From 2be0fa6ecf0b967f09d0f7d7bd8adbe602a915d0 Mon Sep 17 00:00:00 2001
From: Bijoy Jose <bijoy.a.jose@intel.com>
Date: Mon, 17 Dec 2012 23:10:45 -0800
Subject: Dalvik: Use predecoded instruction information from MIR

BZ: 75820

Instead of decoding instructions each time opcode or operands
are needed, this patch allows using predecoded information
available from MIR. This also allows artificial insertion
of bytecodes in a trace because the assumption that bytecodes
come from the same real method is removed.

Category: device-enablement
Domain: AOSP-Dalvik-Compiler-CG
Origin: internal
Upstream-Candidate: no, needs rework

Change-Id: I4d32d37b46c33c464ad7d4c205ae144d32f3edd2
Orig-MCG-Change-Id: I9852d2b9885cb2e201aa19c2f3cb80bf1fc27344
Signed-off-by: Razvan A Lupusoru <razvan.a.lupusoru@intel.com>
Signed-off-by: Qiming Shi <qiming.shi@intel.com>
Signed-off-by: Serguei Katkov <serguei.i.katkov@intel.com>
---
 vm/compiler/codegen/x86/AnalysisO1.cpp       |   61 +-
 vm/compiler/codegen/x86/AnalysisO1.h         |    8 +-
 vm/compiler/codegen/x86/BytecodeVisitor.cpp  |  709 +++++-----
 vm/compiler/codegen/x86/CodegenInterface.cpp |    4 +-
 vm/compiler/codegen/x86/Lower.cpp            |  524 ++++----
 vm/compiler/codegen/x86/Lower.h              |  466 ++++----
 vm/compiler/codegen/x86/LowerAlu.cpp         | 1847 +++++++++++++++-----------
 vm/compiler/codegen/x86/LowerConst.cpp       |  210 ++--
 vm/compiler/codegen/x86/LowerGetPut.cpp      | 1066 ++++++++++-----
 vm/compiler/codegen/x86/LowerInvoke.cpp      |  507 +++++---
 vm/compiler/codegen/x86/LowerJump.cpp        |  300 +++--
 vm/compiler/codegen/x86/LowerMove.cpp        |  147 ++-
 vm/compiler/codegen/x86/LowerObject.cpp      |  569 ++++++---
 vm/compiler/codegen/x86/LowerReturn.cpp      |   46 +-
 14 files changed, 3760 insertions(+), 2704 deletions(-)

diff --git a/vm/compiler/codegen/x86/AnalysisO1.cpp b/vm/compiler/codegen/x86/AnalysisO1.cpp
index 0cbedb3..78abc37 100644
--- a/vm/compiler/codegen/x86/AnalysisO1.cpp
+++ b/vm/compiler/codegen/x86/AnalysisO1.cpp
@@ -289,17 +289,14 @@ void insertAccess(int tableIndex, LiveRange* startP, int rangeStart);
 //register allocation
 int spillLogicalReg(int spill_index, bool updateTable);
 
-/** check whether the current bytecode is IF or GOTO or SWITCH
+/**
+ * @brief Checks whether the opcode can branch or switch
+ * @param opcode the Dalvik mnemonic
+ * @return true if opcode can branch or switch
  */
-bool isCurrentByteCodeJump() {
-    u2 inst_op = INST_INST(inst);
-    if(inst_op == OP_IF_EQ || inst_op == OP_IF_NE || inst_op == OP_IF_LT ||
-       inst_op == OP_IF_GE || inst_op == OP_IF_GT || inst_op == OP_IF_LE) return true;
-    if(inst_op == OP_IF_EQZ || inst_op == OP_IF_NEZ || inst_op == OP_IF_LTZ ||
-       inst_op == OP_IF_GEZ || inst_op == OP_IF_GTZ || inst_op == OP_IF_LEZ) return true;
-    if(inst_op == OP_GOTO || inst_op == OP_GOTO_16 || inst_op == OP_GOTO_32) return true;
-    if(inst_op == OP_PACKED_SWITCH || inst_op == OP_SPARSE_SWITCH) return true;
-    return false;
+static inline bool isCurrentByteCodeJump(Opcode opcode) {
+    int flags = dexGetFlagsFromOpcode(opcode);
+    return (flags & (kInstrCanBranch | kInstrCanSwitch)) != 0;
 }
 
 /* this function is called before code generation of basic blocks
@@ -678,19 +675,16 @@ int collectInfoOfBasicBlock(Method* method, BasicBlock_O1* bb) {
     bb->num_defs = 0;
     bb->defUseTable = NULL;
     bb->defUseTail = NULL;
-    u2* rPC_start = (u2*)method->insns;
     int kk;
     bb->endsWithReturn = false;
     bb->hasAccessToGlue = false;
 
-    MIR* mir;
     int seqNum = 0;
     /* traverse the MIR in basic block
        sequence number is used to make sure next bytecode will have a larger sequence number */
-    for(mir = bb->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
+    for(MIR * mir = bb->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
         offsetPC = seqNum;
         mir->seqNum = seqNum++;
-        rPC = rPC_start + mir->offset;
 #ifdef WITH_JIT_INLINING
         if(mir->dalvikInsn.opcode >= kMirOpFirst &&
            mir->dalvikInsn.opcode != kMirOpCheckInlinePrediction) continue;
@@ -699,8 +693,7 @@ int collectInfoOfBasicBlock(Method* method, BasicBlock_O1* bb) {
 #else
         if(mir->dalvikInsn.opcode >= kNumPackedOpcodes) continue;
 #endif
-        inst = FETCH(0);
-        u2 inst_op = INST_INST(inst);
+        Opcode inst_op = mir->dalvikInsn.opcode;
         /* update bb->hasAccessToGlue */
         if((inst_op >= OP_MOVE_RESULT && inst_op <= OP_RETURN_OBJECT) ||
            (inst_op >= OP_MONITOR_ENTER && inst_op <= OP_INSTANCE_OF) ||
@@ -718,7 +711,7 @@ int collectInfoOfBasicBlock(Method* method, BasicBlock_O1* bb) {
             bb->endsWithReturn = true;
 
         /* get virtual register usage in current bytecode */
-        getVirtualRegInfo(infoByteCode);
+        getVirtualRegInfo(infoByteCode, mir);
         int num_regs = num_regs_per_bytecode;
         for(kk = 0; kk < num_regs; kk++) {
             currentInfo = infoByteCode[kk];
@@ -793,12 +786,10 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
     ALOGI("At start of basic block %d (num of VRs %d) -------", bb->bb_index, bb->num_regs);
 #endif
 
-    u2* rPC_start = (u2*)method->insns;
     bool lastByteCodeIsJump = false;
-    MIR* mir;
-    for(mir = bb->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
+    for(MIR * mir = bb->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
         offsetPC = mir->seqNum;
-        rPC = rPC_start + mir->offset;
+        rPC = const_cast<u2 *>(method->insns) + mir->offset;
 #ifdef WITH_JIT_INLINING
         if(mir->dalvikInsn.opcode >= kMirOpFirst &&
            mir->dalvikInsn.opcode != kMirOpCheckInlinePrediction) {
@@ -809,9 +800,8 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
             continue;
         }
 
-        inst = FETCH(0);
         //before handling a bytecode, import info of temporary registers to compileTable including refCount
-        num_temp_regs_per_bytecode = getTempRegInfo(infoByteCodeTemp);
+        num_temp_regs_per_bytecode = getTempRegInfo(infoByteCodeTemp, mir);
         for(k = 0; k < num_temp_regs_per_bytecode; k++) {
             if(infoByteCodeTemp[k].versionNum > 0) continue;
             insertFromTempInfo(k);
@@ -831,7 +821,7 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
 #endif
         //set isConst to true for CONST & MOVE MOVE_OBJ?
         //clear isConst to true for MOVE, MOVE_OBJ, MOVE_RESULT, MOVE_EXCEPTION ...
-        bool isConst = getConstInfo(bb); //will reset isConst if a VR is updated by the bytecode
+        bool isConst = getConstInfo(bb, mir); //will reset isConst if a VR is updated by the bytecode
         bool isDeadStmt = false;
 #ifdef DSE_OPT
         for(k = 0; k < num_dead_pc; k++) {
@@ -841,7 +831,7 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
             }
         }
 #endif
-        getVirtualRegInfo(infoByteCode);
+        getVirtualRegInfo(infoByteCode, mir);
         //call something similar to mergeEntry2, but only update refCount
         //clear refCount
         for(k = 0; k < num_regs_per_bytecode; k++) {
@@ -877,9 +867,11 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
             dumpCompileTable();
 #endif
             globalShortMap = NULL;
-            if(isCurrentByteCodeJump()) lastByteCodeIsJump = true;
+            if (isCurrentByteCodeJump(mir->dalvikInsn.opcode))
+                lastByteCodeIsJump = true;
             //lowerByteCode will call globalVREndOfBB if it is jump
-            int retCode = lowerByteCodeJit(method, rPC, mir);
+
+            int retCode = lowerByteCodeJit(method, mir, rPC);
             if(gDvmJit.codeCacheByteUsed + (stream - streamStart) +
                  CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
                  ALOGE("JIT code cache full");
@@ -906,6 +898,7 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
                 return retCode;
             }
             freeReg(true); //may dump GL VR to memory (this is necessary)
+
             //after each bytecode, make sure non-VRs have refCount of zero
             for(k = 0; k < num_compile_entries; k++) {
                 if(isTemporary(compileTable[k].physicalType, compileTable[k].regNum)) {
@@ -3179,19 +3172,15 @@ void getDeadStmts() {
     num_dead_pc = 0;
     //traverse each bytecode in the basic block
     //update offsetPC, rPC & inst
-    u2* rPC_start = (u2*)currentMethod->insns;
-    MIR* mir;
-    for(mir = bb->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
+    for(MIR * mir = bb->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
         offsetPC = mir->seqNum;
-        rPC = rPC_start + mir->offset;
         if(mir->dalvikInsn.opcode >= kNumPackedOpcodes) continue;
 #ifdef DEBUG_DSE
         ALOGI("DSE: offsetPC %x", offsetPC);
 #endif
-        inst = FETCH(0);
         bool isDeadStmt = true;
-        getVirtualRegInfo(infoByteCode);
-        u2 inst_op = INST_INST(inst);
+        getVirtualRegInfo(infoByteCode, mir);
+        u2 inst_op = mir->dalvikInsn.opcode;
 	//skip bytecodes with side effect
         if(inst_op != OP_CONST_STRING && inst_op != OP_CONST_STRING_JUMBO &&
            inst_op != OP_MOVE && inst_op != OP_MOVE_OBJECT &&
@@ -3578,6 +3567,9 @@ bool matchType(int typeA, int typeB) {
        (typeB & MASK_FOR_TYPE) == LowOpndRegType_ss) return true;
     return false;
 }
+
+#if 0 /* This code is dead. If reenabling, need to pass proper parameters
+         to getVirtualRegInfo */
 //!check whether a virtual register is used in the current bytecode
 
 //!
@@ -3590,6 +3582,7 @@ bool isUsedInByteCode(int regNum, int type) {
     }
     return false;
 }
+#endif
 //! obsolete
 bool defineFirst(int atype) {
     if(atype == REGACCESS_D || atype == REGACCESS_L || atype == REGACCESS_H || atype == REGACCESS_DU)
diff --git a/vm/compiler/codegen/x86/AnalysisO1.h b/vm/compiler/codegen/x86/AnalysisO1.h
index 7f436d9..ff77c07 100644
--- a/vm/compiler/codegen/x86/AnalysisO1.h
+++ b/vm/compiler/codegen/x86/AnalysisO1.h
@@ -361,10 +361,10 @@ typedef enum GlueVarType {
 void forwardAnalysis(int type);
 
 //functions in bc_visitor.c
-int getByteCodeSize();
-bool getConstInfo(BasicBlock_O1* bb);
-int getVirtualRegInfo(VirtualRegInfo* infoArray);
-int getTempRegInfo(TempRegInfo* infoArray);
+//int getByteCodeSize();
+bool getConstInfo(BasicBlock_O1* bb, const MIR * currentMIR);
+int getVirtualRegInfo(VirtualRegInfo* infoArray, const MIR * currentMIR);
+int getTempRegInfo(TempRegInfo* infoArray, const MIR * currentMIR);
 int createCFGHandler(Method* method);
 
 int findVirtualRegInTable(u2 vA, LowOpndRegType type, bool printError);
diff --git a/vm/compiler/codegen/x86/BytecodeVisitor.cpp b/vm/compiler/codegen/x86/BytecodeVisitor.cpp
index 8e66032..fbacd81 100644
--- a/vm/compiler/codegen/x86/BytecodeVisitor.cpp
+++ b/vm/compiler/codegen/x86/BytecodeVisitor.cpp
@@ -23,6 +23,8 @@
 #include "Lower.h"
 #include "AnalysisO1.h"
 
+#if 0 /* This is dead code and has been disabled. If reenabling,
+         the MIR or opcode must be passed in as a parameter */
 //! Returns size of the current bytecode in u2 unit
 
 //!
@@ -409,9 +411,16 @@ int getByteCodeSize() { //uses inst, unit in u2
         return 2;
     }
 #endif
+    default:
+        ALOGE("ERROR: JIT does not support getting size of bytecode 0x%hx\n",
+                currentMIR->dalvikInsn.opcode);
+        assert(false && "All opcodes should be supported.");
+        break;
     }
     return -1;
 }
+#endif
+
 //! reduces refCount of a virtual register
 
 //!
@@ -527,9 +536,9 @@ void updateConstInfo(BasicBlock_O1* bb) {
 
 //! if yes, update constVRTable; otherwise, update constWorklist
 //! if a bytecode uses vA (const), and updates vA to non const, getConstInfo will return false and update constWorklist to make sure when lowering the bytecode, vA is treated as constant
-bool getConstInfo(BasicBlock_O1* bb) {
+bool getConstInfo(BasicBlock_O1* bb, const MIR * currentMIR) {
     compileTableEntry* infoArray = compileTable;
-    u2 inst_op = INST_INST(inst);
+    Opcode inst_op = currentMIR->dalvikInsn.opcode;
     u2 vA = 0, vB = 0, v1, v2;
     u2 BBBB;
     u2 tmp_u2;
@@ -546,18 +555,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_MOVE_OBJECT_FROM16:
     case OP_MOVE_16:
     case OP_MOVE_OBJECT_16:
-        if(inst_op == OP_MOVE || inst_op == OP_MOVE_OBJECT) {
-            vA = INST_A(inst);
-            vB = INST_B(inst);
-        }
-        else if(inst_op == OP_MOVE_FROM16 || inst_op == OP_MOVE_OBJECT_FROM16) {
-            vA = INST_AA(inst);
-            vB = FETCH(1);
-        }
-        else if(inst_op == OP_MOVE_16 || inst_op == OP_MOVE_OBJECT_16) {
-            vA = FETCH(1);
-            vB = FETCH(2);
-        }
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
             entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
             setVRToConst(vA, OpndSize_32, tmpValue);
@@ -574,18 +573,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_MOVE_WIDE:
     case OP_MOVE_WIDE_FROM16:
     case OP_MOVE_WIDE_16:
-        if(inst_op == OP_MOVE_WIDE) {
-            vA = INST_A(inst);
-            vB = INST_B(inst);
-        }
-        else if(inst_op == OP_MOVE_WIDE_FROM16) {
-            vA = INST_AA(inst);
-            vB = FETCH(1);
-        }
-        else if(inst_op == OP_MOVE_WIDE_16) {
-            vA = FETCH(1);
-            vB = FETCH(2);
-        }
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         if(isVirtualRegConstant(vB, LowOpndRegType_xmm, tmpValue, false) == 3) {
             entry = findVirtualRegInTable(vA, LowOpndRegType_xmm, true);
             setVRToConst(vA, OpndSize_64, tmpValue);
@@ -624,7 +613,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_SGET_BYTE:
     case OP_SGET_CHAR:
     case OP_SGET_SHORT:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         return false;
@@ -632,7 +621,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_AGET_WIDE:
     case OP_SGET_WIDE:
     case OP_SGET_WIDE_VOLATILE:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         constWorklist[num_const_worklist] = vA+1;
@@ -651,14 +640,14 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_IGET_SHORT:
     case OP_IGET_QUICK:
     case OP_IGET_OBJECT_QUICK:
-        vA = INST_A(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         return false;
     case OP_IGET_WIDE:
     case OP_IGET_WIDE_VOLATILE:
     case OP_IGET_WIDE_QUICK:
-        vA = INST_A(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         constWorklist[num_const_worklist] = vA+1;
@@ -670,7 +659,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_MUL_FLOAT:
     case OP_DIV_FLOAT:
     case OP_REM_FLOAT:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         return false;
@@ -679,7 +668,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_MUL_DOUBLE:
     case OP_DIV_DOUBLE:
     case OP_REM_DOUBLE:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         constWorklist[num_const_worklist] = vA+1;
@@ -696,14 +685,14 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_DIV_FLOAT_2ADDR:
     case OP_REM_FLOAT_2ADDR:
     case OP_DOUBLE_TO_FLOAT:
-        vA = INST_A(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA; //change constWorklist to point to vA TODO
         num_const_worklist++;
         return false;
     case OP_FLOAT_TO_LONG:
     case OP_DOUBLE_TO_LONG:
     case OP_FLOAT_TO_DOUBLE:
-        vA = INST_A(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         constWorklist[num_const_worklist] = vA+1;
@@ -718,7 +707,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_DIV_DOUBLE_2ADDR:
     case OP_REM_DOUBLE_2ADDR:
         //ops on float, double
-        vA = INST_A(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         constWorklist[num_const_worklist] = vA+1;
@@ -730,8 +719,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_INT_TO_BYTE:
     case OP_INT_TO_CHAR:
     case OP_INT_TO_SHORT:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
             entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
             infoArray[entry].isConst = true;
@@ -764,7 +753,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_NEG_LONG:
     case OP_NOT_LONG:
     case OP_INT_TO_LONG:
-        vA = INST_A(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         constWorklist[num_const_worklist] = vA+1; //fixed on 10/15/2009
@@ -778,11 +767,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_DIV_INT_LIT8:
     case OP_DIV_INT:
     case OP_REM_INT:
-        if(inst_op == OP_DIV_INT || inst_op == OP_DIV_INT_LIT8 ||
-           inst_op == OP_REM_INT || inst_op == OP_REM_INT_LIT8)
-            vA = INST_AA(inst);
-        else
-            vA = INST_A(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         return false;
@@ -795,8 +780,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_SHL_INT_2ADDR:
     case OP_SHR_INT_2ADDR:
     case OP_USHR_INT_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         if(isVirtualRegConstant(vA, LowOpndRegType_gp, tmpValue, false) == 3 &&
            isVirtualRegConstant(v2, LowOpndRegType_gp, tmpValue2, false) == 3) {
             entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
@@ -843,9 +828,9 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_AND_INT_LIT16:
     case OP_OR_INT_LIT16:
     case OP_XOR_INT_LIT16:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
-        tmp_s4 = (s2)FETCH(1);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        tmp_s4 = currentMIR->dalvikInsn.vC;
         if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
             entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
             infoArray[entry].isConst = true;
@@ -888,9 +873,9 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_SHL_INT:
     case OP_SHR_INT:
     case OP_USHR_INT:
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         if(isVirtualRegConstant(v1, LowOpndRegType_gp, tmpValue, false) == 3 &&
            isVirtualRegConstant(v2, LowOpndRegType_gp, tmpValue2, false) == 3) {
             entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
@@ -941,9 +926,9 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_SHL_INT_LIT8:
     case OP_SHR_INT_LIT8:
     case OP_USHR_INT_LIT8:
-        vA = INST_AA(inst);
-        vB = (u2)FETCH(1) & 0xff;
-        tmp_s4 = (s2)FETCH(1) >> 8;
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        tmp_s4 = currentMIR->dalvikInsn.vC;
         if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
             entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
             infoArray[entry].isConst = true;
@@ -996,14 +981,14 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_USHR_LONG:
         //TODO bytecode is not going to update state registers
         //constant folding
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         constWorklist[num_const_worklist] = vA+1;
         num_const_worklist++;
         return false;
     case OP_CMP_LONG:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         return false;
@@ -1018,15 +1003,15 @@ bool getConstInfo(BasicBlock_O1* bb) {
     case OP_SHL_LONG_2ADDR:
     case OP_SHR_LONG_2ADDR:
     case OP_USHR_LONG_2ADDR:
-        vA = INST_A(inst);
+        vA = currentMIR->dalvikInsn.vA;
         constWorklist[num_const_worklist] = vA;
         num_const_worklist++;
         constWorklist[num_const_worklist] = vA+1;
         num_const_worklist++;
         return false;
     case OP_CONST_4:
-        vA = INST_A(inst);
-        tmp_s4 = (s4) (INST_B(inst) << 28) >> 28;
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_s4 = currentMIR->dalvikInsn.vB;
         entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
         infoArray[entry].value[0] = tmp_s4;
@@ -1038,8 +1023,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
 #endif
         return true;
     case OP_CONST_16:
-        BBBB = FETCH(1);
-        vA = INST_AA(inst);
+        BBBB = currentMIR->dalvikInsn.vB;
+        vA = currentMIR->dalvikInsn.vA;
         entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
         infoArray[entry].value[0] = (s2)BBBB;
@@ -1051,9 +1036,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
 #endif
         return true;
     case OP_CONST:
-        vA = INST_AA(inst);
-        tmp_u4 = FETCH(1);
-        tmp_u4 |= (u4)FETCH(2) << 16;
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u4 = currentMIR->dalvikInsn.vB;
         entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
         infoArray[entry].value[0] = (s4)tmp_u4;
@@ -1065,11 +1049,11 @@ bool getConstInfo(BasicBlock_O1* bb) {
 #endif
         return true;
     case OP_CONST_HIGH16:
-        vA = INST_AA(inst);
-        tmp_u2 = FETCH(1);
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u2 = currentMIR->dalvikInsn.vB;
         entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
-        infoArray[entry].value[0] = (s4)tmp_u2<<16;
+        infoArray[entry].value[0] = ((s4)tmp_u2)<<16;
         tmpValue[0] = infoArray[entry].value[0];
         setVRToConst(vA, OpndSize_32, tmpValue);
         compileTable[entry].refCount--;
@@ -1078,8 +1062,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
 #endif
         return true;
     case OP_CONST_WIDE_16:
-        vA = INST_AA(inst);
-        tmp_u2 = FETCH(1);
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u2 = currentMIR->dalvikInsn.vB;
         entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
         infoArray[entry].value[0] = (s2)tmp_u2;
@@ -1091,7 +1075,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
 
         entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
-        infoArray[entry].value[0] = (s2)tmp_u2>>31;
+        infoArray[entry].value[0] = ((s2)tmp_u2)>>31;
         tmpValue[1] = infoArray[entry].value[0];
         setVRToConst(vA, OpndSize_64, tmpValue);
         compileTable[entry].refCount--;
@@ -1100,9 +1084,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
 #endif
         return true;
     case OP_CONST_WIDE_32:
-        vA = INST_AA(inst);
-        tmp_u4 = FETCH(1);
-        tmp_u4 |= (u4)FETCH(2) << 16;
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u4 = currentMIR->dalvikInsn.vB;
         entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
         infoArray[entry].value[0] = (s4)tmp_u4;
@@ -1114,7 +1097,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
 
         entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
-        infoArray[entry].value[0] = (s4)tmp_u4>>31;
+        infoArray[entry].value[0] = ((s4)tmp_u4)>>31;
         tmpValue[1] = infoArray[entry].value[0];
         setVRToConst(vA, OpndSize_64, tmpValue);
         compileTable[entry].refCount--;
@@ -1123,9 +1106,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
 #endif
         return true;
     case OP_CONST_WIDE:
-        vA = INST_AA(inst);
-        tmp_u4 = FETCH(1);
-        tmp_u4 |= (u8)FETCH(2) << 16;
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u4 = (s4)currentMIR->dalvikInsn.vB_wide;
         entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
         infoArray[entry].value[0] = (s4)tmp_u4;
@@ -1135,8 +1117,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
         LOGD("getConstInfo: set VR %d to %x", vA, infoArray[entry].value[0]);
 #endif
 
-        tmp_u4 = (u8)FETCH(3);
-        tmp_u4 |= (u8)FETCH(4) << 16;
+        tmp_u4 = (s4)(currentMIR->dalvikInsn.vB_wide >> 32);
         entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
         infoArray[entry].value[0] = (s4)tmp_u4;
@@ -1148,8 +1129,8 @@ bool getConstInfo(BasicBlock_O1* bb) {
 #endif
         return true;
     case OP_CONST_WIDE_HIGH16:
-        vA = INST_AA(inst);
-        tmp_u2 = FETCH(1);
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u2 = currentMIR->dalvikInsn.vB;
         entry = findVirtualRegInTable(vA, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
         infoArray[entry].value[0] = 0;
@@ -1161,7 +1142,7 @@ bool getConstInfo(BasicBlock_O1* bb) {
 
         entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp, true);
         infoArray[entry].isConst = true;
-        infoArray[entry].value[0] = (s4)tmp_u2<<16;
+        infoArray[entry].value[0] = ((s4)tmp_u2)<<16;
         tmpValue[1] = infoArray[entry].value[0];
         setVRToConst(vA, OpndSize_64, tmpValue);
         compileTable[entry].refCount--;
@@ -1205,6 +1186,9 @@ bool getConstInfo(BasicBlock_O1* bb) {
         num_const_worklist++;
         return false;
 #endif
+    default:
+        // Bytecode does not generate a const
+        break;
     }
     return false;
 }
@@ -1212,15 +1196,13 @@ bool getConstInfo(BasicBlock_O1* bb) {
 //! This function updates infoArray with virtual registers accessed when lowering the bytecode, and returns size of the bytecode in unit of u2
 
 //! uses of virtual registers are added to infoArray first
-int getVirtualRegInfo(VirtualRegInfo* infoArray) {
-    u2 inst_op = INST_INST(inst);
+int getVirtualRegInfo(VirtualRegInfo* infoArray, const MIR * currentMIR) {
+    u2 inst_op = currentMIR->dalvikInsn.opcode;
     u2 vA = 0, vB = 0, vref, vindex;
     u2 v1, v2, length, vD, vG, vE, vF, count;
     u4 v1_u4, v2_u4;
     int kk, num, num_entry;
-    s4 tmp_s4;
     s2 tmp_s2;
-    u4 tmp_u4;
     int codeSize = 0;
     num_regs_per_bytecode = 0;
     //update infoArray[xx].allocConstraints
@@ -1242,18 +1224,18 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_MOVE_16:
     case OP_MOVE_OBJECT_16:
         if(inst_op == OP_MOVE || inst_op == OP_MOVE_OBJECT) {
-            vA = INST_A(inst);
-            vB = INST_B(inst);
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
             codeSize = 1;
         }
         else if(inst_op == OP_MOVE_FROM16 || inst_op == OP_MOVE_OBJECT_FROM16) {
-            vA = INST_AA(inst);
-            vB = FETCH(1);
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
             codeSize = 2;
         }
         else if(inst_op == OP_MOVE_16 || inst_op == OP_MOVE_OBJECT_16) {
-            vA = FETCH(1);
-            vB = FETCH(2);
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
             codeSize = 3;
         }
         infoArray[1].regNum = vA; //dst
@@ -1270,18 +1252,18 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_MOVE_WIDE_FROM16:
     case OP_MOVE_WIDE_16:
         if(inst_op == OP_MOVE_WIDE) {
-            vA = INST_A(inst);
-            vB = INST_B(inst);
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
             codeSize = 1;
         }
         else if(inst_op == OP_MOVE_WIDE_FROM16) {
-            vA = INST_AA(inst);
-            vB = FETCH(1);
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
             codeSize = 2;
         }
         else if(inst_op == OP_MOVE_WIDE_16) {
-            vA = FETCH(1);
-            vB = FETCH(2);
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
             codeSize = 3;
         }
         infoArray[1].regNum = vA; //dst
@@ -1296,7 +1278,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_MOVE_RESULT: //access memory
     case OP_MOVE_RESULT_OBJECT:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1305,7 +1287,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 1;
         break;
     case OP_MOVE_RESULT_WIDE: //note: 2 destinations
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1314,7 +1296,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 1;
         break;
     case OP_MOVE_EXCEPTION: //access memory
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1330,7 +1312,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_RETURN:
     case OP_RETURN_OBJECT:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         codeSize = 1;
         infoArray[0].regNum = vA; //src
         infoArray[0].refCount = 1;
@@ -1340,7 +1322,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 1;
         break;
     case OP_RETURN_WIDE:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //src
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -1349,8 +1331,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 1;
         break;
     case OP_CONST_4:
-        vA = INST_A(inst);
-        tmp_s4 = (s4) (INST_B(inst) << 28) >> 28;
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1359,7 +1340,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 1;
         break;
     case OP_CONST_16:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1368,9 +1349,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 2;
         break;
     case OP_CONST:
-        vA = INST_AA(inst);
-        tmp_u4 = FETCH(1);
-        tmp_u4 |= (u4)FETCH(2) << 16;
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1379,7 +1358,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 3;
         break;
     case OP_CONST_HIGH16:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1388,7 +1367,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 2;
         break;
     case OP_CONST_WIDE_16:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1401,9 +1380,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_CONST_WIDE_32:
-        vA = INST_AA(inst);
-        tmp_u4 = FETCH(1);
-        tmp_u4 |= (u4)FETCH(2) << 16;
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1416,15 +1393,11 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 3;
         break;
     case OP_CONST_WIDE:
-        vA = INST_AA(inst);
-        tmp_u4 = FETCH(1);
-        tmp_u4 |= (u8)FETCH(2) << 16;
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
         infoArray[0].physicalType = LowOpndRegType_gp;
-        tmp_u4 = (u8)FETCH(3);
-        tmp_u4 |= (u8)FETCH(4) << 16;
         infoArray[1].regNum = vA+1;
         infoArray[1].refCount = 1;
         infoArray[1].accessType = REGACCESS_D;
@@ -1433,7 +1406,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_CONST_WIDE_HIGH16:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1448,7 +1421,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_CONST_STRING:
     case OP_CONST_STRING_JUMBO:
     case OP_CONST_CLASS:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         if(inst_op == OP_CONST_STRING || inst_op == OP_CONST_CLASS)
             codeSize = 2;
         else if(inst_op == OP_CONST_STRING_JUMBO)
@@ -1462,7 +1435,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 1;
         break;
     case OP_MONITOR_ENTER:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         codeSize = 1;
         infoArray[0].regNum = vA; //src
         infoArray[0].refCount = 1;
@@ -1471,7 +1444,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 1;
         break;
     case OP_MONITOR_EXIT:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         codeSize = 1;
         infoArray[0].regNum = vA; //src
         infoArray[0].refCount = 1;
@@ -1483,7 +1456,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_CHECK_CAST:
         codeSize = 2;
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //src
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -1495,8 +1468,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_INSTANCE_OF:
         codeSize = 2;
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[0].regNum = vB; //src
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -1509,8 +1482,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_ARRAY_LENGTH:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[0].regNum = vB; //src
         infoArray[0].refCount = 1;
@@ -1525,7 +1498,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_NEW_INSTANCE:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //dst
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_D;
@@ -1538,8 +1511,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 2;
         break;
     case OP_NEW_ARRAY:
-        vA = INST_A(inst); //destination
-        vB = INST_B(inst); //length
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB; //length
         infoArray[0].regNum = vB; //src
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -1556,13 +1529,12 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_FILLED_NEW_ARRAY: {//update return value
         //can use up to 5 registers to fill the content of array
-        length = INST_B(inst);
-        u2 vv = FETCH(2);
-        v1 = vv & 0xf;
-        v2 = (vv >> 4) & 0xf;
-        u2 v3 = (vv >> 8) & 0xf;
-        u2 v4 = (vv >> 12) & 0xf;
-        u2 v5 = INST_A(inst);
+        length = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.arg[0];
+        v2 = currentMIR->dalvikInsn.arg[1];
+        u2 v3 = currentMIR->dalvikInsn.arg[2];
+        u2 v4 = currentMIR->dalvikInsn.arg[3];
+        u2 v5 = currentMIR->dalvikInsn.arg[4];
         if(length >= 1) {
             infoArray[0].regNum = v1; //src
             infoArray[0].refCount = 1;
@@ -1600,8 +1572,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     }
     case OP_FILLED_NEW_ARRAY_RANGE: {//use "length" virtual registers
-        length = INST_AA(inst);
-        u4 vC = (u4)FETCH(2);
+        length = currentMIR->dalvikInsn.vA;
+        u4 vC = currentMIR->dalvikInsn.vC;
         for(kk = 0; kk < length; kk++) {
             infoArray[kk].regNum = vC+kk; //src
             infoArray[kk].refCount = 1;
@@ -1615,7 +1587,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     }
     case OP_FILL_ARRAY_DATA: //update content of array, read memory
-        vA = INST_AA(inst); //use virtual register, but has side-effect, update memory
+        vA = currentMIR->dalvikInsn.vA; //use virtual register, but has side-effect, update memory
         infoArray[0].regNum = vA; //use
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -1626,7 +1598,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 3;
         break;
     case OP_THROW: //update glue->exception
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //use
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -1653,7 +1625,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_PACKED_SWITCH:
     case OP_SPARSE_SWITCH:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         codeSize = 3;
         infoArray[0].regNum = vA; //use
         infoArray[0].refCount = 1;
@@ -1667,9 +1639,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_CMPL_FLOAT: //move 32 bits from memory to lower part of XMM register
     case OP_CMPG_FLOAT:
         codeSize = 2;
-        vA = INST_AA(inst);
-        v1_u4 = FETCH(1) & 0xff;
-        v2_u4 = FETCH(1) >> 8;
+        vA = currentMIR->dalvikInsn.vA;
+        v1_u4 = currentMIR->dalvikInsn.vB;
+        v2_u4 = currentMIR->dalvikInsn.vC;
         num_regs_per_bytecode = 1;
         infoArray[0].regNum = v1_u4; //use ss or sd CHECK
         infoArray[0].refCount = 1;
@@ -1690,9 +1662,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_CMPG_DOUBLE:
     case OP_CMP_LONG: //load v1, v1+1, v2, v2+1 to gpr
         codeSize = 2;
-        vA = INST_AA(inst);
-        v1_u4 = FETCH(1) & 0xff;
-        v2_u4 = FETCH(1) >> 8;
+        vA = currentMIR->dalvikInsn.vA;
+        v1_u4 = currentMIR->dalvikInsn.vB;
+        v2_u4 = currentMIR->dalvikInsn.vC;
         num_regs_per_bytecode = 1;
         if(inst_op == OP_CMP_LONG) {
             infoArray[0].regNum = v1_u4; //use
@@ -1741,8 +1713,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_IF_GE:
     case OP_IF_GT:
     case OP_IF_LE:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[0].regNum = vA; //use
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -1760,7 +1732,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_IF_GEZ:
     case OP_IF_GTZ:
     case OP_IF_LEZ:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = vA; //use
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -1769,22 +1741,16 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 2;
         break;
     case OP_AGET:
-        codeSize = 2;
     case OP_AGET_WIDE:
-        codeSize = 2;
     case OP_AGET_OBJECT:
-        codeSize = 2;
     case OP_AGET_BOOLEAN: //movez 8
-        codeSize = 2;
     case OP_AGET_BYTE: //moves 8
-        codeSize = 2;
     case OP_AGET_CHAR: //movez 16
-        codeSize = 2;
     case OP_AGET_SHORT: //moves 16
         codeSize = 2;
-        vA = INST_AA(inst);
-        vref = FETCH(1) & 0xff;
-        vindex = FETCH(1) >> 8;
+        vA = currentMIR->dalvikInsn.vA;
+        vref = currentMIR->dalvikInsn.vB;
+        vindex = currentMIR->dalvikInsn.vC;
         if(inst_op == OP_AGET_WIDE) {
             infoArray[2].regNum = vA;
             infoArray[2].refCount = 1;
@@ -1813,9 +1779,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_APUT_BYTE:
     case OP_APUT_CHAR:
     case OP_APUT_SHORT:
-        vA = INST_AA(inst);
-        vref = FETCH(1) & 0xff;
-        vindex = FETCH(1) >> 8;
+        vA = currentMIR->dalvikInsn.vA;
+        vref = currentMIR->dalvikInsn.vB;
+        vindex = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         if(inst_op == OP_APUT_WIDE) {
             infoArray[0].regNum = vA;
@@ -1856,8 +1822,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_IGET_QUICK:
     case OP_IGET_WIDE_QUICK:
     case OP_IGET_OBJECT_QUICK:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 2;
         if(inst_op == OP_IGET_WIDE || inst_op == OP_IGET_WIDE_QUICK) {
             infoArray[1].regNum = vA;
@@ -1903,8 +1869,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_IPUT_QUICK:
     case OP_IPUT_WIDE_QUICK:
     case OP_IPUT_OBJECT_QUICK:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 2;
         if(inst_op == OP_IPUT_WIDE || inst_op == OP_IPUT_WIDE_QUICK || inst_op == OP_IPUT_WIDE_VOLATILE) {
             infoArray[0].regNum = vA;
@@ -1935,7 +1901,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_SGET_BYTE:
     case OP_SGET_CHAR:
     case OP_SGET_SHORT:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         codeSize = 2;
         if(inst_op == OP_SGET_WIDE) {
             infoArray[0].regNum = vA;
@@ -1973,7 +1939,7 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_SPUT_BYTE:
     case OP_SPUT_CHAR:
     case OP_SPUT_SHORT:
-        vA = INST_AA(inst);
+        vA = currentMIR->dalvikInsn.vA;
         codeSize = 2;
         if(inst_op == OP_SPUT_WIDE || inst_op == OP_SPUT_WIDE_VOLATILE) {
             infoArray[0].regNum = vA;
@@ -1998,12 +1964,12 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_INVOKE_VIRTUAL_QUICK:
     case OP_INVOKE_SUPER_QUICK:
         codeSize = 3;
-        vD = FETCH(2) & 0xf; //object for virtual,direct & interface
-        count = INST_B(inst);
-        vE = (FETCH(2) >> 4) & 0xf;
-        vF = (FETCH(2) >> 8) & 0xf;
-        vG = (FETCH(2) >> 12) & 0xf;
-        vA = INST_A(inst); //5th argument
+        vD = currentMIR->dalvikInsn.arg[0]; //object for virtual,direct & interface
+        count = currentMIR->dalvikInsn.vA;
+        vE = currentMIR->dalvikInsn.arg[1];
+        vF = currentMIR->dalvikInsn.arg[2];
+        vG = currentMIR->dalvikInsn.arg[3];
+        vA = currentMIR->dalvikInsn.arg[4]; //5th argument
         if(count == 0) {
             if(inst_op == OP_INVOKE_VIRTUAL || inst_op == OP_INVOKE_DIRECT ||
                inst_op == OP_INVOKE_INTERFACE || inst_op == OP_INVOKE_VIRTUAL_QUICK ||
@@ -2067,8 +2033,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_INVOKE_VIRTUAL_QUICK_RANGE:
     case OP_INVOKE_SUPER_QUICK_RANGE:
         codeSize = 3;
-        vD = FETCH(2);
-        count = INST_AA(inst);
+        vD = currentMIR->dalvikInsn.vC;
+        count = currentMIR->dalvikInsn.vA;
         if(count == 0) {
             if(inst_op == OP_INVOKE_VIRTUAL_RANGE || inst_op == OP_INVOKE_DIRECT_RANGE ||
                inst_op == OP_INVOKE_INTERFACE_RANGE || inst_op == OP_INVOKE_VIRTUAL_QUICK_RANGE ||
@@ -2104,8 +2070,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_NEG_INT:
     case OP_NOT_INT:
     case OP_NEG_FLOAT:
-        vA = INST_A(inst); //destination
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 1;
         infoArray[1].accessType = REGACCESS_D;
@@ -2120,8 +2086,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_NEG_LONG:
     case OP_NOT_LONG:
     case OP_NEG_DOUBLE:
-        vA = INST_A(inst); //destination
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 1;
@@ -2134,8 +2100,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_INT_TO_LONG: //hard-coded registers
-        vA = INST_A(inst); //destination
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 1;
@@ -2162,8 +2128,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_LONG_TO_DOUBLE: //64 to 64
     case OP_FLOAT_TO_DOUBLE: //32 to 64
     case OP_DOUBLE_TO_FLOAT: //64 to 32
-        vA = INST_A(inst); //destination
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 1;
@@ -2182,8 +2148,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_LONG_TO_INT:
-        vA = INST_A(inst); //destination
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 1;
         infoArray[1].accessType = REGACCESS_D;
@@ -2197,8 +2163,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_FLOAT_TO_INT:
     case OP_DOUBLE_TO_INT: //for reaching-def analysis
-        vA = INST_A(inst); //destination
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 3;
@@ -2219,8 +2185,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_FLOAT_TO_LONG:
     case OP_DOUBLE_TO_LONG:
-        vA = INST_A(inst); //destination
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 3;
@@ -2242,8 +2208,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_INT_TO_BYTE:
     case OP_INT_TO_CHAR:
     case OP_INT_TO_SHORT:
-        vA = INST_A(inst); //destination
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 1;
@@ -2262,9 +2228,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_AND_INT:
     case OP_OR_INT:
     case OP_XOR_INT:
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 1;
@@ -2282,9 +2248,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_DIV_INT:
     case OP_REM_INT:
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 2;
@@ -2310,9 +2276,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_SHL_INT:
     case OP_SHR_INT:
     case OP_USHR_INT:
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 1;
@@ -2331,9 +2297,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 3;
         break;
     case OP_ADD_LONG:
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         infoArray[0].regNum = v1;
         infoArray[0].refCount = 1;
@@ -2365,9 +2331,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_AND_LONG:
     case OP_OR_LONG:
     case OP_XOR_LONG:
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 1;
@@ -2384,9 +2350,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 3;
         break;
     case OP_MUL_LONG: //used int
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         infoArray[0].regNum = v1;
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -2416,9 +2382,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_DIV_LONG: //v1: xmm v2,vA:
     case OP_REM_LONG:
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         infoArray[0].regNum = v1;
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -2443,9 +2409,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 2;
         break;
     case OP_SHL_LONG: //v2: 32, move_ss; v1,vA: xmm CHECK
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         infoArray[0].regNum = v1;
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -2462,9 +2428,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 2;
         break;
     case OP_SHR_LONG: //v2: 32, move_ss; v1,vA: xmm CHECK
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         infoArray[0].regNum = v1;
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -2485,9 +2451,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         codeSize = 2;
         break;
     case OP_USHR_LONG: //v2: move_ss; v1,vA: move_sd
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         infoArray[0].regNum = v1;
         infoArray[0].refCount = 1;
         infoArray[0].accessType = REGACCESS_U;
@@ -2507,9 +2473,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_SUB_FLOAT:
     case OP_MUL_FLOAT:
     case OP_DIV_FLOAT:
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 1;
@@ -2526,9 +2492,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 3;
         break;
     case OP_REM_FLOAT: //32 bit GPR, fp_stack for output
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 1;
@@ -2548,9 +2514,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_SUB_DOUBLE:
     case OP_MUL_DOUBLE:
     case OP_DIV_DOUBLE:
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 1;
@@ -2567,9 +2533,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 3;
         break;
     case OP_REM_DOUBLE: //64 bit XMM, fp_stack for output
-        vA = INST_AA(inst);
-        v1 = *((u1*)rPC + 2);
-        v2 = *((u1*)rPC + 3);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
         codeSize = 2;
         infoArray[2].regNum = vA;
         infoArray[2].refCount = 1;
@@ -2592,8 +2558,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_AND_INT_2ADDR:
     case OP_OR_INT_2ADDR:
     case OP_XOR_INT_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 2;
@@ -2607,8 +2573,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_DIV_INT_2ADDR:
     case OP_REM_INT_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 3;
@@ -2630,8 +2596,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_SHL_INT_2ADDR:
     case OP_SHR_INT_2ADDR:
     case OP_USHR_INT_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 2;
@@ -2646,8 +2612,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_ADD_LONG_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 2;
@@ -2671,8 +2637,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_AND_LONG_2ADDR:
     case OP_OR_LONG_2ADDR:
     case OP_XOR_LONG_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 2;
@@ -2685,8 +2651,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_MUL_LONG_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         num_regs_per_bytecode = 4;
         infoArray[0].regNum = v2;
@@ -2708,8 +2674,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_DIV_LONG_2ADDR: //vA used as xmm, then updated as gps
     case OP_REM_LONG_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         num_regs_per_bytecode = 5;
         codeSize = 1;
         infoArray[0].regNum = vA;
@@ -2734,8 +2700,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         infoArray[4].physicalType = LowOpndRegType_gp;
         break;
     case OP_SHL_LONG_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         num_regs_per_bytecode = 2;
         codeSize = 1;
         infoArray[0].regNum = v2; //ss
@@ -2748,8 +2714,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         infoArray[1].physicalType = LowOpndRegType_xmm;
         break;
     case OP_SHR_LONG_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         num_regs_per_bytecode = 3;
         codeSize = 1;
         infoArray[0].regNum = v2; //ss
@@ -2766,8 +2732,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         infoArray[2].physicalType = LowOpndRegType_xmm;
         break;
     case OP_USHR_LONG_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         num_regs_per_bytecode = 2;
         codeSize = 1;
         infoArray[0].regNum = v2;
@@ -2783,8 +2749,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_SUB_FLOAT_2ADDR:
     case OP_MUL_FLOAT_2ADDR:
     case OP_DIV_FLOAT_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 2;
@@ -2797,8 +2763,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_REM_FLOAT_2ADDR: //load vA as GPR, store from fs
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 2;
@@ -2814,8 +2780,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_SUB_DOUBLE_2ADDR:
     case OP_MUL_DOUBLE_2ADDR:
     case OP_DIV_DOUBLE_2ADDR:
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 2;
@@ -2828,8 +2794,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 2;
         break;
     case OP_REM_DOUBLE_2ADDR: //load to xmm, store from fs
-        vA = INST_A(inst);
-        v2 = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
         codeSize = 1;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 2;
@@ -2848,8 +2814,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_AND_INT_LIT16:
     case OP_OR_INT_LIT16:
     case OP_XOR_INT_LIT16:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 2;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 1;
@@ -2863,11 +2829,10 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         break;
     case OP_DIV_INT_LIT16:
     case OP_REM_INT_LIT16:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         codeSize = 2;
-        tmp_s4 = (s2)FETCH(1);
-        tmp_s2 = tmp_s4;
+        tmp_s2 = currentMIR->dalvikInsn.vC;
         if(tmp_s2 == 0) {
             num_regs_per_bytecode = 0;
             break;
@@ -2909,8 +2874,8 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_SHR_INT_LIT8:
     case OP_USHR_INT_LIT8:
         codeSize = 2;
-        vA = INST_AA(inst);
-        vB = (u2)FETCH(1) & 0xff;
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[1].regNum = vA;
         infoArray[1].refCount = 1;
         infoArray[1].accessType = REGACCESS_D;
@@ -2924,9 +2889,9 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_DIV_INT_LIT8:
     case OP_REM_INT_LIT8:
         codeSize = 2;
-        vA = INST_AA(inst);
-        vB = (u2)FETCH(1) & 0xff;
-        tmp_s2 = (s2)FETCH(1) >> 8;
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        tmp_s2 = currentMIR->dalvikInsn.vC;
         if(tmp_s2 == 0) {
             num_regs_per_bytecode = 0;
             break;
@@ -2963,17 +2928,17 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
     case OP_EXECUTE_INLINE: //update glue->retval
     case OP_EXECUTE_INLINE_RANGE:
         u4 vC;
-        if(inst_op == OP_EXECUTE_INLINE)
-            num = INST_B(inst);
-        else
-            num = INST_AA(inst);
+        num = currentMIR->dalvikInsn.vA;
         if(inst_op == OP_EXECUTE_INLINE) {
-            vC = FETCH(2) & 0xf;
-            vD = (FETCH(2) >> 4) & 0xf;
-            vE = (FETCH(2) >> 8) & 0xf;
-            vF = FETCH(2) >> 12;
+            // Note that vC, vD, vE, and vF might have bad values
+            // depending on count. The variable "num" should be
+            // checked before using any of these.
+            vC = currentMIR->dalvikInsn.arg[0];
+            vD = currentMIR->dalvikInsn.arg[1];
+            vE = currentMIR->dalvikInsn.arg[2];
+            vF = currentMIR->dalvikInsn.arg[3];
         } else {
-            vC = FETCH(2);
+            vC = currentMIR->dalvikInsn.vC;
             vD = vC + 1;
             vE = vC + 2;
             vF = vC + 3;
@@ -3013,16 +2978,20 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray) {
         num_regs_per_bytecode = 0;
         break;
 #endif
+    default:
+        ALOGE("ERROR: JIT does not support bytecode 0x%hx when updating\n"
+                "VR accesses\n", currentMIR->dalvikInsn.opcode);
+        assert(false && "All opcodes should be supported.");
+        break;
     }
     return codeSize;
 }
 //! Updates infoArray(TempRegInfo) with temporaries accessed by INVOKE_NO_RANGE
 
 //!
-int updateInvokeNoRange(TempRegInfo* infoArray, int startInd) {
+int updateInvokeNoRange(TempRegInfo* infoArray, int startInd, const MIR * currentMIR) {
     int j = startInd;
-    //invokeMethodNoRange
-    int count = INST_B(inst);
+    int count = currentMIR->dalvikInsn.vA;
     if(count == 5) {
         infoArray[j].regNum = 22;
         infoArray[j].refCount = 2; //DU
@@ -3058,9 +3027,9 @@ int updateInvokeNoRange(TempRegInfo* infoArray, int startInd) {
 //! Updates infoArray(TempRegInfo) with temporaries accessed by INVOKE_RANGE
 
 //! LOOP_COUNT is used to indicate a variable is live through a loop
-int updateInvokeRange(TempRegInfo* infoArray, int startIndex) {
+int updateInvokeRange(TempRegInfo* infoArray, int startIndex, const MIR * currentMIR) {
     int j = startIndex;
-    int count = INST_AA(inst);
+    int count = currentMIR->dalvikInsn.vA;
     infoArray[j].regNum = 21;
     if(count <= 10) {
         infoArray[j].refCount = 1+count; //DU
@@ -3359,7 +3328,7 @@ int iget_obj_inst = -1;
 //! This function updates infoArray with temporaries accessed when lowering the bytecode
 
 //! returns the number of temporaries
-int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
+int getTempRegInfo(TempRegInfo* infoArray, const MIR * currentMIR) { //returns an array of TempRegInfo
     int k;
     int numTmps;
     for(k = 0; k < MAX_TEMP_REG_PER_BYTECODE; k++) {
@@ -3369,16 +3338,9 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[k].is8Bit = false;
     }
     u2 vA, vB, v1, length, num, tmp;
-    u2 inst_op = INST_INST(inst);
+    Opcode inst_op = currentMIR->dalvikInsn.opcode;
     s2 tmp_s2;
-    s4 tmp_s4;
-    switch(inst_op) {
-    case OP_APUT_BYTE:
-        for(k = 0; k < MAX_TEMP_REG_PER_BYTECODE; k++)
-            infoArray[k].shareWithVR = true; //false;
-        break;
-    }
-    switch (INST_INST(inst)) {
+    switch (inst_op) {
     case OP_NOP:
         return 0;
     case OP_MOVE:
@@ -3573,7 +3535,7 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         return 9;
 
     case OP_ARRAY_LENGTH:
-        vA = INST_A(inst);
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 3; //DU
         infoArray[0].physicalType = LowOpndRegType_gp;
@@ -3656,7 +3618,7 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         return 8;
 
     case OP_FILLED_NEW_ARRAY:
-        length = INST_B(inst);
+        length = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = PhysicalReg_EAX;
         //4: class object
         //3: defined by C function, used twice (array object)
@@ -3718,7 +3680,7 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         return 9+length;
 
     case OP_FILLED_NEW_ARRAY_RANGE:
-        length = INST_AA(inst);
+        length = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = PhysicalReg_EAX;
         //4: class object
         //3: defined by C function, used twice (array object)
@@ -3918,7 +3880,21 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_AGET_BYTE:
     case OP_AGET_CHAR:
     case OP_AGET_SHORT:
-        vA = INST_AA(inst);
+#ifdef INC_NCG_O0
+        if(gDvm.helper_switch[7]) {
+            infoArray[0].regNum = PhysicalReg_EBX;
+            infoArray[0].refCount = 2;
+            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            infoArray[1].regNum = PhysicalReg_ECX;
+            infoArray[1].refCount = 2;
+            infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            infoArray[2].regNum = PhysicalReg_EDX;
+            infoArray[2].refCount = 2;
+            infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            return 3;
+        }
+#endif
+        vA = currentMIR->dalvikInsn.vA;
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 4; //DU
         infoArray[0].physicalType = LowOpndRegType_gp;
@@ -3955,10 +3931,12 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[4].refCount = 2;
         infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
         return 5;
-
+    case OP_APUT_BYTE:
+        for(k = 0; k < MAX_TEMP_REG_PER_BYTECODE; k++)
+            infoArray[k].shareWithVR = true; //false;
+        // Intentional fall through
     case OP_APUT:
     case OP_APUT_BOOLEAN:
-    case OP_APUT_BYTE:
     case OP_APUT_CHAR:
     case OP_APUT_SHORT:
         infoArray[0].regNum = 1;
@@ -4354,9 +4332,9 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[numTmps].physicalType = LowOpndRegType_gp;
         numTmps++;
         if(inst_op == OP_INVOKE_VIRTUAL)
-            k = updateInvokeNoRange(infoArray, numTmps);
+            k = updateInvokeNoRange(infoArray, numTmps, currentMIR);
         else
-            k = updateInvokeRange(infoArray, numTmps);
+            k = updateInvokeRange(infoArray, numTmps, currentMIR);
         return k;
 #else
         infoArray[0].regNum = 3;
@@ -4439,9 +4417,9 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[11].refCount = 2; //DU
         infoArray[11].physicalType = LowOpndRegType_scratch;
         if(inst_op == OP_INVOKE_SUPER)
-            k = updateInvokeNoRange(infoArray, 12);
+            k = updateInvokeNoRange(infoArray, 12, currentMIR);
         else
-            k = updateInvokeRange(infoArray, 12);
+            k = updateInvokeRange(infoArray, 12, currentMIR);
         return k;
     case OP_INVOKE_DIRECT:
     case OP_INVOKE_DIRECT_RANGE:
@@ -4469,9 +4447,9 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[6].refCount = 2; //DU
         infoArray[6].physicalType = LowOpndRegType_scratch;
         if(inst_op == OP_INVOKE_DIRECT)
-            k = updateInvokeNoRange(infoArray, 7);
+            k = updateInvokeNoRange(infoArray, 7, currentMIR);
         else
-            k = updateInvokeRange(infoArray, 7);
+            k = updateInvokeRange(infoArray, 7, currentMIR);
         return k;
     case OP_INVOKE_STATIC:
     case OP_INVOKE_STATIC_RANGE:
@@ -4496,9 +4474,9 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[5].refCount = 2; //DU
         infoArray[5].physicalType = LowOpndRegType_scratch;
         if(inst_op == OP_INVOKE_STATIC)
-            k = updateInvokeNoRange(infoArray, 6);
+            k = updateInvokeNoRange(infoArray, 6, currentMIR);
         else
-            k = updateInvokeRange(infoArray, 6);
+            k = updateInvokeRange(infoArray, 6, currentMIR);
         return k;
     case OP_INVOKE_INTERFACE:
     case OP_INVOKE_INTERFACE_RANGE:
@@ -4509,9 +4487,9 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[numTmps].physicalType = LowOpndRegType_gp;
         numTmps++;
         if(inst_op == OP_INVOKE_INTERFACE)
-            k = updateInvokeNoRange(infoArray, numTmps);
+            k = updateInvokeNoRange(infoArray, numTmps, currentMIR);
         else
-            k = updateInvokeRange(infoArray, numTmps);
+            k = updateInvokeRange(infoArray, numTmps, currentMIR);
         return k;
 #else
         infoArray[0].regNum = 1;
@@ -4555,8 +4533,8 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         ////////////////////////////////////////////// ALU
     case OP_NEG_INT:
     case OP_NOT_INT:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 3; //define, update, use
         infoArray[0].physicalType = LowOpndRegType_gp;
@@ -4572,8 +4550,8 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[1].physicalType = LowOpndRegType_xmm;
         return 2;
     case OP_NOT_LONG:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 3; //define, update, use
         infoArray[0].physicalType = LowOpndRegType_xmm;
@@ -4584,8 +4562,8 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[1].physicalType = LowOpndRegType_xmm;
         return 2;
     case OP_NEG_FLOAT:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 3; //define, update, use
         infoArray[0].physicalType = LowOpndRegType_gp;
@@ -4593,8 +4571,8 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
             infoArray[0].shareWithVR = false;
         return 1;
     case OP_NEG_DOUBLE:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 3; //define, use
         infoArray[0].physicalType = LowOpndRegType_xmm;
@@ -4644,8 +4622,8 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_INT_TO_BYTE:
     case OP_INT_TO_CHAR:
     case OP_INT_TO_SHORT:
-        vA = INST_A(inst);
-        vB = INST_B(inst);
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 4; //define, update, update, use
         infoArray[0].physicalType = LowOpndRegType_gp;
@@ -4667,10 +4645,10 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_XOR_INT_2ADDR:
         if(inst_op == OP_ADD_INT || inst_op == OP_SUB_INT || inst_op == OP_MUL_INT ||
            inst_op == OP_AND_INT || inst_op == OP_OR_INT || inst_op == OP_XOR_INT) {
-            vA = INST_AA(inst);
-            v1 = *((u1*)rPC + 2);
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
         } else {
-            vA = INST_A(inst);
+            vA = currentMIR->dalvikInsn.vA;
             v1 = vA;
         }
         infoArray[0].regNum = 1;
@@ -4687,10 +4665,10 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_SHR_INT_2ADDR:
     case OP_USHR_INT_2ADDR: //use %cl or %ecx?
         if(inst_op == OP_SHL_INT || inst_op == OP_SHR_INT || inst_op == OP_USHR_INT) {
-            vA = INST_AA(inst);
-            v1 = *((u1*)rPC + 2);
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
         } else {
-            vA = INST_A(inst);
+            vA = currentMIR->dalvikInsn.vA;
             v1 = vA;
         }
         infoArray[0].regNum = 1;
@@ -4740,14 +4718,8 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_SHL_INT_LIT8:
     case OP_SHR_INT_LIT8:
     case OP_USHR_INT_LIT8:
-        if(inst_op == OP_ADD_INT_LIT16 || inst_op == OP_MUL_INT_LIT16 ||
-           inst_op == OP_AND_INT_LIT16 || inst_op == OP_OR_INT_LIT16 || inst_op == OP_XOR_INT_LIT16) {
-            vA = INST_A(inst);
-            v1 = INST_B(inst);
-        } else {
-            vA = INST_AA(inst);
-            v1 = (u2)FETCH(1) & 0xff;
-        }
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 3; //define, update, use
         infoArray[0].physicalType = LowOpndRegType_gp;
@@ -4757,8 +4729,8 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
 
     case OP_RSUB_INT_LIT8:
     case OP_RSUB_INT:
-        vA = INST_AA(inst);
-        v1 = (inst_op == OP_RSUB_INT) ? INST_B(inst) : ((u2)FETCH(1) & 0xff);
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 2;
         infoArray[0].physicalType = LowOpndRegType_gp;
@@ -4775,13 +4747,7 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_REM_INT_LIT16:
     case OP_DIV_INT_LIT8:
     case OP_REM_INT_LIT8:
-        if(inst_op == OP_DIV_INT_LIT8 || inst_op == OP_REM_INT_LIT8) {
-            tmp_s2 = (s2)FETCH(1) >> 8;
-        }
-        else {
-            tmp_s4 = (s2)FETCH(1);
-            tmp_s2 = tmp_s4;
-        }
+        tmp_s2 = currentMIR->dalvikInsn.vC;
         if((inst_op == OP_DIV_INT_LIT8 || inst_op == OP_DIV_INT_LIT16)) {
             int power = isPowerOfTwo(tmp_s2);
             if(power >= 1) { /* divide by a power of 2 constant */
@@ -4848,10 +4814,10 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_XOR_LONG_2ADDR:
         if(inst_op == OP_ADD_LONG || inst_op == OP_SUB_LONG || inst_op == OP_AND_LONG ||
            inst_op == OP_OR_LONG || inst_op == OP_XOR_LONG) {
-            vA = INST_AA(inst);
-            v1 = *((u1*)rPC + 2);
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
         } else {
-            vA = INST_A(inst);
+            vA = currentMIR->dalvikInsn.vA;
             v1 = vA;
         }
         infoArray[0].regNum = 1;
@@ -4867,10 +4833,10 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_SHL_LONG:
     case OP_SHL_LONG_2ADDR:
         if(inst_op == OP_SHL_LONG) {
-            vA = INST_AA(inst);
-            v1 = *((u1*)rPC + 2);
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
         } else {
-            vA = INST_A(inst);
+            vA = currentMIR->dalvikInsn.vA;
             v1 = vA;
         }
         infoArray[0].regNum = 1;
@@ -4890,10 +4856,10 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_SHR_LONG:
     case OP_SHR_LONG_2ADDR:
         if(inst_op == OP_SHR_LONG) {
-            vA = INST_AA(inst);
-            v1 = *((u1*)rPC + 2);
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
         } else {
-            vA = INST_A(inst);
+            vA = currentMIR->dalvikInsn.vA;
             v1 = vA;
         }
         infoArray[0].regNum = 1;
@@ -4919,10 +4885,10 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
     case OP_USHR_LONG:
     case OP_USHR_LONG_2ADDR:
         if(inst_op == OP_USHR_LONG) {
-            vA = INST_AA(inst);
-            v1 = *((u1*)rPC + 2);
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
         } else {
-            vA = INST_A(inst);
+            vA = currentMIR->dalvikInsn.vA;
             v1 = vA;
         }
         infoArray[0].regNum = 1;
@@ -5114,11 +5080,9 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
 
     case OP_EXECUTE_INLINE:
     case OP_EXECUTE_INLINE_RANGE:
-        if(inst_op == OP_EXECUTE_INLINE)
-            num = INST_B(inst);
-        else
-            num = INST_AA(inst);
-        tmp = FETCH(1);
+        num = currentMIR->dalvikInsn.vA;
+#if defined(WITH_JIT)
+        tmp = currentMIR->dalvikInsn.vB;
         switch (tmp) {
             case INLINE_STRING_LENGTH:
                 infoArray[0].regNum = 1;
@@ -5290,7 +5254,7 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
             default:
                 break;
         }
-
+#endif
         infoArray[0].regNum = 1;
         infoArray[0].refCount = 4;
         infoArray[0].physicalType = LowOpndRegType_gp;
@@ -5340,9 +5304,9 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[numTmps].physicalType = LowOpndRegType_gp;
         numTmps++;
         if(inst_op == OP_INVOKE_VIRTUAL_QUICK)
-            k = updateInvokeNoRange(infoArray, numTmps);
+            k = updateInvokeNoRange(infoArray, numTmps, currentMIR);
         else
-            k = updateInvokeRange(infoArray, numTmps);
+            k = updateInvokeRange(infoArray, numTmps, currentMIR);
         return k;
 #else
         infoArray[0].regNum = 1;
@@ -5393,9 +5357,9 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         infoArray[6].refCount = 2;
         infoArray[6].physicalType = LowOpndRegType_scratch;
         if(inst_op == OP_INVOKE_SUPER_QUICK_RANGE)
-            k = updateInvokeRange(infoArray, 7);
+            k = updateInvokeRange(infoArray, 7, currentMIR);
         else
-            k = updateInvokeNoRange(infoArray, 7);
+            k = updateInvokeNoRange(infoArray, 7, currentMIR);
         return k;
 #ifdef SUPPORT_HLO
     case kExtInstruction:
@@ -5589,6 +5553,11 @@ int getTempRegInfo(TempRegInfo* infoArray) { //returns an array of TempRegInfo
         return 7;
     }
 #endif
+    default:
+        ALOGE("ERROR: JIT does not support bytecode 0x%hx when updating\n"
+                "temp accesses\n", currentMIR->dalvikInsn.opcode);
+        assert(false && "All opcodes should be supported.");
+        break;
     }
     return -1;
 }
diff --git a/vm/compiler/codegen/x86/CodegenInterface.cpp b/vm/compiler/codegen/x86/CodegenInterface.cpp
index e8a47f6..65ac3be 100644
--- a/vm/compiler/codegen/x86/CodegenInterface.cpp
+++ b/vm/compiler/codegen/x86/CodegenInterface.cpp
@@ -1331,7 +1331,6 @@ void dvmCompilerMIR2LIR(CompilationUnit *cUnit, JitTranslationInfo *info)
         //LowOp *headLIR = NULL;
         const DexCode *dexCode = dvmGetMethodCode(cUnit->method);
         const u2 *startCodePtr = dexCode->insns;
-        const u2 *codePtr;
         labelList[i].lop.generic.offset = (stream - streamMethodStart);
         ALOGV("get ready to handle JIT bb %d type %d hidden %d",
               bb->id, bb->blockType, bb->hidden);
@@ -1392,9 +1391,8 @@ void dvmCompilerMIR2LIR(CompilationUnit *cUnit, JitTranslationInfo *info)
                  0);
             if (singleStepMe || cUnit->allSingleStep) {
             } else {
-                codePtr = startCodePtr + mir->offset;
                 //lower each byte code, update LIR
-                notHandled = lowerByteCodeJit(cUnit->method, rPC , mir);
+                notHandled = lowerByteCodeJit(cUnit->method, mir, rPC);
                 if(gDvmJit.codeCacheByteUsed + (stream - streamStart) +
                    CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
                     ALOGI("JIT code cache full after lowerByteCodeJit (trace uses %uB)", (stream - streamStart));
diff --git a/vm/compiler/codegen/x86/Lower.cpp b/vm/compiler/codegen/x86/Lower.cpp
index 68a3994..d67a7eb 100644
--- a/vm/compiler/codegen/x86/Lower.cpp
+++ b/vm/compiler/codegen/x86/Lower.cpp
@@ -44,7 +44,6 @@ PhysicalReg scratchRegs[4];
 LowOp* ops[BUFFER_SIZE];
 LowOp* op;
 u2* rPC; //PC pointer to bytecode
-u2 inst; //current bytecode
 int offsetPC/*offset in bytecode*/, offsetNCG/*byte offset in native code*/;
 int ncg_rPC;
 //! map from PC in bytecode to PC in native code
@@ -58,12 +57,9 @@ Method* currentMethod = NULL;
 int currentExceptionBlockIdx = -1;
 LowOpBlockLabel* traceLabelList = NULL;
 BasicBlock* traceCurrentBB = NULL;
-MIR* traceCurrentMIR = NULL;
 JitMode traceMode = kJitTrace;
 bool branchInLoop = false;
 
-int common_invokeMethodNoRange();
-int common_invokeMethodRange();
 int common_invokeArgsDone(ArgsDoneType, bool);
 
 //data section of .ia32:
@@ -246,6 +242,9 @@ int performCGWorklist() {
 int aput_object_count;
 int common_periodicChecks_entry();
 int common_periodicChecks4();
+
+#if 0 /* Commented out because it is dead code. If re-enabling, this needs to be updated
+         to take MIR as parameter */
 /*!
 \brief for debugging purpose, dump the sequence of native code for each bytecode
 
@@ -283,6 +282,7 @@ int ncgMethodFake(Method* method) {
     }
     exit(0);
 }
+#endif
 
 bool existATryBlock(Method* method, int startPC, int endPC) {
     const DexCode* pCode = dvmGetMethodCode(method);
@@ -428,13 +428,16 @@ void initGlobalMethods() {
 }
 
 ExecutionMode origMode;
-//when to update streamMethodStart
-bool lowerByteCodeJit(const Method* method, const u2* codePtr, MIR* mir) {
-    rPC = (u2*)codePtr;
-    inst = FETCH(0);
-    traceCurrentMIR = mir;
-    int retCode = lowerByteCode(method);
-    traceCurrentMIR = NULL;
+
+/**
+ * @brief Lowers bytecode to native code
+ * @param method parent method of trace
+ * @param mir bytecode representation
+ * @return true when NOT handled and false when it IS handled
+ */
+bool lowerByteCodeJit(const Method* method, const MIR * mir,
+        const u2 * dalvikPC) {
+    int retCode = lowerByteCode(method, mir, dalvikPC);
     freeShortMap();
     if(retCode >= 0) return false; //handled
     return true; //not handled
@@ -491,17 +494,21 @@ void endOfTrace(bool freeOnly) {
     gDvm.executionMode = origMode;
 }
 
-///////////////////////////////////////////////////////////////////
-//!
-//! each bytecode is translated to a sequence of machine codes
-int lowerByteCode(const Method* method) { //inputs: rPC & inst & stream & streamMethodStart
+/**
+ * @brief Generates native code for the bytecode.
+ * @details May update code stream.
+ * @param method parent method of trace
+ * @param mir bytecode representation
+ * @return 0 or greater when handled
+ */
+int lowerByteCode(const Method* method, const MIR * mir, const u2 * dalvikPC) {
     /* offsetPC is used in O1 code generator, where it is defined as the sequence number
        use a local version to avoid overwriting */
-    int offsetPC = rPC - (u2*)method->insns;
+    int offsetPC = mir->offset;
 
     if(dump_x86_inst)
-        ALOGI("LOWER bytecode %x at offsetPC %x offsetNCG %x @%p",
-              INST_INST(inst), offsetPC, stream - streamMethodStart, stream);
+        LOGI("LOWER bytecode %x at offsetPC %x offsetNCG %x @%p\n",
+             mir->dalvikInsn.opcode, offsetPC, stream - streamMethodStart, stream);
 
     //update mapFromBCtoNCG
     offsetNCG = stream - streamMethodStart;
@@ -511,494 +518,497 @@ int lowerByteCode(const Method* method) { //inputs: rPC & inst & stream & stream
     insertMapWorklist(offsetPC, mapFromBCtoNCG[offsetPC], 1);
 #endif
     //return number of LowOps generated
-    switch (INST_INST(inst)) {
+    switch (mir->dalvikInsn.opcode) {
     case OP_NOP:
-        return op_nop();
+        return op_nop(mir);
     case OP_MOVE:
     case OP_MOVE_OBJECT:
-        return op_move();
+        return op_move(mir);
     case OP_MOVE_FROM16:
     case OP_MOVE_OBJECT_FROM16:
-        return op_move_from16();
+        return op_move_from16(mir);
     case OP_MOVE_16:
     case OP_MOVE_OBJECT_16:
-        return op_move_16();
+        return op_move_16(mir);
     case OP_MOVE_WIDE:
-        return op_move_wide();
+        return op_move_wide(mir);
     case OP_MOVE_WIDE_FROM16:
-        return op_move_wide_from16();
+        return op_move_wide_from16(mir);
     case OP_MOVE_WIDE_16:
-        return op_move_wide_16();
+        return op_move_wide_16(mir);
     case OP_MOVE_RESULT:
     case OP_MOVE_RESULT_OBJECT:
-        return op_move_result();
+        return op_move_result(mir);
     case OP_MOVE_RESULT_WIDE:
-        return op_move_result_wide();
+        return op_move_result_wide(mir);
     case OP_MOVE_EXCEPTION:
-        return op_move_exception();
+        return op_move_exception(mir);
     case OP_RETURN_VOID:
     case OP_RETURN_VOID_BARRIER:
-        return op_return_void();
+        return op_return_void(mir);
     case OP_RETURN:
     case OP_RETURN_OBJECT:
-        return op_return();
+        return op_return(mir);
     case OP_RETURN_WIDE:
-        return op_return_wide();
+        return op_return_wide(mir);
     case OP_CONST_4:
-        return op_const_4();
+        return op_const_4(mir);
     case OP_CONST_16:
-        return op_const_16();
+        return op_const_16(mir);
     case OP_CONST:
-        return op_const();
+        return op_const(mir);
     case OP_CONST_HIGH16:
-        return op_const_high16();
+        return op_const_high16(mir);
     case OP_CONST_WIDE_16:
-        return op_const_wide_16();
+        return op_const_wide_16(mir);
     case OP_CONST_WIDE_32:
-        return op_const_wide_32();
+        return op_const_wide_32(mir);
     case OP_CONST_WIDE:
-        return op_const_wide();
+        return op_const_wide(mir);
     case OP_CONST_WIDE_HIGH16:
-        return op_const_wide_high16();
+        return op_const_wide_high16(mir);
     case OP_CONST_STRING:
-        return op_const_string();
+        return op_const_string(mir);
     case OP_CONST_STRING_JUMBO:
-        return op_const_string_jumbo();
+        return op_const_string_jumbo(mir);
     case OP_CONST_CLASS:
-        return op_const_class();
+        return op_const_class(mir);
     case OP_MONITOR_ENTER:
-        return op_monitor_enter();
+        return op_monitor_enter(mir);
     case OP_MONITOR_EXIT:
-        return op_monitor_exit();
+        return op_monitor_exit(mir);
     case OP_CHECK_CAST:
-        return op_check_cast();
+        return op_check_cast(mir);
     case OP_INSTANCE_OF:
-        return op_instance_of();
+        return op_instance_of(mir);
     case OP_ARRAY_LENGTH:
-        return op_array_length();
+        return op_array_length(mir);
     case OP_NEW_INSTANCE:
-        return op_new_instance();
+        return op_new_instance(mir);
     case OP_NEW_ARRAY:
-        return op_new_array();
+        return op_new_array(mir);
     case OP_FILLED_NEW_ARRAY:
-        return op_filled_new_array();
+        return op_filled_new_array(mir);
     case OP_FILLED_NEW_ARRAY_RANGE:
-        return op_filled_new_array_range();
+        return op_filled_new_array_range(mir);
     case OP_FILL_ARRAY_DATA:
-        return op_fill_array_data();
+        return op_fill_array_data(mir, dalvikPC);
     case OP_THROW:
-        return op_throw();
+        return op_throw(mir);
     case OP_THROW_VERIFICATION_ERROR:
-        return op_throw_verification_error();
+        return op_throw_verification_error(mir);
     case OP_GOTO:
-        return op_goto();
+        return op_goto(mir);
     case OP_GOTO_16:
-        return op_goto_16();
+        return op_goto_16(mir);
     case OP_GOTO_32:
-        return op_goto_32();
+        return op_goto_32(mir);
     case OP_PACKED_SWITCH:
-        return op_packed_switch();
+        return op_packed_switch(mir, dalvikPC);
     case OP_SPARSE_SWITCH:
-        return op_sparse_switch();
+        return op_sparse_switch(mir, dalvikPC);
     case OP_CMPL_FLOAT:
-        return op_cmpl_float();
+        return op_cmpl_float(mir);
     case OP_CMPG_FLOAT:
-        return op_cmpg_float();
+        return op_cmpg_float(mir);
     case OP_CMPL_DOUBLE:
-        return op_cmpl_double();
+        return op_cmpl_double(mir);
     case OP_CMPG_DOUBLE:
-        return op_cmpg_double();
+        return op_cmpg_double(mir);
     case OP_CMP_LONG:
-        return op_cmp_long();
+        return op_cmp_long(mir);
     case OP_IF_EQ:
-        return op_if_eq();
+        return op_if_eq(mir);
     case OP_IF_NE:
-        return op_if_ne();
+        return op_if_ne(mir);
     case OP_IF_LT:
-        return op_if_lt();
+        return op_if_lt(mir);
     case OP_IF_GE:
-        return op_if_ge();
+        return op_if_ge(mir);
     case OP_IF_GT:
-        return op_if_gt();
+        return op_if_gt(mir);
     case OP_IF_LE:
-        return op_if_le();
+        return op_if_le(mir);
     case OP_IF_EQZ:
-        return op_if_eqz();
+        return op_if_eqz(mir);
     case OP_IF_NEZ:
-        return op_if_nez();
+        return op_if_nez(mir);
     case OP_IF_LTZ:
-        return op_if_ltz();
+        return op_if_ltz(mir);
     case OP_IF_GEZ:
-        return op_if_gez();
+        return op_if_gez(mir);
     case OP_IF_GTZ:
-        return op_if_gtz();
+        return op_if_gtz(mir);
     case OP_IF_LEZ:
-        return op_if_lez();
+        return op_if_lez(mir);
     case OP_AGET:
-        return op_aget();
+        return op_aget(mir);
     case OP_AGET_WIDE:
-        return op_aget_wide();
+        return op_aget_wide(mir);
     case OP_AGET_OBJECT:
-        return op_aget_object();
+        return op_aget_object(mir);
     case OP_AGET_BOOLEAN:
-        return op_aget_boolean();
+        return op_aget_boolean(mir);
     case OP_AGET_BYTE:
-        return op_aget_byte();
+        return op_aget_byte(mir);
     case OP_AGET_CHAR:
-        return op_aget_char();
+        return op_aget_char(mir);
     case OP_AGET_SHORT:
-        return op_aget_short();
+        return op_aget_short(mir);
     case OP_APUT:
-        return op_aput();
+        return op_aput(mir);
     case OP_APUT_WIDE:
-        return op_aput_wide();
+        return op_aput_wide(mir);
     case OP_APUT_OBJECT:
-        return op_aput_object();
+        return op_aput_object(mir);
     case OP_APUT_BOOLEAN:
-        return op_aput_boolean();
+        return op_aput_boolean(mir);
     case OP_APUT_BYTE:
-        return op_aput_byte();
+        return op_aput_byte(mir);
     case OP_APUT_CHAR:
-        return op_aput_char();
+        return op_aput_char(mir);
     case OP_APUT_SHORT:
-        return op_aput_short();
+        return op_aput_short(mir);
     case OP_IGET:
     case OP_IGET_VOLATILE:
-        return op_iget();
+        return op_iget(mir);
     case OP_IGET_WIDE:
-        return op_iget_wide(false); // isVolatile==false
+        return op_iget_wide(mir, false /*isVolatile*/);
     case OP_IGET_WIDE_VOLATILE:
-        return op_iget_wide(true);  // isVolatile==true
+        return op_iget_wide(mir, true /*isVolatile*/);
     case OP_IGET_OBJECT:
     case OP_IGET_OBJECT_VOLATILE:
-        return op_iget_object();
+        return op_iget_object(mir);
     case OP_IGET_BOOLEAN:
-        return op_iget_boolean();
+        return op_iget_boolean(mir);
     case OP_IGET_BYTE:
-        return op_iget_byte();
+        return op_iget_byte(mir);
     case OP_IGET_CHAR:
-        return op_iget_char();
+        return op_iget_char(mir);
     case OP_IGET_SHORT:
-        return op_iget_short();
+        return op_iget_short(mir);
     case OP_IPUT:
     case OP_IPUT_VOLATILE:
-        return op_iput();
+        return op_iput(mir);
     case OP_IPUT_WIDE:
-        return op_iput_wide(false); // isVolatile==false
+        return op_iput_wide(mir, false /*isVolatile*/);
     case OP_IPUT_WIDE_VOLATILE:
-        return op_iput_wide(true);  // isVolatile==true
+        return op_iput_wide(mir, true /*isVolatile*/);
     case OP_IPUT_OBJECT:
     case OP_IPUT_OBJECT_VOLATILE:
-        return op_iput_object();
+        return op_iput_object(mir);
     case OP_IPUT_BOOLEAN:
-        return op_iput_boolean();
+        return op_iput_boolean(mir);
     case OP_IPUT_BYTE:
-        return op_iput_byte();
+        return op_iput_byte(mir);
     case OP_IPUT_CHAR:
-        return op_iput_char();
+        return op_iput_char(mir);
     case OP_IPUT_SHORT:
-        return op_iput_short();
+        return op_iput_short(mir);
     case OP_SGET:
     case OP_SGET_VOLATILE:
-        return op_sget();
+        return op_sget(mir);
     case OP_SGET_WIDE:
-        return op_sget_wide(false); // isVolatile==false
+        return op_sget_wide(mir, false /*isVolatile*/);
     case OP_SGET_WIDE_VOLATILE:
-        return op_sget_wide(true);  // isVolatile==true
+        return op_sget_wide(mir, true /*isVolatile*/);
     case OP_SGET_OBJECT:
     case OP_SGET_OBJECT_VOLATILE:
-        return op_sget_object();
+        return op_sget_object(mir);
     case OP_SGET_BOOLEAN:
-        return op_sget_boolean();
+        return op_sget_boolean(mir);
     case OP_SGET_BYTE:
-        return op_sget_byte();
+        return op_sget_byte(mir);
     case OP_SGET_CHAR:
-        return op_sget_char();
+        return op_sget_char(mir);
     case OP_SGET_SHORT:
-        return op_sget_short();
+        return op_sget_short(mir);
     case OP_SPUT:
     case OP_SPUT_VOLATILE:
-        return op_sput(false);
+        return op_sput(mir, false /*isObj*/);
     case OP_SPUT_WIDE:
-        return op_sput_wide(false); // isVolatile==false
+        return op_sput_wide(mir, false /*isVolatile*/);
     case OP_SPUT_WIDE_VOLATILE:
-        return op_sput_wide(true);  // isVolatile==true
+        return op_sput_wide(mir, true /*isVolatile*/);
     case OP_SPUT_OBJECT:
     case OP_SPUT_OBJECT_VOLATILE:
-        return op_sput_object();
+        return op_sput_object(mir);
     case OP_SPUT_BOOLEAN:
-        return op_sput_boolean();
+        return op_sput_boolean(mir);
     case OP_SPUT_BYTE:
-        return op_sput_byte();
+        return op_sput_byte(mir);
     case OP_SPUT_CHAR:
-        return op_sput_char();
+        return op_sput_char(mir);
     case OP_SPUT_SHORT:
-        return op_sput_short();
+        return op_sput_short(mir);
     case OP_INVOKE_VIRTUAL:
-        return op_invoke_virtual();
+        return op_invoke_virtual(mir);
     case OP_INVOKE_SUPER:
-        return op_invoke_super();
+        return op_invoke_super(mir);
     case OP_INVOKE_DIRECT:
-        return op_invoke_direct();
+        return op_invoke_direct(mir);
     case OP_INVOKE_STATIC:
-        return op_invoke_static();
+        return op_invoke_static(mir);
     case OP_INVOKE_INTERFACE:
-        return op_invoke_interface();
+        return op_invoke_interface(mir);
     case OP_INVOKE_VIRTUAL_RANGE:
-        return op_invoke_virtual_range();
+        return op_invoke_virtual_range(mir);
     case OP_INVOKE_SUPER_RANGE:
-        return op_invoke_super_range();
+        return op_invoke_super_range(mir);
     case OP_INVOKE_DIRECT_RANGE:
-        return op_invoke_direct_range();
+        return op_invoke_direct_range(mir);
     case OP_INVOKE_STATIC_RANGE:
-        return op_invoke_static_range();
+        return op_invoke_static_range(mir);
     case OP_INVOKE_INTERFACE_RANGE:
-        return op_invoke_interface_range();
+        return op_invoke_interface_range(mir);
     case OP_NEG_INT:
-        return op_neg_int();
+        return op_neg_int(mir);
     case OP_NOT_INT:
-        return op_not_int();
+        return op_not_int(mir);
     case OP_NEG_LONG:
-        return op_neg_long();
+        return op_neg_long(mir);
     case OP_NOT_LONG:
-        return op_not_long();
+        return op_not_long(mir);
     case OP_NEG_FLOAT:
-        return op_neg_float();
+        return op_neg_float(mir);
     case OP_NEG_DOUBLE:
-        return op_neg_double();
+        return op_neg_double(mir);
     case OP_INT_TO_LONG:
-        return op_int_to_long();
+        return op_int_to_long(mir);
     case OP_INT_TO_FLOAT:
-        return op_int_to_float();
+        return op_int_to_float(mir);
     case OP_INT_TO_DOUBLE:
-        return op_int_to_double();
+        return op_int_to_double(mir);
     case OP_LONG_TO_INT:
-        return op_long_to_int();
+        return op_long_to_int(mir);
     case OP_LONG_TO_FLOAT:
-        return op_long_to_float();
+        return op_long_to_float(mir);
     case OP_LONG_TO_DOUBLE:
-        return op_long_to_double();
+        return op_long_to_double(mir);
     case OP_FLOAT_TO_INT:
-        return op_float_to_int();
+        return op_float_to_int(mir);
     case OP_FLOAT_TO_LONG:
-        return op_float_to_long();
+        return op_float_to_long(mir);
     case OP_FLOAT_TO_DOUBLE:
-        return op_float_to_double();
+        return op_float_to_double(mir);
     case OP_DOUBLE_TO_INT:
-        return op_double_to_int();
+        return op_double_to_int(mir);
     case OP_DOUBLE_TO_LONG:
-        return op_double_to_long();
+        return op_double_to_long(mir);
     case OP_DOUBLE_TO_FLOAT:
-        return op_double_to_float();
+        return op_double_to_float(mir);
     case OP_INT_TO_BYTE:
-        return op_int_to_byte();
+        return op_int_to_byte(mir);
     case OP_INT_TO_CHAR:
-        return op_int_to_char();
+        return op_int_to_char(mir);
     case OP_INT_TO_SHORT:
-        return op_int_to_short();
+        return op_int_to_short(mir);
     case OP_ADD_INT:
-        return op_add_int();
+        return op_add_int(mir);
     case OP_SUB_INT:
-        return op_sub_int();
+        return op_sub_int(mir);
     case OP_MUL_INT:
-        return op_mul_int();
+        return op_mul_int(mir);
     case OP_DIV_INT:
-        return op_div_int();
+        return op_div_int(mir);
     case OP_REM_INT:
-        return op_rem_int();
+        return op_rem_int(mir);
     case OP_AND_INT:
-        return op_and_int();
+        return op_and_int(mir);
     case OP_OR_INT:
-        return op_or_int();
+        return op_or_int(mir);
     case OP_XOR_INT:
-        return op_xor_int();
+        return op_xor_int(mir);
     case OP_SHL_INT:
-        return op_shl_int();
+        return op_shl_int(mir);
     case OP_SHR_INT:
-        return op_shr_int();
+        return op_shr_int(mir);
     case OP_USHR_INT:
-        return op_ushr_int();
+        return op_ushr_int(mir);
     case OP_ADD_LONG:
-        return op_add_long();
+        return op_add_long(mir);
     case OP_SUB_LONG:
-        return op_sub_long();
+        return op_sub_long(mir);
     case OP_MUL_LONG:
-        return op_mul_long();
+        return op_mul_long(mir);
     case OP_DIV_LONG:
-        return op_div_long();
+        return op_div_long(mir);
     case OP_REM_LONG:
-        return op_rem_long();
+        return op_rem_long(mir);
     case OP_AND_LONG:
-        return op_and_long();
+        return op_and_long(mir);
     case OP_OR_LONG:
-        return op_or_long();
+        return op_or_long(mir);
     case OP_XOR_LONG:
-        return op_xor_long();
+        return op_xor_long(mir);
     case OP_SHL_LONG:
-        return op_shl_long();
+        return op_shl_long(mir);
     case OP_SHR_LONG:
-        return op_shr_long();
+        return op_shr_long(mir);
     case OP_USHR_LONG:
-        return op_ushr_long();
+        return op_ushr_long(mir);
     case OP_ADD_FLOAT:
-        return op_add_float();
+        return op_add_float(mir);
     case OP_SUB_FLOAT:
-        return op_sub_float();
+        return op_sub_float(mir);
     case OP_MUL_FLOAT:
-        return op_mul_float();
+        return op_mul_float(mir);
     case OP_DIV_FLOAT:
-        return op_div_float();
+        return op_div_float(mir);
     case OP_REM_FLOAT:
-        return op_rem_float();
+        return op_rem_float(mir);
     case OP_ADD_DOUBLE:
-        return op_add_double();
+        return op_add_double(mir);
     case OP_SUB_DOUBLE:
-        return op_sub_double();
+        return op_sub_double(mir);
     case OP_MUL_DOUBLE:
-        return op_mul_double();
+        return op_mul_double(mir);
     case OP_DIV_DOUBLE:
-        return op_div_double();
+        return op_div_double(mir);
     case OP_REM_DOUBLE:
-        return op_rem_double();
+        return op_rem_double(mir);
     case OP_ADD_INT_2ADDR:
-        return op_add_int_2addr();
+        return op_add_int_2addr(mir);
     case OP_SUB_INT_2ADDR:
-        return op_sub_int_2addr();
+        return op_sub_int_2addr(mir);
     case OP_MUL_INT_2ADDR:
-        return op_mul_int_2addr();
+        return op_mul_int_2addr(mir);
     case OP_DIV_INT_2ADDR:
-        return op_div_int_2addr();
+        return op_div_int_2addr(mir);
     case OP_REM_INT_2ADDR:
-        return op_rem_int_2addr();
+        return op_rem_int_2addr(mir);
     case OP_AND_INT_2ADDR:
-        return op_and_int_2addr();
+        return op_and_int_2addr(mir);
     case OP_OR_INT_2ADDR:
-        return op_or_int_2addr();
+        return op_or_int_2addr(mir);
     case OP_XOR_INT_2ADDR:
-        return op_xor_int_2addr();
+        return op_xor_int_2addr(mir);
     case OP_SHL_INT_2ADDR:
-        return op_shl_int_2addr();
+        return op_shl_int_2addr(mir);
     case OP_SHR_INT_2ADDR:
-        return op_shr_int_2addr();
+        return op_shr_int_2addr(mir);
     case OP_USHR_INT_2ADDR:
-        return op_ushr_int_2addr();
+        return op_ushr_int_2addr(mir);
     case OP_ADD_LONG_2ADDR:
-        return op_add_long_2addr();
+        return op_add_long_2addr(mir);
     case OP_SUB_LONG_2ADDR:
-        return op_sub_long_2addr();
+        return op_sub_long_2addr(mir);
     case OP_MUL_LONG_2ADDR:
-        return op_mul_long_2addr();
+        return op_mul_long_2addr(mir);
     case OP_DIV_LONG_2ADDR:
-        return op_div_long_2addr();
+        return op_div_long_2addr(mir);
     case OP_REM_LONG_2ADDR:
-        return op_rem_long_2addr();
+        return op_rem_long_2addr(mir);
     case OP_AND_LONG_2ADDR:
-        return op_and_long_2addr();
+        return op_and_long_2addr(mir);
     case OP_OR_LONG_2ADDR:
-        return op_or_long_2addr();
+        return op_or_long_2addr(mir);
     case OP_XOR_LONG_2ADDR:
-        return op_xor_long_2addr();
+        return op_xor_long_2addr(mir);
     case OP_SHL_LONG_2ADDR:
-        return op_shl_long_2addr();
+        return op_shl_long_2addr(mir);
     case OP_SHR_LONG_2ADDR:
-        return op_shr_long_2addr();
+        return op_shr_long_2addr(mir);
     case OP_USHR_LONG_2ADDR:
-        return op_ushr_long_2addr();
+        return op_ushr_long_2addr(mir);
     case OP_ADD_FLOAT_2ADDR:
-        return op_add_float_2addr();
+        return op_add_float_2addr(mir);
     case OP_SUB_FLOAT_2ADDR:
-        return op_sub_float_2addr();
+        return op_sub_float_2addr(mir);
     case OP_MUL_FLOAT_2ADDR:
-        return op_mul_float_2addr();
+        return op_mul_float_2addr(mir);
     case OP_DIV_FLOAT_2ADDR:
-        return op_div_float_2addr();
+        return op_div_float_2addr(mir);
     case OP_REM_FLOAT_2ADDR:
-        return op_rem_float_2addr();
+        return op_rem_float_2addr(mir);
     case OP_ADD_DOUBLE_2ADDR:
-        return op_add_double_2addr();
+        return op_add_double_2addr(mir);
     case OP_SUB_DOUBLE_2ADDR:
-        return op_sub_double_2addr();
+        return op_sub_double_2addr(mir);
     case OP_MUL_DOUBLE_2ADDR:
-        return op_mul_double_2addr();
+        return op_mul_double_2addr(mir);
     case OP_DIV_DOUBLE_2ADDR:
-        return op_div_double_2addr();
+        return op_div_double_2addr(mir);
     case OP_REM_DOUBLE_2ADDR:
-        return op_rem_double_2addr();
+        return op_rem_double_2addr(mir);
     case OP_ADD_INT_LIT16:
-        return op_add_int_lit16();
+        return op_add_int_lit16(mir);
     case OP_RSUB_INT:
-        return op_rsub_int();
+        return op_rsub_int(mir);
     case OP_MUL_INT_LIT16:
-        return op_mul_int_lit16();
+        return op_mul_int_lit16(mir);
     case OP_DIV_INT_LIT16:
-        return op_div_int_lit16();
+        return op_div_int_lit16(mir);
     case OP_REM_INT_LIT16:
-        return op_rem_int_lit16();
+        return op_rem_int_lit16(mir);
     case OP_AND_INT_LIT16:
-        return op_and_int_lit16();
+        return op_and_int_lit16(mir);
     case OP_OR_INT_LIT16:
-        return op_or_int_lit16();
+        return op_or_int_lit16(mir);
     case OP_XOR_INT_LIT16:
-        return op_xor_int_lit16();
+        return op_xor_int_lit16(mir);
     case OP_ADD_INT_LIT8:
-        return op_add_int_lit8();
+        return op_add_int_lit8(mir);
     case OP_RSUB_INT_LIT8:
-        return op_rsub_int_lit8();
+        return op_rsub_int_lit8(mir);
     case OP_MUL_INT_LIT8:
-        return op_mul_int_lit8();
+        return op_mul_int_lit8(mir);
     case OP_DIV_INT_LIT8:
-        return op_div_int_lit8();
+        return op_div_int_lit8(mir);
     case OP_REM_INT_LIT8:
-        return op_rem_int_lit8();
+        return op_rem_int_lit8(mir);
     case OP_AND_INT_LIT8:
-        return op_and_int_lit8();
+        return op_and_int_lit8(mir);
     case OP_OR_INT_LIT8:
-        return op_or_int_lit8();
+        return op_or_int_lit8(mir);
     case OP_XOR_INT_LIT8:
-        return op_xor_int_lit8();
+        return op_xor_int_lit8(mir);
     case OP_SHL_INT_LIT8:
-        return op_shl_int_lit8();
+        return op_shl_int_lit8(mir);
     case OP_SHR_INT_LIT8:
-        return op_shr_int_lit8();
+        return op_shr_int_lit8(mir);
     case OP_USHR_INT_LIT8:
-        return op_ushr_int_lit8();
+        return op_ushr_int_lit8(mir);
     case OP_EXECUTE_INLINE:
-        return op_execute_inline(false);
+        return op_execute_inline(mir, false /*isRange*/);
     case OP_EXECUTE_INLINE_RANGE:
-        return op_execute_inline(true);
+        return op_execute_inline(mir, true /*isRange*/);
     case OP_BREAKPOINT:
         ALOGE("found bytecode OP_BREAKPOINT");
-        dvmAbort();
-    case OP_INVOKE_OBJECT_INIT_RANGE:
-        return op_invoke_object_init_range();
+        exit(-1);
+//  case OP_INVOKE_OBJECT_INIT_RANGE:
+//      return op_invoke_object_init_range();
     case OP_IGET_QUICK:
-        return op_iget_quick();
+        return op_iget_quick(mir);
     case OP_IGET_WIDE_QUICK:
-        return op_iget_wide_quick();
+        return op_iget_wide_quick(mir);
     case OP_IGET_OBJECT_QUICK:
-        return op_iget_object_quick();
+        return op_iget_object_quick(mir);
     case OP_IPUT_QUICK:
-        return op_iput_quick();
+        return op_iput_quick(mir);
     case OP_IPUT_WIDE_QUICK:
-        return op_iput_wide_quick();
+        return op_iput_wide_quick(mir);
     case OP_IPUT_OBJECT_QUICK:
-        return op_iput_object_quick();
+        return op_iput_object_quick(mir);
     case OP_INVOKE_VIRTUAL_QUICK:
-        return op_invoke_virtual_quick();
+        return op_invoke_virtual_quick(mir);
     case OP_INVOKE_VIRTUAL_QUICK_RANGE:
-        return op_invoke_virtual_quick_range();
+        return op_invoke_virtual_quick_range(mir);
     case OP_INVOKE_SUPER_QUICK:
-        return op_invoke_super_quick();
+        return op_invoke_super_quick(mir);
     case OP_INVOKE_SUPER_QUICK_RANGE:
-        return op_invoke_super_quick_range();
+        return op_invoke_super_quick_range(mir);
+    default:
+        ALOGE("ERROR: JIT does not support bytecode 0x%hx\n",
+                mir->dalvikInsn.opcode);
+        assert(false && "All opcodes should be supported.");
+        break;
     }
-
-    ALOGE("No JIT support for bytecode %x at offsetPC %x",
-          INST_INST(inst), offsetPC);
     return -1;
 }
-int op_nop() {
-    rPC++;
+
+int op_nop(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NOP);
     return 0;
 }
 
diff --git a/vm/compiler/codegen/x86/Lower.h b/vm/compiler/codegen/x86/Lower.h
index 33ce761..088d02a 100644
--- a/vm/compiler/codegen/x86/Lower.h
+++ b/vm/compiler/codegen/x86/Lower.h
@@ -652,7 +652,6 @@ extern PhysicalReg scratchRegs[4];
 extern LowOp* ops[BUFFER_SIZE];
 extern bool isScratchPhysical;
 extern u2* rPC;
-extern u2 inst;
 extern int offsetPC;
 extern int offsetNCG;
 extern int mapFromBCtoNCG[BYTECODE_SIZE_PER_METHOD];
@@ -1020,7 +1019,6 @@ void freeDataWorklist();
 void freeLabelWorklist();
 void freeChainingWorklist();
 
-int common_invokeArgsDone(ArgsDoneType form, bool isJitFull);
 int common_backwardBranch();
 int common_exceptionThrown();
 int common_errNullObject();
@@ -1041,236 +1039,236 @@ void sendLabelInfoToVTune(int startStreamPtr, int endStreamPtr, const char* labe
 #endif
 
 //lower a bytecode
-int lowerByteCode(const Method* method);
-
-int op_nop();
-int op_move();
-int op_move_from16();
-int op_move_16();
-int op_move_wide();
-int op_move_wide_from16();
-int op_move_wide_16();
-int op_move_result();
-int op_move_result_wide();
-int op_move_exception();
-
-int op_return_void();
-int op_return();
-int op_return_wide();
-int op_const_4();
-int op_const_16();
-int op_const();
-int op_const_high16();
-int op_const_wide_16();
-int op_const_wide_32();
-int op_const_wide();
-int op_const_wide_high16();
-int op_const_string();
-int op_const_string_jumbo();
-int op_const_class();
-int op_monitor_enter();
-int op_monitor_exit();
-int op_check_cast();
-int op_instance_of();
-
-int op_array_length();
-int op_new_instance();
-int op_new_array();
-int op_filled_new_array();
-int op_filled_new_array_range();
-int op_fill_array_data();
-int op_throw();
-int op_throw_verification_error();
-int op_goto();
-int op_goto_16();
-int op_goto_32();
-int op_packed_switch();
-int op_sparse_switch();
-int op_if_ge();
-int op_aget();
-int op_aget_wide();
-int op_aget_object();
-int op_aget_boolean();
-int op_aget_byte();
-int op_aget_char();
-int op_aget_short();
-int op_aput();
-int op_aput_wide();
-int op_aput_object();
-int op_aput_boolean();
-int op_aput_byte();
-int op_aput_char();
-int op_aput_short();
-int op_iget();
-int op_iget_wide(bool isVolatile);
-int op_iget_object();
-int op_iget_boolean();
-int op_iget_byte();
-int op_iget_char();
-int op_iget_short();
-int op_iput();
-int op_iput_wide(bool isVolatile);
-int op_iput_object();
-int op_iput_boolean();
-int op_iput_byte();
-int op_iput_char();
-int op_iput_short();
-int op_sget();
-int op_sget_wide(bool isVolatile);
-int op_sget_object();
-int op_sget_boolean();
-int op_sget_byte();
-int op_sget_char();
-int op_sget_short();
-int op_sput(bool isObj);
-int op_sput_wide(bool isVolatile);
-int op_sput_object();
-int op_sput_boolean();
-int op_sput_byte();
-int op_sput_char();
-int op_sput_short();
-int op_invoke_virtual();
-int op_invoke_super();
-int op_invoke_direct();
-int op_invoke_static();
-int op_invoke_interface();
-int op_invoke_virtual_range();
-int op_invoke_super_range();
-int op_invoke_direct_range();
-int op_invoke_static_range();
-int op_invoke_interface_range();
-int op_int_to_long();
-int op_add_long_2addr();
-int op_add_int_lit8();
-int op_cmpl_float();
-int op_cmpg_float();
-int op_cmpl_double();
-int op_cmpg_double();
-int op_cmp_long();
-int op_if_eq();
-int op_if_ne();
-int op_if_lt();
-int op_if_gt();
-int op_if_le();
-int op_if_eqz();
-int op_if_nez();
-int op_if_ltz();
-int op_if_gez();
-int op_if_gtz();
-int op_if_lez();
-int op_neg_int();
-int op_not_int();
-int op_neg_long();
-int op_not_long();
-int op_neg_float();
-int op_neg_double();
-int op_int_to_float();
-int op_int_to_double();
-int op_long_to_int();
-int op_long_to_float();
-int op_long_to_double();
-int op_float_to_int();
-int op_float_to_long();
-int op_float_to_double();
-int op_double_to_int();
-int op_double_to_long();
-int op_double_to_float();
-int op_int_to_byte();
-int op_int_to_char();
-int op_int_to_short();
-int op_add_int();
-int op_sub_int();
-int op_mul_int();
-int op_div_int();
-int op_rem_int();
-int op_and_int();
-int op_or_int();
-int op_xor_int();
-int op_shl_int();
-int op_shr_int();
-int op_ushr_int();
-int op_add_long();
-int op_sub_long();
-int op_mul_long();
-int op_div_long();
-int op_rem_long();
-int op_and_long();
-int op_or_long();
-int op_xor_long();
-int op_shl_long();
-int op_shr_long();
-int op_ushr_long();
-int op_add_float();
-int op_sub_float();
-int op_mul_float();
-int op_div_float();
-int op_rem_float();
-int op_add_double();
-int op_sub_double();
-int op_mul_double();
-int op_div_double();
-int op_rem_double();
-int op_add_int_2addr();
-int op_sub_int_2addr();
-int op_mul_int_2addr();
-int op_div_int_2addr();
-int op_rem_int_2addr();
-int op_and_int_2addr();
-int op_or_int_2addr();
-int op_xor_int_2addr();
-int op_shl_int_2addr();
-int op_shr_int_2addr();
-int op_ushr_int_2addr();
-int op_sub_long_2addr();
-int op_mul_long_2addr();
-int op_div_long_2addr();
-int op_rem_long_2addr();
-int op_and_long_2addr();
-int op_or_long_2addr();
-int op_xor_long_2addr();
-int op_shl_long_2addr();
-int op_shr_long_2addr();
-int op_ushr_long_2addr();
-int op_add_float_2addr();
-int op_sub_float_2addr();
-int op_mul_float_2addr();
-int op_div_float_2addr();
-int op_rem_float_2addr();
-int op_add_double_2addr();
-int op_sub_double_2addr();
-int op_mul_double_2addr();
-int op_div_double_2addr();
-int op_rem_double_2addr();
-int op_add_int_lit16();
-int op_rsub_int();
-int op_mul_int_lit16();
-int op_div_int_lit16();
-int op_rem_int_lit16();
-int op_and_int_lit16();
-int op_or_int_lit16();
-int op_xor_int_lit16();
-int op_rsub_int_lit8();
-int op_mul_int_lit8();
-int op_div_int_lit8();
-int op_rem_int_lit8();
-int op_and_int_lit8();
-int op_or_int_lit8();
-int op_xor_int_lit8();
-int op_shl_int_lit8();
-int op_shr_int_lit8();
-int op_ushr_int_lit8();
-int op_execute_inline(bool isRange);
-int op_invoke_object_init_range();
-int op_iget_quick();
-int op_iget_wide_quick();
-int op_iget_object_quick();
-int op_iput_quick();
-int op_iput_wide_quick();
-int op_iput_object_quick();
-int op_invoke_virtual_quick();
-int op_invoke_virtual_quick_range();
-int op_invoke_super_quick();
-int op_invoke_super_quick_range();
+int lowerByteCode(const Method* method, const MIR * mir, const u2 * dalvikPC);
+
+int op_nop(const MIR * mir);
+int op_move(const MIR * mir);
+int op_move_from16(const MIR * mir);
+int op_move_16(const MIR * mir);
+int op_move_wide(const MIR * mir);
+int op_move_wide_from16(const MIR * mir);
+int op_move_wide_16(const MIR * mir);
+int op_move_result(const MIR * mir);
+int op_move_result_wide(const MIR * mir);
+int op_move_exception(const MIR * mir);
+
+int op_return_void(const MIR * mir);
+int op_return(const MIR * mir);
+int op_return_wide(const MIR * mir);
+int op_const_4(const MIR * mir);
+int op_const_16(const MIR * mir);
+int op_const(const MIR * mir);
+int op_const_high16(const MIR * mir);
+int op_const_wide_16(const MIR * mir);
+int op_const_wide_32(const MIR * mir);
+int op_const_wide(const MIR * mir);
+int op_const_wide_high16(const MIR * mir);
+int op_const_string(const MIR * mir);
+int op_const_string_jumbo(const MIR * mir);
+int op_const_class(const MIR * mir);
+int op_monitor_enter(const MIR * mir);
+int op_monitor_exit(const MIR * mir);
+int op_check_cast(const MIR * mir);
+int op_instance_of(const MIR * mir);
+
+int op_array_length(const MIR * mir);
+int op_new_instance(const MIR * mir);
+int op_new_array(const MIR * mir);
+int op_filled_new_array(const MIR * mir);
+int op_filled_new_array_range(const MIR * mir);
+int op_fill_array_data(const MIR * mir, const u2 * dalvikPC);
+int op_throw(const MIR * mir);
+int op_throw_verification_error(const MIR * mir);
+int op_goto(const MIR * mir);
+int op_goto_16(const MIR * mir);
+int op_goto_32(const MIR * mir);
+int op_packed_switch(const MIR * mir, const u2 * dalvikPC);
+int op_sparse_switch(const MIR * mir, const u2 * dalvikPC);
+int op_if_ge(const MIR * mir);
+int op_aget(const MIR * mir);
+int op_aget_wide(const MIR * mir);
+int op_aget_object(const MIR * mir);
+int op_aget_boolean(const MIR * mir);
+int op_aget_byte(const MIR * mir);
+int op_aget_char(const MIR * mir);
+int op_aget_short(const MIR * mir);
+int op_aput(const MIR * mir);
+int op_aput_wide(const MIR * mir);
+int op_aput_object(const MIR * mir);
+int op_aput_boolean(const MIR * mir);
+int op_aput_byte(const MIR * mir);
+int op_aput_char(const MIR * mir);
+int op_aput_short(const MIR * mir);
+int op_iget(const MIR * mir);
+int op_iget_wide(const MIR * mir, bool isVolatile);
+int op_iget_object(const MIR * mir);
+int op_iget_boolean(const MIR * mir);
+int op_iget_byte(const MIR * mir);
+int op_iget_char(const MIR * mir);
+int op_iget_short(const MIR * mir);
+int op_iput(const MIR * mir);
+int op_iput_wide(const MIR * mir, bool isVolatile);
+int op_iput_object(const MIR * mir);
+int op_iput_boolean(const MIR * mir);
+int op_iput_byte(const MIR * mir);
+int op_iput_char(const MIR * mir);
+int op_iput_short(const MIR * mir);
+int op_sget(const MIR * mir);
+int op_sget_wide(const MIR * mir, bool isVolatile);
+int op_sget_object(const MIR * mir);
+int op_sget_boolean(const MIR * mir);
+int op_sget_byte(const MIR * mir);
+int op_sget_char(const MIR * mir);
+int op_sget_short(const MIR * mir);
+int op_sput(const MIR * mir, bool isObj);
+int op_sput_wide(const MIR * mir, bool isVolatile);
+int op_sput_object(const MIR * mir);
+int op_sput_boolean(const MIR * mir);
+int op_sput_byte(const MIR * mir);
+int op_sput_char(const MIR * mir);
+int op_sput_short(const MIR * mir);
+int op_invoke_virtual(const MIR * mir);
+int op_invoke_super(const MIR * mir);
+int op_invoke_direct(const MIR * mir);
+int op_invoke_static(const MIR * mir);
+int op_invoke_interface(const MIR * mir);
+int op_invoke_virtual_range(const MIR * mir);
+int op_invoke_super_range(const MIR * mir);
+int op_invoke_direct_range(const MIR * mir);
+int op_invoke_static_range(const MIR * mir);
+int op_invoke_interface_range(const MIR * mir);
+int op_int_to_long(const MIR * mir);
+int op_add_long_2addr(const MIR * mir);
+int op_add_int_lit8(const MIR * mir);
+int op_cmpl_float(const MIR * mir);
+int op_cmpg_float(const MIR * mir);
+int op_cmpl_double(const MIR * mir);
+int op_cmpg_double(const MIR * mir);
+int op_cmp_long(const MIR * mir);
+int op_if_eq(const MIR * mir);
+int op_if_ne(const MIR * mir);
+int op_if_lt(const MIR * mir);
+int op_if_gt(const MIR * mir);
+int op_if_le(const MIR * mir);
+int op_if_eqz(const MIR * mir);
+int op_if_nez(const MIR * mir);
+int op_if_ltz(const MIR * mir);
+int op_if_gez(const MIR * mir);
+int op_if_gtz(const MIR * mir);
+int op_if_lez(const MIR * mir);
+int op_neg_int(const MIR * mir);
+int op_not_int(const MIR * mir);
+int op_neg_long(const MIR * mir);
+int op_not_long(const MIR * mir);
+int op_neg_float(const MIR * mir);
+int op_neg_double(const MIR * mir);
+int op_int_to_float(const MIR * mir);
+int op_int_to_double(const MIR * mir);
+int op_long_to_int(const MIR * mir);
+int op_long_to_float(const MIR * mir);
+int op_long_to_double(const MIR * mir);
+int op_float_to_int(const MIR * mir);
+int op_float_to_long(const MIR * mir);
+int op_float_to_double(const MIR * mir);
+int op_double_to_int(const MIR * mir);
+int op_double_to_long(const MIR * mir);
+int op_double_to_float(const MIR * mir);
+int op_int_to_byte(const MIR * mir);
+int op_int_to_char(const MIR * mir);
+int op_int_to_short(const MIR * mir);
+int op_add_int(const MIR * mir);
+int op_sub_int(const MIR * mir);
+int op_mul_int(const MIR * mir);
+int op_div_int(const MIR * mir);
+int op_rem_int(const MIR * mir);
+int op_and_int(const MIR * mir);
+int op_or_int(const MIR * mir);
+int op_xor_int(const MIR * mir);
+int op_shl_int(const MIR * mir);
+int op_shr_int(const MIR * mir);
+int op_ushr_int(const MIR * mir);
+int op_add_long(const MIR * mir);
+int op_sub_long(const MIR * mir);
+int op_mul_long(const MIR * mir);
+int op_div_long(const MIR * mir);
+int op_rem_long(const MIR * mir);
+int op_and_long(const MIR * mir);
+int op_or_long(const MIR * mir);
+int op_xor_long(const MIR * mir);
+int op_shl_long(const MIR * mir);
+int op_shr_long(const MIR * mir);
+int op_ushr_long(const MIR * mir);
+int op_add_float(const MIR * mir);
+int op_sub_float(const MIR * mir);
+int op_mul_float(const MIR * mir);
+int op_div_float(const MIR * mir);
+int op_rem_float(const MIR * mir);
+int op_add_double(const MIR * mir);
+int op_sub_double(const MIR * mir);
+int op_mul_double(const MIR * mir);
+int op_div_double(const MIR * mir);
+int op_rem_double(const MIR * mir);
+int op_add_int_2addr(const MIR * mir);
+int op_sub_int_2addr(const MIR * mir);
+int op_mul_int_2addr(const MIR * mir);
+int op_div_int_2addr(const MIR * mir);
+int op_rem_int_2addr(const MIR * mir);
+int op_and_int_2addr(const MIR * mir);
+int op_or_int_2addr(const MIR * mir);
+int op_xor_int_2addr(const MIR * mir);
+int op_shl_int_2addr(const MIR * mir);
+int op_shr_int_2addr(const MIR * mir);
+int op_ushr_int_2addr(const MIR * mir);
+int op_sub_long_2addr(const MIR * mir);
+int op_mul_long_2addr(const MIR * mir);
+int op_div_long_2addr(const MIR * mir);
+int op_rem_long_2addr(const MIR * mir);
+int op_and_long_2addr(const MIR * mir);
+int op_or_long_2addr(const MIR * mir);
+int op_xor_long_2addr(const MIR * mir);
+int op_shl_long_2addr(const MIR * mir);
+int op_shr_long_2addr(const MIR * mir);
+int op_ushr_long_2addr(const MIR * mir);
+int op_add_float_2addr(const MIR * mir);
+int op_sub_float_2addr(const MIR * mir);
+int op_mul_float_2addr(const MIR * mir);
+int op_div_float_2addr(const MIR * mir);
+int op_rem_float_2addr(const MIR * mir);
+int op_add_double_2addr(const MIR * mir);
+int op_sub_double_2addr(const MIR * mir);
+int op_mul_double_2addr(const MIR * mir);
+int op_div_double_2addr(const MIR * mir);
+int op_rem_double_2addr(const MIR * mir);
+int op_add_int_lit16(const MIR * mir);
+int op_rsub_int(const MIR * mir);
+int op_mul_int_lit16(const MIR * mir);
+int op_div_int_lit16(const MIR * mir);
+int op_rem_int_lit16(const MIR * mir);
+int op_and_int_lit16(const MIR * mir);
+int op_or_int_lit16(const MIR * mir);
+int op_xor_int_lit16(const MIR * mir);
+int op_rsub_int_lit8(const MIR * mir);
+int op_mul_int_lit8(const MIR * mir);
+int op_div_int_lit8(const MIR * mir);
+int op_rem_int_lit8(const MIR * mir);
+int op_and_int_lit8(const MIR * mir);
+int op_or_int_lit8(const MIR * mir);
+int op_xor_int_lit8(const MIR * mir);
+int op_shl_int_lit8(const MIR * mir);
+int op_shr_int_lit8(const MIR * mir);
+int op_ushr_int_lit8(const MIR * mir);
+int op_execute_inline(const MIR * mir, bool isRange);
+int op_invoke_direct_empty(const MIR * mir);
+int op_iget_quick(const MIR * mir);
+int op_iget_wide_quick(const MIR * mir);
+int op_iget_object_quick(const MIR * mir);
+int op_iput_quick(const MIR * mir);
+int op_iput_wide_quick(const MIR * mir);
+int op_iput_object_quick(const MIR * mir);
+int op_invoke_virtual_quick(const MIR * mir);
+int op_invoke_virtual_quick_range(const MIR * mir);
+int op_invoke_super_quick(const MIR * mir);
+int op_invoke_super_quick_range(const MIR * mir);
 
 ///////////////////////////////////////////////
 void set_reg_opnd(LowOpndReg* op_reg, int reg, bool isPhysical, LowOpndRegType type);
@@ -1336,10 +1334,11 @@ LowOpLabel* dump_label(Mnemonic m, OpndSize size, int imm,
 
 unsigned getJmpCallInstSize(OpndSize size, JmpCall_type type);
 bool lowerByteCodeJit(const Method* method, const u2* codePtr, MIR* mir);
+#if defined(WITH_JIT)
+bool lowerByteCodeJit(const Method* method, const MIR * mir, const u2 * dalvikPC);
 void startOfBasicBlock(struct BasicBlock* bb);
 extern LowOpBlockLabel* traceLabelList;
 extern struct BasicBlock* traceCurrentBB;
-extern struct MIR* traceCurrentMIR;
 extern JitMode traceMode;
 extern bool branchInLoop;
 void startOfTrace(const Method* method, LowOpBlockLabel* labelList, int, CompilationUnit*);
@@ -1353,6 +1352,7 @@ void handleExtendedMIR(CompilationUnit *cUnit, MIR *mir);
 int insertChainingWorklist(int bbId, char * codeStart);
 void startOfTraceO1(const Method* method, LowOpBlockLabel* labelList, int exceptionBlockId, CompilationUnit *cUnit);
 void endOfTraceO1();
+#endif
 int isPowerOfTwo(int imm);
 void move_chain_to_mem(OpndSize size, int imm,
                         int disp, int base_reg, bool isBasePhysical);
diff --git a/vm/compiler/codegen/x86/LowerAlu.cpp b/vm/compiler/codegen/x86/LowerAlu.cpp
index d83f1e7..43f3249 100644
--- a/vm/compiler/codegen/x86/LowerAlu.cpp
+++ b/vm/compiler/codegen/x86/LowerAlu.cpp
@@ -26,175 +26,227 @@
 
 /////////////////////////////////////////////
 #define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode NEG_INT
 
-//!
-int op_neg_int() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode neg-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_neg_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEG_INT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false);
     alu_unary_reg(OpndSize_32, neg_opc, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 1;
     return 0;
 }
-//! lower bytecode NOT_INT
 
-//!
-int op_not_int() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode not-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_not_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NOT_INT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false);
     alu_unary_reg(OpndSize_32, not_opc, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 1;
     return 0;
 }
 #undef P_GPR_1
-//! lower bytecode NEG_LONG
 
-//! This implementation uses XMM registers
-int op_neg_long() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode neg-long
+ * @details Implementation uses XMM registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_neg_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEG_LONG);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_64, 1, false);
     alu_binary_reg_reg(OpndSize_64, xor_opc, 2, false, 2, false);
     alu_binary_reg_reg(OpndSize_64, sub_opc, 1, false, 2, false);
     set_virtual_reg(vA, OpndSize_64, 2, false);
-    rPC += 1;
     return 0;
 }
-//! lower bytecode NOT_LONG
 
-//! This implementation uses XMM registers
-int op_not_long() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode not-long
+ * @details Implementation uses XMM registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_not_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NOT_LONG);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_64, 1, false);
     load_global_data_API("64bits", OpndSize_64, 2, false);
     alu_binary_reg_reg(OpndSize_64, andn_opc, 2, false, 1, false);
     set_virtual_reg(vA, OpndSize_64, 1, false);
-    rPC += 1;
     return 0;
 }
-#define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode NEG_FLOAT
 
-//! This implementation uses GPR
-int op_neg_float() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+#define P_GPR_1 PhysicalReg_EBX
+/**
+ * @brief Generate native code for bytecode neg-float
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_neg_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEG_FLOAT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false);
     alu_binary_imm_reg(OpndSize_32, add_opc, 0x80000000, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 1;
     return 0;
 }
 #undef P_GPR_1
 
-//! lower bytecode NEG_DOUBLE
-
-//! This implementation uses XMM registers
-int op_neg_double() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode neg-double
+ * @details Implementation uses XMM registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_neg_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEG_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_64, 1, false);
     load_global_data_API("doubNeg", OpndSize_64, 2, false);
     alu_binary_reg_reg(OpndSize_64, xor_opc, 2, false, 1, false);
     set_virtual_reg(vA, OpndSize_64, 1, false);
-    rPC += 1;
     return 0;
 }
 
-//! lower bytecode INT_TO_LONG
-
-//! It uses native instruction cdq
-int op_int_to_long() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode int-to-long
+ * @details Implementation uses native instruction cdq
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_LONG);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, PhysicalReg_EAX, true);
     convert_integer(OpndSize_32, OpndSize_64);
     set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
     set_virtual_reg(vA+1, OpndSize_32, PhysicalReg_EDX, true);
-    rPC += 1;
     return 0;
 }
-//! lower bytecode INT_TO_FLOAT
 
-//! This implementation uses FP stack
-int op_int_to_float() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode int-to-float
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_FLOAT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     load_int_fp_stack_VR(OpndSize_32, vB); //fildl
     store_fp_stack_VR(true, OpndSize_32, vA); //fstps
-    rPC += 1;
     return 0;
 }
-//! lower bytecode INT_TO_DOUBLE
 
-//! This implementation uses FP stack
-int op_int_to_double() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode int-to-double
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     load_int_fp_stack_VR(OpndSize_32, vB); //fildl
     store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
-    rPC += 1;
     return 0;
 }
-//! lower bytecode LONG_TO_FLOAT
 
-//! This implementation uses FP stack
-int op_long_to_float() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode long-to-float
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_long_to_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_LONG_TO_FLOAT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     load_int_fp_stack_VR(OpndSize_64, vB); //fildll
     store_fp_stack_VR(true, OpndSize_32, vA); //fstps
-    rPC += 1;
     return 0;
 }
-//! lower bytecode LONG_TO_DOUBLE
 
-//! This implementation uses FP stack
-int op_long_to_double() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode long-to-double
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_long_to_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_LONG_TO_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     load_int_fp_stack_VR(OpndSize_64, vB); //fildll
     store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
-    rPC += 1;
     return 0;
 }
-//! lower bytecode FLOAT_TO_DOUBLE
 
-//! This implementation uses FP stack
-int op_float_to_double() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode float-to-double
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_float_to_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FLOAT_TO_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     load_fp_stack_VR(OpndSize_32, vB); //flds
     store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
-    rPC += 1;
     return 0;
 }
-//! lower bytecode DOUBLE_TO_FLOAT
 
-//! This implementation uses FP stack
-int op_double_to_float() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode double-to-float
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_double_to_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DOUBLE_TO_FLOAT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     load_fp_stack_VR(OpndSize_64, vB); //fldl
     store_fp_stack_VR(true, OpndSize_32, vA); //fstps
-    rPC += 1;
     return 0;
 }
-#define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode LONG_TO_INT
 
-//! This implementation uses GPR
-int op_long_to_int() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+#define P_GPR_1 PhysicalReg_EBX
+/**
+ * @brief Generate native code for bytecode long-to-int
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_long_to_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_LONG_TO_INT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 1;
     return 0;
 }
 #undef P_GPR_1
@@ -259,24 +311,30 @@ int common_fp_to_int(bool isDouble, u2 vA, u2 vB) {
     insertLabel(".float_to_int_okay", true);
     return 0;
 }
-//! lower bytecode FLOAT_TO_INT by calling common_fp_to_int
 
-//!
-int op_float_to_int() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode float-to-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_float_to_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FLOAT_TO_INT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     int retval = common_fp_to_int(false, vA, vB);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode DOUBLE_TO_INT by calling common_fp_to_int
 
-//!
-int op_double_to_int() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode double-to-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_double_to_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DOUBLE_TO_INT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     int retval = common_fp_to_int(true, vA, vB);
-    rPC += 1;
     return retval;
 }
 
@@ -362,64 +420,82 @@ int common_fp_to_long(bool isDouble, u2 vA, u2 vB) {
     insertLabel(".float_to_long_okay", true);
     return 0;
 }
-//! lower bytecode FLOAT_TO_LONG by calling common_fp_to_long
 
-//!
-int op_float_to_long() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode float-to-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_float_to_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FLOAT_TO_LONG);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     int retval = common_fp_to_long(false, vA, vB);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode DOUBLE_TO_LONG by calling common_fp_to_long
 
-//!
-int op_double_to_long() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode double-to-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_double_to_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DOUBLE_TO_LONG);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     int retval = common_fp_to_long(true, vA, vB);
-    rPC += 1;
     return retval;
 }
-#define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode INT_TO_BYTE
 
-//! It uses GPR
-int op_int_to_byte() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+#define P_GPR_1 PhysicalReg_EBX
+/**
+ * @brief Generate native code for bytecode int-to-byte
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_BYTE);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false);
     alu_binary_imm_reg(OpndSize_32, sal_opc, 24, 1, false);
     alu_binary_imm_reg(OpndSize_32, sar_opc, 24, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 1;
     return 0;
 }
-//! lower bytecode INT_TO_CHAR
 
-//! It uses GPR
-int op_int_to_char() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode int-to-char
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_CHAR);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false);
     alu_binary_imm_reg(OpndSize_32, sal_opc, 16, 1, false);
     alu_binary_imm_reg(OpndSize_32, shr_opc, 16, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 1;
     return 0;
 }
-//! lower bytecode INT_TO_SHORT
 
-//! It uses GPR
-int op_int_to_short() {
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode int-to-short
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_SHORT);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false);
     alu_binary_imm_reg(OpndSize_32, sal_opc, 16, 1, false);
     alu_binary_imm_reg(OpndSize_32, sar_opc, 16, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 1;
     return 0;
 }
 //! common code to handle integer ALU ops
@@ -450,222 +526,277 @@ int common_shift_int(ALU_Opcode opc, u2 vA, u2 v1, u2 v2) {
     return 0;
 }
 #undef p_GPR_1
-//! lower bytecode ADD_INT by calling common_alu_int
 
-//!
-int op_add_int() {
+/**
+ * @brief Generate native code for bytecode add-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_alu_int(add_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode SUB_INT by calling common_alu_int
 
-//!
-int op_sub_int() {
+/**
+ * @brief Generate native code for bytecode sub-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_alu_int(sub_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode MUL_INT by calling common_alu_int
 
-//!
-int op_mul_int() {
+/**
+ * @brief Generate native code for bytecode mul-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_alu_int(imul_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode AND_INT by calling common_alu_int
 
-//!
-int op_and_int() {
+/**
+ * @brief Generate native code for bytecode and-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_alu_int(and_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode OR_INT by calling common_alu_int
 
-//!
-int op_or_int() {
+/**
+ * @brief Generate native code for bytecode or-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_alu_int(or_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode XOR_INT by calling common_alu_int
 
-//!
-int op_xor_int() {
+/**
+ * @brief Generate native code for bytecode xor-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_alu_int(xor_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode SHL_INT by calling common_shift_int
 
-//!
-int op_shl_int() {
+/**
+ * @brief Generate native code for bytecode shl-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_shift_int(shl_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode SHR_INT by calling common_shift_int
 
-//!
-int op_shr_int() {
+/**
+ * @brief Generate native code for bytecode shr-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_shift_int(sar_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode USHR_INT by calling common_shift_int
 
-//!
-int op_ushr_int() {
+/**
+ * @brief Generate native code for bytecode ushr-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_shift_int(shr_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode ADD_INT_2ADDR by calling common_alu_int
 
-//!
-int op_add_int_2addr() {
+/**
+ * @brief Generate native code for bytecode add-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_INT_2ADDR);
     u2 vA, v1, v2;
-    vA = INST_A(inst);
+    vA = mir->dalvikInsn.vA;
     v1 = vA;
-    v2 = INST_B(inst);
+    v2 = mir->dalvikInsn.vB;
     int retval = common_alu_int(add_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode SUB_INT_2ADDR by calling common_alu_int
 
-//!
-int op_sub_int_2addr() {
+/**
+ * @brief Generate native code for bytecode sub-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_INT_2ADDR);
     u2 vA, v1, v2;
-    vA = INST_A(inst);
+    vA = mir->dalvikInsn.vA;
     v1 = vA;
-    v2 = INST_B(inst);
+    v2 = mir->dalvikInsn.vB;
     int retval = common_alu_int(sub_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode MUL_INT_2ADDR by calling common_alu_int
 
-//!
-int op_mul_int_2addr() {
+/**
+ * @brief Generate native code for bytecode mul-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_INT_2ADDR);
     u2 vA, v1, v2;
-    vA = INST_A(inst);
+    vA = mir->dalvikInsn.vA;
     v1 = vA;
-    v2 = INST_B(inst);
+    v2 = mir->dalvikInsn.vB;
     int retval = common_alu_int(imul_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode AND_INT_2ADDR by calling common_alu_int
 
-//!
-int op_and_int_2addr() {
+/**
+ * @brief Generate native code for bytecode and-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_INT_2ADDR);
     u2 vA, v1, v2;
-    vA = INST_A(inst);
+    vA = mir->dalvikInsn.vA;
     v1 = vA;
-    v2 = INST_B(inst);
+    v2 = mir->dalvikInsn.vB;
     int retval = common_alu_int(and_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode OR_INT_2ADDR by calling common_alu_int
 
-//!
-int op_or_int_2addr() {
+/**
+ * @brief Generate native code for bytecode or-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_INT_2ADDR);
     u2 vA, v1, v2;
-    vA = INST_A(inst);
+    vA = mir->dalvikInsn.vA;
     v1 = vA;
-    v2 = INST_B(inst);
+    v2 = mir->dalvikInsn.vB;
     int retval = common_alu_int(or_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode XOR_INT_2ADDR by calling common_alu_int
 
-//!
-int op_xor_int_2addr() {
+/**
+ * @brief Generate native code for bytecode xor-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_INT_2ADDR);
     u2 vA, v1, v2;
-    vA = INST_A(inst);
+    vA = mir->dalvikInsn.vA;
     v1 = vA;
-    v2 = INST_B(inst);
+    v2 = mir->dalvikInsn.vB;
     int retval = common_alu_int(xor_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode SHL_INT_2ADDR by calling common_shift_int
 
-//!
-int op_shl_int_2addr() {
+/**
+ * @brief Generate native code for bytecode shl-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_INT_2ADDR);
     u2 vA, v1, v2;
-    vA = INST_A(inst);
+    vA = mir->dalvikInsn.vA;
     v1 = vA;
-    v2 = INST_B(inst);
+    v2 = mir->dalvikInsn.vB;
     int retval = common_shift_int(shl_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode SHR_INT_2ADDR by calling common_shift_int
 
-//!
-int op_shr_int_2addr() {
+/**
+ * @brief Generate native code for bytecode shr-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_INT_2ADDR);
     u2 vA, v1, v2;
-    vA = INST_A(inst);
+    vA = mir->dalvikInsn.vA;
     v1 = vA;
-    v2 = INST_B(inst);
+    v2 = mir->dalvikInsn.vB;
     int retval = common_shift_int(sar_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode USHR_INT_2ADDR by calling common_shift_int
 
-//!
-int op_ushr_int_2addr() {
+/**
+ * @brief Generate native code for bytecode ushr-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_INT_2ADDR);
     u2 vA, v1, v2;
-    vA = INST_A(inst);
+    vA = mir->dalvikInsn.vA;
     v1 = vA;
-    v2 = INST_B(inst);
+    v2 = mir->dalvikInsn.vB;
     int retval = common_shift_int(shr_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
+
 #define P_GPR_1 PhysicalReg_EBX
 //!common code to handle integer DIV & REM, it used GPR
 
@@ -713,50 +844,62 @@ int common_div_rem_int(bool isRem, u2 vA, u2 v1, u2 v2) {
     return 0;
 }
 #undef P_GPR_1
-//! lower bytecode DIV_INT by calling common_div_rem_int
 
-//!
-int op_div_int() {
+/**
+ * @brief Generate native code for bytecode div-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_div_rem_int(false, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode REM_INT by calling common_div_rem_int
 
-//!
-int op_rem_int() {
+/**
+ * @brief Generate native code for bytecode rem-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_INT);
     u2 vA, v1, v2;
-    vA = INST_AA(inst);
-    v1 = *((u1*)rPC + 2);
-    v2 = *((u1*)rPC + 3);
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
     int retval = common_div_rem_int(true, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode DIV_INT_2ADDR by calling common_div_rem_int
 
-//!
-int op_div_int_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode div-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_INT_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_div_rem_int(false, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode REM_INT_2ADDR by calling common_div_rem_int
 
-//!
-int op_rem_int_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode rem-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_INT_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_div_rem_int(true, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 
@@ -775,15 +918,18 @@ int common_shift_int_lit(ALU_Opcode opc, u2 vA, u2 vB, s2 imm) {
     return common_alu_int_lit(opc, vA, vB, imm);
 }
 #undef p_GPR_1
-//! lower bytecode ADD_INT_LIT16 by calling common_alu_int_lit
 
-//!
-int op_add_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_alu_int_lit(add_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode add-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_INT_LIT16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(add_opc, vA, vB, literal);
     return retval;
 }
 
@@ -795,192 +941,199 @@ int alu_rsub_int(ALU_Opcode opc, u2 vA, s2 imm, u2 vB) {
     return 0;
 }
 
-
-//! lower bytecode RSUB_INT by calling common_alu_int_lit
-
-//!
-int op_rsub_int() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = alu_rsub_int(sub_opc, vA, tmp, vB);
-    rPC += 2;
-    return retval;
-}
-//! lower bytecode MUL_INT_LIT16 by calling common_alu_int_lit
-
-//!
-int op_mul_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_alu_int_lit(imul_opc, vA, vB, tmp);
-    rPC += 2;
-    return retval;
-}
-//! lower bytecode AND_INT_LIT16 by calling common_alu_int_lit
-
-//!
-int op_and_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_alu_int_lit(and_opc, vA, vB, tmp);
-    rPC += 2;
-    return retval;
-}
-//! lower bytecode OR_INT_LIT16 by calling common_alu_int_lit
-
-//!
-int op_or_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_alu_int_lit(or_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode rsub-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rsub_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RSUB_INT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = alu_rsub_int(sub_opc, vA, literal, vB);
     return retval;
 }
-//! lower bytecode XOR_INT_LIT16 by calling common_alu_int_lit
 
-//!
-int op_xor_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_alu_int_lit(xor_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode mul-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_INT_LIT16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(imul_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode SHL_INT_LIT16 by calling common_shift_int_lit
 
-//!
-int op_shl_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_shift_int_lit(shl_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode and-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_INT_LIT16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(and_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode SHR_INT_LIT16 by calling common_shift_int_lit
 
-//!
-int op_shr_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_shift_int_lit(sar_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode or-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_INT_LIT16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(or_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode USHR_INT_LIT16 by calling common_shift_int_lit
 
-//!
-int op_ushr_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_shift_int_lit(shr_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode xor-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_INT_LIT16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(xor_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode ADD_INT_LIT8 by calling common_alu_int_lit
 
-//!
-int op_add_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_alu_int_lit(add_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode add-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(add_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode RSUB_INT_LIT8 by calling common_alu_int_lit
 
-//!
-int op_rsub_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = alu_rsub_int(sub_opc, vA, tmp, vB);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode rsub-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rsub_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RSUB_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = alu_rsub_int(sub_opc, vA, literal, vB);
     return retval;
 }
-//! lower bytecode MUL_INT_LIT8 by calling common_alu_int_lit
 
-//!
-int op_mul_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_alu_int_lit(imul_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode mul-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(imul_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode AND_INT_LIT8 by calling common_alu_int_lit
 
-//!
-int op_and_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_alu_int_lit(and_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode and-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(and_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode OR_INT_LIT8 by calling common_alu_int_lit
 
-//!
-int op_or_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_alu_int_lit(or_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode or-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(or_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode XOR_INT_LIT8 by calling common_alu_int_lit
 
-//!
-int op_xor_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_alu_int_lit(xor_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode xor-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(xor_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode SHL_INT_LIT8 by calling common_shift_int_lit
 
-//!
-int op_shl_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_shift_int_lit(shl_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode shl-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_shift_int_lit(shl_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode SHR_INT_LIT8 by calling common_shift_int_lit
 
-//!
-int op_shr_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_shift_int_lit(sar_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode shr-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_shift_int_lit(sar_opc, vA, vB, literal);
     return retval;
 }
-//! lower bytecode USHR_INT_LIT8 by calling common_shift_int_lit
 
-//!
-int op_ushr_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_shift_int_lit(shr_opc, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode ushr-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_shift_int_lit(shr_opc, vA, vB, literal);
     return retval;
 }
 
@@ -1083,48 +1236,60 @@ int common_div_rem_int_lit(bool isRem, u2 vA, u2 vB, s2 imm) {
     return 0;
 }
 #undef P_GPR_1
-//! lower bytecode DIV_INT_LIT16 by calling common_div_rem_int_lit
 
-//!
-int op_div_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_div_rem_int_lit(false, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode div-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_INT_LIT16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int_lit(false, vA, vB, literal);
     return retval;
 }
-//! lower bytecode REM_INT_LIT16 by calling common_div_rem_int_lit
 
-//!
-int op_rem_int_lit16() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s4 tmp = (s2)FETCH(1);
-    int retval = common_div_rem_int_lit(true, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode rem-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_INT_LIT16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int_lit(true, vA, vB, literal);
     return retval;
 }
-//! lower bytecode DIV_INT_LIT8 by calling common_div_rem_int_lit
 
-//!
-int op_div_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_div_rem_int_lit(false, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode div-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int_lit(false, vA, vB, literal);
     return retval;
 }
-//! lower bytecode REM_INT_LIT8 by calling common_div_rem_int_lit
 
-//!
-int op_rem_int_lit8() {
-    u2 vA = INST_AA(inst);
-    u2 vB = (u2)FETCH(1) & 0xff;
-    s2 tmp = (s2)FETCH(1) >> 8;
-    int retval = common_div_rem_int_lit(true, vA, vB, tmp);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode rem-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_INT_LIT8);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int_lit(true, vA, vB, literal);
     return retval;
 }
 //! common code to hanle long ALU ops
@@ -1149,114 +1314,143 @@ int common_add_long(u2 vA, u2 v1, u2 v2) {
     return 0;
 }
 
-//! lower bytecode ADD_LONG by calling common_add_long
-
-//!
-int op_add_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode add-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_add_long(vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode SUB_LONG by calling common_alu_long
 
-//!
-int op_sub_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode sub-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_long(sub_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode AND_LONG by calling common_alu_long
 
-//!
-int op_and_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode and-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_long(and_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode OR_LONG by calling common_alu_long
 
-//!
-int op_or_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode or-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_long(or_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode XOR_LONG by calling common_alu_long
 
-//!
-int op_xor_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode xor-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_long(xor_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode ADD_LONG_2ADDR by calling common_add_long
 
-//!
-int op_add_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode add-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_add_long(vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode SUB_LONG_2ADDR by calling common_alu_long
 
-//!
-int op_sub_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode sub-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_long(sub_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode AND_LONG_2ADDR by calling common_alu_long
 
-//!
-int op_and_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode and-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_long(and_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode OR_LONG_2ADDR by calling common_alu_long
 
-//!
-int op_or_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode or-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_long(or_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode XOR_LONG_2ADDR by calling common_alu_long
 
-//!
-int op_xor_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode xor-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_long(xor_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 
@@ -1286,26 +1480,32 @@ int common_mul_long(u2 vA, u2 v1, u2 v2) {
 #undef P_GPR_1
 #undef P_GPR_2
 #undef P_GPR_3
-//! lower bytecode MUL_LONG by calling common_mul_long
 
-//!
-int op_mul_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode mul-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_mul_long(vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode MUL_LONG_2ADDR by calling common_mul_long
 
-//!
-int op_mul_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode mul-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_mul_long(vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 
@@ -1341,48 +1541,60 @@ int common_div_rem_long(bool isRem, u2 vA, u2 v1, u2 v2) {
 }
 #undef P_GPR_1
 #undef P_GPR_2
-//! lower bytecode DIV_LONG by calling common_div_rem_long
 
-//!
-int op_div_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode div-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_div_rem_long(false, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode REM_LONG by calling common_div_rem_long
 
-//!
-int op_rem_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode rem-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_div_rem_long(true, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode DIV_LONG_2ADDR by calling common_div_rem_long
 
-//!
-int op_div_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode div-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_div_rem_long(false, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode REM_LONG_2ADDR by calling common_div_rem_long
 
-//!
-int op_rem_long_2addr() { //call __moddi3 instead of __divdi3
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode rem-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_div_rem_long(true, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 
@@ -1450,70 +1662,88 @@ int common_ushr_long(u2 vA, u2 v1, u2 v2) {
     set_VR_sd(vA, 1, false);
     return 0;
 }
-//! lower bytecode SHL_LONG by calling common_shl_long
 
-//!
-int op_shl_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode shl-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_shl_long(vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode SHL_LONG_2ADDR by calling common_shl_long
 
-//!
-int op_shl_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode shl-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_shl_long(vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode SHR_LONG by calling common_shr_long
 
-//!
-int op_shr_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode shr-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_shr_long(vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode SHR_LONG_2ADDR by calling common_shr_long
 
-//!
-int op_shr_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode shr-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_shr_long(vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode USHR_LONG by calling common_ushr_long
 
-//!
-int op_ushr_long() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode ushr-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_ushr_long(vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode USHR_LONG_2ADDR by calling common_ushr_long
 
-//!
-int op_ushr_long_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode ushr-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_LONG_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_ushr_long(vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 #define USE_MEM_OPERAND
@@ -1532,70 +1762,88 @@ int common_alu_float(ALU_Opcode opc, u2 vA, u2 v1, u2 v2) {//add, sub, mul
     set_VR_ss(vA, 1, false);
     return 0;
 }
-//! lower bytecode ADD_FLOAT by calling common_alu_float
 
-//!
-int op_add_float() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode add-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_FLOAT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_float(add_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode SUB_FLOAT by calling common_alu_float
 
-//!
-int op_sub_float() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode sub-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_FLOAT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_float(sub_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode MUL_FLOAT by calling common_alu_float
 
-//!
-int op_mul_float() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode mul-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_FLOAT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_float(mul_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode ADD_FLOAT_2ADDR by calling common_alu_float
 
-//!
-int op_add_float_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode add-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_FLOAT_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_float(add_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode SUB_FLOAT_2ADDR by calling common_alu_float
 
-//!
-int op_sub_float_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode sub-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_FLOAT_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_float(sub_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode MUL_FLOAT_2ADDR by calling common_alu_float
 
-//!
-int op_mul_float_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode mul-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_FLOAT_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_float(mul_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 //! common code to handle DIV of float
@@ -1607,26 +1855,32 @@ int common_div_float(u2 vA, u2 v1, u2 v2) {
     store_fp_stack_VR(true, OpndSize_32, vA); //fstps
     return 0;
 }
-//! lower bytecode DIV_FLOAT by calling common_div_float
 
-//!
-int op_div_float() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode div-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_FLOAT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_float(div_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode DIV_FLOAT_2ADDR by calling common_div_float
 
-//!
-int op_div_float_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode div-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_FLOAT_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_float(div_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 //! common code to handle DIV of double
@@ -1643,70 +1897,88 @@ int common_alu_double(ALU_Opcode opc, u2 vA, u2 v1, u2 v2) {//add, sub, mul
     set_VR_sd(vA, 1, false);
     return 0;
 }
-//! lower bytecode ADD_DOUBLE by calling common_alu_double
 
-//!
-int op_add_double() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode add-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_double(add_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode SUB_DOUBLE by calling common_alu_double
 
-//!
-int op_sub_double() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode sub-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_double(sub_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode MUL_DOUBLE by calling common_alu_double
 
-//!
-int op_mul_double() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode mul-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_double(mul_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode ADD_DOUBLE_2ADDR by calling common_alu_double
 
-//!
-int op_add_double_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode add-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_DOUBLE_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_double(add_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode SUB_DOUBLE_2ADDR by calling common_alu_double
 
-//!
-int op_sub_double_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode sub-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_DOUBLE_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_double(sub_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode MUL_DOUBLE_2ADDR by calling common_alu_double
 
-//!
-int op_mul_double_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode mul-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_DOUBLE_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_double(mul_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 //! common code to handle DIV of double
@@ -1718,26 +1990,32 @@ int common_div_double(u2 vA, u2 v1, u2 v2) {
     store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
     return 0;
 }
-//! lower bytecode DIV_DOUBLE by calling common_div_double
 
-//!
-int op_div_double() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode div-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_alu_double(div_opc, vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode DIV_DOUBLE_2ADDR by calling common_div_double
 
-//!
-int op_div_double_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode div-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_DOUBLE_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_alu_double(div_opc, vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 #define P_GPR_1 PhysicalReg_EBX
@@ -1759,26 +2037,32 @@ int common_rem_float(u2 vA, u2 v1, u2 v2) {
 }
 #undef P_GPR_1
 #undef P_GPR_2
-//! lower bytecode REM_FLOAT by calling common_rem_float
 
-//!
-int op_rem_float() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode rem-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_FLOAT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_rem_float(vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode REM_FLOAT_2ADDR by calling common_rem_float
 
-//!
-int op_rem_float_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode rem-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_FLOAT_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_rem_float(vA, v1, v2);
-    rPC += 1;
     return retval;
 }
 //! common code to handle REM of double
@@ -1796,35 +2080,45 @@ int common_rem_double(u2 vA, u2 v1, u2 v2) {
     store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
     return 0;
 }
-//! lower bytecode REM_DOUBLE by calling common_rem_double
 
-//!
-int op_rem_double() {
-    u2 vA = INST_AA(inst);
-    u2 v1 = *((u1*)rPC + 2);
-    u2 v2 = *((u1*)rPC + 3);
+/**
+ * @brief Generate native code for bytecode rem-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 v1 = mir->dalvikInsn.vB;
+    u2 v2 = mir->dalvikInsn.vC;
     int retval = common_rem_double(vA, v1, v2);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode REM_DOUBLE_2ADDR by calling common_rem_double
 
-//!
-int op_rem_double_2addr() {
-    u2 vA = INST_A(inst);
+/**
+ * @brief Generate native code for bytecode rem-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_DOUBLE_2ADDR);
+    u2 vA = mir->dalvikInsn.vA;
     u2 v1 = vA;
-    u2 v2 = INST_B(inst);
+    u2 v2 = mir->dalvikInsn.vB;
     int retval = common_rem_double(vA, v1, v2);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode CMPL_FLOAT
 
-//!
-int op_cmpl_float() {
-    u2 vA = INST_AA(inst);
-    u4 v1 = FETCH(1) & 0xff;
-    u4 v2 = FETCH(1) >> 8;
+/**
+ * @brief Generate native code for bytecode cmpl-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmpl_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMPL_FLOAT);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 v1 = mir->dalvikInsn.vB;
+    u4 v2 = mir->dalvikInsn.vC;
     get_VR_ss(v1, 1, false); //xmm
     move_imm_to_reg(OpndSize_32, 0, 1, false);
     move_imm_to_reg(OpndSize_32, 1, 2, false);
@@ -1842,16 +2136,19 @@ int op_cmpl_float() {
     conditional_move_reg_to_reg(OpndSize_32, Condition_A,
                                              2, false, 4, false);
     set_virtual_reg(vA, OpndSize_32, 4, false);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode CMPG_FLOAT
 
-//!
-int op_cmpg_float() {
-    u2 vA = INST_AA(inst);
-    u4 v1 = FETCH(1) & 0xff;
-    u4 v2 = FETCH(1) >> 8;
+/**
+ * @brief Generate native code for bytecode cmpg-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmpg_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMPG_FLOAT);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 v1 = mir->dalvikInsn.vB;
+    u4 v2 = mir->dalvikInsn.vC;
     get_VR_ss(v1, 1, false);
     compare_VR_ss_reg(v2, 1, false);
     move_imm_to_reg(OpndSize_32, 0, 1, false);
@@ -1866,16 +2163,19 @@ int op_cmpg_float() {
     conditional_move_reg_to_reg(OpndSize_32, Condition_A,
                                 2, false, 3, false);
     set_virtual_reg(vA, OpndSize_32, 3, false);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode CMPL_DOUBLE
 
-//!
-int op_cmpl_double() {
-    u2 vA = INST_AA(inst);
-    u4 v1 = FETCH(1) & 0xff;
-    u4 v2 = FETCH(1) >> 8;
+/**
+ * @brief Generate native code for bytecode cmpl-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmpl_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMPL_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 v1 = mir->dalvikInsn.vB;
+    u4 v2 = mir->dalvikInsn.vC;
     get_VR_sd(v1, 1, false);
     compare_VR_sd_reg(v2, 1, false);
     move_imm_to_reg(OpndSize_32, 0, 1, false);
@@ -1891,16 +2191,19 @@ int op_cmpl_double() {
     conditional_move_reg_to_reg(OpndSize_32, Condition_A,
                                              2, false, 4, false);
     set_virtual_reg(vA, OpndSize_32, 4, false);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode CMPG_DOUBLE
 
-//!
-int op_cmpg_double() {
-    u2 vA = INST_AA(inst);
-    u4 v1 = FETCH(1) & 0xff;
-    u4 v2 = FETCH(1) >> 8;
+/**
+ * @brief Generate native code for bytecode cmpg-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmpg_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMPG_DOUBLE);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 v1 = mir->dalvikInsn.vB;
+    u4 v2 = mir->dalvikInsn.vC;
     get_VR_sd(v1, 1, false);
     compare_VR_sd_reg(v2, 1, false);
     move_imm_to_reg(OpndSize_32, 0, 1, false);
@@ -1916,8 +2219,7 @@ int op_cmpg_double() {
                                              2, false, 3, false);
     conditional_move_reg_to_reg(OpndSize_32, Condition_A,
                                              2, false, 3, false);
-   set_virtual_reg(vA, OpndSize_32, 3, false);
-    rPC += 2;
+    set_virtual_reg(vA, OpndSize_32, 3, false);
     return 0;
 }
 #define P_GPR_1 PhysicalReg_EBX
@@ -1926,13 +2228,17 @@ int op_cmpg_double() {
 #define P_SCRATCH_1 PhysicalReg_EDX
 #define P_SCRATCH_2 PhysicalReg_EAX
 #define OPTION_OLD //for simpler cfg
-//! lower bytecode CMP_LONG
 
-//!
-int op_cmp_long() {
-    u2 vA = INST_AA(inst);
-    u4 v1 = FETCH(1) & 0xff;
-    u4 v2 = FETCH(1) >> 8;
+/**
+ * @brief Generate native code for bytecode cmp-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmp_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMP_LONG);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 v1 = mir->dalvikInsn.vB;
+    u4 v2 = mir->dalvikInsn.vC;
     get_virtual_reg(v1+1, OpndSize_32, 2, false);
 #ifdef OPTION_OLD
     move_imm_to_reg(OpndSize_32, 0xffffffff, 3, false);
@@ -1984,7 +2290,6 @@ int op_cmp_long() {
     set_VR_to_imm(vA, OpndSize_32, 1);
 #endif
     insertLabel(".cmp_long_okay", true);
-    rPC += 2;
     return 0;
 }
 #undef P_GPR_1
diff --git a/vm/compiler/codegen/x86/LowerConst.cpp b/vm/compiler/codegen/x86/LowerConst.cpp
index 62e0ed1..23d7450 100644
--- a/vm/compiler/codegen/x86/LowerConst.cpp
+++ b/vm/compiler/codegen/x86/LowerConst.cpp
@@ -61,135 +61,177 @@ int const_string_common(u4 tmp, u2 vA) {
 #undef P_GPR_1
 #undef P_GPR_2
 
-//! lower bytecode CONST_4
-
-//!
-int op_const_4() {
-    u2 vA = INST_A(inst);
-    s4 tmp = (s4) (INST_B(inst) << 28) >> 28;
+/**
+ * @brief Generate native code for bytecode const/4
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_4(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_4);
+    u2 vA = mir->dalvikInsn.vA;
+    s4 tmp = mir->dalvikInsn.vB;
     set_VR_to_imm(vA, OpndSize_32, tmp);
-    rPC += 1;
     return 1;
 }
-//! lower bytecode CONST_16
 
-//!
-int op_const_16() {
-    u2 BBBB = FETCH(1);
-    u2 vA = INST_AA(inst);
+/**
+ * @brief Generate native code for bytecode const/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_16);
+    u2 BBBB = mir->dalvikInsn.vB;
+    u2 vA = mir->dalvikInsn.vA;
     set_VR_to_imm(vA, OpndSize_32, (s2)BBBB);
-    rPC += 2;
     return 1;
 }
-//! lower bytecode CONST
 
-//!
-int op_const() {
-    u2 vA = INST_AA(inst);
-    u4 tmp = FETCH(1);
-    tmp |= (u4)FETCH(2) << 16;
+/**
+ * @brief Generate native code for bytecode const
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
     set_VR_to_imm(vA, OpndSize_32, (s4)tmp);
-    rPC += 3;
     return 1;
 }
-//! lower bytecode CONST_HIGH16
 
-//!
-int op_const_high16() {
-    u2 vA = INST_AA(inst);
-    u2 tmp = FETCH(1);
-    set_VR_to_imm(vA, OpndSize_32, (s4)tmp<<16); //??
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode const/high16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_high16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_HIGH16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 tmp = mir->dalvikInsn.vB;
+    set_VR_to_imm(vA, OpndSize_32, ((s4)tmp)<<16);
     return 1;
 }
-//! lower bytecode CONST_WIDE_16
 
-//!
-int op_const_wide_16() {
-    u2 vA = INST_AA(inst);
-    u2 tmp = FETCH(1);
+/**
+ * @brief Generate native code for bytecode const-wide/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_wide_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE_16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 tmp = mir->dalvikInsn.vB;
     set_VR_to_imm(vA, OpndSize_32, (s2)tmp);
-    set_VR_to_imm(vA+1, OpndSize_32, (s2)tmp>>31);
-    rPC += 2;
+    set_VR_to_imm(vA+1, OpndSize_32, ((s2)tmp)>>31);
     return 2;
 }
-//! lower bytecode CONST_WIDE_32
 
-//!
-int op_const_wide_32() {
-    u2 vA = INST_AA(inst);
-    u4 tmp = FETCH(1);
-    tmp |= (u4)FETCH(2) << 16;
+/**
+ * @brief Generate native code for bytecode const-wide/32
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_wide_32(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE_32);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
     set_VR_to_imm(vA, OpndSize_32, (s4)tmp);
-    set_VR_to_imm(vA+1, OpndSize_32, (s4)tmp>>31);
-    rPC += 3;
+    set_VR_to_imm(vA+1, OpndSize_32, ((s4)tmp)>>31);
     return 2;
 }
-//! lower bytecode CONST_WIDE
 
-//!
-int op_const_wide() {
-    u2 vA = INST_AA(inst);
-    u4 tmp = FETCH(1);
-    tmp |= (u8)FETCH(2) << 16;
+/**
+ * @brief Generate native code for bytecode const-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE);
+    u2 vA = mir->dalvikInsn.vA;
+    u8 tmp = mir->dalvikInsn.vB_wide;
     set_VR_to_imm(vA, OpndSize_32, (s4)tmp);
-    tmp = (u8)FETCH(3);
-    tmp |= (u8)FETCH(4) << 16;
-    set_VR_to_imm(vA+1, OpndSize_32, (s4)tmp);
-    rPC += 5;
+    set_VR_to_imm(vA+1, OpndSize_32, (s4)(tmp >> 32));
     return 2;
 }
-//! lower bytecode CONST_WIDE_HIGH16
 
-//!
-int op_const_wide_high16() {
-    u2 vA = INST_AA(inst);
-    u2 tmp = FETCH(1);
+/**
+ * @brief Generate native code for bytecode const-wide/high16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_wide_high16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE_HIGH16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 tmp = mir->dalvikInsn.vB;
     set_VR_to_imm(vA, OpndSize_32, 0);
-    set_VR_to_imm(vA+1, OpndSize_32, (s4)tmp<<16);
-    rPC += 2;
+    set_VR_to_imm(vA+1, OpndSize_32, ((s4)tmp)<<16);
     return 2;
 }
-//! lower bytecode CONST_STRING
 
-//!
-int op_const_string() {
-    u2 vB = FETCH(1);
-    u2 vA = INST_AA(inst);
-    u4 tmp = vB;
+/**
+ * @brief Generate native code for bytecode const-string
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_string(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_STRING);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
     int retval = const_string_common(tmp, vA);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode CONST_STRING_JUMBO
 
-//!
-int op_const_string_jumbo() {
-    u2 vA = INST_AA(inst);
-    u4 tmp = FETCH(1);
-    tmp |= (u4)FETCH(2) << 16;
+/**
+ * @brief Generate native code for bytecode const-string/jumbo
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_string_jumbo(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_STRING_JUMBO);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
     int retval = const_string_common(tmp, vA);
-    rPC += 3;
     return retval;
 }
 
 #define P_GPR_1 PhysicalReg_EBX
-//! LOWER bytecode CONST_CLASS
-
-//! It calls class_resolve (%ebx is live across the call)
-//! Since the register allocator does not handle control flow within the lowered native sequence,
-//!   we define an interface between the lowering module and register allocator:
-//!     rememberState, gotoState, transferToState
-//!   to make sure at the control flow merge point the state of registers is the same
-int op_const_class() {
-    u2 vA = INST_AA(inst);
-    u4 tmp = (u4)FETCH(1);
+/**
+ * @brief Generate native code for bytecode const-class
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_class(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_CLASS);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+#if !defined(WITH_JIT)
+    // It calls class_resolve (%ebx is live across the call)
+    // Since the register allocator does not handle control flow within the lowered native sequence,
+    //   we define an interface between the lowering module and register allocator:
+    //     rememberState, gotoState, transferToState
+    //   to make sure at the control flow merge point the state of registers is the same
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+    get_res_classes(3, false);
+    move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_NE, ".const_class_resolved", true);
+    rememberState(1);
+    export_pc();
+    move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+    call_helper_API(".class_resolve");
+    transferToState(1);
+    insertLabel(".const_class_resolved", true);
+    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+#else
     /* for trace-based JIT, the class is already resolved since this code has been executed */
     void *classPtr = (void*)
        (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
     assert(classPtr != NULL);
     set_VR_to_imm(vA, OpndSize_32, (int) classPtr );
-    rPC += 2;
+#endif
+
     return 0;
 }
 
diff --git a/vm/compiler/codegen/x86/LowerGetPut.cpp b/vm/compiler/codegen/x86/LowerGetPut.cpp
index be519b1..14cea48 100644
--- a/vm/compiler/codegen/x86/LowerGetPut.cpp
+++ b/vm/compiler/codegen/x86/LowerGetPut.cpp
@@ -28,15 +28,23 @@
 #define P_GPR_2 PhysicalReg_ECX
 #define P_GPR_3 PhysicalReg_ESI
 #define P_GPR_4 PhysicalReg_EDX
-//! LOWER bytecode AGET without usage of helper function
 
-//! It has null check and length check
-int aget_common_nohelper(int flag, u2 vA, u2 vref, u2 vindex) {
+/**
+ * @brief Common function for generating native code for aget variants
+ * @details Includes null check and bound check.
+ * @param flag
+ * @param vA destination VR
+ * @param vref VR holding reference
+ * @param vindex VR holding index
+ * @param mirOptFlags optimization flags for current bytecode
+ * @return value >= 0 when handled
+ */
+int aget_common_nohelper(ArrayAccess flag, u2 vA, u2 vref, u2 vindex, int mirOptFlags) {
     ////////////////////////////
     // Request VR free delays before register allocation for the temporaries
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_NULL_CHECK))
+    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK))
         requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
+    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
         requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
         requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
     }
@@ -44,7 +52,7 @@ int aget_common_nohelper(int flag, u2 vA, u2 vref, u2 vindex) {
     get_virtual_reg(vref, OpndSize_32, 1, false); //array
     get_virtual_reg(vindex, OpndSize_32, 2, false); //index
 
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_NULL_CHECK)) {
+    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK)) {
         //last argument is the exception number for this bytecode
         nullCheck(1, false, 1, vref); //maybe optimized away, if not, call
         cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
@@ -52,7 +60,7 @@ int aget_common_nohelper(int flag, u2 vA, u2 vref, u2 vindex) {
         updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
     }
 
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
+    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
         boundCheck(vref, 1, false,
                              vindex, 2, false,
                              2);
@@ -90,86 +98,118 @@ int aget_common_nohelper(int flag, u2 vA, u2 vref, u2 vindex) {
     //////////////////////////////////
     return 0;
 }
+#if 0 /* Code is deprecated. If reenabled, needs additional parameter
+         for optimization flags*/
 //! wrapper to call either aget_common_helper or aget_common_nohelper
 
 //!
 int aget_common(int flag, u2 vA, u2 vref, u2 vindex) {
     return aget_common_nohelper(flag, vA, vref, vindex);
 }
+#endif
 #undef P_GPR_1
 #undef P_GPR_2
 #undef P_GPR_3
 #undef P_GPR_4
-//! lower bytecode AGET by calling aget_common
 
-//!
-int op_aget() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aget_common(AGET, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aget and aget-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET
+            || mir->dalvikInsn.opcode == OP_AGET_OBJECT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode AGET_WIDE by calling aget_common
 
-//!
-int op_aget_wide() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aget_common(AGET_WIDE, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aget-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_WIDE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_WIDE, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode AGET_OBJECT by calling aget_common
 
-//!
-int op_aget_object() {
-    return op_aget();
+/**
+ * @brief Generate native code for bytecode aget-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_OBJECT);
+    return op_aget(mir);
 }
-//! lower bytecode BOOLEAN by calling aget_common
 
-//!
-int op_aget_boolean() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aget_common(AGET_BOOLEAN, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aget-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_BOOLEAN);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_BOOLEAN, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode AGET_BYTE by calling aget_common
 
-//!
-int op_aget_byte() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aget_common(AGET_BYTE, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aget-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_BYTE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_BYTE, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode AGET_CHAR by calling aget_common
 
-//!
-int op_aget_char() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aget_common(AGET_CHAR, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aget-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_CHAR);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_CHAR, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode AGET_SHORT by calling aget_common
 
-//!
-int op_aget_short() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aget_common(AGET_SHORT, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aget-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_SHORT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_SHORT, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
 
@@ -177,17 +217,25 @@ int op_aget_short() {
 #define P_GPR_2 PhysicalReg_ECX
 #define P_GPR_3 PhysicalReg_ESI
 #define P_GPR_4 PhysicalReg_EDX
-//! LOWER bytecode APUT without usage of helper function
 
-//! It has null check and length check
-int aput_common_nohelper(int flag, u2 vA, u2 vref, u2 vindex) {
+/**
+ * @brief Common function for generating native code for aput variants
+ * @details Includes null check and bound check.
+ * @param flag
+ * @param vA destination VR
+ * @param vref VR holding reference
+ * @param vindex VR holding index
+ * @param mirOptFlags optimization flags for current bytecode
+ * @return value >= 0 when handled
+ */
+int aput_common_nohelper(ArrayAccess flag, u2 vA, u2 vref, u2 vindex, int mirOptFlags) {
     //////////////////////////////////////
     // Request VR free delays before register allocation for the temporaries.
     // No need to request delay for vA since it will be transferred to temporary
     // after the null check and bound check.
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_NULL_CHECK))
+    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK))
         requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
+    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
         requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
         requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
     }
@@ -195,7 +243,7 @@ int aput_common_nohelper(int flag, u2 vA, u2 vref, u2 vindex) {
     get_virtual_reg(vref, OpndSize_32, 1, false); //array
     get_virtual_reg(vindex, OpndSize_32, 2, false); //index
 
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_NULL_CHECK)) {
+    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK)) {
         //last argument is the exception number for this bytecode
         nullCheck(1, false, 1, vref); //maybe optimized away, if not, call
         cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
@@ -203,7 +251,7 @@ int aput_common_nohelper(int flag, u2 vA, u2 vref, u2 vindex) {
         updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
     }
 
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
+    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
         boundCheck(vref, 1, false,
                              vindex, 2, false,
                              2);
@@ -231,80 +279,107 @@ int aput_common_nohelper(int flag, u2 vA, u2 vref, u2 vindex) {
     //////////////////////////////////
     return 0;
 }
+#if 0 /* Code is deprecated. If reenabled, needs additional parameter
+         for optimization flags*/
 //! wrapper to call either aput_common_helper or aput_common_nohelper
 
 //!
 int aput_common(int flag, u2 vA, u2 vref, u2 vindex) {
     return aput_common_nohelper(flag, vA, vref, vindex);
 }
+#endif
 #undef P_GPR_1
 #undef P_GPR_2
 #undef P_GPR_3
 #undef P_GPR_4
-//! lower bytecode APUT by calling aput_common
 
-//!
-int op_aput() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aput_common(APUT, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aput
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode APUT_WIDE by calling aput_common
 
-//!
-int op_aput_wide() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aput_common(APUT_WIDE, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aput-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_WIDE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_WIDE, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode APUT_BOOLEAN by calling aput_common
 
-//!
-int op_aput_boolean() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aput_common(APUT_BOOLEAN, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aput-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_BOOLEAN);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_BOOLEAN, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode APUT_BYTE by calling aput_common
 
-//!
-int op_aput_byte() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aput_common(APUT_BYTE, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aput-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_BYTE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_BYTE, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode APUT_CHAR by calling aput_common
 
-//!
-int op_aput_char() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aput_common(APUT_CHAR, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aput-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_CHAR);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_CHAR, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
-//! lower bytecode APUT_SHORT by calling aput_common
 
-//!
-int op_aput_short() {
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-    int retval = aput_common(APUT_SHORT, vA, vref, vindex);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode aput-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_SHORT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_SHORT, vA, vref, vindex,
+            mir->OptimizationFlags);
     return retval;
 }
 
@@ -317,46 +392,63 @@ int op_aput_short() {
 
 void markCard_notNull(int tgtAddrReg, int scratchReg, bool isPhysical);
 
-//! lower bytecode APUT_OBJECT
-
-//! Lower the bytecode using helper function ".aput_obj_helper" if helper switch is on
-int op_aput_object() { //type checking
-    u2 vA = INST_AA(inst);
-    u2 vref = FETCH(1) & 0xff;
-    u2 vindex = FETCH(1) >> 8;
-
-    ///////////////////////////
-    // Request VR free delays before register allocation for the temporaries
-    // No need to request delay for vA since it will be transferred to temporary
-    // after the null check and bound check.
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_NULL_CHECK))
-        requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
-        requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
-        requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
+/**
+ * @brief Generate native code for bytecode aput-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_object(const MIR * mir) { //type checking
+    assert(mir->dalvikInsn.opcode == OP_APUT_OBJECT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vref = mir->dalvikInsn.vB;
+    u2 vindex = mir->dalvikInsn.vC;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[6]) {
+        export_pc(); //use %edx
+        move_imm_to_reg(OpndSize_32, vA, P_SCRATCH_1, true);
+        move_imm_to_reg(OpndSize_32, vref, P_SCRATCH_2, true);
+        move_imm_to_reg(OpndSize_32, vindex, P_GPR_2, true);
+
+        spillVirtualReg(vref, LowOpndRegType_gp, true);
+        spillVirtualReg(vindex, LowOpndRegType_gp, true);
+        spillVirtualReg(vA, LowOpndRegType_gp, true);
+        call_helper_API(".aput_obj_helper");
     }
+    else
+#endif
+    {
+        ///////////////////////////
+        // Request VR free delays before register allocation for the temporaries
+        // No need to request delay for vA since it will be transferred to temporary
+        // after the null check and bound check.
+        if(!(mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK))
+            requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
+        if(!(mir->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
+            requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
+            requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
+        }
 
-    get_virtual_reg(vref, OpndSize_32, 1, false); //array
-    export_pc(); //use %edx
+        get_virtual_reg(vref, OpndSize_32, 1, false); //array
+        export_pc(); //use %edx
 
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_NULL_CHECK)) {
-        compare_imm_reg(OpndSize_32, 0, 1, false);
-        conditional_jump_global_API(Condition_E, "common_errNullObject", false);
-        cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
-    } else {
-        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
-    }
+        if(!(mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK)) {
+            compare_imm_reg(OpndSize_32, 0, 1, false);
+            conditional_jump_global_API(Condition_E, "common_errNullObject", false);
+            cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
+        } else {
+            updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
+        }
 
-    get_virtual_reg(vindex, OpndSize_32, 2, false); //index
-    if(!(traceCurrentMIR->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
-        compare_mem_reg(OpndSize_32, offArrayObject_length, 1, false, 2, false);
-        conditional_jump_global_API(Condition_NC, "common_errArrayIndex", false);
-        cancelVRFreeDelayRequest(vref,VRDELAY_BOUNDCHECK);
-        cancelVRFreeDelayRequest(vindex,VRDELAY_BOUNDCHECK);
-    } else {
-        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
-        updateRefCount2(2, LowOpndRegType_gp, false); //update reference count for tmp2
-    }
+        get_virtual_reg(vindex, OpndSize_32, 2, false); //index
+        if(!(mir->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
+            compare_mem_reg(OpndSize_32, offArrayObject_length, 1, false, 2, false);
+            conditional_jump_global_API(Condition_NC, "common_errArrayIndex", false);
+            cancelVRFreeDelayRequest(vref,VRDELAY_BOUNDCHECK);
+            cancelVRFreeDelayRequest(vindex,VRDELAY_BOUNDCHECK);
+        } else {
+            updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
+            updateRefCount2(2, LowOpndRegType_gp, false); //update reference count for tmp2
+        }
 
     get_virtual_reg(vA, OpndSize_32, 4, false);
     compare_imm_reg(OpndSize_32, 0, 4, false);
@@ -389,8 +481,8 @@ int op_aput_object() { //type checking
     transferToState(2);
     insertLabel(".aput_object_after_check", true);
     ///////////////////////////////
-    rPC += 2;
-    return 0;
+  }
+  return 0;
 }
 #undef P_GPR_1
 #undef P_GPR_2
@@ -438,24 +530,54 @@ void markCard_filled(int tgtAddrReg, bool isTgtPhysical, int scratchReg, bool is
    alu_binary_imm_reg(OpndSize_32, shr_opc, GC_CARD_SHIFT, tgtAddrReg, isTgtPhysical);
    move_reg_to_mem_disp_scale(OpndSize_8, scratchReg, isScratchPhysical, scratchReg, isScratchPhysical, 0, tgtAddrReg, isTgtPhysical, 1);
 }
-//! LOWER bytecode IGET,IPUT without usage of helper function
 
-//! It has null check and calls assembly function inst_field_resolve
-int iget_iput_common_nohelper(int tmp, int flag, u2 vA, u2 vB, int isObj, bool isVolatile) {
+/**
+ * @brief Common function for generating native code for iget and iput variants
+ * @details Includes null check
+ * @param referenceIndex instance field index
+ * @param flag type of instance access
+ * @param vA value register
+ * @param vB object register
+ * @param isObj true iff mnemonic is object variant
+ * @param isVolatile iff mnemonic is volatile variant
+ * @return value >= 0 when handled
+ */
+int iget_iput_common_nohelper(u2 referenceIndex, InstanceAccess flag, u2 vA,
+        u2 vB, bool isObj, bool isVolatile) {
+#if !defined(WITH_JIT)
+    ///////////////////////////////
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+    get_res_fields(3, false);
+    //move_mem_to_reg(OpndSize_32, referenceIndex*4, 3, false, 4, false);
+    compare_imm_mem(OpndSize_32, 0, referenceIndex*4, 3, false);
+    move_mem_to_reg(OpndSize_32, referenceIndex*4, 3, false, PhysicalReg_EAX, true);
+    /*********************************
+    compare_imm_reg(OpndSize_32, 0, 4, false);
+    **********************************/
+    export_pc(); //use %edx
+    conditional_jump(Condition_NE, ".iget_iput_resolved", true);
+    rememberState(1);
+    move_imm_to_reg(OpndSize_32, referenceIndex, PhysicalReg_EAX, true);
+    call_helper_API(".inst_field_resolve");
+    transferToState(1);
+    insertLabel(".iget_iput_resolved", true);
+#else
 #ifdef WITH_JIT_INLINING
-    const Method *method = (traceCurrentMIR->OptimizationFlags & MIR_CALLEE) ?
-        traceCurrentMIR->meta.calleeMethod : currentMethod;
+    const Method *method = (mir->OptimizationFlags & MIR_CALLEE) ?
+        mir->meta.calleeMethod : currentMethod;
     InstField *pInstField = (InstField *)
-            method->clazz->pDvmDex->pResFields[tmp];
+            method->clazz->pDvmDex->pResFields[referenceIndex];
 #else
     InstField *pInstField = (InstField *)
-            currentMethod->clazz->pDvmDex->pResFields[tmp];
+            currentMethod->clazz->pDvmDex->pResFields[referenceIndex];
 #endif
     int fieldOffset;
 
     assert(pInstField != NULL);
     fieldOffset = pInstField->byteOffset;
     move_imm_to_reg(OpndSize_32, fieldOffset, 8, false);
+#endif
     // Request VR delay before transfer to temporary. Only vB needs delay.
     // vA will have non-zero reference count since transfer to temporary for
     // it happens after null check, thus no delay is needed.
@@ -472,7 +594,7 @@ int iget_iput_common_nohelper(int tmp, int flag, u2 vA, u2 vB, int isObj, bool i
             load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
             move_reg_to_mem(OpndSize_32, 9, false, 12, PhysicalReg_ESP, true); //field
             move_reg_to_mem(OpndSize_32, 7, false, 8, PhysicalReg_ESP, true); //object
-            move_imm_to_mem(OpndSize_32, tmp, 4, PhysicalReg_ESP, true); //field
+            move_imm_to_mem(OpndSize_32, referenceIndex, 4, PhysicalReg_ESP, true); //field
             move_imm_to_mem(OpndSize_32, 0, 0, PhysicalReg_ESP, true); //iget
             call_dvmDebugIgetIput();
             load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
@@ -522,151 +644,260 @@ int iget_iput_common_nohelper(int tmp, int flag, u2 vA, u2 vB, int isObj, bool i
     ///////////////////////////
     return 0;
 }
+
+#if 0 /* Code is deprecated. If reenabled, needs additional parameter
+         for optimization flags*/
 //! wrapper to call either iget_iput_common_helper or iget_iput_common_nohelper
 
 //!
 int iget_iput_common(int tmp, int flag, u2 vA, u2 vB, int isObj, bool isVolatile) {
     return iget_iput_common_nohelper(tmp, flag, vA, vB, isObj, isVolatile);
 }
+#endif
 #undef P_GPR_1
 #undef P_GPR_2
 #undef P_GPR_3
 #undef P_SCRATCH_1
-//! lower bytecode IGET by calling iget_iput_common
 
-//!
-int op_iget() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    u2 tmp = FETCH(1);
-    int retval = iget_iput_common(tmp, IGET, vA, vB, 0, false);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes iget, iget-boolean,
+ * iget-byte, iget-char, iget-short, and iget/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET
+            || mir->dalvikInsn.opcode == OP_IGET_BOOLEAN
+            || mir->dalvikInsn.opcode == OP_IGET_BYTE
+            || mir->dalvikInsn.opcode == OP_IGET_CHAR
+            || mir->dalvikInsn.opcode == OP_IGET_SHORT
+            || mir->dalvikInsn.opcode == OP_IGET_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IGET, vA, vB, false,
+            false);
     return retval;
 }
-//! lower bytecode IGET_WIDE by calling iget_iput_common
 
-//!
-int op_iget_wide(bool isVolatile) {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    u2 tmp = FETCH(1);
-    int retval = iget_iput_common(tmp, IGET_WIDE, vA, vB, 0, isVolatile);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes iget-wide
+ * and iget-wide/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_wide(const MIR * mir, bool isVolatile) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_WIDE
+            || mir->dalvikInsn.opcode == OP_IGET_WIDE_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IGET_WIDE, vA, vB,
+            false, isVolatile);
     return retval;
 }
-//! lower bytecode IGET_OBJECT by calling iget_iput_common
 
-//!
-int op_iget_object() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    u2 tmp = FETCH(1);
-    int retval = iget_iput_common(tmp, IGET, vA, vB, 1, false);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes iget-object
+ * and iget-object/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_OBJECT
+            || mir->dalvikInsn.opcode == OP_IGET_OBJECT_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IGET, vA, vB, true,
+            false);
     return retval;
 }
-//! lower bytecode IGET_BOOLEAN by calling iget_iput_common
 
-//!
-int op_iget_boolean() {
-    return op_iget();
+/**
+ * @brief Generate native code for bytecode iget-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_BOOLEAN);
+    return op_iget(mir);
 }
-//! lower bytecode IGET_BYTE by calling iget_iput_common
 
-//!
-int op_iget_byte() {
-    return op_iget();
+/**
+ * @brief Generate native code for bytecode iget-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_BYTE);
+    return op_iget(mir);
 }
-//! lower bytecode IGET_CHAR by calling iget_iput_common
 
-//!
-int op_iget_char() {
-    return op_iget();
+/**
+ * @brief Generate native code for bytecode iget-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_CHAR);
+    return op_iget(mir);
 }
-//! lower bytecode IGET_SHORT by calling iget_iput_common
 
-//!
-int op_iget_short() {
-    return op_iget();
+/**
+ * @brief Generate native code for bytecode iget-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_SHORT);
+    return op_iget(mir);
 }
-//! lower bytecode IPUT by calling iget_iput_common
 
-//!
-int op_iput() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    u2 tmp = FETCH(1);
-    int retval = iget_iput_common(tmp, IPUT, vA, vB, 0, false);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes iput, iput-boolean,
+ * iput-byte, iput-char, iput-short, and iput/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT
+            || mir->dalvikInsn.opcode == OP_IPUT_BOOLEAN
+            || mir->dalvikInsn.opcode == OP_IPUT_BYTE
+            || mir->dalvikInsn.opcode == OP_IPUT_CHAR
+            || mir->dalvikInsn.opcode == OP_IPUT_SHORT
+            || mir->dalvikInsn.opcode == OP_IPUT_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IPUT, vA, vB, false,
+            false);
     return retval;
 }
-//! lower bytecode IPUT_WIDE by calling iget_iput_common
 
-//!
-int op_iput_wide(bool isVolatile) {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    u2 tmp = FETCH(1);
-    int retval = iget_iput_common(tmp, IPUT_WIDE, vA, vB, 0, isVolatile);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes iput-wide
+ * and iput-wide/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_wide(const MIR * mir, bool isVolatile) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_WIDE
+            || mir->dalvikInsn.opcode == OP_IPUT_WIDE_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IPUT_WIDE, vA, vB,
+            false, isVolatile);
     return retval;
 }
-//! lower bytecode IPUT_OBJECT by calling iget_iput_common
 
-//!
-int op_iput_object() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    u2 tmp = FETCH(1);
-    int retval = iget_iput_common(tmp, IPUT, vA, vB, 1, false);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes iput-object
+ * and iput-object/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_OBJECT
+            || mir->dalvikInsn.opcode == OP_IPUT_OBJECT_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IPUT, vA, vB, true,
+            false);
     return retval;
 }
-//! lower bytecode IPUT_BOOLEAN by calling iget_iput_common
 
-//!
-int op_iput_boolean() {
-    return op_iput();
+/**
+ * @brief Generate native code for bytecode iput-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_BOOLEAN);
+    return op_iput(mir);
 }
-//! lower bytecode IPUT_BYTE by calling iget_iput_common
 
-//!
-int op_iput_byte() {
-    return op_iput();
+/**
+ * @brief Generate native code for bytecode iput-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_BYTE);
+    return op_iput(mir);
 }
-//! lower bytecode IPUT_CHAR by calling iget_iput_common
 
-//!
-int op_iput_char() {
-    return op_iput();
+/**
+ * @brief Generate native code for bytecode iput-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_CHAR);
+    return op_iput(mir);
 }
-//! lower bytecode IPUT_SHORT by calling iget_iput_common
 
-//!
-int op_iput_short() {
-    return op_iput();
+/**
+ * @brief Generate native code for bytecode iput-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_SHORT);
+    return op_iput(mir);
 }
 
 #define P_GPR_1 PhysicalReg_EBX
 #define P_GPR_2 PhysicalReg_ECX
 #define P_GPR_3 PhysicalReg_EDX //used by helper only
 
-//! common section to lower IGET & IPUT
-
-//! It will use helper function sget_helper if the switch is on
-int sget_sput_common(int flag, u2 vA, u2 tmp, bool isObj, bool isVolatile) {
-    //call assembly static_field_resolve
-    //no exception
-    //glue: get_res_fields
-    //hard-coded: eax (one version?)
-    //////////////////////////////////////////
+/**
+ * @brief Common function for generating native code for sget and sput variants
+ * @details Includes null check
+ * @param flag type of static access
+ * @param vA value register
+ * @param referenceIndex static field index
+ * @param isObj true iff mnemonic is object variant
+ * @param isVolatile iff mnemonic is volatile variant
+ * @return value >= 0 when handled
+ */
+int sget_sput_common(StaticAccess flag, u2 vA, u2 referenceIndex, bool isObj, bool isVolatile) {
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[5]) {
+        return sget_sput_common_helper(flag, vA, referenceIndex, isObj);
+    }
+    else
+#endif
+    {
+        //call assembly static_field_resolve
+        //no exception
+        //glue: get_res_fields
+        //hard-coded: eax (one version?)
+        //////////////////////////////////////////
+#if !defined(WITH_JIT)
+        scratchRegs[2] = PhysicalReg_EDX; scratchRegs[3] = PhysicalReg_Null;
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        get_res_fields(3, false);
+        move_mem_to_reg(OpndSize_32, referenceIndex*4, 3, false, PhysicalReg_EAX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //InstanceField
+        conditional_jump(Condition_NE, ".sget_sput_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, referenceIndex, PhysicalReg_EAX, true);
+
+        export_pc(); //use %edx
+        call_helper_API(".static_field_resolve");
+        transferToState(1);
+        insertLabel(".sget_sput_resolved", true);
+#else
 #ifdef WITH_JIT_INLINING
-    const Method *method = (traceCurrentMIR->OptimizationFlags & MIR_CALLEE) ? traceCurrentMIR->meta.calleeMethod : currentMethod;
-    void *fieldPtr = (void*)
-        (method->clazz->pDvmDex->pResFields[tmp]);
+        const Method *method = (mir->OptimizationFlags & MIR_CALLEE) ? mir->meta.calleeMethod : currentMethod;
+        void *fieldPtr = (void*)
+              (method->clazz->pDvmDex->pResFields[referenceIndex]);
 #else
-    void *fieldPtr = (void*)
-        (currentMethod->clazz->pDvmDex->pResFields[tmp]);
+        void *fieldPtr = (void*)
+              (currentMethod->clazz->pDvmDex->pResFields[referenceIndex]);
 #endif
 
     /* Usually, fieldPtr should not be null. The interpreter should resolve
@@ -680,6 +911,7 @@ int sget_sput_common(int flag, u2 vA, u2 tmp, bool isObj, bool isVolatile) {
     }
 
     move_imm_to_reg(OpndSize_32, (int)fieldPtr, PhysicalReg_EAX, true);
+#endif
     if(flag == SGET) {
         move_mem_to_reg(OpndSize_32, offStaticField_value, PhysicalReg_EAX, true, 7, false); //access field
         set_virtual_reg(vA, OpndSize_32, 7, false);
@@ -727,168 +959,260 @@ int sget_sput_common(int flag, u2 vA, u2 tmp, bool isObj, bool isVolatile) {
         }
     }
     //////////////////////////////////////////////
-    return 0;
+  }
+  return 0;
 }
 #undef P_GPR_1
 #undef P_GPR_2
 #undef P_GPR_3
-//! lower bytecode SGET by calling sget_sput_common
 
-//!
-int op_sget() {
-    u2 vA = INST_AA(inst);
-    u2 tmp = FETCH(1);
-    int retval = sget_sput_common(SGET, vA, tmp, false, false);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes sget, sget-boolean,
+ * sget-byte, sget-char, sget-object, sget-short, and sget/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET
+            || mir->dalvikInsn.opcode == OP_SGET_BOOLEAN
+            || mir->dalvikInsn.opcode == OP_SGET_BYTE
+            || mir->dalvikInsn.opcode == OP_SGET_CHAR
+            || mir->dalvikInsn.opcode == OP_SGET_OBJECT
+            || mir->dalvikInsn.opcode == OP_SGET_SHORT
+            || mir->dalvikInsn.opcode == OP_SGET_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 referenceIndex = mir->dalvikInsn.vB;
+    int retval = sget_sput_common(SGET, vA, referenceIndex, false, false);
     return retval;
 }
-//! lower bytecode SGET_WIDE by calling sget_sput_common
 
-//!
-int op_sget_wide(bool isVolatile) {
-    u2 vA = INST_AA(inst);
-    u2 tmp = FETCH(1);
-    int retval = sget_sput_common(SGET_WIDE, vA, tmp, false, isVolatile);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes sget-wide
+ * and sget-wide/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_wide(const MIR * mir, bool isVolatile) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_WIDE
+            || mir->dalvikInsn.opcode == OP_SGET_WIDE_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 referenceIndex = mir->dalvikInsn.vB;
+    int retval = sget_sput_common(SGET_WIDE, vA, referenceIndex, false,
+            isVolatile);
     return retval;
 }
-//! lower bytecode SGET_OBJECT by calling sget_sput_common
 
-//!
-int op_sget_object() {
-    return op_sget();
+/**
+ * @brief Generate native code for bytecodes sget-object and
+ * sget-object/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_OBJECT
+            || mir->dalvikInsn.opcode == OP_SGET_OBJECT_VOLATILE);
+    return op_sget(mir);
 }
-//! lower bytecode SGET_BOOLEAN by calling sget_sput_common
 
-//!
-int op_sget_boolean() {
-    return op_sget();
+/**
+ * @brief Generate native code for bytecode sget-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_BOOLEAN);
+    return op_sget(mir);
 }
-//! lower bytecode SGET_BYTE by calling sget_sput_common
 
-//!
-int op_sget_byte() {
-    return op_sget();
+/**
+ * @brief Generate native code for bytecode sget-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_BYTE);
+    return op_sget(mir);
 }
-//! lower bytecode SGET_CHAR by calling sget_sput_common
 
-//!
-int op_sget_char() {
-    return op_sget();
+/**
+ * @brief Generate native code for bytecode sget-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_CHAR);
+    return op_sget(mir);
 }
-//! lower bytecode SGET_SHORT by calling sget_sput_common
 
-//!
-int op_sget_short() {
-    return op_sget();
+/**
+ * @brief Generate native code for bytecode sget-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_SHORT);
+    return op_sget(mir);
 }
-//! lower bytecode SPUT by calling sget_sput_common
 
-//!
-int op_sput(bool isObj) {
-    u2 vA = INST_AA(inst);
-    u2 tmp = FETCH(1);
-    int retval = sget_sput_common(SPUT, vA, tmp, isObj, false);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes sput, sput-boolean,
+ * sput-byte, sput-char, sput-object, sput-short, and sput/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput(const MIR * mir, bool isObj) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT
+            || mir->dalvikInsn.opcode == OP_SPUT_BOOLEAN
+            || mir->dalvikInsn.opcode == OP_SPUT_BYTE
+            || mir->dalvikInsn.opcode == OP_SPUT_CHAR
+            || mir->dalvikInsn.opcode == OP_SPUT_OBJECT
+            || mir->dalvikInsn.opcode == OP_SPUT_SHORT
+            || mir->dalvikInsn.opcode == OP_SPUT_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 referenceIndex = mir->dalvikInsn.vB;
+    int retval = sget_sput_common(SPUT, vA, referenceIndex, isObj, false);
     return retval;
 }
-//! lower bytecode SPUT_WIDE by calling sget_sput_common
 
-//!
-int op_sput_wide(bool isVolatile) {
-    u2 vA = INST_AA(inst);
-    u2 tmp = FETCH(1);
-    int retval = sget_sput_common(SPUT_WIDE, vA, tmp, false, isVolatile);
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecodes sput-wide
+ * and sput-wide/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_wide(const MIR * mir, bool isVolatile) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_WIDE
+            || mir->dalvikInsn.opcode == OP_SPUT_WIDE_VOLATILE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 referenceIndex = mir->dalvikInsn.vB;
+    int retval = sget_sput_common(SPUT_WIDE, vA, referenceIndex, false,
+            isVolatile);
     return retval;
 }
-//! lower bytecode SPUT_OBJECT by calling sget_sput_common
 
-//!
-int op_sput_object() {
-    return op_sput(true);
+/**
+ * @brief Generate native code for bytecodes sput-object and
+ * sput-object/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_OBJECT
+            || mir->dalvikInsn.opcode == OP_SPUT_OBJECT_VOLATILE);
+    return op_sput(mir, true /*isObj*/);
 }
-//! lower bytecode SPUT_OBJECT by calling sget_sput_common
 
-//!
-int op_sput_boolean() {
-    return op_sput(false);
+/**
+ * @brief Generate native code for bytecode sput-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_BOOLEAN);
+    return op_sput(mir, false /*isObj*/);
 }
-//! lower bytecode SPUT_BOOLEAN by calling sget_sput_common
 
-//!
-int op_sput_byte() {
-    return op_sput(false);
+/**
+ * @brief Generate native code for bytecode sput-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_BYTE);
+    return op_sput(mir, false /*isObj*/);
 }
-//! lower bytecode SPUT_BYTE by calling sget_sput_common
 
-//!
-int op_sput_char() {
-    return op_sput(false);
+/**
+ * @brief Generate native code for bytecode sput-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_CHAR);
+    return op_sput(mir, false /*isObj*/);
 }
-//! lower bytecode SPUT_SHORT by calling sget_sput_common
 
-//!
-int op_sput_short() {
-    return op_sput(false);
+/**
+ * @brief Generate native code for bytecode sput-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_SHORT);
+    return op_sput(mir, false /*isObj*/);
 }
 #define P_GPR_1 PhysicalReg_EBX
 #define P_GPR_2 PhysicalReg_ECX
-//! lower bytecode IGET_QUICK
 
-//!
-int op_iget_quick() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst); //object
-    u2 tmp = FETCH(1);
+/**
+ * @brief Generate native code for bytecodes iget-quick and iget-object-quick
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_QUICK
+            || mir->dalvikInsn.opcode == OP_IGET_OBJECT_QUICK);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB; //object
+    u2 fieldByteOffset = mir->dalvikInsn.vC;
 
     requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
     get_virtual_reg(vB, OpndSize_32, 1, false);
     nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
     cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
 
-    move_mem_to_reg(OpndSize_32, tmp, 1, false, 2, false);
+    move_mem_to_reg(OpndSize_32, fieldByteOffset, 1, false, 2, false);
     set_virtual_reg(vA, OpndSize_32, 2, false);
-    rPC += 2;
     return 0;
 }
 #undef P_GPR_1
 #undef P_GPR_2
 #define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode IGET_WIDE_QUICK
 
-//!
-int op_iget_wide_quick() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst); //object
-    u2 tmp = FETCH(1);
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_wide_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_WIDE_QUICK);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB; //object
+    u2 fieldByteOffset = mir->dalvikInsn.vC;
 
     requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
     get_virtual_reg(vB, OpndSize_32, 1, false);
     nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
     cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
 
-    move_mem_to_reg(OpndSize_64, tmp, 1, false, 1, false);
+    move_mem_to_reg(OpndSize_64, fieldByteOffset, 1, false, 1, false);
     set_virtual_reg(vA, OpndSize_64, 1, false);
-    rPC += 2;
     return 0;
 }
 #undef P_GPR_1
-//! lower bytecode IGET_OBJECT_QUICK
 
-//!
-int op_iget_object_quick() {
-    return op_iget_quick();
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_object_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_OBJECT_QUICK);
+    return op_iget_quick(mir);
 }
 #define P_GPR_1 PhysicalReg_EBX
 #define P_GPR_2 PhysicalReg_ECX
-//! lower bytecode IPUT_QUICK
 
-//!
-int iput_quick_common(bool isObj) {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst); //object
-    u2 tmp = FETCH(1);
+/**
+ *
+ * @param mir
+ * @param isObj
+ * @return
+ */
+int iput_quick_common(const MIR * mir, bool isObj) {
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB; //object
+    u2 fieldByteOffset = mir->dalvikInsn.vC;
 
     // Request VR delay before transfer to temporary. Only vB needs delay.
     // vA will have non-zero reference count since transfer to temporary for
@@ -899,26 +1223,35 @@ int iput_quick_common(bool isObj) {
     cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
 
     get_virtual_reg(vA, OpndSize_32, 2, false);
-    move_reg_to_mem(OpndSize_32, 2, false, tmp, 1, false);
+    move_reg_to_mem(OpndSize_32, 2, false, fieldByteOffset, 1, false);
     if(isObj) {
         markCard(2/*valReg*/, 1, false, 11, false);
     }
-    rPC += 2;
     return 0;
 }
-int op_iput_quick() {
-    return iput_quick_common(false);
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_QUICK);
+    return iput_quick_common(mir, false /*isObj*/);
 }
 #undef P_GPR_1
 #undef P_GPR_2
 #define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode IPUT_WIDE_QUICK
 
-//!
-int op_iput_wide_quick() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst); //object
-    u2 tmp = FETCH(1); //byte offset
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_wide_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_WIDE_QUICK);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB; //object
+    u2 fieldByteOffset = mir->dalvikInsn.vC;
 
     // Request VR delay before transfer to temporary. Only vB needs delay.
     // vA will have non-zero reference count since transfer to temporary for
@@ -929,15 +1262,18 @@ int op_iput_wide_quick() {
     cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
 
     get_virtual_reg(vA, OpndSize_64, 1, false);
-    move_reg_to_mem(OpndSize_64, 1, false, tmp, 1, false);
-    rPC += 2;
+    move_reg_to_mem(OpndSize_64, 1, false, fieldByteOffset, 1, false);
     return 0;
 }
 #undef P_GPR_1
-//! lower bytecode IPUT_OBJECT_QUICK
 
-//!
-int op_iput_object_quick() {
-    return iput_quick_common(true);
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_object_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_OBJECT_QUICK);
+    return iput_quick_common(mir, true /*isObj*/);
 }
 
diff --git a/vm/compiler/codegen/x86/LowerInvoke.cpp b/vm/compiler/codegen/x86/LowerInvoke.cpp
index 9df1ee7..15414c7 100644
--- a/vm/compiler/codegen/x86/LowerInvoke.cpp
+++ b/vm/compiler/codegen/x86/LowerInvoke.cpp
@@ -39,9 +39,12 @@ ArgsDoneType convertCalleeToType(const Method* calleeMethod) {
         return ArgsDone_Native;
     return ArgsDone_Normal;
 }
-int common_invokeMethodRange(ArgsDoneType);
-int common_invokeMethodNoRange(ArgsDoneType);
-void gen_predicted_chain(bool isRange, u2 tmp, int IMMC, bool isInterface, int inputReg);
+int common_invokeMethodRange(ArgsDoneType,
+        const DecodedInstruction &decodedInst);
+int common_invokeMethodNoRange(ArgsDoneType,
+        const DecodedInstruction &decodedInst);
+void gen_predicted_chain(bool isRange, u2 tmp, int IMMC, bool isInterface,
+        int inputReg, const DecodedInstruction &decodedInst);
 
 //inputs to common_invokeMethodRange: %ecx
 //          common_errNoSuchMethod: %edx
@@ -79,14 +82,15 @@ static void genLandingPadForMispredictedCallee(MIR* mir) {
 //! LOWER bytecode INVOKE_VIRTUAL without usage of helper function
 
 //!
-int common_invoke_virtual_nohelper(bool isRange, u2 tmp, u2 vD) {
+int common_invoke_virtual_nohelper(bool isRange, u2 tmp, u2 vD,
+        const DecodedInstruction &decodedInst) {
 #ifdef WITH_JIT_INLINING
     /*
      * If the invoke has non-null misPredBranchOver, we need to generate
      * the non-inlined version of the invoke here to handle the
      * mispredicted case.
      */
-    if (traceCurrentMIR->meta.callsiteInfo->misPredBranchOver) {
+    if (mir->meta.callsiteInfo->misPredBranchOver) {
         genLandingPadForMispredictedCallee(); //cUnit, mir, bb, labelList);
     }
 #endif
@@ -113,17 +117,21 @@ int common_invoke_virtual_nohelper(bool isRange, u2 tmp, u2 vD) {
 #else
     int methodIndex =
                 currentMethod->clazz->pDvmDex->pResMethods[tmp]->methodIndex;
-    gen_predicted_chain(isRange, tmp, methodIndex*4, false, 5/*tmp5*/);
+    gen_predicted_chain(isRange, tmp, methodIndex * 4, false, 5/*tmp5*/,
+            decodedInst);
 #endif
     ///////////////////////////////////
     return 0;
 }
+
+#if 0 /* Code is deprecated. If reenabling, must add parameter for decoded instruction */
 //! wrapper to call either common_invoke_virtual_helper or common_invoke_virtual_nohelper
 
 //!
 int common_invoke_virtual(bool isRange, u2 tmp, u2 vD) {
     return common_invoke_virtual_nohelper(isRange, tmp, vD);
 }
+#endif
 #undef P_GPR_1
 #undef P_GPR_2
 #undef P_GPR_3
@@ -143,25 +151,54 @@ int common_invoke_virtual(bool isRange, u2 tmp, u2 vD) {
 //! common section to lower INVOKE_SUPER
 
 //! It will use helper function if the switch is on
-int common_invoke_super(bool isRange, u2 tmp) {
+int common_invoke_super(bool isRange, u2 tmp,
+        const DecodedInstruction &decodedInst) {
     export_pc();
     constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
-    ///////////////////////
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    /* method is already resolved in trace-based JIT */
-    int mIndex = currentMethod->clazz->pDvmDex->pResMethods[tmp]->methodIndex;
-    const Method *calleeMethod =
-        currentMethod->clazz->super->vtable[mIndex];
-    move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
-    if(isRange) {
-        common_invokeMethodRange(convertCalleeToType(calleeMethod));
-    }
-    else {
-        common_invokeMethodNoRange(convertCalleeToType(calleeMethod));
-    }
-    ///////////////////////////////
-    return 0;
+        ///////////////////////
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+#if !defined(WITH_JIT)
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        get_res_methods(3, false);
+        //LR[4] = vB*4(LR[3])
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
+        //cmp $0, LR[4]
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+
+        conditional_jump(Condition_NE, ".LinvokeSuper_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call(".virtual_method_resolve"); //in %eax
+        transferToState(1);
+        insertLabel(".LinvokeSuper_resolved", true);
+        scratchRegs[0] = PhysicalReg_SCRATCH_3; scratchRegs[1] = PhysicalReg_SCRATCH_4;
+        get_glue_method_class(6, false);
+        move_mem_to_reg(OpndSize_32, offClassObject_super, 6, false, 7, false);
+        movez_mem_to_reg(OpndSize_16, offMethod_methodIndex, PhysicalReg_EAX, true, 8, false);
+        compare_mem_reg(OpndSize_32, offClassObject_vtableCount, 7, false, 8, false);
+
+        conditional_jump_global_API(Condition_NC, ".invoke_super_nsm", false);
+        move_mem_to_reg(OpndSize_32, offClassObject_vtable, 7, false, 9, false);
+        move_mem_scale_to_reg(OpndSize_32, 9, false, 8, false, 4, PhysicalReg_ECX, true);
+        const Method *calleeMethod = NULL;
+#else
+        /* method is already resolved in trace-based JIT */
+        int mIndex = currentMethod->clazz->pDvmDex->
+                pResMethods[tmp]->methodIndex;
+        const Method *calleeMethod =
+                currentMethod->clazz->super->vtable[mIndex];
+        move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
+#endif
+        if(isRange) {
+            common_invokeMethodRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+        else {
+            common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+ return 0;
 }
 #undef PP_GPR_1
 #undef PP_GPR_2
@@ -192,26 +229,47 @@ int invoke_super_nsm() {
 //! common section to lower INVOKE_DIRECT
 
 //! It will use helper function if the switch is on
-int common_invoke_direct(bool isRange, u2 tmp, u2 vD) {
+int common_invoke_direct(bool isRange, u2 tmp, u2 vD,
+        const DecodedInstruction &decodedInst) {
     //%ecx can be used as scratch when calling export_pc, get_res_methods and resolve_method
     export_pc();
     constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
-    ////////////////////////////////////
-    get_virtual_reg(vD, OpndSize_32, 5, false);
-    simpleNullCheck(5, false, vD);
-    /* method is already resolved in trace-based JIT */
-    const Method *calleeMethod =
-        currentMethod->clazz->pDvmDex->pResMethods[tmp];
-    move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
-    //%ecx passed to common_invokeMethod...
-    if(isRange) {
-        common_invokeMethodRange(convertCalleeToType(calleeMethod));
-    }
-    else {
-        common_invokeMethodNoRange(convertCalleeToType(calleeMethod));
-    }
-    ////////////////////////////
+#if !defined(WITH_JIT)
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+        get_res_methods(3, false);
+        //LR[4] = vB*4(LR[3])
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_ECX, true);
+#endif
+        get_virtual_reg(vD, OpndSize_32, 5, false);
+        simpleNullCheck(5, false, vD);
+#if !defined(WITH_JIT)
+        //cmp $0, LR[4]
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_ECX, true);
+        conditional_jump(Condition_NE, ".LinvokeDirect_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call_helper_API(".direct_method_resolve"); //in %eax
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
+        transferToState(1);
+        insertLabel(".LinvokeDirect_resolved", true);
+        const Method *calleeMethod = NULL;
+#else
+        /* method is already resolved in trace-based JIT */
+        const Method *calleeMethod =
+                currentMethod->clazz->pDvmDex->pResMethods[tmp];
+        move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
+#endif
+        //%ecx passed to common_invokeMethod...
+        if(isRange) {
+            common_invokeMethodRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+        else {
+            common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
     return 0;
 }
 #undef P_GPR_1
@@ -231,24 +289,43 @@ int common_invoke_direct(bool isRange, u2 tmp, u2 vD) {
 //! common section to lower INVOKE_STATIC
 
 //! It will use helper function if the switch is on
-int common_invoke_static(bool isRange, u2 tmp) {
+int common_invoke_static(bool isRange, u2 tmp,
+        const DecodedInstruction &decodedInst) {
     //%ecx can be used as scratch when calling export_pc, get_res_methods and resolve_method
     export_pc();
     constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
-    ////////////////////////////
-    /* method is already resolved in trace-based JIT */
-    const Method *calleeMethod =
-        currentMethod->clazz->pDvmDex->pResMethods[tmp];
-    move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
-    //%ecx passed to common_invokeMethod...
-    if(isRange) {
-        common_invokeMethodRange(convertCalleeToType(calleeMethod));
-    }
-    else {
-        common_invokeMethodNoRange(convertCalleeToType(calleeMethod));
-    }
-    ////////////////////////
+#if !defined(WITH_JIT)
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null;      scratchRegs[3] = PhysicalReg_Null;
+        get_res_methods(3, false);
+        //LR[4] = vB*4(LR[3])
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_ECX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_ECX, true);
+
+        conditional_jump(Condition_NE, ".LinvokeStatic_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call(".static_method_resolve"); //in %eax
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
+        transferToState(1);
+        insertLabel(".LinvokeStatic_resolved", true);
+        const Method *calleeMethod = NULL;
+#else
+        /* method is already resolved in trace-based JIT */
+        const Method *calleeMethod =
+                currentMethod->clazz->pDvmDex->pResMethods[tmp];
+        move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
+#endif
+        //%ecx passed to common_invokeMethod...
+        if(isRange) {
+            common_invokeMethodRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+        else {
+            common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
     return 0;
 }
 #undef P_GPR_1
@@ -268,14 +345,15 @@ int common_invoke_static(bool isRange, u2 tmp) {
 //! common section to lower INVOKE_INTERFACE
 
 //! It will use helper function if the switch is on
-int common_invoke_interface(bool isRange, u2 tmp, u2 vD) {
+int common_invoke_interface(bool isRange, u2 tmp, u2 vD,
+        const DecodedInstruction &decodedInst) {
 #ifdef WITH_JIT_INLINING
     /*
      * If the invoke has non-null misPredBranchOver, we need to generate
      * the non-inlined version of the invoke here to handle the
      * mispredicted case.
      */
-    if (traceCurrentMIR->meta.callsiteInfo->misPredBranchOver) {
+    if (mir->meta.callsiteInfo->misPredBranchOver) {
         genLandingPadForMispredictedCallee(); //cUnit, mir, bb, labelList);
     }
 #endif
@@ -312,7 +390,8 @@ int common_invoke_interface(bool isRange, u2 tmp, u2 vD) {
         common_invokeMethodNoRange(ArgsDone_Full);
     }
 #else
-    gen_predicted_chain(isRange, tmp, -1, true /*interface*/, 1/*tmp1*/);
+        gen_predicted_chain(isRange, tmp, -1, true /*interface*/, 1/*tmp1*/,
+                decodedInst);
 #endif
     ///////////////////////
     return 0;
@@ -328,200 +407,217 @@ int common_invoke_interface(bool isRange, u2 tmp, u2 vD) {
 //! lower bytecode INVOKE_VIRTUAL by calling common_invoke_virtual
 
 //!
-int op_invoke_virtual() {
+int op_invoke_virtual(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
-    //B|A|op CCCC G|F|E|D
-    //D: the first argument, which is the "this" pointer
-    //B: argument count
-    //D,E,F,G,A: arguments
-    u2 vD = FETCH(2) & 0xf;
-    u2 tmp = FETCH(1); //method index
-    int retval = common_invoke_virtual(false/*not range*/, tmp, vD);
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    u2 vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vC */
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_virtual_nohelper(false/*not range*/, tmp, vD,
+            mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_SUPER by calling common_invoke_super
 
 //!
-int op_invoke_super() {
+int op_invoke_super(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
-    //B|A|op CCCC G|F|E|D
-    //D: the first argument
-    //B: argument count
-    //D,E,F,G,A: arguments
-    u2 tmp = FETCH(1); //method index
-    int retval = common_invoke_super(false/*not range*/, tmp);
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_super(false/*not range*/, tmp, mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_DIRECT by calling common_invoke_direct
 
 //!
-int op_invoke_direct() {
+int op_invoke_direct(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_DIRECT);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
-    //B|A|op CCCC G|F|E|D
-    //D: the first argument, which is the "this" pointer
-    //B: argument count
-    //D,E,F,G,A: arguments
-    u2 vD = FETCH(2) & 0xf;
-    u2 tmp = FETCH(1); //method index
-    int retval = common_invoke_direct(false/*not range*/, tmp, vD);
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    u2 vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vC */
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_direct(false/*not range*/, tmp, vD,
+            mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_STATIC by calling common_invoke_static
 
 //!
-int op_invoke_static() {
+int op_invoke_static(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_STATIC);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
-    //B|A|op CCCC G|F|E|D
-    //D: the first argument
-    //B: argument count
-    //D,E,F,G,A: arguments
-    u2 tmp = FETCH(1); //method index
-    int retval = common_invoke_static(false/*not range*/, tmp);
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_static(false/*not range*/, tmp, mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_INTERFACE by calling common_invoke_interface
 
 //!
-int op_invoke_interface() {
+int op_invoke_interface(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_INTERFACE);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
-    //B|A|op CCCC G|F|E|D
-    //D: the first argument, which is the "this" pointer
-    //B: argument count
-    //D,E,F,G,A: arguments
-    u2 vD = FETCH(2) & 0xf;
-    u2 tmp = FETCH(1); //method index
-    int retval = common_invoke_interface(false/*not range*/, tmp, vD);
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    u2 vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vC */
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_interface(false/*not range*/, tmp, vD,
+            mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_VIRTUAL_RANGE by calling common_invoke_virtual
 
 //!
-int op_invoke_virtual_range() {
+int op_invoke_virtual_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL_RANGE);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
     //AA|op BBBB CCCC
     //CCCC: the first argument, which is the "this" pointer
     //AA: argument count
-    u2 tmp = FETCH(1); //BBBB, method index
-    u2 vD = FETCH(2); //the first argument
-    int retval = common_invoke_virtual(true/*range*/, tmp, vD);
+    u2 vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vCCCC */
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_virtual_nohelper(true/*range*/, tmp, vD,
+            mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_SUPER_RANGE by calling common_invoke_super
 
 //!
-int op_invoke_super_range() {
+int op_invoke_super_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER_RANGE);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
-    u2 tmp = FETCH(1); //BBBB, method index
-    int retval = common_invoke_super(true/*range*/, tmp);
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_super(true/*range*/, tmp, mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_DIRECT_RANGE by calling common_invoke_direct
 
 //!
-int op_invoke_direct_range() {
+int op_invoke_direct_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_DIRECT_RANGE);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
-    u2 tmp = FETCH(1); //BBBB, method index
-    u2 vD = FETCH(2); //the first argument
-    int retval = common_invoke_direct(true/*range*/, tmp, vD);
+    u2 vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vCCCC */
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_direct(true/*range*/, tmp, vD, mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_STATIC_RANGE by calling common_invoke_static
 
 //!
-int op_invoke_static_range() {
+int op_invoke_static_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_STATIC_RANGE);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
-    u2 tmp = FETCH(1); //BBBB, method index
-    int retval = common_invoke_static(true/*range*/, tmp);
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_static(true/*range*/, tmp, mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_INTERFACE_RANGE by calling common_invoke_interface
 
 //!
-int op_invoke_interface_range() {
+int op_invoke_interface_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_INTERFACE_RANGE);
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
 #endif
-    u2 tmp = FETCH(1); //BBBB, method index
-    u2 vD = FETCH(2); //the first argument
-    int retval = common_invoke_interface(true/*range*/, tmp, vD);
+    u2 vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vCCCC */
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_interface(true/*range*/, tmp, vD,
+            mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 
@@ -535,17 +631,21 @@ int op_invoke_interface_range() {
 //! pass the arguments for invoking method without range
 
 //!
-int common_invokeMethodNoRange_noJmp() {
+int common_invokeMethodNoRange_noJmp(const DecodedInstruction &decodedInst) {
 #if defined VTUNE_DALVIK
     int startStreamPtr = (int)stream;
 #endif
 
-    u2 count = INST_B(inst);
-    u2 vD = FETCH(2) & 0xf;
-    u2 vE = (FETCH(2) >> 4) & 0xf;
-    u2 vF = (FETCH(2) >> 8) & 0xf;
-    u2 vG = (FETCH(2) >> 12) & 0xf;
-    u2 vA = INST_A(inst); //5th argument
+    u2 count = decodedInst.vA;
+
+    // Please note that vA, vD, vE, vF, and vG might have
+    // invalid values. Check variable "count" before accessing.
+    u2 vD = decodedInst.arg[0];
+    u2 vE = decodedInst.arg[1];
+    u2 vF = decodedInst.arg[2];
+    u2 vG = decodedInst.arg[3];
+    u2 vA = decodedInst.arg[4];
+
     int offsetFromSaveArea = -4;
     if(count == 5) {
         get_virtual_reg(vA, OpndSize_32, 22, false);
@@ -620,8 +720,8 @@ int common_invokeMethod_Jmp(ArgsDoneType form) {
     return 0;
 }
 
-int common_invokeMethodNoRange(ArgsDoneType form) {
-    common_invokeMethodNoRange_noJmp();
+int common_invokeMethodNoRange(ArgsDoneType form, const DecodedInstruction &decodedInst) {
+    common_invokeMethodNoRange_noJmp(decodedInst);
     common_invokeMethod_Jmp(form);
     return 0;
 }
@@ -650,14 +750,14 @@ int common_invokeMethodNoRange(ArgsDoneType form) {
 //! pass the arguments for invoking method with range
 
 //! loop is unrolled when count <= 10
-int common_invokeMethodRange_noJmp() {
-
+int common_invokeMethodRange_noJmp(const DecodedInstruction &decodedInst) {
 #if defined VTUNE_DALVIK
     int startStreamPtr = (int)stream;
 #endif
 
-    u2 count = INST_AA(inst);
-    u2 vD = FETCH(2); //the first argument
+    u2 count = decodedInst.vA;
+    u2 vD = decodedInst.vC; //the first argument
+
     savearea_from_fp(21, false);
     //vD to rFP-4*count-20
     //vD+1 to rFP-4*count-20+4 = rFP-20-4*(count-1)
@@ -732,8 +832,8 @@ int common_invokeMethodRange_noJmp() {
     return 0;
 }
 
-int common_invokeMethodRange(ArgsDoneType form) {
-    common_invokeMethodRange_noJmp();
+int common_invokeMethodRange(ArgsDoneType form, const DecodedInstruction &decodedInst) {
+    common_invokeMethodRange_noJmp(decodedInst);
     common_invokeMethod_Jmp(form);
     return 0;
 }
@@ -1004,20 +1104,22 @@ void generate_stackOverflow() {
 //! lower bytecode EXECUTE_INLINE
 
 //!
-int op_execute_inline(bool isRange) {
-    //tmp, vC, vD, vE, vF
-    int num;
-    if(!isRange) num = INST_B(inst);
-    else num = INST_AA(inst);
-    u2 tmp = FETCH(1);
+int op_execute_inline(const MIR * mir, bool isRange) {
+    assert(mir->dalvikInsn.opcode == OP_EXECUTE_INLINE
+            || mir->dalvikInsn.opcode == OP_EXECUTE_INLINE_RANGE);
+    int num = mir->dalvikInsn.vA;
+    u2 tmp = mir->dalvikInsn.vB;
     u2 vC, vD, vE, vF;
     if(!isRange) {
-        vC = FETCH(2) & 0xf;
-        vD = (FETCH(2) >> 4) & 0xf;
-        vE = (FETCH(2) >> 8) & 0xf;
-        vF = FETCH(2) >> 12;
+        // Note that vC, vD, vE, and vF might have bad values
+        // depending on count. The variable "num" should be
+        // checked before using any of these.
+        vC = mir->dalvikInsn.arg[0];
+        vD = mir->dalvikInsn.arg[1];
+        vE = mir->dalvikInsn.arg[2];
+        vF = mir->dalvikInsn.arg[3];
     } else {
-        vC = FETCH(2);
+        vC = mir->dalvikInsn.vC;
         vD = vC + 1;
         vE = vC + 2;
         vF = vC + 3;
@@ -1240,7 +1342,6 @@ int op_execute_inline(bool isRange) {
     scratchRegs[0] = PhysicalReg_SCRATCH_1;
     jumpToExceptionThrown(1/*exception number*/);
     insertLabel(".execute_inline_done", true);
-    rPC += 3;
     return 0;
 }
 #undef P_GPR_1
@@ -1250,12 +1351,6 @@ int op_execute_inline(bool isRange) {
 #undef P_SCRATCH_3
 #undef P_SCRATCH_4
 
-//! lower bytecode INVOKE_OBJECT_INIT_RANGE
-
-//!
-int op_invoke_object_init_range() {
-    return -1;
-}
 
 #define P_GPR_1 PhysicalReg_EBX
 #define P_SCRATCH_1 PhysicalReg_ESI
@@ -1267,17 +1362,18 @@ int op_invoke_object_init_range() {
 //! common code for INVOKE_VIRTUAL_QUICK
 
 //! It uses helper function if the switch is on
-int common_invoke_virtual_quick(bool hasRange, u2 vD, u2 IMMC) {
+int common_invoke_virtual_quick(bool hasRange, u2 vD, u2 IMMC,
+        const DecodedInstruction &decodedInst) {
 #ifdef WITH_JIT_INLINING
     /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return false;
     /*
      * If the invoke has non-null misPredBranchOver, we need to generate
      * the non-inlined version of the invoke here to handle the
      * mispredicted case.
      */
-    if (traceCurrentMIR->meta.callsiteInfo->misPredBranchOver) {
+    if (mir->meta.callsiteInfo->misPredBranchOver) {
         genLandingPadForMispredictedCallee(); //cUnit, mir, bb, labelList);
     }
 #endif
@@ -1299,7 +1395,7 @@ int common_invoke_virtual_quick(bool hasRange, u2 vD, u2 IMMC) {
         common_invokeMethodNoRange(ArgsDone_Full);
     }
 #else
-    gen_predicted_chain(hasRange, -1, IMMC, false, 1/*tmp1*/);
+    gen_predicted_chain(hasRange, -1, IMMC, false, 1/*tmp1*/, decodedInst);
 #endif
     ////////////////////////
     return 0;
@@ -1314,27 +1410,27 @@ int common_invoke_virtual_quick(bool hasRange, u2 vD, u2 IMMC) {
 //! lower bytecode INVOKE_VIRTUAL_QUICK by calling common_invoke_virtual_quick
 
 //!
-int op_invoke_virtual_quick() {
-    u2 vD = FETCH(2) & 0xf;
-    u2 IMMC = 4*FETCH(1);
-    int retval = common_invoke_virtual_quick(false, vD, IMMC);
+int op_invoke_virtual_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL_QUICK);
+    u2 vD = mir->dalvikInsn.vC;
+    u2 IMMC = 4 * mir->dalvikInsn.vB;
+    int retval = common_invoke_virtual_quick(false, vD, IMMC, mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_VIRTUAL_QUICK_RANGE by calling common_invoke_virtual_quick
 
 //!
-int op_invoke_virtual_quick_range() {
-    u2 vD = FETCH(2);
-    u2 IMMC = 4*FETCH(1);
-    int retval = common_invoke_virtual_quick(true, vD, IMMC);
+int op_invoke_virtual_quick_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL_QUICK_RANGE);
+    u2 vD = mir->dalvikInsn.vC;
+    u2 IMMC = 4 * mir->dalvikInsn.vB;
+    int retval = common_invoke_virtual_quick(true, vD, IMMC, mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 #define P_GPR_1 PhysicalReg_EBX
@@ -1343,7 +1439,8 @@ int op_invoke_virtual_quick_range() {
 //! common code to lower INVOKE_SUPER_QUICK
 
 //!
-int common_invoke_super_quick(bool hasRange, u2 vD, u2 IMMC) {
+int common_invoke_super_quick(bool hasRange, u2 vD, u2 IMMC,
+        const DecodedInstruction &decodedInst) {
     export_pc();
     constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
@@ -1355,10 +1452,12 @@ int common_invoke_super_quick(bool hasRange, u2 vD, u2 IMMC) {
     const Method *calleeMethod = currentMethod->clazz->super->vtable[mIndex];
     move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
     if(hasRange) {
-        common_invokeMethodRange(convertCalleeToType(calleeMethod));
+        common_invokeMethodRange(convertCalleeToType(calleeMethod),
+                decodedInst);
     }
     else {
-        common_invokeMethodNoRange(convertCalleeToType(calleeMethod));
+        common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
+                decodedInst);
     }
     return 0;
 }
@@ -1368,27 +1467,27 @@ int common_invoke_super_quick(bool hasRange, u2 vD, u2 IMMC) {
 //! lower bytecode INVOKE_SUPER_QUICK by calling common_invoke_super_quick
 
 //!
-int op_invoke_super_quick() {
-    u2 vD = FETCH(2) & 0xf;
-    u2 IMMC = 4*FETCH(1);
-    int retval = common_invoke_super_quick(false, vD, IMMC);
+int op_invoke_super_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER_QUICK);
+    u2 vD = mir->dalvikInsn.vC;
+    u2 IMMC = 4 * mir->dalvikInsn.vB;
+    int retval = common_invoke_super_quick(false, vD, IMMC, mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 //! lower bytecode INVOKE_SUPER_QUICK_RANGE by calling common_invoke_super_quick
 
 //!
-int op_invoke_super_quick_range() {
-    u2 vD = FETCH(2);
-    u2 IMMC = 4*FETCH(1);
-    int retval = common_invoke_super_quick(true, vD, IMMC);
+int op_invoke_super_quick_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER_QUICK_RANGE);
+    u2 vD = mir->dalvikInsn.vC;
+    u2 IMMC = 4 * mir->dalvikInsn.vB;
+    int retval = common_invoke_super_quick(true, vD, IMMC, mir->dalvikInsn);
 #if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
     insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
 #endif
-    rPC += 3;
     return retval;
 }
 /////// code to predict the callee method for invoke_virtual & invoke_interface
@@ -1605,8 +1704,9 @@ void predicted_chain_virtual_O1(u2 IMMC) {
 
 static int invokeChain_inst = 0;
 /* object "this" is in %ebx */
-void gen_predicted_chain_O0(bool isRange, u2 tmp, int IMMC, bool isInterface, int inputReg) {
-    ALOGI("TODO predicted_chain_O0");
+void gen_predicted_chain_O0(bool isRange, u2 tmp, int IMMC, bool isInterface,
+        int inputReg, const DecodedInstruction &decodedInst) {
+    LOGI("TODO predicted_chain_O0");
 
     /* get current class object */
     move_mem_to_reg(OpndSize_32, offObject_clazz, PhysicalReg_EBX, true,
@@ -1674,10 +1774,10 @@ void gen_predicted_chain_O0(bool isRange, u2 tmp, int IMMC, bool isInterface, in
 #endif
 
     if(isRange) {
-        common_invokeMethodRange(ArgsDone_Full);
+        common_invokeMethodRange(ArgsDone_Full, decodedInst);
     }
     else {
-        common_invokeMethodNoRange(ArgsDone_Full);
+        common_invokeMethodNoRange(ArgsDone_Full, decodedInst);
     }
 
     insertLabel(".invokeChain", true);
@@ -1694,15 +1794,16 @@ void gen_predicted_chain_O0(bool isRange, u2 tmp, int IMMC, bool isInterface, in
 #endif
 
     if(isRange) {
-        common_invokeMethodRange(ArgsDone_Normal);
+        common_invokeMethodRange(ArgsDone_Normal, decodedInst);
     }
     else {
-        common_invokeMethodNoRange(ArgsDone_Normal);
+        common_invokeMethodNoRange(ArgsDone_Normal, decodedInst);
     }
 }
 
 /* object "this" is in inputReg: 5 for virtual, 1 for interface, 1 for virtual_quick */
-void gen_predicted_chain_O1(bool isRange, u2 tmp, int IMMC, bool isInterface, int inputReg) {
+void gen_predicted_chain_O1(bool isRange, u2 tmp, int IMMC, bool isInterface,
+        int inputReg, const DecodedInstruction &decodedInst) {
 
     /* get current class object */
     move_mem_to_reg(OpndSize_32, offObject_clazz, inputReg, false,
@@ -1719,8 +1820,8 @@ void gen_predicted_chain_O1(bool isRange, u2 tmp, int IMMC, bool isInterface, in
     move_mem_to_reg(OpndSize_32, offChainingCell_method, 41, false, PhysicalReg_ECX, true);//predicted method
 
     /* update stack with parameters first, then decide the callee */
-    if(isRange) common_invokeMethodRange_noJmp();
-    else common_invokeMethodNoRange_noJmp();
+    if(isRange) common_invokeMethodRange_noJmp(decodedInst);
+    else common_invokeMethodNoRange_noJmp(decodedInst);
 
     /* compare current class object against predicted clazz
        if equal, prediction is still valid, jump to .invokeChain */
@@ -1743,8 +1844,10 @@ void gen_predicted_chain_O1(bool isRange, u2 tmp, int IMMC, bool isInterface, in
     common_invokeMethod_Jmp(ArgsDone_Normal);
 }
 
-void gen_predicted_chain(bool isRange, u2 tmp, int IMMC, bool isInterface, int inputReg) {
-    return gen_predicted_chain_O1(isRange, tmp, IMMC, isInterface, inputReg);
+void gen_predicted_chain(bool isRange, u2 tmp, int IMMC, bool isInterface,
+        int inputReg, const DecodedInstruction &decodedInst) {
+    return gen_predicted_chain_O1(isRange, tmp, IMMC, isInterface, inputReg,
+            decodedInst);
 }
 #undef P_GPR_1
 #undef P_GPR_2
diff --git a/vm/compiler/codegen/x86/LowerJump.cpp b/vm/compiler/codegen/x86/LowerJump.cpp
index c4b6493..18422bc 100644
--- a/vm/compiler/codegen/x86/LowerJump.cpp
+++ b/vm/compiler/codegen/x86/LowerJump.cpp
@@ -1356,48 +1356,74 @@ int throw_exception(int exceptionPtrReg, int immReg,
     return 0;
 }
 
-//! lower bytecode GOTO
-
-//!
-int op_goto() {
+/**
+ * @brief Generate native code for bytecode goto
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_goto(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_GOTO);
+#if !defined(WITH_JIT)
+    u2 tt = INST_AA(inst);
+    s2 tmp = (s2)((s2)tt << 8) >> 8; //00AA --> AA00 --> xxAA
+#else
     s2 tmp = traceCurrentBB->taken->id;
+#endif
     int retval = common_goto(tmp);
-    rPC += 1;
     return retval;
 }
-//! lower bytecode GOTO_16
 
-//!
-int op_goto_16() {
+/**
+ * @brief Generate native code for bytecode goto/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_goto_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_GOTO_16);
+#if !defined(WITH_JIT)
+    s2 tmp = (s2)FETCH(1);
+#else
     s2 tmp = traceCurrentBB->taken->id;
+#endif
     int retval = common_goto(tmp);
-    rPC += 2;
     return retval;
 }
-//! lower bytecode GOTO_32
 
-//!
-int op_goto_32() {
+/**
+ * @brief Generate native code for bytecode goto/32
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_goto_32(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_GOTO_32);
+#if !defined(WITH_JIT)
+    u4 tmp = (u4)FETCH(1);
+    tmp |= (u4)FETCH(2) << 16;
+#else
     s2 tmp = traceCurrentBB->taken->id;
+#endif
     int retval = common_goto((s4)tmp);
-    rPC += 3;
     return retval;
 }
 #define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode PACKED_SWITCH
 
-//!
-int op_packed_switch() {
-    u4 tmp = (u4)FETCH(1);
-    tmp |= (u4)FETCH(2) << 16;
-    u2 vA = INST_AA(inst);
+/**
+ * @brief Generate native code for bytecode packed-switch
+ * @param mir bytecode representation
+ * @param dalvikPC program counter for Dalvik bytecode
+ * @return value >= 0 when handled
+ */
+int op_packed_switch(const MIR * mir, const u2 * dalvikPC) {
+    assert(mir->dalvikInsn.opcode == OP_PACKED_SWITCH);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
 
 #ifdef DEBUG_EACH_BYTECODE
     u2 tSize = 0;
     s4 firstKey = 0;
     s4* entries = NULL;
 #else
-    u2* switchData = rPC + (s4)tmp;
+    u2* switchData = const_cast<u2 *>(dalvikPC) + (s4)tmp;
     if (*switchData++ != kPackedSwitchSignature) {
         /* should have been caught by verifier */
         dvmThrowInternalError(
@@ -1433,7 +1459,7 @@ int op_packed_switch() {
     constVREndOfBB();
     globalVREndOfBB(currentMethod); //update GG VRs
     //get rPC, %eax has the relative PC offset
-    alu_binary_imm_reg(OpndSize_32, add_opc, (int)rPC, PhysicalReg_EAX, true);
+    alu_binary_imm_reg(OpndSize_32, add_opc, (int)dalvikPC, PhysicalReg_EAX, true);
     scratchRegs[0] = PhysicalReg_SCRATCH_2;
 #if defined(WITH_JIT_TUNING)
     /* Fall back to interpreter after resolving address of switch target.
@@ -1443,25 +1469,28 @@ int op_packed_switch() {
     move_imm_to_mem(OpndSize_32, kSwitchOverflow, 0, PhysicalReg_ESP, true);
 #endif
     jumpToInterpNoChain();
-    rPC += 3;
     return 0;
 }
 #undef P_GPR_1
 
 #define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode SPARSE_SWITCH
 
-//!
-int op_sparse_switch() {
-    u4 tmp = (u4)FETCH(1);
-    tmp |= (u4)FETCH(2) << 16;
-    u2 vA = INST_AA(inst);
+/**
+ * @brief Generate native code for bytecode sparse-switch
+ * @param mir bytecode representation
+ * @param dalvikPC program counter for Dalvik bytecode
+ * @return value >= 0 when handled
+ */
+int op_sparse_switch(const MIR * mir, const u2 * dalvikPC) {
+    assert(mir->dalvikInsn.opcode == OP_SPARSE_SWITCH);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
 #ifdef DEBUG_EACH_BYTECODE
     u2 tSize = 0;
     const s4* keys = NULL;
     s4* entries = NULL;
 #else
-    u2* switchData = rPC + (s4)tmp;
+    u2* switchData = const_cast<u2 *>(dalvikPC) + (s4)tmp;
 
     if (*switchData++ != kSparseSwitchSignature) {
         /* should have been caught by verifier */
@@ -1497,7 +1526,7 @@ int op_sparse_switch() {
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     //get rPC, %eax has the relative PC offset
-    alu_binary_imm_reg(OpndSize_32, add_opc, (int)rPC, PhysicalReg_EAX, true);
+    alu_binary_imm_reg(OpndSize_32, add_opc, (int)dalvikPC, PhysicalReg_EAX, true);
     scratchRegs[0] = PhysicalReg_SCRATCH_2;
 #if defined(WITH_JIT_TUNING)
     /* Fall back to interpreter after resolving address of switch target.
@@ -1507,186 +1536,215 @@ int op_sparse_switch() {
     move_imm_to_mem(OpndSize_32, kSwitchOverflow, 0, PhysicalReg_ESP, true);
 #endif
     jumpToInterpNoChain();
-    rPC += 3;
     return 0;
 }
 
 #undef P_GPR_1
 
 #define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode IF_EQ
 
-//!
-int op_if_eq() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s2 tmp = (s2)FETCH(1);
+/**
+ * @brief Generate native code for bytecode if-eq
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_eq(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_EQ);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_NE, Condition_E);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_NE
 
-//!
-int op_if_ne() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s2 tmp = (s2)FETCH(1);
+/**
+ * @brief Generate native code for bytecode if-ne
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_ne(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_NE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_E, Condition_NE);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_LT
 
-//!
-int op_if_lt() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s2 tmp = (s2)FETCH(1);
+/**
+ * @brief Generate native code for bytecode if-lt
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_lt(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_LT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_GE, Condition_L);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_GE
 
-//!
-int op_if_ge() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s2 tmp = (s2)FETCH(1);
+/**
+ * @brief Generate native code for bytecode if-ge
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_ge(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_GE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_L, Condition_GE);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_GT
 
-//!
-int op_if_gt() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s2 tmp = (s2)FETCH(1);
+/**
+ * @brief Generate native code for bytecode if-gt
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_gt(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_GT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_LE, Condition_G);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_LE
 
-//!
-int op_if_le() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    s2 tmp = (s2)FETCH(1);
+/**
+ * @brief Generate native code for bytecode if-le
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_le(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_LE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_G, Condition_LE);
-    rPC += 2;
     return 0;
 }
 #undef P_GPR_1
-//! lower bytecode IF_EQZ
 
-//!
-int op_if_eqz() {
-    u2 vA = INST_AA(inst);
-    s2 tmp = (s2)FETCH(1);
-    compare_imm_VR(OpndSize_32,
-                                  0, vA);
+/**
+ * @brief Generate native code for bytecode if-eqz
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_eqz(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_EQZ);
+    u2 vA = mir->dalvikInsn.vA;
+    s2 tmp = mir->dalvikInsn.vB;
+    compare_imm_VR(OpndSize_32, 0, vA);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_NE, Condition_E);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_NEZ
 
-//!
-int op_if_nez() {
-    u2 vA = INST_AA(inst);
-    s2 tmp = (s2)FETCH(1);
-    compare_imm_VR(OpndSize_32,
-                                  0, vA);
+/**
+ * @brief Generate native code for bytecode if-nez
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_nez(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_NEZ);
+    u2 vA = mir->dalvikInsn.vA;
+    s2 tmp = mir->dalvikInsn.vB;
+    compare_imm_VR(OpndSize_32, 0, vA);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_E, Condition_NE);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_LTZ
 
-//!
-int op_if_ltz() {
-    u2 vA = INST_AA(inst);
-    s2 tmp = (s2)FETCH(1);
-    compare_imm_VR(OpndSize_32,
-                                  0, vA);
+/**
+ * @brief Generate native code for bytecode if-ltz
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_ltz(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_LTZ);
+    u2 vA = mir->dalvikInsn.vA;
+    s2 tmp = mir->dalvikInsn.vB;
+    compare_imm_VR(OpndSize_32, 0, vA);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_GE, Condition_L);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_GEZ
 
-//!
-int op_if_gez() {
-    u2 vA = INST_AA(inst);
-    s2 tmp = (s2)FETCH(1);
-    compare_imm_VR(OpndSize_32,
-                                  0, vA);
+/**
+ * @brief Generate native code for bytecode if-gez
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_gez(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_GEZ);
+    u2 vA = mir->dalvikInsn.vA;
+    s2 tmp = mir->dalvikInsn.vB;
+    compare_imm_VR(OpndSize_32, 0, vA);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_L, Condition_GE);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_GTZ
 
-//!
-int op_if_gtz() {
-    u2 vA = INST_AA(inst);
-    s2 tmp = (s2)FETCH(1);
-    compare_imm_VR(OpndSize_32,
-                                  0, vA);
+/**
+ * @brief Generate native code for bytecode if-gtz
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_gtz(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_GTZ);
+    u2 vA = mir->dalvikInsn.vA;
+    s2 tmp = mir->dalvikInsn.vB;
+    compare_imm_VR(OpndSize_32, 0, vA);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_LE, Condition_G);
-    rPC += 2;
     return 0;
 }
-//! lower bytecode IF_LEZ
 
-//!
-int op_if_lez() {
-    u2 vA = INST_AA(inst);
-    s2 tmp = (s2)FETCH(1);
-    compare_imm_VR(OpndSize_32,
-                                  0, vA);
+/**
+ * @brief Generate native code for bytecode if-lez
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_lez(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_LEZ);
+    u2 vA = mir->dalvikInsn.vA;
+    s2 tmp = mir->dalvikInsn.vB;
+    compare_imm_VR(OpndSize_32, 0, vA);
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
     common_if(tmp, Condition_G, Condition_LE);
-    rPC += 2;
     return 0;
 }
 
diff --git a/vm/compiler/codegen/x86/LowerMove.cpp b/vm/compiler/codegen/x86/LowerMove.cpp
index 2f0b5bc..a2a6183 100644
--- a/vm/compiler/codegen/x86/LowerMove.cpp
+++ b/vm/compiler/codegen/x86/LowerMove.cpp
@@ -24,124 +24,157 @@
 #include "enc_wrapper.h"
 
 #define P_GPR_1 PhysicalReg_EBX
-//! lower bytecode MOVE
 
-//!
-int op_move() {
-    u2 vA, vB;
-    vA = INST_A(inst);
-    vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecodes move and
+ * move-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE
+            || mir->dalvikInsn.opcode == OP_MOVE_OBJECT);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false/*isPhysical*/);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 1;
     return 2;
 }
-//! lower bytecode MOVE_FROM16
 
-//!
-int op_move_from16() {
-    u2 vA, vB;
-    vA = INST_AA(inst);
-    vB = FETCH(1);
+/**
+ * @brief Generate native code for bytecodes move/from16
+ * and move-object/from16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_from16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_FROM16
+            || mir->dalvikInsn.opcode == OP_MOVE_OBJECT_FROM16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 2;
     return 2;
 }
-//! lower bytecode MOVE_16
 
-//!
-int op_move_16() {
-    u2 vA, vB;
-    vA = FETCH(1);
-    vB = FETCH(2);
+/**
+ * @brief Generate native code for bytecodes move/16 and
+ * move-object/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_16
+            || mir->dalvikInsn.opcode == OP_MOVE_OBJECT_16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_32, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 3;
     return 2;
 }
 #undef P_GPR_1
-//! lower bytecode MOVE_WIDE
 
-//!
-int op_move_wide() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
+/**
+ * @brief Generate native code for bytecode move-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_WIDE);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_64, 1, false);
     set_virtual_reg(vA, OpndSize_64, 1, false);
-    rPC += 1;
     return 2;
 }
-//! lower bytecode MOVE_WIDE_FROM16
 
-//!
-int op_move_wide_from16() {
-    u2 vA = INST_AA(inst);
-    u2 vB = FETCH(1);
+/**
+ * @brief Generate native code for bytecode move-wide/from16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_wide_from16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_WIDE_FROM16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_64, 1, false);
     set_virtual_reg(vA, OpndSize_64, 1, false);
-    rPC += 2;
     return 2;
 }
-//! lower bytecode MOVE_WIDE_16
 
-//!
-int op_move_wide_16() {
-    u2 vA = FETCH(1);
-    u2 vB = FETCH(2);
+/**
+ * @brief Generate native code for bytecode move-wide/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_wide_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_WIDE_16);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
     get_virtual_reg(vB, OpndSize_64, 1, false);
     set_virtual_reg(vA, OpndSize_64, 1, false);
-    rPC += 3;
     return 2;
 }
-//! lower bytecode MOVE_RESULT.
 
-//! the return value from bytecode INVOKE is stored in the glue structure
-int op_move_result() {
+/**
+ * @brief Generate native code for bytecodes move-result
+ * and move-result-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_result(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_RESULT
+            || mir->dalvikInsn.opcode == OP_MOVE_RESULT_OBJECT);
 #ifdef WITH_JIT_INLINING
     /* An inlined move result is effectively no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return 0;
 #endif
-    u2 vA = INST_AA(inst);
+    u2 vA = mir->dalvikInsn.vA;
     scratchRegs[0] = PhysicalReg_SCRATCH_1;
     get_return_value(OpndSize_32, 1, false);
     set_virtual_reg(vA, OpndSize_32, 1, false);
-    rPC += 1;
     return 0;
 }
-//! lower bytecode MOVE_RESULT_WIDE.
 
-//! the return value from bytecode INVOKE is stored in the glue structure
-int op_move_result_wide() {
+/**
+ * @brief Generate native code for bytecode move-result-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_result_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_RESULT_WIDE);
 #ifdef WITH_JIT_INLINING
     /* An inlined move result is effectively no-op */
-    if (traceCurrentMIR->OptimizationFlags & MIR_INLINED)
+    if (mir->OptimizationFlags & MIR_INLINED)
         return 0;
 #endif
-    u2 vA = INST_AA(inst);
+    u2 vA = mir->dalvikInsn.vA;
     scratchRegs[0] = PhysicalReg_SCRATCH_1;
     get_return_value(OpndSize_64, 1, false);
     set_virtual_reg(vA, OpndSize_64, 1, false);
-    rPC += 1;
     return 0;
 }
 
 #define P_GPR_1 PhysicalReg_EBX
 #define P_GPR_2 PhysicalReg_ECX
-//!lower bytecode MOVE_RESULT_EXCEPTION
 
-//!update a virtual register with exception from glue structure;
-//!clear the exception from glue structure
-int op_move_exception() {
-    u2 vA = INST_AA(inst);
+/**
+ * @brief Generate native code for bytecode move-exception
+ * @details Updates virtual register with exception from Thread and then
+ * clear the exception from Thread.
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_exception(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_EXCEPTION);
+    u2 vA = mir->dalvikInsn.vA;
     scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
     scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_Null;
     get_self_pointer(2, false);
     move_mem_to_reg(OpndSize_32, offThread_exception, 2, false, 3, false);
     move_imm_to_mem(OpndSize_32, 0, offThread_exception, 2, false);
     set_virtual_reg(vA, OpndSize_32, 3, false);
-    rPC += 1;
     return 0;
 }
 #undef P_GPR_1
diff --git a/vm/compiler/codegen/x86/LowerObject.cpp b/vm/compiler/codegen/x86/LowerObject.cpp
index 67c4044..1a5b348 100644
--- a/vm/compiler/codegen/x86/LowerObject.cpp
+++ b/vm/compiler/codegen/x86/LowerObject.cpp
@@ -173,25 +173,30 @@ int common_check_cast_instance_of(u2 vA, u4 tmp, bool instance, u2 vDest) {
 #undef P_GPR_2
 #undef P_GPR_3
 
-//! LOWER bytecode CHECK_CAST
-
-//!
-int op_check_cast() {
-    u2 vA = INST_AA(inst);
-    u4 tmp = (u4)FETCH(1);
+/**
+ * @brief Generate native code for bytecode check-cast
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_check_cast(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CHECK_CAST);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
     common_check_cast_instance_of(vA, tmp, false, 0);
-    rPC += 2;
     return 0;
 }
-//!LOWER bytecode INSTANCE_OF
 
-//!
-int op_instance_of() {
-    u2 vB = INST_B(inst);
-    u2 vA = INST_A(inst);
-    u4 tmp = (u4)FETCH(1);
+/**
+ * @brief Generate native code for bytecode instance-of
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_instance_of(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INSTANCE_OF);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+    u4 tmp = mir->dalvikInsn.vC;
     common_check_cast_instance_of(vB, tmp, true, vA);
-    rPC += 2;
     return 0;
 }
 
@@ -234,14 +239,32 @@ int monitor_enter_nohelper(u2 vA) {
     /////////////////////////////
     return 0;
 }
-//! lower bytecode MONITOR_ENTER
 
-//! It will use helper function if switch is on
-int op_monitor_enter() {
-    u2 vA = INST_AA(inst);
-    export_pc();
-    monitor_enter_nohelper(vA);
-    rPC += 1;
+/**
+ * @brief Generate native code for bytecode monitor-enter
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_monitor_enter(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MONITOR_ENTER);
+    u2 vA = mir->dalvikInsn.vA;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[11]) {
+        // .monitor_enter_helper
+        //   INPUT: P_GPR_1 (virtual register for object)
+        //   OUTPUT: none
+        //   %esi is live through function monitor_enter_helper
+        export_pc(); //use %edx
+        move_imm_to_reg(OpndSize_32, vA, P_GPR_1, true);
+        spillVirtualReg(vA, LowOpndRegType_gp, true);
+        call_helper_API(".monitor_enter_helper");
+    }
+    else
+#endif
+    {
+        export_pc();
+        monitor_enter_nohelper(vA);
+    }
     return 0;
 }
 #undef P_GPR_1
@@ -249,37 +272,65 @@ int op_monitor_enter() {
 
 #define P_GPR_1 PhysicalReg_EBX
 #define P_GPR_2 PhysicalReg_ECX
-//! lower bytecode MONITOR_EXIT
-
-//! It will use helper function if switch is on
-int op_monitor_exit() {
-    u2 vA = INST_AA(inst);
-    ////////////////////
-    //LOWER bytecode MONITOR_EXIT without helper function
-    //   CALL dvmUnlockObject
-    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    requestVRFreeDelay(vA,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    nullCheck(1, false, 1, vA); //maybe optimized away
-    cancelVRFreeDelayRequest(vA,VRDELAY_NULLCHECK);
-
-    /////////////////////////////
-    //prepare to call dvmUnlockObject, inputs: object reference and self
-    push_reg_to_stack(OpndSize_32, 1, false);
-    push_mem_to_stack(OpndSize_32, offEBP_self, PhysicalReg_EBP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_2;
-    call_dvmUnlockObject();
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
 
-    conditional_jump(Condition_NE, ".unlock_object_done", true);
-    //jump to dvmJitToExceptionThrown
-    scratchRegs[0] = PhysicalReg_SCRATCH_3;
-    jumpToExceptionThrown(2/*exception number*/);
-    insertLabel(".unlock_object_done", true);
-    ///////////////////////////
-    rPC += 1;
+/**
+ * @brief Generate native code for bytecode monitor-exit
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_monitor_exit(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MONITOR_EXIT);
+    u2 vA = mir->dalvikInsn.vA;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[11]) {
+        export_pc();
+        // .moniter_exit_helper
+        //   INPUT: in P_GPR_1 (virtual register for object)
+        //   OUTPUT: none
+        //   %esi is live through function moniter_exit_helper
+        move_imm_to_reg(OpndSize_32, vA, P_GPR_1, true);
+        spillVirtualReg(vA, LowOpndRegType_gp, true);
+        call_helper_API(".monitor_exit_helper");
+    }
+    else
+#endif
+    {
+        ////////////////////
+        //LOWER bytecode MONITOR_EXIT without helper function
+        //   CALL dvmUnlockObject
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+
+        requestVRFreeDelay(vA,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
+        get_virtual_reg(vA, OpndSize_32, 1, false);
+        nullCheck(1, false, 1, vA); //maybe optimized away
+        cancelVRFreeDelayRequest(vA,VRDELAY_NULLCHECK);
+
+        /////////////////////////////
+        //prepare to call dvmUnlockObject, inputs: object reference and self
+        push_reg_to_stack(OpndSize_32, 1, false);
+        push_mem_to_stack(OpndSize_32, offEBP_self, PhysicalReg_EBP, true);
+        scratchRegs[0] = PhysicalReg_SCRATCH_2;
+        call_dvmUnlockObject();
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+#if defined(WITH_JIT)
+        conditional_jump(Condition_NE, ".unlock_object_done", true);
+        //jump to dvmJitToExceptionThrown
+        scratchRegs[0] = PhysicalReg_SCRATCH_3;
+        jumpToExceptionThrown(2/*exception number*/);
+#else
+        //throw exception if dvmUnlockObject returns 0
+        char errName[256];
+        sprintf(errName, "common_exceptionThrown");
+        handlePotentialException(
+                                           Condition_E, Condition_NE,
+                                           2, errName);
+#endif
+        insertLabel(".unlock_object_done", true);
+        ///////////////////////////
+    }
     return 0;
 }
 #undef P_GPR_1
@@ -288,23 +339,42 @@ int op_monitor_exit() {
 #define P_GPR_1 PhysicalReg_EBX
 #define P_GPR_2 PhysicalReg_ECX
 #define P_GPR_3 PhysicalReg_EDX /*vA*/
-//! LOWER bytecode ARRAY_LENGTH
-
-//! It will use helper function if switch is on
-int op_array_length() {
-    u2 vA = INST_A(inst);
-    u2 vB = INST_B(inst);
-    ////////////////////
-    //no usage of helper function
-    requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    nullCheck(1, false, 1, vB); //maybe optimized away
-    cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
-
-    move_mem_to_reg(OpndSize_32, offArrayObject_length, 1, false, 2, false);
-    set_virtual_reg(vA, OpndSize_32, 2, false);
-    ///////////////////////
-    rPC += 1;
+
+/**
+ * @brief Generate native code for bytecode array-length
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_array_length(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ARRAY_LENGTH);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[14]) {
+        // .array_length_helper
+        //   INPUT: P_GPR_1 (virtual register for array object)
+        //          P_GPR_3 (virtual register for length)
+        //   OUTPUT: none
+        //   %eax, %esi, %ebx: live through function array_length_helper
+        export_pc(); //use %edx
+        move_imm_to_reg(OpndSize_32, vA, P_GPR_3, true);
+        move_imm_to_reg(OpndSize_32, vB, P_GPR_1, true);
+        call_helper_API(".array_length_helper");
+    }
+    else
+#endif
+    {
+        ////////////////////
+        //no usage of helper function
+        requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
+        get_virtual_reg(vB, OpndSize_32, 1, false);
+        nullCheck(1, false, 1, vB); //maybe optimized away
+        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
+
+        move_mem_to_reg(OpndSize_32, offArrayObject_length, 1, false, 2, false);
+        set_virtual_reg(vA, OpndSize_32, 2, false);
+        ///////////////////////
+    }
     return 0;
 }
 #undef P_GPR_1
@@ -314,42 +384,108 @@ int op_array_length() {
 #define P_GPR_1 PhysicalReg_EBX
 #define P_GPR_2 PhysicalReg_ECX
 #define P_GPR_3 PhysicalReg_ESI
-//! lower bytecode NEW_INSTANCE
 
-//! It will use helper function if switch is on
-int op_new_instance() {
-    u4 tmp = (u4)FETCH(1);
-    u2 vA = INST_AA(inst);
-    export_pc();
-    /* for trace-based JIT, class is already resolved */
-    ClassObject *classPtr =
-        (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
-    assert(classPtr != NULL);
-    assert(classPtr->status & CLASS_INITIALIZED);
-    /*
-     * If it is going to throw, it should not make to the trace to begin
-     * with.  However, Alloc might throw, so we need to genExportPC()
-     */
-    assert((classPtr->accessFlags & (ACC_INTERFACE|ACC_ABSTRACT)) == 0);
-    //prepare to call dvmAllocObject, inputs: resolved class & flag ALLOC_DONT_TRACK
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    /* 1st argument to dvmAllocObject at -8(%esp) */
-    move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 4, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_3;
-    nextVersionOfHardReg(PhysicalReg_EAX, 3); //next version has 3 refs
-    call_dvmAllocObject();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    //return value of dvmAllocObject is in %eax
-    //if return value is null, throw exception
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_NE, ".new_instance_done", true);
-    //jump to dvmJitToExceptionThrown
-    scratchRegs[0] = PhysicalReg_SCRATCH_4;
-    jumpToExceptionThrown(3/*exception number*/);
+/**
+ * @brief Generate native code for bytecode new-instance
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_new_instance(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEW_INSTANCE);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[4]) {
+        // .new_instance_helper
+        //   INPUT: P_GPR_3 (const pool index)
+        //   OUTPUT: %eax
+        //   no register is live through function array_length_helper
+        export_pc();
+        move_imm_to_reg(OpndSize_32, tmp, P_GPR_3, true);
+        call_helper_API(".new_instance_helper");
+    }
+    else
+#endif
+    {
+        export_pc();
+#if defined(WITH_JIT)
+        /* for trace-based JIT, class is already resolved */
+        ClassObject *classPtr =
+              (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
+        assert(classPtr != NULL);
+        assert(classPtr->status & CLASS_INITIALIZED);
+        /*
+         * If it is going to throw, it should not make to the trace to begin
+         * with.  However, Alloc might throw, so we need to genExportPC()
+        */
+        assert((classPtr->accessFlags & (ACC_INTERFACE|ACC_ABSTRACT)) == 0);
+#else
+        //////////////////////////////////////////
+        //resolve class, check whether it has been resolved
+        //if yes, jump to resolved
+        //if no, call class_resolve
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+        get_res_classes(3, false);
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //resolved class
+        conditional_jump(Condition_NE, ".new_instance_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call_helper_API(".class_resolve");
+        transferToState(1);
+
+        //here, class is resolved
+        insertLabel(".new_instance_resolved", true);
+        //check whether the class is initialized
+        //if yes, jump to initialized
+        //if no, call new_instance_needinit
+        movez_mem_to_reg(OpndSize_8, offClassObject_status, PhysicalReg_EAX, true, 5, false);
+        compare_imm_reg(OpndSize_32, CLASS_INITIALIZED, 5, false);
+        conditional_jump(Condition_E, ".new_instance_initialized", true);
+        rememberState(2);
+        call_helper_API(".new_instance_needinit");
+        transferToState(2);
+        //here, class is already initialized
+        insertLabel(".new_instance_initialized", true);
+        //check whether the class is an interface or abstract, if yes, throw exception
+        move_mem_to_reg(OpndSize_32, offClassObject_accessFlags, PhysicalReg_EAX, true, 6, false);
+        test_imm_reg(OpndSize_32, ACC_INTERFACE|ACC_ABSTRACT, 6, false); //access flags
+
+        //two inputs for common_throw_message: object reference in eax, exception pointer in ecx
+        handlePotentialException(
+                                           Condition_NE, Condition_E,
+                                           2, "common_throw_message");
+#endif
+        //prepare to call dvmAllocObject, inputs: resolved class & flag ALLOC_DONT_TRACK
+        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+#if defined(WITH_JIT)
+        /* 1st argument to dvmAllocObject at -8(%esp) */
+        move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
+#else
+        move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true); //resolved class
+#endif
+        move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 4, PhysicalReg_ESP, true);
+        scratchRegs[0] = PhysicalReg_SCRATCH_3;
+        nextVersionOfHardReg(PhysicalReg_EAX, 3); //next version has 3 refs
+        call_dvmAllocObject();
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        //return value of dvmAllocObject is in %eax
+        //if return value is null, throw exception
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+#if defined(WITH_JIT)
+        conditional_jump(Condition_NE, ".new_instance_done", true);
+        //jump to dvmJitToExceptionThrown
+        scratchRegs[0] = PhysicalReg_SCRATCH_4;
+        jumpToExceptionThrown(3/*exception number*/);
+#else
+        handlePotentialException(
+                                           Condition_E, Condition_NE,
+                                           3, "common_exceptionThrown");
+#endif
+    }
     insertLabel(".new_instance_done", true);
     set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-    rPC += 2;
     return 0;
 }
 
@@ -382,49 +518,96 @@ int new_instance_needinit() {
 #define P_GPR_1 PhysicalReg_EBX //live through C function, must in callee-saved reg
 #define P_GPR_2 PhysicalReg_ECX
 #define P_GPR_3 PhysicalReg_EDX
-//! lower bytecode NEW_ARRAY
-
-//! It will use helper function if switch is on
-int op_new_array() {
-    u4 tmp = (u4)FETCH(1);
-    u2 vA = INST_A(inst); //destination
-    u2 vB = INST_B(inst); //length
-    /////////////////////////
-    //   REGS used: %esi, %eax, P_GPR_1, P_GPR_2
-    //   CALL class_resolve, dvmAllocArrayByClass
-    export_pc(); //use %edx
-    //check size of the array, if negative, throw exception
-    get_virtual_reg(vB, OpndSize_32, 5, false);
-    compare_imm_reg(OpndSize_32, 0, 5, false);
-    handlePotentialException(Condition_S, Condition_NS,
-                             1, "common_errNegArraySize");
-    void *classPtr = (void*)
-        (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
-    assert(classPtr != NULL);
-    //here, class is already resolved, the class object is in %eax
-    //prepare to call dvmAllocArrayByClass with inputs: resolved class, array length, flag ALLOC_DONT_TRACK
-    insertLabel(".new_array_resolved", true);
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    /* 1st argument to dvmAllocArrayByClass at 0(%esp) */
-    move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 5, false, 4, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 8, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_3;
-    nextVersionOfHardReg(PhysicalReg_EAX, 3); //next version has 3 refs
-    call_dvmAllocArrayByClass();
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
 
-    //the allocated object is in %eax
-    //check whether it is null, throw exception if null
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_NE, ".new_array_done", true);
-    //jump to dvmJitToExceptionThrown
-    scratchRegs[0] = PhysicalReg_SCRATCH_4;
-    jumpToExceptionThrown(2/*exception number*/);
-    insertLabel(".new_array_done", true);
-    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-    //////////////////////////////////////
-    rPC += 2;
+/**
+ * @brief Generate native code for bytecode new-array
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_new_array(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEW_ARRAY);
+    u2 vA = mir->dalvikInsn.vA; //destination
+    u2 vB = mir->dalvikInsn.vB; //length
+    u4 tmp = mir->dalvikInsn.vC;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[17]) {
+        // .new_array_helper
+        //   INPUT: P_GPR_3 (const pool index)
+        //          P_GPR_1 (virtual register with size of the array)
+        //   OUTPUT: %eax
+        //   no reg is live through function new_array_helper
+        export_pc(); //use %edx
+        move_imm_to_reg(OpndSize_32, tmp, P_GPR_3, true);
+        move_imm_to_reg(OpndSize_32, vB, P_GPR_1, true);
+        spillVirtualReg(vB, LowOpndRegType_gp, true);
+        call_helper_API(".new_array_helper");
+        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+    }
+    else
+#endif
+    {
+        /////////////////////////
+        //   REGS used: %esi, %eax, P_GPR_1, P_GPR_2
+        //   CALL class_resolve, dvmAllocArrayByClass
+        export_pc(); //use %edx
+        //check size of the array, if negative, throw exception
+        get_virtual_reg(vB, OpndSize_32, 5, false);
+        compare_imm_reg(OpndSize_32, 0, 5, false);
+        handlePotentialException(
+                                           Condition_S, Condition_NS,
+                                           1, "common_errNegArraySize");
+#if defined(WITH_JIT)
+       void *classPtr = (void*)
+            (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
+       assert(classPtr != NULL);
+#else
+        //try to resolve class, if already resolved, jump to resolved
+        //if not, call class_resolve
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+        get_res_classes(3, false);
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+        conditional_jump(Condition_NE, ".new_array_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call_helper_API(".class_resolve");
+        transferToState(1);
+#endif
+        //here, class is already resolved, the class object is in %eax
+        //prepare to call dvmAllocArrayByClass with inputs: resolved class, array length, flag ALLOC_DONT_TRACK
+        insertLabel(".new_array_resolved", true);
+        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+#if defined(WITH_JIT)
+        /* 1st argument to dvmAllocArrayByClass at 0(%esp) */
+        move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
+#else
+        move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
+#endif
+        move_reg_to_mem(OpndSize_32, 5, false, 4, PhysicalReg_ESP, true);
+        move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 8, PhysicalReg_ESP, true);
+        scratchRegs[0] = PhysicalReg_SCRATCH_3;
+        nextVersionOfHardReg(PhysicalReg_EAX, 3); //next version has 3 refs
+        call_dvmAllocArrayByClass();
+        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+        //the allocated object is in %eax
+        //check whether it is null, throw exception if null
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+#if defined(WITH_JIT)
+        conditional_jump(Condition_NE, ".new_array_done", true);
+        //jump to dvmJitToExceptionThrown
+        scratchRegs[0] = PhysicalReg_SCRATCH_4;
+        jumpToExceptionThrown(2/*exception number*/);
+#else
+        handlePotentialException(
+                                           Condition_E, Condition_NE,
+                                           2, "common_exceptionThrown");
+#endif
+        insertLabel(".new_array_done", true);
+        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+        //////////////////////////////////////
+    }
     return 0;
 }
 #undef P_GPR_1
@@ -506,18 +689,26 @@ int common_filled_new_array(u2 length, u4 tmp, bool hasRange) {
     set_return_value(OpndSize_32, PhysicalReg_EAX, true);
     return 0;
 }
-//! LOWER bytecode FILLED_NEW_ARRAY
 
-//!
-int op_filled_new_array() {
-    u2 length = INST_B(inst);
-    u4 tmp = (u4)FETCH(1);
-    u2 v5 = INST_A(inst);
-    u2 vv = FETCH(2);
-    u2 v1 = vv & 0xf;
-    u2 v2 = (vv >> 4) & 0xf;
-    u2 v3 = (vv >> 8) & 0xf;
-    u2 v4 = (vv >> 12) & 0xf;
+/**
+ * @brief Generate native code for bytecode filled-new-array
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_filled_new_array(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FILLED_NEW_ARRAY);
+    u2 length = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+    u2 v1, v2, v3, v4, v5;
+
+    // Note that v1, v2, v3, v4, and/or v5 may not be valid.
+    // Always check "length" before using any of them.
+    v5 = mir->dalvikInsn.arg[4];
+    v4 = mir->dalvikInsn.arg[3];
+    v3 = mir->dalvikInsn.arg[2];
+    v2 = mir->dalvikInsn.arg[1];
+    v1 = mir->dalvikInsn.arg[0];
+
     common_filled_new_array(length, tmp, false);
     if(length >= 1) {
         //move from virtual register to contents of array object
@@ -544,7 +735,6 @@ int op_filled_new_array() {
         get_virtual_reg(v5, OpndSize_32, 11, false);
         move_reg_to_mem(OpndSize_32, 11, false, offArrayObject_contents+16, PhysicalReg_EAX, true);
     }
-    rPC += 3;
     return 0;
 }
 //! function to handle the error of array not implemented
@@ -560,13 +750,17 @@ int filled_new_array_notimpl() {
 }
 
 #define P_SCRATCH_1 PhysicalReg_EDX
-//! LOWER bytecode FILLED_NEW_ARRAY_RANGE
 
-//!
-int op_filled_new_array_range() {
-    u2 length = INST_AA(inst);
-    u4 tmp = (u4)FETCH(1);
-    u4 vC = (u4)FETCH(2);
+/**
+ * @brief Generate native code for bytecode filled-new-array/range
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_filled_new_array_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FILLED_NEW_ARRAY_RANGE);
+    u2 length = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+    u4 vC = mir->dalvikInsn.vC;
     common_filled_new_array(length, tmp, true/*hasRange*/);
     //here, %eax points to the array object
     if(length >= 1) {
@@ -593,7 +787,6 @@ int op_filled_new_array_range() {
         //jump back to the loop start
         conditional_jump(Condition_NS, ".filled_new_array_range_loop1", true);
     }
-    rPC += 3;
     return 0;
 }
 #undef P_GPR_1
@@ -602,14 +795,18 @@ int op_filled_new_array_range() {
 #undef P_SCRATCH_1
 
 #define P_GPR_1 PhysicalReg_EBX
-//! LOWER bytecode FILL_ARRAY_DATA
-
-//!use 1 GPR and scratch regs (export_pc dvmInterpHandleFillArrayData)
-//!CALL: dvmInterpHandleFillArrayData
-int op_fill_array_data() {
-    u2 vA = INST_AA(inst);
-    u4 tmp = (u4)FETCH(1);
-    tmp |= (u4)FETCH(2) << 16;
+
+/**
+ * @brief Generate native code for bytecode fill-array-data
+ * @details Calls dvmInterpHandleFillArrayData
+ * @param mir bytecode representation
+ * @param dalvikPC program counter for Dalvik bytecode
+ * @return value >= 0 when handled
+ */
+int op_fill_array_data(const MIR * mir, const u2 * dalvikPC) {
+    assert(mir->dalvikInsn.opcode == OP_FILL_ARRAY_DATA);
+    u2 vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
     scratchRegs[0] = PhysicalReg_SCRATCH_1;
     scratchRegs[1] = PhysicalReg_Null;
     scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
@@ -618,7 +815,7 @@ int op_fill_array_data() {
     load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
     move_reg_to_mem(OpndSize_32, 1, false, 0, PhysicalReg_ESP, true);
     /* 2nd argument to dvmInterpHandleFillArrayData at 4(%esp) */
-    move_imm_to_mem(OpndSize_32, (int)(rPC+tmp), 4, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, (int)(dalvikPC+tmp), 4, PhysicalReg_ESP, true);
     call_dvmInterpHandleFillArrayData();
     load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
 
@@ -629,17 +826,20 @@ int op_fill_array_data() {
     scratchRegs[0] = PhysicalReg_SCRATCH_2;
     jumpToExceptionThrown(2/*exception number*/);
     insertLabel(".fill_array_data_done", true);
-    rPC += 3;
     return 0;
 }
 #undef P_GPR_1
 
 #define P_GPR_1 PhysicalReg_EBX
-//! LOWER bytecode THROW
 
-//!
-int op_throw() {
-    u2 vA = INST_AA(inst);
+/**
+ * @brief Generate native code for bytecode throw
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_throw(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_THROW);
+    u2 vA = mir->dalvikInsn.vA;
     export_pc();
     get_virtual_reg(vA, OpndSize_32, 1, false);
     //null check
@@ -650,18 +850,20 @@ int op_throw() {
     scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
     set_exception(1, false);
     unconditional_jump("common_exceptionThrown", false);
-    rPC += 1;
     return 0;
 }
 #undef P_GPR_1
 #define P_GPR_1 PhysicalReg_EBX
-//! LOWER bytecode THROW_VERIFICATION_ERROR
 
-//! op AA, ref@BBBB
-int op_throw_verification_error() {
-    u2 vA, vB;
-    vA = INST_AA(inst);
-    vB = FETCH(1);
+/**
+ * @brief Generate native code for bytecode throw-verification-error
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_throw_verification_error(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_THROW_VERIFICATION_ERROR);
+    u2 vA = mir->dalvikInsn.vA;
+    u2 vB = mir->dalvikInsn.vB;
 
     export_pc();
     scratchRegs[0] = PhysicalReg_SCRATCH_1;
@@ -676,7 +878,6 @@ int op_throw_verification_error() {
     load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
 
     unconditional_jump("common_exceptionThrown", false);
-    rPC += 2;
     return 0;
 }
 #undef P_GPR_1
diff --git a/vm/compiler/codegen/x86/LowerReturn.cpp b/vm/compiler/codegen/x86/LowerReturn.cpp
index 294d6b5..ecc9789 100644
--- a/vm/compiler/codegen/x86/LowerReturn.cpp
+++ b/vm/compiler/codegen/x86/LowerReturn.cpp
@@ -108,44 +108,52 @@ int common_returnFromMethod() {
 #undef P_GPR_2
 #undef P_SCRATCH_1
 
-//! lower bytecode RETURN_VOID
-
-//! It seems that shared code cache does not support helper switch
-int op_return_void() {
+/**
+ * @brief Generate native code for bytecodes return-void
+ * and return-void-barrier
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_return_void(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RETURN_VOID
+            || mir->dalvikInsn.opcode == OP_RETURN_VOID_BARRIER);
     int retval;
     retval = common_returnFromMethod();
     rPC += 1;
     return retval;
 }
 
-//! lower bytecode RETURN
-
-//! It seems that shared code cache does not support helper switch
-//! The return value is stored to glue->retval first
-int op_return() {
-    u2 vA = INST_AA(inst);
-
+/**
+ * @brief Generate native code for bytecodes return
+ * and return-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_return(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RETURN
+            || mir->dalvikInsn.opcode == OP_RETURN_OBJECT);
+    u2 vA = mir->dalvikInsn.vA;
     get_virtual_reg(vA, OpndSize_32, 22, false);
     scratchRegs[0] = PhysicalReg_SCRATCH_1;
     set_return_value(OpndSize_32, 22, false);
 
     common_returnFromMethod();
-    rPC += 1;
     return 0;
 }
 
-//! lower bytecode RETURN_WIDE
-
-//! It seems that shared code cache does not support helper switch
-//! The return value is stored to glue->retval first
-int op_return_wide() {
-    u2 vA = INST_AA(inst);
+/**
+ * @brief Generate native code for bytecode return-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_return_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RETURN_WIDE);
+    u2 vA = mir->dalvikInsn.vA;
     get_virtual_reg(vA, OpndSize_64, 1, false);
     scratchRegs[0] = PhysicalReg_SCRATCH_10; scratchRegs[1] = PhysicalReg_Null;
     scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
     set_return_value(OpndSize_64, 1, false);
 
     common_returnFromMethod();
-    rPC += 1;
     return 0;
 }
-- 
1.7.4.1

