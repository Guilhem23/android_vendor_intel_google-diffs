From a6d26189a1982f28f32d8c9b1ae546cf0b018a6e Mon Sep 17 00:00:00 2001
From: Razvan A Lupusoru <razvan.a.lupusoru@intel.com>
Date: Tue, 5 Mar 2013 14:11:28 -0800
Subject: Dalvik: Backend registerization

BZ: 86952

This patch adds backend support for registerization. This means that when it
is enabled, we attempt to keep virtual registers in physical registers across
multiple basic blocks. This is especially important for loops because we
preload the VR in physical register before the loop.

The flag -Xjitnobackendregisterization can be used to turn off backend
registerization. This puts the backend in a retro compatibility mode which
ensures to write back all dirty VRs at end of basic block.

Note that retro compatibility mode is enabled by default with this patch,
which means VRs are not kept live across basic blocks.

Globals.h/Init.cpp
    - Adds options for turning on/off backend registerizations.

Frontend.cpp
    - Allows backend to make additions to the dot printing of CFG.

IntermediateRep.cpp
    - Updates dvmCompilerNewBB to call backend specific basic block creator.

CodegenFactory.cpp/CompilerCodegen.h
    - Added forward declarationsfor backend BB creator and backend BB printer.
    - Added stubs for ARM and MIPS.

AnalysisO1.cpp/AnalysisO1.h
    - Refactored BasicBlock_O1 to extend BasicBlock. Both BB_O1 and
    compileTableEntry were cleaned up to remove unused fields.
    - Implements BasicBlock_O1 creation and clearing. Since arrays in the BB_O1
    struct were replaced with vectors, there are changes in this file to allow
    correct access to the new fields.
    - Adds initialization logic for compile table, memVRTable, and constVRTable.
    - Handles case when BE registerization is turned off by ensuring to add
    spills for every dirty VR at end of BB.
    - Turns off registerization whenever we find that we registerized a VR
    that will be used in x87 because that tries to use VR on stack
    - Updates freeReg to not eagerly free VRs and also to assume live range
    of VR is until end of BB. Whenever freeReg is called with a true argument,
    it writes back all dirty VRs (constants too) back to stack.
    - Fixes transferToState to allow writing back constant VRs to stack if
    they were marked as in memory in the other state.
    - Added function handleVRsEndOfBB which is called at end of basic block
    to ensure either VRs are dumped to stack or synced up to match state
    child expects.
    - Adds several helper functions:
        - updatePhysicalRegForVR which updates physical register for VR
        - writeBackVRIfConstant which writes back constant VRs to stack
        - findFreeRegisters which looks through compile table and finds
        unused physical registers
        - getScratch which looks through a list of candidates and find
        a free register that matches desired type

BytecodeVisitor.cpp
    - Added ref counts to support kMirOpRegisterize.

CodegenErrors.cpp/CodegenErrors.h
    - Added new error for BE registerization and handler for this error.

CodegenInterface.cpp
    - Added a new field to Backward Branch Chaining Cell to allow saving
    preloop address
    - Added helper which creates an intermediary basic block whenever the
    BE has a taken branch to a child whose state has been finalized and thus
    its associations must be satisfied.

InstructionGeneration.cpp/InstructionGeneration.h
    - Implements handling of registerization requests (kMirOpRegisterize).

Lower.cpp
    - Added helper getLabelOffset which gets address where BB was lowered.

LowerAlu.cpp/LowerInvoke.cpp/NcgAot.cpp
    - Removed calls to constVREndOfBB because that is now handled whenever
    freeReg is called with true.

LowerHelper.cpp
    - Updated most calls to freeReg to call with false argument so we don't
    write back VRs when it is not needed.

LowerJump.cpp
    - Removed calls to constVREndOfBB and globalVREndOfBB from all if
    implementation.
    - Refactored logic in common_if to properly handle backward branch
    chaining cells when registerization is used and also for properly
    syncing association tables to both children.

RegisterizationBE.cpp/RegisterizationBE.h
    - Added class AssociationTable which keeps track of constant VRs and
    VRs that are in physical registers. It also keeps track of whether the
    VR is dirty and hasn't been written to stack yet.
    - Added logic which syncs compile table with association table and
    vice versa.
    - Added logic which handles the VR spill requests from the middle end
    - Added logic which syncs state of parent to the state child expects.
    Namely it can set VRs to stack, load VRs into physical registers, and
    move contents from physical registers to others. However, it currently
    cannot handle GP to xmm moves, XMMs to XMMd moves, and loading VR to
    register when when parent doesn't have VR in compile table.

enc_wrapper.cpp/enc_wrapper.h
    - Added helper to allow converting from PhysicalReg enum to string.

InterpAsm-x86.S/footer.S
    - When going through interpreter on first iteration of loop via
    backward branch chaining cell, we now jump back to preloop so that
    we can reload registerization requests in case interpreter cloberred them.

Category: device-enablement
Domain: AOSP-Dalvik-Compiler-CG; AOSP-Dalvik-Compiler-ME; AOSP-Dalvik-Interpreter; AOSP-Dalvik-Runtime
Origin: internal
Upstream-Candidate: no, needs rework

Change-Id: Ia7bd8bbf656d8713ee31240542a212906c218bde
Orig-MCG-Change-Id: Ib68c1d21a5f2ab944b6fbab4db670a433e24798a
Signed-off-by: Razvan A Lupusoru <razvan.a.lupusoru@intel.com>
Signed-off-by: Jean Christophe Beyler <jean.christophe.beyler@intel.com>
Signed-off-by: Qiming Shi <qiming.shi@intel.com>
Signed-off-by: Serguei Katkov <serguei.i.katkov@intel.com>
---
 vm/Dvm.mk                                         |    1 +
 vm/Globals.h                                      |    6 +
 vm/Init.cpp                                       |   17 +-
 vm/compiler/Frontend.cpp                          |    3 +
 vm/compiler/IntermediateRep.cpp                   |   11 +-
 vm/compiler/codegen/CodegenFactory.cpp            |   24 +
 vm/compiler/codegen/CompilerCodegen.h             |   12 +
 vm/compiler/codegen/x86/AnalysisO1.cpp            | 1990 ++++++++++++---------
 vm/compiler/codegen/x86/AnalysisO1.h              |  183 ++-
 vm/compiler/codegen/x86/BytecodeVisitor.cpp       |   97 +-
 vm/compiler/codegen/x86/CodegenErrors.cpp         |   45 +-
 vm/compiler/codegen/x86/CodegenErrors.h           |   14 +-
 vm/compiler/codegen/x86/CodegenInterface.cpp      |  383 +++--
 vm/compiler/codegen/x86/InstructionGeneration.cpp |   32 +
 vm/compiler/codegen/x86/InstructionGeneration.h   |    8 +
 vm/compiler/codegen/x86/Lower.cpp                 |   36 +-
 vm/compiler/codegen/x86/Lower.h                   |   44 +-
 vm/compiler/codegen/x86/LowerAlu.cpp              |    1 -
 vm/compiler/codegen/x86/LowerHelper.cpp           |  214 ++-
 vm/compiler/codegen/x86/LowerInvoke.cpp           |   15 +-
 vm/compiler/codegen/x86/LowerJump.cpp             |  306 ++--
 vm/compiler/codegen/x86/NcgAot.cpp                |    1 -
 vm/compiler/codegen/x86/RegisterizationBE.cpp     |  999 +++++++++++
 vm/compiler/codegen/x86/RegisterizationBE.h       |  269 +++
 vm/compiler/codegen/x86/libenc/enc_wrapper.cpp    |   42 +
 vm/compiler/codegen/x86/libenc/enc_wrapper.h      |   29 +-
 vm/mterp/out/InterpAsm-x86.S                      |    9 +-
 vm/mterp/x86/footer.S                             |    9 +-
 28 files changed, 3395 insertions(+), 1405 deletions(-)
 create mode 100644 vm/compiler/codegen/x86/RegisterizationBE.cpp
 create mode 100644 vm/compiler/codegen/x86/RegisterizationBE.h

diff --git a/vm/Dvm.mk b/vm/Dvm.mk
index f7000bf..724aac0 100644
--- a/vm/Dvm.mk
+++ b/vm/Dvm.mk
@@ -353,6 +353,7 @@ ifeq ($(dvm_arch),x86)
               compiler/codegen/$(dvm_arch_variant)/InstructionGeneration.cpp \
               compiler/codegen/$(dvm_arch_variant)/ExceptionHandling.cpp \
               compiler/codegen/$(dvm_arch_variant)/CodegenErrors.cpp \
+              compiler/codegen/$(dvm_arch_variant)/RegisterizationBE.cpp \
               compiler/LoopOpt.cpp \
               compiler/Checks.cpp \
               compiler/Pass.cpp \
diff --git a/vm/Globals.h b/vm/Globals.h
index db56d70..7640dc9 100644
--- a/vm/Globals.h
+++ b/vm/Globals.h
@@ -943,6 +943,12 @@ struct DvmJitGlobals {
     bool branchLoops;
 
     /* Flag to control backend registerization */
+    bool backEndRegisterization;
+
+    /*
+     * Flag to control maximum number of registerization requests when
+     * backend registerization is enabled.
+     */
     unsigned int maximumRegisterization;
 
     /* Flag to control which loop detection system is being used */
diff --git a/vm/Init.cpp b/vm/Init.cpp
index c13eb38..cb1f3cc 100644
--- a/vm/Init.cpp
+++ b/vm/Init.cpp
@@ -1173,8 +1173,17 @@ static int processOptions(int argc, const char* const argv[],
             gDvmJit.scheduling = false;
         } else if (strncmp(argv[i], "-Xjitnestedloops", 16) == 0) {
             gDvmJit.nestedLoops = true;
+            //For the moment turn off registerization in the back-end in the case of nested loops
+            gDvmJit.backEndRegisterization = false;
+            gDvmJit.maximumRegisterization = 0;
         } else if (strncmp(argv[i], "-Xjitbranchloops", 16) == 0) {
             gDvmJit.branchLoops = true;
+            //For the moment turn off registerization in the back-end in the case of nested loops
+            gDvmJit.backEndRegisterization = false;
+            gDvmJit.maximumRegisterization = 0;
+        } else if (strncmp(argv[i], "-Xjitnobackendregisterization", 29) == 0) {
+            gDvmJit.backEndRegisterization = false;
+            gDvmJit.maximumRegisterization = 0;
         } else if (strncmp(argv[i], "-Xjitregisterization:", 21) == 0) {
             char *endptr = NULL;
             //Get requested style
@@ -1418,10 +1427,16 @@ static void setCommandLineDefaults()
      */
 #if defined(WITH_JIT)
     gDvm.executionMode = kExecutionModeJit;
+
     gDvmJit.nestedLoops = false;
     gDvmJit.branchLoops = false;
-    gDvmJit.maximumRegisterization = UINT_MAX;
     gDvmJit.oldLoopDetection = false;
+
+    // TODO Backend registerization is disabled because it does not pass
+    // MTBF testing. It should be enabled once problem is addressed.
+    gDvmJit.backEndRegisterization = false;
+    gDvmJit.maximumRegisterization = 0;
+
     gDvmJit.ignorePasses = 0;
 #if defined(ARCH_IA32)
     gDvmJit.num_entries_pcTable = 0;
diff --git a/vm/compiler/Frontend.cpp b/vm/compiler/Frontend.cpp
index 4a9d626..5180265 100644
--- a/vm/compiler/Frontend.cpp
+++ b/vm/compiler/Frontend.cpp
@@ -793,6 +793,9 @@ void dvmDumpBasicBlock (CompilationUnit *cUnit, BasicBlock *bb, FILE *file, bool
                 fprintf(file, "    {%s\\l}|\\\n", bbName);
             }
 
+            //Then dump any architecture specific information
+            dvmCompilerDumpArchSpecificBB (cUnit, bb, file, true);
+
             //Then dump the instructions if any
             dumpMIRInstructions (cUnit, bb, file);
 
diff --git a/vm/compiler/IntermediateRep.cpp b/vm/compiler/IntermediateRep.cpp
index 1e136aa..5b59355 100644
--- a/vm/compiler/IntermediateRep.cpp
+++ b/vm/compiler/IntermediateRep.cpp
@@ -20,7 +20,16 @@
 /* Allocate a new basic block */
 BasicBlock *dvmCompilerNewBB(BBType blockType, int blockId)
 {
-    BasicBlock *bb = (BasicBlock *)dvmCompilerNew(sizeof(BasicBlock), true);
+    //Call the backend, it might want to allocate the BasicBlock itself
+    BasicBlock *bb = dvmCompilerArchSpecificNewBB();
+
+    // If architecture specific BB creator returns null, we need to actually
+    // create a BasicBlock.
+    if (bb == 0)
+    {
+        bb = static_cast<BasicBlock *> (dvmCompilerNew(sizeof(BasicBlock), true));
+    }
+
     bb->blockType = blockType;
     bb->id = blockId;
     bb->predecessors = dvmCompilerAllocBitVector(blockId > 32 ? blockId : 32,
diff --git a/vm/compiler/codegen/CodegenFactory.cpp b/vm/compiler/codegen/CodegenFactory.cpp
index 790b258..56f1345 100644
--- a/vm/compiler/codegen/CodegenFactory.cpp
+++ b/vm/compiler/codegen/CodegenFactory.cpp
@@ -276,4 +276,28 @@ bool dvmCompilerFindRegClass (MIR *mir, int vR, RegisterClass &regClass)
     (void) regClass;
     return false;
 }
+
+/**
+ * @brief Architecture specific BasicBlock printing
+ * @param cUnit the CompilationUnit
+ * @param bb the BasicBlock
+ * @param file the File in which to dump the BasicBlock
+ */
+void dvmCompilerDumpArchSpecificBB(CompilationUnit *cUnit, BasicBlock *bb, FILE *file, bool beforeMIRs) {
+    // Empty stub for non x86
+    (void) cUnit;
+    (void) bb;
+    (void) file;
+    (void) beforeMIRs;
+}
+
+/**
+ * @brief Architecture specific BasicBlock creator
+ * @details Initializes x86 specific BasicBlock fields
+ * @return newly created BasicBlock
+ */
+BasicBlock * dvmCompilerArchSpecificNewBB(void) {
+    // Empty stub since non x86 do not use a specialized BasicBlock
+    return NULL;
+}
 #endif
diff --git a/vm/compiler/codegen/CompilerCodegen.h b/vm/compiler/codegen/CompilerCodegen.h
index 8161316..e08eaef 100644
--- a/vm/compiler/codegen/CompilerCodegen.h
+++ b/vm/compiler/codegen/CompilerCodegen.h
@@ -78,4 +78,16 @@ int dvmCompilerTargetOptHint(int key);
 /* Implemented in codegen/<target>/<target_variant>/ArchVariant.c */
 void dvmCompilerGenMemBarrier(CompilationUnit *cUnit, int barrierKind);
 
+/*
+ * Implemented in codegen/<target>/CodegenDriver.cpp
+ * Architecture-specific BasicBlock initialization
+ */
+BasicBlock *dvmCompilerArchSpecificNewBB();
+
+/*
+ * Implemented in the codegen/<target>/ArchUtility.c
+ * Dumps architecture specific Basic Block information into the CFG dot file.
+ */
+void dvmCompilerDumpArchSpecificBB(CompilationUnit *cUnit, BasicBlock *bb, FILE *file, bool beforeMIRs = false);
+
 #endif  // DALVIK_VM_COMPILERCODEGEN_H_
diff --git a/vm/compiler/codegen/x86/AnalysisO1.cpp b/vm/compiler/codegen/x86/AnalysisO1.cpp
index 11328e6..b8b578a 100644
--- a/vm/compiler/codegen/x86/AnalysisO1.cpp
+++ b/vm/compiler/codegen/x86/AnalysisO1.cpp
@@ -24,8 +24,10 @@
 #include "interp/InterpState.h"
 #include "interp/InterpDefs.h"
 #include "libdex/Leb128.h"
+#include "../../RegisterizationME.h"
 #include "Scheduler.h"
 #include "Singleton.h"
+#include <set>
 
 
 /* compilation flags to turn on debug printout */
@@ -37,22 +39,21 @@
 //#define DEBUG_REACHING_DEF2
 //#define DEBUG_REACHING_DEF
 //#define DEBUG_LIVE_RANGE
-//#define DEBUG_MOVE_OPT
-//#define DEBUG_SPILL
 //#define DEBUG_ENDOFBB
 //#define DEBUG_CONST
-/*
-  #define DEBUG_XFER_POINTS
-  #define DEBUG_DSE
-  #define DEBUG_CFG
-  #define DEBUG_GLOBALTYPE
-  #define DEBUG_STATE
-  #define DEBUG_COMPILE_TABLE
-  #define DEBUG_VIRTUAL_INFO
-  #define DEBUG_MOVE_OPT
-  #define DEBUG_MERGE_ENTRY
-  #define DEBUG_INVALIDATE
-*/
+//#define DEBUG_XFER_POINTS
+//#define DEBUG_DSE
+//#define DEBUG_CFG
+//#define DEBUG_GLOBALTYPE
+//#define DEBUG_STATE
+//#define DEBUG_COMPILE_TABLE
+//#define DEBUG_VIRTUAL_INFO
+//#define DEBUG_MERGE_ENTRY
+//#define DEBUG_INVALIDATE
+#define DEBUG_MEMORYVR(X)
+#define DEBUG_MOVE_OPT(X)
+#define DEBUG_SPILL(X)
+
 #include "AnalysisO1.h"
 
 void dumpCompileTable();
@@ -62,15 +63,18 @@ void dumpCompileTable();
    2> temporary (!isVirtualReg() && regNum < PhysicalReg_GLUE_DVMDEX)
    3> glue variables: regNum >= PhysicalReg_GLUE_DVMDEX
 */
-/** check whether a variable is a virtual register
+
+/**
+ * @brief Check whether a type is a virtual register type
+ * @param type the type of the register
+ * @return whether type is a virtual register or not
  */
 bool isVirtualReg(int type) {
-    if((type & LowOpndRegType_virtual) != 0) return true;
-    return false;
+    return ((type & LowOpndRegType_virtual) != 0);
 }
+
 bool isTemporary(int type, int regNum) {
-    if(!isVirtualReg(type) && regNum < PhysicalReg_GLUE_DVMDEX) return true;
-    return false;
+    return (!isVirtualReg(type) && regNum < PhysicalReg_GLUE_DVMDEX);
 }
 
 /** convert type defined in lowering module to type defined in register allocator
@@ -202,7 +206,6 @@ int num_regs_per_bytecode;
 TempRegInfo infoByteCodeTemp[MAX_TEMP_REG_PER_BYTECODE];
 int num_temp_regs_per_bytecode;
 //! array of MemoryVRInfo to store whether a VR is in memory
-#define NUM_MEM_VR_ENTRY 140
 MemoryVRInfo memVRTable[NUM_MEM_VR_ENTRY];
 int num_memory_vr;
 
@@ -223,14 +226,6 @@ int indexForGlue = -1;
 int num_bbs_for_method;
 //! array of basic blocks in a method in program order
 BasicBlock_O1* method_bbs_sorted[MAX_NUM_BBS_PER_METHOD];
-//! the entry basic block
-BasicBlock_O1* bb_entry;
-int pc_start = -1;
-int pc_end = -1;
-
-//!array of PCs for exception handlers
-int exceptionHandlers[10];
-int num_exception_handlers;
 
 bool canSpillReg[PhysicalReg_Null]; //physical registers that should not be spilled
 int inGetVR_num = -1;
@@ -256,7 +251,7 @@ int sortAllocConstraint(RegAllocConstraint* allocConstraints,
 //used in codeGenBasicBlock
 int insertFromVirtualInfo(BasicBlock_O1* bb, int k); //update compileTable
 int insertFromTempInfo(int k); //update compileTable
-int updateXferPoints();
+int updateXferPoints(void);
 static int updateLiveTable();
 void printDefUseTable();
 bool isFirstOfHandler(BasicBlock_O1* bb);
@@ -288,15 +283,19 @@ RegAccessType setAccessTypeOfUse(OverlapCase isDefPartiallyOverlapUse, RegAccess
 DefUsePair* searchDefUseTable(int offsetPC, int regNum, LowOpndRegType pType);
 void insertAccess(int tableIndex, LiveRange* startP, int rangeStart);
 
-//register allocation
-int spillLogicalReg(int spill_index, bool updateTable);
-
 /**
  * @brief Checks whether the opcode can branch or switch
  * @param opcode the Dalvik mnemonic
  * @return true if opcode can branch or switch
  */
 static inline bool isCurrentByteCodeJump(Opcode opcode) {
+    // Since extended MIRs are not normal dex opcodes, we cannot
+    // obtain any flag information (dexGetFlagsFromOpcode will overflow)
+    // As of now, none of the Extended MIRs branch so state it with this test
+    if ((int) opcode >= (int) kMirOpFirst)
+        return false;
+
+    // For regular MIRs, just find it if it can branch or switch
     int flags = dexGetFlagsFromOpcode(opcode);
     return (flags & (kInstrCanBranch | kInstrCanSwitch)) != 0;
 }
@@ -359,21 +358,120 @@ void syncAllRegs() {
     return;
 }
 
-//! \brief sync up spillIndexUsed with compileTable
+/**
+ * @brief Looks through all physical registers and determines what is used
+ * @param outScratchRegisters is a set that is updated with the unused physical
+ * registers
+ */
+void findFreeRegisters(std::set<PhysicalReg> & outFreeRegisters) {
+    // Go through all GPs
+    for (int reg = PhysicalReg_StartOfGPMarker;
+            reg <= PhysicalReg_EndOfGPMarker; reg++) {
+        // If it is one of the unusable registers, continue
+        if (reg == PhysicalReg_EDI || reg == PhysicalReg_ESP
+                || reg == PhysicalReg_EBP)
+            continue;
 
-//! \return -1 if error, 0 otherwise
+        if (allRegs[reg].isUsed == false) {
+            outFreeRegisters.insert(static_cast<PhysicalReg>(reg));
+        }
+    }
+
+    // Go through all XMMs
+    for (int reg = PhysicalReg_StartOfXmmMarker;
+            reg <= PhysicalReg_EndOfXmmMarker; reg++) {
+        if (allRegs[reg].isUsed == false) {
+            outFreeRegisters.insert(static_cast<PhysicalReg>(reg));
+        }
+    }
+}
+
+/**
+ * @brief Given a list of scratch register candidates and a register type,
+ * it returns a scratch register of that type
+ * @param scratchCandidates registers that can be used for scratch
+ * @param type xmm or gp
+ * @return physical register which can be used as scratch
+ */
+PhysicalReg getScratch(std::set<PhysicalReg> & scratchCandidates,
+        LowOpndRegType type) {
+    if (type != LowOpndRegType_gp && type != LowOpndRegType_xmm) {
+        return PhysicalReg_Null;
+    }
+
+    int start = (
+            type == LowOpndRegType_gp ?
+                    PhysicalReg_StartOfGPMarker : PhysicalReg_StartOfXmmMarker);
+    int end = (
+            type == LowOpndRegType_gp ?
+                    PhysicalReg_EndOfGPMarker : PhysicalReg_EndOfXmmMarker);
+
+    PhysicalReg candidate = PhysicalReg_Null;
+    std::set<PhysicalReg>::const_iterator iter;
+    for (iter = scratchCandidates.begin(); iter != scratchCandidates.end();
+            iter++) {
+        PhysicalReg scratch = *iter;
+        if (scratch >= start && scratch <= end) {
+            candidate = scratch;
+            break;
+        }
+    }
+
+    return candidate;
+}
+
+/**
+ * @brief Given a physical register, it returns its physical type
+ * @param reg physical register to check
+ * @return Returns LowOpndRegType_gp if register general purpose.
+ * Returns LowOpndRegType_xmm if register is XMM. Returns
+ * LowOpndRegType_fs is register is stack register for x87.
+ * Otherwise, returns LowOpndRegType_invalid for anything else.
+ */
+LowOpndRegType getTypeOfRegister(PhysicalReg reg) {
+    if (reg >= PhysicalReg_StartOfGPMarker && reg <= PhysicalReg_EndOfGPMarker)
+        return LowOpndRegType_gp;
+    else if (reg >= PhysicalReg_StartOfXmmMarker
+            && reg <= PhysicalReg_EndOfXmmMarker)
+        return LowOpndRegType_xmm;
+    else if (reg >= PhysicalReg_StartOfX87Marker
+            && reg <= PhysicalReg_EndOfX87Marker)
+        return LowOpndRegType_fs;
+
+    return LowOpndRegType_invalid;
+}
+
+/**
+ * @brief Synchronize the spillIndexUsed table with the informatino from compileTable
+ * @return 0 on success, -1 on error
+ */
 static int updateSpillIndexUsed(void) {
     int k;
-    for(k = 0; k <= MAX_SPILL_JIT_IA-1; k++) spillIndexUsed[k] = 0;
+
+    /* First: for each entry, set the index used to 0, in order to reset it */
+    for(k = 0; k <= MAX_SPILL_JIT_IA - 1; k++) {
+        spillIndexUsed[k] = 0;
+    }
+
+    /* Second: go through each compile entry */
     for(k = 0; k < num_compile_entries; k++) {
-        if(isVirtualReg(compileTable[k].physicalType)) continue;
+        /** If it is a virtual register, we skip it, we don't need a special spill region for VRs */
+        if(isVirtualReg(compileTable[k].physicalType)) {
+            continue;
+        }
+
+        /** It might have been spilled, let's see if we have the spill_loc_index filled */
         if(compileTable[k].spill_loc_index >= 0) {
-            if(compileTable[k].spill_loc_index > 4*(MAX_SPILL_JIT_IA-1)) {
+
+            /* We do have it, but is the index correct (and will fit in our table) */
+            if(compileTable[k].spill_loc_index > 4 * (MAX_SPILL_JIT_IA - 1)) {
                 ALOGI("JIT_INFO: spill_loc_index is wrong for entry %d: %d\n",
                       k, compileTable[k].spill_loc_index);
                 SET_JIT_ERROR(kJitErrorRegAllocFailed);
                 return -1;
             }
+
+            /* The spill index is correct, we use the higher bits as a hash for it and set it to 1 */
             spillIndexUsed[compileTable[k].spill_loc_index >> 2] = 1;
         }
     }
@@ -381,94 +479,31 @@ static int updateSpillIndexUsed(void) {
 }
 
 /* free memory used in all basic blocks */
-void freeCFG() {
+void freeCFG(void) {
     int k;
     for(k = 0; k < num_bbs_for_method; k++) {
-        /* free defUseTable for method_bbs_sorted[k] */
-        DefUsePair* ptr = method_bbs_sorted[k]->defUseTable;
-        while(ptr != NULL) {
-            DefUsePair* tmp = ptr->next;
-            /* free ptr->uses */
-            DefOrUseLink* ptrUse = ptr->uses;
-            while(ptrUse != NULL) {
-                DefOrUseLink* tmp2 = ptrUse->next;
-                free(ptrUse);
-                ptrUse = tmp2;
-            }
-            free(ptr);
-            ptr = tmp;
-        }
-        free(method_bbs_sorted[k]);
-    }
-}
-
-/* update compileTable.physicalReg, compileTable.spill_loc_index & allRegs.isUsed
-   for glue-related variables, they do not exist
-       not in a physical register (physicalReg is Null)
-       not in a spilled memory location (spill_loc_index is -1)
-*/
-void initializeRegStateOfBB(BasicBlock_O1* bb) {
-    //for GLUE variables, do not exist
-    int k;
-    for(k = 0; k < num_compile_entries; k++) {
-        /* trace-based JIT: there is no VR with GG type */
-        if(isVirtualReg(compileTable[k].physicalType) && compileTable[k].gType == GLOBALTYPE_GG) {
-            if(bb->bb_index > 0) { //non-entry block
-                if(isFirstOfHandler(bb)) {
-                    /* at the beginning of an exception handler, GG VR is in the interpreted stack */
-                    compileTable[k].physicalReg = PhysicalReg_Null;
-#ifdef DEBUG_COMPILE_TABLE
-                    ALOGI("At the first basic block of an exception handler, GG VR %d type %d is in memory",
-                          compileTable[k].regNum, compileTable[k].physicalType);
-#endif
-                } else {
-                    if(compileTable[k].physicalReg == PhysicalReg_Null) {
-                        /* GG VR is in a specific physical register */
-                        compileTable[k].physicalReg = compileTable[k].physicalReg_prev;
-                    }
-                    int tReg = compileTable[k].physicalReg;
-                    allRegs[tReg].isUsed = true;
-#ifdef DEBUG_REG_USED
-                    ALOGI("REGALLOC: physical reg %d is used by a GG VR %d %d at beginning of BB", tReg, compileTable[k].regNum, compileTable[k].physicalType);
-#endif
-                }
-            } //non-entry block
-        } //if GG VR
-        if(compileTable[k].regNum != PhysicalReg_GLUE &&
-           compileTable[k].regNum >= PhysicalReg_GLUE_DVMDEX) {
-            /* glue related registers */
-            compileTable[k].physicalReg = PhysicalReg_Null;
-            compileTable[k].spill_loc_index = -1;
-        }
-    }
-}
-
-/* update memVRTable[].nullCheckDone */
-void initializeNullCheck(int indexToMemVR) {
-    bool found = false;
-#ifdef GLOBAL_NULLCHECK_OPT
-    /* search nullCheck_inB of the current Basic Block */
-    for(k = 0; k < nullCheck_inSize[currentBB->bb_index2]; k++) {
-        if(nullCheck_inB[currentBB->bb_index2][k] == memVRTable[indexToMemVR].regNum) {
-            found = true;
-            break;
-        }
+        //Call the BasicBlock_O1 clear function
+        method_bbs_sorted[k]->freeIt ();
     }
-#endif
-    memVRTable[indexToMemVR].nullCheckDone = found;
 }
 
-/* @brief Initializes the MemVRTable
- *
- * @return -1 if error happened, 0 otherwise
+/**
+ * @brief Initializes the MemVRTable
+ * @return True if successful, False if error occurred
  */
-static int initializeMemVRTable(void) {
+bool initializeMemVRTable(void) {
     num_memory_vr = 0;
     int k;
     for(k = 0; k < num_compile_entries; k++) {
         if(!isVirtualReg(compileTable[k].physicalType)) continue;
         /* VRs in compileTable */
-        bool setToInMemory = (compileTable[k].physicalReg == PhysicalReg_Null);
+
+        bool setToInMemory =
+                (compileTable[k].physicalReg == PhysicalReg_Null) ?
+                        true :
+                        currentBB->associationTable.wasVRInMemory(
+                                compileTable[k].regNum);
+
         int regNum = compileTable[k].regNum;
         OpndSize sizeVR = getRegSize(compileTable[k].physicalType);
         /* search memVRTable for the VR in compileTable */
@@ -491,15 +526,20 @@ static int initializeMemVRTable(void) {
             if(num_memory_vr >= NUM_MEM_VR_ENTRY) {
                 ALOGI("JIT_INFO: Index %d exceeds size of memVRTable\n", num_memory_vr);
                 SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                return -1;
+                return false;
             }
             memVRTable[num_memory_vr].regNum = regNum;
             memVRTable[num_memory_vr].inMemory = setToInMemory;
-            initializeNullCheck(num_memory_vr); //set nullCheckDone
+            memVRTable[num_memory_vr].nullCheckDone = false;
             memVRTable[num_memory_vr].boundCheck.checkDone = false;
             memVRTable[num_memory_vr].num_ranges = 0;
             memVRTable[num_memory_vr].ranges = NULL;
             memVRTable[num_memory_vr].delayFreeFlags = VRDELAY_NONE;
+
+            DEBUG_MEMORYVR(ALOGD("Initializing state of v%d %sin memory",
+                    memVRTable[num_memory_vr].regNum,
+                    (memVRTable[k].inMemory ? "" : "NOT ")));
+
             num_memory_vr++;
         }
         if(sizeVR == OpndSize_64 && indexH < 0) {
@@ -508,65 +548,374 @@ static int initializeMemVRTable(void) {
             if(num_memory_vr >= NUM_MEM_VR_ENTRY) {
                 ALOGI("JIT_INFO: Index %d exceeds size of memVRTable for 64-bit OpndSize\n", num_memory_vr);
                 SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                return -1;
+                return false;
             }
             memVRTable[num_memory_vr].regNum = regNum+1;
             memVRTable[num_memory_vr].inMemory = setToInMemory;
-            initializeNullCheck(num_memory_vr);
+            memVRTable[num_memory_vr].nullCheckDone = false;
             memVRTable[num_memory_vr].boundCheck.checkDone = false;
             memVRTable[num_memory_vr].num_ranges = 0;
             memVRTable[num_memory_vr].ranges = NULL;
             memVRTable[num_memory_vr].delayFreeFlags = VRDELAY_NONE;
+
+            DEBUG_MEMORYVR(ALOGD("Initializing state of v%d %sin memory",
+                    memVRTable[num_memory_vr].regNum,
+                    (memVRTable[k].inMemory ? "" : "NOT ")));
+
             num_memory_vr++;
         }
     }
-    return 0;
+    return true;
+}
+
+/**
+ * @brief Initializes constant table
+ * @return True if successful
+ */
+bool initializeConstVRTable(void) {
+    // Set number of entries to zero since we are initializing
+    num_const_vr = 0;
+
+    for (int k = 0; k < num_compile_entries; k++) {
+        // Only initialize entries that are VRs
+        if (isVirtualReg(compileTable[k].physicalType) == false)
+            continue;
+
+        OpndSize size = getRegSize(compileTable[k].physicalType);
+        int VR = compileTable[k].regNum;
+        int constValue;
+
+        // First take care of case when VR entry in table is 64-bit
+        if (size == OpndSize_64) {
+            if (currentBB->associationTable.wasVRConstant(VR + 1)) {
+                constValue = currentBB->associationTable.getVRConstValue(
+                        VR + 1);
+
+                // We can eagerly just set the high 32-bits of constant.
+                // Since we are updating just 32-bits, we are safe to not
+                // use an array for constValue.
+                setVRToConst(VR + 1, OpndSize_32, &constValue);
+            }
+        }
+
+        // Now look at the low 32-bits to see if they are constant
+        if (currentBB->associationTable.wasVRConstant(VR)) {
+            constValue = currentBB->associationTable.getVRConstValue(VR);
+
+            setVRToConst(VR, OpndSize_32, &constValue);
+        }
+    }
+
+    return true;
+}
+
+/**
+ * @brief Initializes entries in the compile table at start of BB.
+ * @details It ensures that it updates compile table based on the
+ * given associations from its parent.
+ * @param bb
+ */
+bool initializeRegStateOfBB(BasicBlock_O1* bb) {
+    //for GLUE variables, do not exist
+
+    if(AssociationTable::syncCompileTableWithAssociations(bb->associationTable) == false)
+        return false;
+
+    // Now initialize constants and inMemory tracking
+    if (initializeMemVRTable() == false)
+        return false;
+    if (initializeConstVRTable() == false)
+        return false;
+
+    for(int k = 0; k < num_compile_entries; k++) {
+        // If we have copied any associations into compile table,
+        // lets make sure that the reference count is 0
+        compileTable[k].refCount = 0;
+
+        if(compileTable[k].regNum != PhysicalReg_GLUE &&
+           compileTable[k].regNum >= PhysicalReg_GLUE_DVMDEX) {
+            /* glue related registers */
+            compileTable[k].physicalReg = PhysicalReg_Null;
+            compileTable[k].spill_loc_index = -1;
+        }
+    }
+
+    return true;
+}
+
+
+/**
+ * @brief Constructor for BasicBlock_O1
+ */
+BasicBlock_O1::BasicBlock_O1 (void)
+{
+    //We set the defUseTable to 0 to make sure it doesn't try to free it in the clear function
+    defUseTable = 0;
+    clear (true);
+}
+
+/**
+ * @brief Clear function for BasicBlock_O1
+ * @param allocateLabel do we allocate the label
+ */
+void BasicBlock_O1::clear (bool allocateLabel)
+{
+    // Free defUseTable
+    DefUsePair* ptr = defUseTable;
+    defUseTable = 0;
+
+    //Go through each entry
+    while(ptr != NULL) {
+
+        //Get next
+        DefUsePair* tmp = ptr->next;
+
+        //Free its uses
+        DefOrUseLink* ptrUse = ptr->uses;
+
+        while(ptrUse != NULL) {
+            DefOrUseLink* tmp2 = ptrUse->next;
+            free(ptrUse), ptrUse = 0;
+            ptrUse = tmp2;
+        }
+
+        free(ptr), ptr = 0;
+        ptr = tmp;
+    }
+
+    //Reset variables
+    pc_start = 0;
+    pc_end = 0;
+    streamStart = 0;
+    defUseTable = 0;
+    defUseTail = 0;
+    num_defs = 0;
+    endsWithReturn = 0;
+    hasAccessToGlue = 0;
+    defUseTail = 0;
+
+    //Clear the vectors
+    xferPoints.clear ();
+    associationTable.clear ();
+    infoBasicBlock.clear ();
+
+    //Allocate label
+    if (allocateLabel == true)
+    {
+        label = static_cast<LowOpBlockLabel *> (dvmCompilerNew (sizeof (*label), true));
+    }
+
+    //Paranoid
+    assert (label != NULL);
+
+    //Default value for the offset
+    label->lop.generic.offset = -1;
+
+    //! The logic below assumes that PhysicalReg_EAX is first entry in
+    //! PhysicalReg enum so let's assert it
+    assert(static_cast<int>(PhysicalReg_EAX) == 0);
+
+    // Initialize allocation constraints
+    for (PhysicalReg reg = PhysicalReg_StartOfGPMarker;
+            reg <= PhysicalReg_EndOfGPMarker;
+            reg = static_cast<PhysicalReg>(reg + 1)) {
+        allocConstraints[static_cast<int>(reg)].physicalReg = reg;
+        allocConstraints[static_cast<int>(reg)].count = 0;
+    }
+}
+
+/**
+ * @brief Free everything in the BasicBlock that requires it
+ */
+void BasicBlock_O1::freeIt (void) {
+    //First call clear
+    clear (false);
+
+    //Now free anything that is C++ oriented
+    std::vector<XferPoint> emptyXfer;
+    xferPoints.swap(emptyXfer);
+
+    std::vector<VirtualRegInfo> emptyVRI;
+    infoBasicBlock.swap(emptyVRI);
+}
+
+/**
+ * @brief Architecture specific BasicBlock creator
+ * @details Initializes x86 specific BasicBlock fields
+ * @return newly created BasicBlock
+ */
+BasicBlock * dvmCompilerArchSpecificNewBB() {
+    // Make space on arena for this BB
+    void * space = dvmCompilerNew(sizeof(BasicBlock_O1), true);
+    // Ensure that constructor is called
+    BasicBlock_O1 * newBB = new (space) BasicBlock_O1;
+
+    // Paranoid because dvmCompilerNew should never return NULL
+    assert(newBB != NULL);
+
+    return newBB;
+}
+
+/**
+ * @brief Architecture specific BasicBlock printing
+ * @param cUnit the CompilationUnit
+ * @param bb the BasicBlock
+ * @param file the File in which to dump the BasicBlock
+ */
+void dvmCompilerDumpArchSpecificBB(CompilationUnit *cUnit, BasicBlock *bb,
+        FILE *file, bool beforeMIRs) {
+    // We have already created the x86 specific BB so cast is okay
+    BasicBlock_O1 * curBB = reinterpret_cast<BasicBlock_O1 *>(bb);
+
+    if (beforeMIRs == true) {
+        curBB->associationTable.printToDot(file);
+    }
 }
 
-/* create a O1 basic block from basic block constructed in JIT MIR */
-BasicBlock_O1* createBasicBlockO1(BasicBlock* bb) {
-    BasicBlock_O1* bb1 = createBasicBlock(0, -1);
-    if (bb1 == NULL) return bb1;
-    bb1->jitBasicBlock = bb;
-    return bb1;
+/**
+ * @brief Do we have enough of a given class of registers to registerize
+ * @param cUnit the BasicBlock
+ * @param reg the RegisterClass to consider
+ * @param cnt the number of this type we have right now
+ */
+static bool isEnoughRegisterization (CompilationUnit *cUnit, RegisterClass reg, int cnt)
+{
+    // Get the max set in the cUnit
+    const int max = cUnit->maximumRegisterization;
+
+    return cnt > max;
 }
 
-/* @brief Pre-process BasicBlocks
- * @detail A basic block in JIT MIR can contain bytecodes
- * that are not in program order. For example, a "goto"
- * bytecode will be followed by the goto target
+/**
+ * @brief Handles registerization decision before lowering.
+ * @details If registerization is disabled, set the writeBack vector to all 1s.
+ * Registerization extended MIRs are then removed in order to only registerize
+ * maximum set by the cUnit.
+ * @param cUnit the BasicBlock
+ * @param bb the BasicBlock
+ */
+static void handleRegisterizationPrework (CompilationUnit *cUnit, BasicBlock *bb)
+{
+    //Handle no registerization option first
+    if (gDvmJit.backEndRegisterization == false)
+    {
+        //In this case, we are going to rewrite the requestWriteBack to spilling everything
+        dvmCompilerWriteBackAll (cUnit, bb);
+    }
+
+    //A counter for the kOpRegisterize requests
+    MIR *mir = bb->firstMIRInsn;
+    std::map<RegisterClass, int> counters;
+
+    //Go through the instructions
+    while (mir != 0)
+    {
+        //Did we remove an instruction? Suppose no
+        bool removed = false;
+
+        //If it's a registerize request, we might have to ignore it
+        if (mir->dalvikInsn.opcode == static_cast<Opcode> (kMirOpRegisterize))
+        {
+            //Get the class for it
+            RegisterClass reg = static_cast<RegisterClass> (mir->dalvikInsn.vB);
+
+            //Increment counter
+            counters[reg]++;
+
+            //If we've had enough
+            if (isEnoughRegisterization (cUnit, reg, counters[reg]) == true)
+            {
+                //We are going to remove it, remember it
+                MIR *toRemove = mir;
+                //Remove the instruction but first remember the next one
+                //Note: we are removing this registerization request knowing that the system might request a recompile
+                //TODO: Most likely a flag to ignore it would be better
+                mir = mir->next;
+                //Call the helper function
+                dvmCompilerRemoveMIR (bb, toRemove);
+                //Set removed
+                removed = true;
+            }
+        }
+
+        if (removed == false)
+        {
+            //Go to next
+            mir = mir->next;
+        }
+    }
+}
+
+/**
+ * @brief Parse the BasicBlock and perform pre-lowering work
+ * @param cUnit the BasicBlock
+ * @param bb the BasicBlock
+ */
+static void parseBlock (CompilationUnit *cUnit, BasicBlock *bb)
+{
+    //Always handle registerization request
+    handleRegisterizationPrework (cUnit, bb);
+}
+
+/** @brief Pre-process BasicBlocks
+ * @detail The function registers the BasicBlock in the method_bbs_sorted
+   It checks the number of BasicBlocks can be handled by the back-end
+   It then also collects information about the BasicBlock such as virtual register usage
+   Finally, it parses the block to perform some pre-code generation tasks
+ * @param cUnit the BasicBlock
+ * @param bb the BasicBlock
  * @return -1 if error happened, 0 otherwise
  */
-int preprocessingBB(BasicBlock* bb) {
-    currentBB = createBasicBlockO1(bb);
+int preprocessingBB(CompilationUnit *cUnit, BasicBlock* bb) {
+    // We should have already created the x86 specific BB
+    currentBB = reinterpret_cast<BasicBlock_O1 *> (bb);
+
     if (currentBB == NULL)
         return -1;
-    /* initialize currentBB->allocConstraints */
-    int ii;
-    for(ii = 0; ii < 8; ii++) {
-        currentBB->allocConstraints[ii].physicalReg = (PhysicalReg)ii;
-        currentBB->allocConstraints[ii].count = 0;
+
+    // Keep track of this newly created BB_O1
+    method_bbs_sorted[num_bbs_for_method] = currentBB;
+    num_bbs_for_method++;
+
+    if (num_bbs_for_method >= MAX_NUM_BBS_PER_METHOD) {
+        ALOGE("JIT_ERROR: Exceeded maximum number of basic blocks\n");
+        SET_JIT_ERROR(kJitErrorTraceFormation);
+        return -1;
     }
+
+    // If BB does not have any bytecode we do not need to collect
+    // any information about it
+    if (bb->blockType != kDalvikByteCode && bb->firstMIRInsn == NULL)
+        return 0;
+
     collectInfoOfBasicBlock(currentMethod, currentBB);
 #ifdef DEBUG_COMPILE_TABLE
     dumpVirtualInfoOfBasicBlock(currentBB);
 #endif
+
+    //Parse the BasicBlock, we might have some pre work to do
+    parseBlock (cUnit, bb);
+
     currentBB = NULL;
     return 0;
 }
 
-//! \brief preprocess a trace
-//!
-//! \return -1 on error, 0 on success
-int preprocessingTrace() {
-    int k, k2, k3, jj;
+/**
+ * @brief Perform some preprocessing when getting a trace
+ * @return -1 on error, 0 on success
+ */
+int preprocessingTrace(void) {
+    int k, k2;
+    unsigned int jj;
     int retCode = 0;
     /* this is a simplified verson of setTypeOfVR()
         all VRs are assumed to be GL, no VR will be GG
     */
-    for(k = 0; k < num_bbs_for_method; k++)
-        for(jj = 0; jj < method_bbs_sorted[k]->num_regs; jj++)
+    for(k = 0; k < num_bbs_for_method; k++) {
+        unsigned int max = method_bbs_sorted[k]->infoBasicBlock.size ();
+        for(jj = 0; jj < max; jj++)
             method_bbs_sorted[k]->infoBasicBlock[jj].gType = GLOBALTYPE_GL;
+    }
 
     /* insert a glue-related register GLUE_DVMDEX to compileTable */
     insertGlueReg();
@@ -574,8 +923,9 @@ int preprocessingTrace() {
     int compile_entries_old = num_compile_entries;
     for(k2 = 0; k2 < num_bbs_for_method; k2++) {
         currentBB = method_bbs_sorted[k2];
+        unsigned int max = method_bbs_sorted[k2]->infoBasicBlock.size ();
         /* update compileTable with virtual register from currentBB */
-        for(k3 = 0; k3 < currentBB->num_regs; k3++) {
+        for(unsigned int k3 = 0; k3 < max; k3++) {
             retCode = insertFromVirtualInfo(currentBB, k3);
             if (retCode < 0)
                 return retCode;
@@ -592,10 +942,6 @@ int preprocessingTrace() {
                 /* update defUseTable by assuming a fake usage at END of a basic block for variable @ currentInfo */
                 fakeUsageAtEndOfBB(currentBB);
             }
-            if(isVirtualReg(compileTable[k].physicalType) &&
-               compileTable[k].gType == GLOBALTYPE_GG) {
-                fakeUsageAtEndOfBB(currentBB);
-            }
         }
         offsetPC = offsetPC_back;
         num_compile_entries = compile_entries_old;
@@ -605,6 +951,7 @@ int preprocessingTrace() {
 #ifdef DEBUG_COMPILE_TABLE
     dumpCompileTable();
 #endif
+
     currentBB = NULL;
     return 0;
 }
@@ -613,12 +960,9 @@ void printJitTraceInfoAtRunTime(const Method* method, int offset) {
     ALOGI("execute trace for %s%s at offset %x", method->clazz->descriptor, method->name, offset);
 }
 
-void startOfTraceO1(const Method* method, LowOpBlockLabel* labelList, int exceptionBlockId, CompilationUnit *cUnit) {
-    num_exception_handlers = 0;
+void startOfTraceO1(const Method* method, int exceptionBlockId, CompilationUnit *cUnit) {
     num_compile_entries = 0;
     currentBB = NULL;
-    pc_start = -1;
-    bb_entry = NULL;
     num_bbs_for_method = 0;
     currentUnit = cUnit;
     lowOpTimeStamp = 0;
@@ -647,42 +991,44 @@ void startOfTraceO1(const Method* method, LowOpBlockLabel* labelList, int except
        BasicBlock defined in vm/compiler by JIT
        BasicBlock_O1 defined in o1 */
 int codeGenBasicBlockJit(const Method* method, BasicBlock* bb) {
-    /* search method_bbs_sorted to find the O1 basic block corresponding to bb */
-    int k;
-    for(k = 0; k < num_bbs_for_method; k++) {
-        if(method_bbs_sorted[k]->jitBasicBlock == bb) {
-            lowOpTimeStamp = 0; //reset time stamp at start of a basic block
-            currentBB = method_bbs_sorted[k];
-#if 0
-            if(indexForGlue >= 0) {
-                /* at start of a basic block, GLUE is mapped to %ebp */
-                compileTable[indexForGlue].physicalReg = PhysicalReg_EBP;
-                compileTable[indexForGlue].spill_loc_index = -1;
-                if(!currentBB->hasAccessToGlue) {
-                    /* since GLUE is not used in this basic block, spill it to free one GPR */
-                    spillLogicalReg(indexForGlue, true);
-                    syncAllRegs(); /* sync up allRegs with compileTable */
-                }
-            }
-#endif
-            // Basic block here also means new native basic block
-            if (gDvmJit.scheduling)
-                singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+    lowOpTimeStamp = 0; //reset time stamp at start of a basic block
+    int cg_ret = 0;
+
+    // For x86, the BasicBlock should be the specialized one
+    currentBB = reinterpret_cast<BasicBlock_O1 *>(bb);
+
+    // Basic block here also means new native basic block
+    if (gDvmJit.scheduling)
+        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+
+    // If basic block does not have any MIRs, we do not need to generate
+    // code for the MIRs. However, let's make sure to update its association
+    // table. Also, it might need to update register/memory state before it
+    // it goes into the next BB.
+    if (currentBB->firstMIRInsn == NULL) {
+        // Sync compile table with associations
+        if (initializeRegStateOfBB(currentBB) == false)
+            return -1;
 
-            int cg_ret = codeGenBasicBlock(method, currentBB);
+        // Assert because a BB with no MIRs has no way to branch to two BBs
+        assert (currentBB->taken == NULL);
 
-            // End of managed basic block means end of native basic block
-            if (gDvmJit.scheduling)
-                singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+        if (AssociationTable::createOrSyncTable(currentBB, true) == false)
+            return -1;
 
-            currentBB = NULL;
-            return cg_ret;
-        }
+        // FIXME This is a workaround to reset compile table after processing
+        // an empty BB. This leaves the GLUE_DVMDEX entry still in table.
+        num_compile_entries = 1;
+    } else {
+        cg_ret = codeGenBasicBlock(method, currentBB);
     }
-    ALOGI("JIT_INFO: Cannot find the corresponding O1 basic block for id %d type %d",
-         bb->id, bb->blockType);
-    SET_JIT_ERROR(kJitErrorInvalidBBId);
-    return -1;
+
+    // End of managed basic block means end of native basic block
+    if (gDvmJit.scheduling)
+        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+
+    currentBB = NULL;
+    return cg_ret;
 }
 void endOfBasicBlock(BasicBlock* bb) {
     isScratchPhysical = true;
@@ -699,28 +1045,20 @@ void endOfTraceO1() {
     Global variables accessed: offsetPC, rPC
 */
 int collectInfoOfBasicBlock(Method* method, BasicBlock_O1* bb) {
-    bb->num_regs = 0;
-    bb->num_defs = 0;
-    bb->defUseTable = NULL;
-    bb->defUseTail = NULL;
-    int kk;
-    bb->endsWithReturn = false;
-    bb->hasAccessToGlue = false;
-
     int seqNum = 0;
     /* traverse the MIR in basic block
        sequence number is used to make sure next bytecode will have a larger sequence number */
-    for(MIR * mir = bb->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
+    for(MIR * mir = bb->firstMIRInsn; mir; mir = mir->next) {
         offsetPC = seqNum;
         mir->seqNum = seqNum++;
-#ifdef WITH_JIT_INLINING_PHASE2
-        if(mir->dalvikInsn.opcode >= kMirOpFirst &&
-           mir->dalvikInsn.opcode != kMirOpCheckInlinePrediction) continue;
-        if(mir->dalvikInsn.opcode == kMirOpCheckInlinePrediction) { //TODO
+
+        // Skip extended MIRs whose implementation uses NcgO0 mode
+        if ((int) mir->dalvikInsn.opcode >= (int) kMirOpFirst
+                && (int) mir->dalvikInsn.opcode
+                        != (int) kMirOpRegisterize) {
+            continue;
         }
-#else
-        if(mir->dalvikInsn.opcode >= kNumPackedOpcodes) continue;
-#endif
+
         Opcode inst_op = mir->dalvikInsn.opcode;
         /* update bb->hasAccessToGlue */
         if((inst_op >= OP_MOVE_RESULT && inst_op <= OP_RETURN_OBJECT) ||
@@ -741,7 +1079,7 @@ int collectInfoOfBasicBlock(Method* method, BasicBlock_O1* bb) {
         /* get virtual register usage in current bytecode */
         getVirtualRegInfo(infoByteCode, mir);
         int num_regs = num_regs_per_bytecode;
-        for(kk = 0; kk < num_regs; kk++) {
+        for(int kk = 0; kk < num_regs; kk++) {
             currentInfo = infoByteCode[kk];
 #ifdef DEBUG_MERGE_ENTRY
             ALOGI("Call mergeEntry2 at offsetPC %x kk %d VR %d %d\n", offsetPC, kk,
@@ -758,7 +1096,8 @@ int collectInfoOfBasicBlock(Method* method, BasicBlock_O1* bb) {
     bb->pc_end = seqNum;
 
     //sort allocConstraints of each basic block
-    for(kk = 0; kk < bb->num_regs; kk++) {
+    unsigned int max = bb->infoBasicBlock.size ();
+    for(unsigned int kk = 0; kk < max; kk++) {
 #ifdef DEBUG_ALLOC_CONSTRAINT
         ALOGI("Sort virtual reg %d type %d -------", bb->infoBasicBlock[kk].regNum,
               bb->infoBasicBlock[kk].physicalType);
@@ -773,6 +1112,81 @@ int collectInfoOfBasicBlock(Method* method, BasicBlock_O1* bb) {
     return 0;
 }
 
+/**
+ * @brief Looks through a basic block for any reasons to reject it
+ * @param bb Basic Block to look at
+ * @return true if Basic Block cannot be handled safely by Backend
+ */
+static bool shouldRejectBasicBlock(BasicBlock_O1* bb) {
+    // Assume that we do not want to reject the BB
+    bool shouldReject = false;
+
+    /**
+     * Rejection Scenario 1:
+     * If the basic block has incoming virtual registers that are in physical
+     * registers but we have a usage of that VR in x87, we should reject trace
+     * until we properly handle the Xfer point.
+     */
+
+    // We will need to call getVirtualRegInfo but we do not want to update
+    // register constraints so temporarily set the global currentBB to null
+    BasicBlock_O1 * savedCurrentBB = currentBB;
+    currentBB = NULL;
+
+    std::set<int> registerizedVRs;
+
+    // Find all of the VRs that have been registerized at entry into this BB
+    for (AssociationTable::const_iterator iter = bb->associationTable.begin();
+            iter != bb->associationTable.end(); iter++) {
+        // If there is a physical register for this VR, then it has been
+        // registerized
+        if (iter->second.physicalReg != PhysicalReg_Null) {
+            registerizedVRs.insert(iter->first);
+        }
+    }
+
+    for (MIR * mir = bb->firstMIRInsn; mir != NULL; mir = mir->next) {
+        if ((int) mir->dalvikInsn.opcode >= (int) kMirOpFirst
+                && (int) mir->dalvikInsn.opcode != (int) kMirOpRegisterize) {
+            continue;
+        }
+
+        // Get VR usage for bytecode
+        int numVRs = getVirtualRegInfo(infoByteCode, mir);
+
+        // Go through each VR of the MIR
+        for (int vrIter = 0; vrIter < numVRs; vrIter++) {
+            int VR = infoByteCode[vrIter].regNum;
+            LowOpndRegType type = infoByteCode[vrIter].physicalType;
+
+            // Has this VR been registerized?
+            if (registerizedVRs.find(VR) != registerizedVRs.end()) {
+                // If we will be using x87 for this registerized VR, we cannot
+                // handle
+                if (type == LowOpndRegType_fs || type == LowOpndRegType_fs_s) {
+                    ALOGE("Found x87 usage for VR that has been registerized.");
+                    shouldReject = true;
+                    break;
+                }
+            }
+        }
+
+        // We already know we need to reject, break out of loop
+        if (shouldReject == true) {
+            break;
+        }
+    }
+
+    // Restore currentBB
+    currentBB = savedCurrentBB;
+
+    if (shouldReject) {
+        SET_JIT_ERROR(kJitErrorBBCannotBeHandled);
+    }
+
+    return shouldReject;
+}
+
 /** entry point to generate native code for a O1 basic block
     There are 3 kinds of virtual registers in a O1 basic block:
     1> L VR: local within the basic block
@@ -790,8 +1204,9 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
        all GL VRs reside in memory and all GG VRs reside in predefined physical registers,
        so at the end of a basic block, recover a spilled GG VR, store a GL VR to memory */
     /* update compileTable with entries in bb->infoBasicBlock */
-    int k;
-    for(k = 0; k < bb->num_regs; k++) {
+    int k, max;
+    max = bb->infoBasicBlock.size ();
+    for(k = 0; k < max; k++) {
         retCode = insertFromVirtualInfo(bb, k);
         if (retCode < 0)
             return retCode;
@@ -799,6 +1214,11 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
     retCode = updateXferPoints(); //call fakeUsageAtEndOfBB
     if (retCode < 0)
         return retCode;
+
+    // If we should reject the BB, return that it has not been handled
+    if (shouldRejectBasicBlock(bb) == true)
+        return -1;
+
 #ifdef DEBUG_REACHING_DEF
     printDefUseTable();
 #endif
@@ -808,36 +1228,34 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
         return retCode;
     printDefUseTable();
 #endif
-    //clear const section of compileTable
-    for(k = 0; k < num_compile_entries; k++) compileTable[k].isConst = false;
-    num_const_vr = 0;
 #ifdef DEBUG_COMPILE_TABLE
-    ALOGI("At start of basic block %d (num of VRs %d) -------", bb->bb_index, bb->num_regs);
+    ALOGI("At start of basic block %d (num of VRs %d) -------", bb->bb_index, bb->infoBasicBlock.size ());
     dumpCompileTable();
 #endif
-    initializeRegStateOfBB(bb);
-    retCode = initializeMemVRTable();
-    if (retCode < 0)
-        return retCode;
+
+    if (initializeRegStateOfBB(bb) == false)
+        return -1;
+
     retCode = updateLiveTable();
     if (retCode < 0)
         return retCode;
-    freeReg(true);  //before code gen of a basic block, also called at end of a basic block?
+    freeReg(false);  //before code gen of a basic block, also called at end of a basic block?
 #ifdef DEBUG_COMPILE_TABLE
-    ALOGI("At start of basic block %d (num of VRs %d) -------", bb->bb_index, bb->num_regs);
+    ALOGI("At start of basic block %d (num of VRs %d) -------", bb->bb_index, bb->infoBasicBlock.size ());
 #endif
 
     bool lastByteCodeIsJump = false;
-    for(MIR * mir = bb->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
+    for(MIR * mir = bb->firstMIRInsn; mir; mir = mir->next) {
         offsetPC = mir->seqNum;
         rPC = const_cast<u2 *>(method->insns) + mir->offset;
-#ifdef WITH_JIT_INLINING_PHASE2
-        if(mir->dalvikInsn.opcode >= kMirOpFirst &&
-           mir->dalvikInsn.opcode != kMirOpCheckInlinePrediction) {
-#else
-        if(mir->dalvikInsn.opcode >= kNumPackedOpcodes) {
-#endif
+
+        // Handle extended MIRs whose implementation uses NcgO0 mode
+        if ((int) mir->dalvikInsn.opcode >= (int) kMirOpFirst
+                && (int) mir->dalvikInsn.opcode
+                        != (int) kMirOpRegisterize) {
             handleExtendedMIR(currentUnit, mir);
+            // The rest of logic is for handling mirs that use NCG01 so
+            // we can safely skip
             continue;
         }
 
@@ -850,7 +1268,7 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
                 return retCode;
         }
         startNativeCode(-1, -1);
-        for(k = 0; k <= MAX_SPILL_JIT_IA-1; k++) spillIndexUsed[k] = 0;
+        for(k = 0; k <= MAX_SPILL_JIT_IA - 1; k++) spillIndexUsed[k] = 0;
         //update spillIndexUsed if a glue variable was spilled
         for(k = 0; k < num_compile_entries; k++) {
             if(compileTable[k].regNum >= PhysicalReg_GLUE_DVMDEX) {
@@ -860,7 +1278,7 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
         }
 #ifdef DEBUG_COMPILE_TABLE
         ALOGI("compile table size after importing temporary info %d", num_compile_entries);
-        ALOGI("before one bytecode %d (num of VRs %d) -------", bb->bb_index, bb->num_regs);
+        ALOGI("before one bytecode %d (num of VRs %d) -------", bb->bb_index, bb->infoBasicBlock.size ());
 #endif
         //set isConst to true for CONST & MOVE MOVE_OBJ?
         //clear isConst to true for MOVE, MOVE_OBJ, MOVE_RESULT, MOVE_EXCEPTION ...
@@ -921,9 +1339,13 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
             freeShortMap();
             if (isCurrentByteCodeJump(mir->dalvikInsn.opcode))
                 lastByteCodeIsJump = true;
-            //lowerByteCode will call globalVREndOfBB if it is jump
 
-            bool retCode = lowerByteCodeJit(method, mir, rPC);
+            bool notHandled = false;
+            if ((int) mir->dalvikInsn.opcode >= (int) kMirOpFirst)
+                handleExtendedMIR(currentUnit, mir);
+            else
+                notHandled = lowerByteCodeJit(method, mir, rPC);
+
             if(gDvmJit.codeCacheByteUsed + (stream - streamStart) +
                  CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
                  ALOGI("JIT_INFO: Code cache full while lowering bytecode %s", dexGetOpcodeName(mir->dalvikInsn.opcode));
@@ -932,7 +1354,7 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
                  return -1;
             }
 
-            if (retCode){
+            if (notHandled){
                 SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
                 ALOGI("JIT_INFO: Unsupported bytecode %s\n", dexGetOpcodeName(mir->dalvikInsn.opcode));
                 return -1;
@@ -946,7 +1368,7 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
 
             updateConstInfo(bb);
             freeShortMap();
-            freeReg(true); //may dump GL VR to memory (this is necessary)
+            freeReg(false); //may dump GL VR to memory (this is necessary)
 
             //after each bytecode, make sure non-VRs have refCount of zero
             for(k = 0; k < num_compile_entries; k++) {
@@ -966,27 +1388,26 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
 #ifdef DEBUG_COMPILE_TABLE
             ALOGI("Bytecode %s generates a constant and has no side effect\n", dexGetOpcodeName(mir->dalvikInsn.opcode));
 #endif
-            freeReg(true); //may dump GL VR to memory (this is necessary)
+            freeReg(false); //may dump GL VR to memory (this is necessary)
         }
 #ifdef DEBUG_COMPILE_TABLE
-        ALOGI("After one bytecode BB %d (num of VRs %d)", bb->bb_index, bb->num_regs);
+        ALOGI("After one bytecode BB %d (num of VRs %d)", bb->bb_index, bb->infoBasicBlock.size ());
 #endif
     }//for each bytecode
 #ifdef DEBUG_COMPILE_TABLE
     dumpCompileTable();
 #endif
-    if(!lastByteCodeIsJump) constVREndOfBB();
+
     //at end of a basic block, get spilled GG VR & dump GL VR
-    if(!lastByteCodeIsJump) {
-        retCode = globalVREndOfBB(method);
-        if (retCode < 0)
-            return retCode;
+    retCode = handleVRsEndOfBB(lastByteCodeIsJump);
+    if (retCode < 0) {
+        return retCode;
     }
+
     //remove entries for temporary registers, L VR and GL VR
-    int jj;
     for(k = 0; k < num_compile_entries; ) {
         bool removeEntry = false;
-        if(isVirtualReg(compileTable[k].physicalType) && compileTable[k].gType != GLOBALTYPE_GG) {
+        if(isVirtualReg(compileTable[k].physicalType)) {
             removeEntry = true;
         }
         if(isTemporary(compileTable[k].physicalType, compileTable[k].regNum))
@@ -997,7 +1418,7 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
                 ALOGW("refCount for REG %d %d is %d at end of a basic block", compileTable[k].regNum, compileTable[k].physicalType, compileTable[k].refCount);
 #endif
             compileTable[k].refCount = 0;
-            for(jj = k+1; jj < num_compile_entries; jj++) {
+            for(int jj = k+1; jj < num_compile_entries; jj++) {
                 compileTable[jj-1] = compileTable[jj];
             }
             num_compile_entries--;
@@ -1005,7 +1426,7 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb) {
             k++;
         }
     }
-    freeReg(true);
+    freeReg(false);
     //free LIVE TABLE
     for(k = 0; k < num_memory_vr; k++) {
         LiveRange* ptr2 = memVRTable[k].ranges;
@@ -1038,7 +1459,7 @@ int mergeEntry2(BasicBlock_O1* bb) {
     LowOpndRegType typeB = currentInfo.physicalType;
     int regB = currentInfo.regNum;
     int jj, k;
-    int jjend = bb->num_regs;
+    int jjend = bb->infoBasicBlock.size ();
     bool isMerged = false;
     bool hasAlias = false;
     OverlapCase isBPartiallyOverlapA, isAPartiallyOverlapB;
@@ -1105,38 +1526,41 @@ int mergeEntry2(BasicBlock_O1* bb) {
     }//for each variable A in infoBasicBlock
     if(!isMerged) {
         /* create a new entry in infoBasicBlock */
-        bb->infoBasicBlock[bb->num_regs].refCount = currentInfo.refCount;
-        bb->infoBasicBlock[bb->num_regs].physicalType = typeB;
+        VirtualRegInfo info;
+        info.refCount = currentInfo.refCount;
+        info.physicalType = typeB;
         if(hasAlias)
-            bb->infoBasicBlock[bb->num_regs].accessType = updateAccess3(tmpType, currentInfo.accessType);
+            info.accessType = updateAccess3(tmpType, currentInfo.accessType);
         else
-            bb->infoBasicBlock[bb->num_regs].accessType = currentInfo.accessType;
+            info.accessType = currentInfo.accessType;
 #ifdef DEBUG_MERGE_ENTRY
-        ALOGI("Update accessType in case 3: VR %d %d accessType %d", regB, typeB, bb->infoBasicBlock[bb->num_regs].accessType);
+        ALOGI("Update accessType in case 3: VR %d %d accessType %d", regB, typeB, info.accessType);
 #endif
-        bb->infoBasicBlock[bb->num_regs].regNum = regB;
+        info.regNum = regB;
         for(k = 0; k < 8; k++)
-            bb->infoBasicBlock[bb->num_regs].allocConstraints[k] = currentInfo.allocConstraints[k];
+            info.allocConstraints[k] = currentInfo.allocConstraints[k];
 #ifdef DEBUG_MERGE_ENTRY
         ALOGI("isMerged is false, call updateDefUseTable");
 #endif
         updateDefUseTable(); //use currentInfo to update defUseTable
         updateReachingDefB3(); //update currentInfo.reachingDefs if currentInfo defines variable B
 
-        //copy from currentInfo.reachingDefs to bb->infoBasicBlock[bb->num_regs]
-        bb->infoBasicBlock[bb->num_regs].num_reaching_defs = currentInfo.num_reaching_defs;
+        //copy from currentInfo.reachingDefs to info
+        info.num_reaching_defs = currentInfo.num_reaching_defs;
         for(k = 0; k < currentInfo.num_reaching_defs; k++)
-            bb->infoBasicBlock[bb->num_regs].reachingDefs[k] = currentInfo.reachingDefs[k];
+            info.reachingDefs[k] = currentInfo.reachingDefs[k];
 #ifdef DEBUG_MERGE_ENTRY
         ALOGI("Try to update reaching defs for VR %d %d", regB, typeB);
-        for(k = 0; k < bb->infoBasicBlock[bb->num_regs].num_reaching_defs; k++)
+        for(k = 0; k < info.num_reaching_defs; k++)
             ALOGI("reaching def %d @ %d for VR %d %d access %d", k, currentInfo.reachingDefs[k].offsetPC,
                   currentInfo.reachingDefs[k].regNum, currentInfo.reachingDefs[k].physicalType,
                   currentInfo.reachingDefs[k].accessType);
 #endif
-        bb->num_regs++;
-        if(bb->num_regs >= MAX_REG_PER_BASICBLOCK) {
-            ALOGI("JIT_INFO: Number of VRs (%d) in a basic block, exceed maximum (%d)\n", bb->num_regs, MAX_REG_PER_BASICBLOCK);
+        //Push it in the vector
+        bb->infoBasicBlock.push_back (info);
+
+        if(bb->infoBasicBlock.size () >= MAX_REG_PER_BASICBLOCK) {
+            ALOGI("JIT_INFO: Number of VRs (%d) in a basic block, exceed maximum (%d)\n", bb->infoBasicBlock.size (), MAX_REG_PER_BASICBLOCK);
             SET_JIT_ERROR(kJitErrorMaxVR);
             return -1;
         }
@@ -1531,24 +1955,32 @@ RegAccessType insertDefUsePair(int reachingDefIndex) {
  */
 static int insertLoadXfer(int offset, int regNum, LowOpndRegType pType) {
     //check whether it is already in currentBB->xferPoints
-    int k;
-    for(k = 0; k < currentBB->num_xfer_points; k++) {
+    unsigned int k, max;
+    max = currentBB->xferPoints.size ();
+    for(k = 0; k < max; k++) {
         if(currentBB->xferPoints[k].xtype == XFER_MEM_TO_XMM &&
            currentBB->xferPoints[k].offsetPC == offset &&
            currentBB->xferPoints[k].regNum == regNum &&
            currentBB->xferPoints[k].physicalType == pType)
             return 0;
     }
-    currentBB->xferPoints[currentBB->num_xfer_points].xtype = XFER_MEM_TO_XMM;
-    currentBB->xferPoints[currentBB->num_xfer_points].regNum = regNum;
-    currentBB->xferPoints[currentBB->num_xfer_points].offsetPC = offset;
-    currentBB->xferPoints[currentBB->num_xfer_points].physicalType = pType;
+
+    //We are going to create a new one
+    XferPoint point;
+    point.xtype = XFER_MEM_TO_XMM;
+    point.regNum = regNum;
+    point.offsetPC = offset;
+    point.physicalType = pType;
 #ifdef DEBUG_XFER_POINTS
-    ALOGI("Insert to xferPoints %d: XFER_MEM_TO_XMM of VR %d %d at %d", currentBB->num_xfer_points, regNum, pType, offset);
+    ALOGI("Insert to xferPoints %d: XFER_MEM_TO_XMM of VR %d %d at %d", max, regNum, pType, offset);
 #endif
-    currentBB->num_xfer_points++;
-    if(currentBB->num_xfer_points >= MAX_XFER_PER_BB) {
-        ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", currentBB->num_xfer_points, MAX_XFER_PER_BB);
+
+    //Insert new point
+    currentBB->xferPoints.push_back (point);
+
+    //Paranoid
+    if(max + 1 >= MAX_XFER_PER_BB) {
+        ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", max + 1, MAX_XFER_PER_BB);
         SET_JIT_ERROR(kJitErrorMaxXferPoints);
         return -1;
     }
@@ -1566,9 +1998,11 @@ int fakeUsageAtEndOfBB(BasicBlock_O1* bb) {
     currentInfo.accessType = REGACCESS_U;
     LowOpndRegType typeB = currentInfo.physicalType;
     int regB = currentInfo.regNum;
-    int jj, k;
+    unsigned int jj;
+    int k;
     currentInfo.num_reaching_defs = 0;
-    for(jj = 0; jj < bb->num_regs; jj++) {
+    unsigned int max = bb->infoBasicBlock.size ();
+    for(jj = 0; jj < max; jj++) {
         int regA = bb->infoBasicBlock[jj].regNum;
         LowOpndRegType typeA = bb->infoBasicBlock[jj].physicalType;
         OverlapCase isBPartiallyOverlapA = getBPartiallyOverlapA(regB, typeB, regA, typeA);
@@ -1595,15 +2029,18 @@ int fakeUsageAtEndOfBB(BasicBlock_O1* bb) {
     return 0;
 }
 
-/** update xferPoints of currentBB
-    Traverse currentBB->defUseTable
+/**
+ * @brief Update xferPoints of currentBB
+ * @return -1 on error, 0 otherwise
 */
-int updateXferPoints() {
-    int k = 0;
-    currentBB->num_xfer_points = 0;
+int updateXferPoints(void) {
+    //First clear the XferPoints
+    currentBB->xferPoints.clear ();
+
+    //Get a local version of defUseTable
     DefUsePair* ptr = currentBB->defUseTable;
-    DefOrUseLink* ptrUse = NULL;
-    /* traverse the def use chain of the basic block */
+
+    /* Traverse the def-use chain of the basic block */
     while(ptr != NULL) {
         LowOpndRegType defType = ptr->def.physicalType;
         //if definition is for a variable of 32 bits
@@ -1614,7 +2051,9 @@ int updateXferPoints() {
             bool hasXmmUsage = false;
             bool hasFSUsage = false;
             bool hasSSUsage = false;
-            ptrUse = ptr->uses;
+
+            //Get the uses
+            DefOrUseLink* ptrUse = ptr->uses;
             while(ptrUse != NULL) {
                 if(ptrUse->physicalType == LowOpndRegType_gp) {
                     hasGpUsage = true;
@@ -1647,22 +2086,26 @@ int updateXferPoints() {
                                      if def is on a SS, usage is on a GPR, XMM or FS
                    transfer type is XFER_DEF_TO_GP_MEM if a real GPR usage exisits
                    transfer type is XFER_DEF_TO_GP otherwise*/
-                currentBB->xferPoints[currentBB->num_xfer_points].offsetPC = ptr->def.offsetPC;
-                currentBB->xferPoints[currentBB->num_xfer_points].regNum = ptr->def.regNum;
-                currentBB->xferPoints[currentBB->num_xfer_points].physicalType = ptr->def.physicalType;
+                XferPoint point;
+                point.offsetPC = ptr->def.offsetPC;
+                point.regNum = ptr->def.regNum;
+                point.physicalType = ptr->def.physicalType;
                 if(hasGpUsage2) { //create an entry XFER_DEF_TO_GP_MEM
-                    currentBB->xferPoints[currentBB->num_xfer_points].xtype = XFER_DEF_TO_GP_MEM;
+                    point.xtype = XFER_DEF_TO_GP_MEM;
                 }
                 else { //create an entry XFER_DEF_TO_MEM
-                    currentBB->xferPoints[currentBB->num_xfer_points].xtype = XFER_DEF_TO_MEM;
+                    point.xtype = XFER_DEF_TO_MEM;
                 }
-                currentBB->xferPoints[currentBB->num_xfer_points].tableIndex = k;
+                point.tableIndex = 0;
 #ifdef DEBUG_XFER_POINTS
-                ALOGI("Insert XFER %d at def %d: V%d %d", currentBB->num_xfer_points, ptr->def.offsetPC, ptr->def.regNum, defType);
+                ALOGI("Insert XFER %d at def %d: V%d %d", currentBB->xferPoints.size (), ptr->def.offsetPC, ptr->def.regNum, defType);
 #endif
-                currentBB->num_xfer_points++;
-                if(currentBB->num_xfer_points >= MAX_XFER_PER_BB) {
-                    ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", currentBB->num_xfer_points, MAX_XFER_PER_BB);
+
+                //Push new point
+                currentBB->xferPoints.push_back (point);
+
+                if(currentBB->xferPoints.size () >= MAX_XFER_PER_BB) {
+                    ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", currentBB->xferPoints.size (), MAX_XFER_PER_BB);
                     SET_JIT_ERROR(kJitErrorMaxXferPoints);
                     return -1;
                 }
@@ -1677,7 +2120,9 @@ int updateXferPoints() {
             bool hasAligned = false;
             bool hasFSUsage = false;
             bool hasSSUsage = false;
-            ptrUse = ptr->uses;
+
+            //Get the uses
+            DefOrUseLink* ptrUse = ptr->uses;
             while(ptrUse != NULL) {
                 if(ptrUse->physicalType == LowOpndRegType_gp &&
                    ptrUse->regNum == ptr->def.regNum) {
@@ -1734,33 +2179,47 @@ int updateXferPoints() {
                 continue;
             }
             /* insert a XFER_DEF_IS_XMM */
-            currentBB->xferPoints[currentBB->num_xfer_points].regNum = ptr->def.regNum;
-            currentBB->xferPoints[currentBB->num_xfer_points].offsetPC = ptr->def.offsetPC;
-            currentBB->xferPoints[currentBB->num_xfer_points].physicalType = ptr->def.physicalType;
-            currentBB->xferPoints[currentBB->num_xfer_points].xtype = XFER_DEF_IS_XMM;
-            currentBB->xferPoints[currentBB->num_xfer_points].vr_gpl = -1;
-            currentBB->xferPoints[currentBB->num_xfer_points].vr_gph = -1;
-            if(hasGpUsageOfL2) currentBB->xferPoints[currentBB->num_xfer_points].vr_gpl = ptr->def.regNum;
-            if(hasGpUsageOfH2) currentBB->xferPoints[currentBB->num_xfer_points].vr_gph = ptr->def.regNum+1;
-            currentBB->xferPoints[currentBB->num_xfer_points].dumpToMem = true;
-            currentBB->xferPoints[currentBB->num_xfer_points].dumpToXmm = false; //not used in updateVirtualReg
-            if(hasAligned) currentBB->xferPoints[currentBB->num_xfer_points].dumpToXmm = true;
-            currentBB->xferPoints[currentBB->num_xfer_points].tableIndex = k;
+            XferPoint point;
+
+            point.regNum = ptr->def.regNum;
+            point.offsetPC = ptr->def.offsetPC;
+            point.physicalType = ptr->def.physicalType;
+            point.xtype = XFER_DEF_IS_XMM;
+            point.vr_gpl = -1;
+            point.vr_gph = -1;
+            if(hasGpUsageOfL2) {
+                point.vr_gpl = ptr->def.regNum;
+            }
+            if(hasGpUsageOfH2) {
+                point.vr_gph = ptr->def.regNum+1;
+            }
+            point.dumpToMem = true;
+            point.dumpToXmm = false; //not used in updateVirtualReg
+            if(hasAligned) {
+                point.dumpToXmm = true;
+            }
+            point.tableIndex = 0;
 #ifdef DEBUG_XFER_POINTS
-            ALOGI("Insert XFER %d at def %d: V%d %d", currentBB->num_xfer_points, ptr->def.offsetPC, ptr->def.regNum, defType);
+            ALOGI("Insert XFER %d at def %d: V%d %d", currentBB->xferPoints.size (), ptr->def.offsetPC, ptr->def.regNum, defType);
 #endif
-            currentBB->num_xfer_points++;
-            if(currentBB->num_xfer_points >= MAX_XFER_PER_BB) {
-                ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", currentBB->num_xfer_points, MAX_XFER_PER_BB);
+            //Push new point
+            currentBB->xferPoints.push_back (point);
+
+            if(currentBB->xferPoints.size () >= MAX_XFER_PER_BB) {
+                ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", currentBB->xferPoints.size (), MAX_XFER_PER_BB);
                 SET_JIT_ERROR(kJitErrorMaxXferPoints);
                 return -1;
             }
         }
+
+        //Get next pointer
         ptr = ptr->next;
     } //while ptr
+
 #ifdef DEBUG_XFER_POINTS
     ALOGI("XFER points for current basic block ------");
-    for(k = 0; k < currentBB->num_xfer_points; k++) {
+    unsigned int max = currentBB->xferPoints.size ();
+    for(unsigned int k = 0; k < max; k++) {
         ALOGI("  at offset %x, VR %d %d: type %d, vr_gpl %d, vr_gph %d, dumpToMem %d, dumpToXmm %d",
               currentBB->xferPoints[k].offsetPC, currentBB->xferPoints[k].regNum,
               currentBB->xferPoints[k].physicalType, currentBB->xferPoints[k].xtype,
@@ -1768,6 +2227,8 @@ int updateXferPoints() {
               currentBB->xferPoints[k].dumpToMem, currentBB->xferPoints[k].dumpToXmm);
     }
 #endif
+
+    //Report success
     return 0;
 }
 
@@ -2095,7 +2556,7 @@ inline void loadFromSpillRegion_with_self(OpndSize size, int reg_self, bool self
 }
 inline void loadFromSpillRegion(OpndSize size, int reg, int offset) {
     get_self_pointer(C_SCRATCH_1, isScratchPhysical);
-    int reg_self = registerAlloc(LowOpndRegType_scratch, C_SCRATCH_1, isScratchPhysical, false);
+    int reg_self = registerAlloc(LowOpndRegType_scratch, C_SCRATCH_1, isScratchPhysical, false, true);
     /* only 1 instruction is generated by move_mem_to_reg_noalloc */
     move_mem_to_reg_noalloc(size,
                             getSpillLocDisp(offset), reg_self, true,
@@ -2132,36 +2593,59 @@ inline void saveToSpillRegion(OpndSize size, int reg, int offset) {
 }
 #endif
 
-//! dump an immediate to memory, set inMemory to true
-
-//!
-void dumpImmToMem(int vrNum, OpndSize size, int value) {
-    if(isInMemory(vrNum, size)) {
-#ifdef DEBUG_SPILL
-        ALOGI("Skip dumpImmToMem vA %d size %d", vrNum, size);
-#endif
+/**
+ * @brief If VR is dirty, it writes the constant value to the VR on stack
+ * @details If VR is wide, this function should be called separately twice,
+ * once with the low bits and once with the high bits.
+ * @param vR virtual register
+ * @param value constant value of the VR
+ */
+void writeBackConstVR(int vR, int value) {
+    // If VR is already in memory, we do not need to write it back
+    if(isInMemory(vR, OpndSize_32)) {
+        DEBUG_SPILL(ALOGD("Skip dumpImmToMem v%d size %d", vR, size));
         return;
     }
-    set_VR_to_imm_noalloc(vrNum, size, value);
-    setVRToMemory(vrNum, size);
+
+    // Do the actual move to set constant to VR in stack
+    set_VR_to_imm_noalloc(vR, OpndSize_32, value);
+
+    // Mark the VR as in memory now
+    setVRToMemory(vR, OpndSize_32);
 }
-//! dump content of a VR to memory, set inMemory to true
 
-//!
-void dumpToMem(int vrNum, LowOpndRegType type, int regAll) { //ss,gp,xmm
-    if(isInMemory(vrNum, getRegSize(type))) {
-#ifdef DEBUG_SPILL
-        ALOGI("Skip dumpToMem vA %d type %d", vrNum, type);
-#endif
+/**
+ * @brief Writes back VR to memory if dirty.
+ * @param vR virtual register
+ * @param type physical type as noted by compile table
+ * @param physicalReg physical register
+ */
+void writeBackVR(int vR, LowOpndRegType type, int physicalReg) {
+    int physicalType = type & MASK_FOR_TYPE;
+
+    // Paranoid check because we only handle writing back if in either
+    // GP or XMM registers
+    assert((physicalReg >= PhysicalReg_StartOfGPMarker &&
+            physicalReg <= PhysicalReg_StartOfGPMarker) ||
+            (physicalReg >= PhysicalReg_StartOfXmmMarker &&
+                    physicalReg <= PhysicalReg_StartOfXmmMarker));
+
+    // If VR is already in memory, we can skip writing it back
+    if (isInMemory(vR, getRegSize(physicalType))) {
+        DEBUG_SPILL(ALOGD("Skip writeBackVR v%d type %d", vR, physicalType));
         return;
     }
-    if(type == LowOpndRegType_gp || type == LowOpndRegType_xmm)
-        set_virtual_reg_noalloc(vrNum, getRegSize(type), regAll, true);
-    if(type == LowOpndRegType_ss)
-        move_ss_reg_to_mem_noalloc(regAll, true,
-                                   4*vrNum, PhysicalReg_FP, true,
-                                   MemoryAccess_VR, vrNum);
-    setVRToMemory(vrNum, getRegSize(type));
+
+    // Handle writing back different types of VRs
+    if (physicalType == LowOpndRegType_gp || physicalType == LowOpndRegType_xmm)
+        set_virtual_reg_noalloc(vR, getRegSize(physicalType), physicalReg,
+                true);
+    if (physicalType == LowOpndRegType_ss)
+        move_ss_reg_to_mem_noalloc(physicalReg, true, 4 * vR, PhysicalReg_FP,
+                true, MemoryAccess_VR, vR);
+
+    // Mark it in memory because we have written it back
+    setVRToMemory(vR, getRegSize(physicalType));
 }
 //! dump part of a 64-bit VR to memory and update inMemory
 
@@ -2169,17 +2653,13 @@ void dumpToMem(int vrNum, LowOpndRegType type, int regAll) { //ss,gp,xmm
 void dumpPartToMem(int reg /*xmm physical reg*/, int vA, bool isLow) {
     if(isLow) {
         if(isInMemory(vA, OpndSize_32)) {
-#ifdef DEBUG_SPILL
-            ALOGI("Skip dumpPartToMem isLow %d vA %d", isLow, vA);
-#endif
+            DEBUG_SPILL(ALOGD("Skip dumpPartToMem isLow %d v%d", isLow, vA));
             return;
         }
     }
     else {
         if(isInMemory(vA+1, OpndSize_32)) {
-#ifdef DEBUG_SPILL
-            ALOGI("Skip dumpPartToMem isLow %d vA %d", isLow, vA);
-#endif
+            DEBUG_SPILL(ALOGD("Skip dumpPartToMem isLow %d v%d", isLow, vA));
             return;
         }
     }
@@ -2238,6 +2718,7 @@ void invalidateVRDueToConst(int reg, OpndSize size) {
 #endif
         if(size == OpndSize_32)
             dumpPartToMem(compileTable[index].physicalReg, reg, false); //dump high of xmm to memory
+
         compileTable[index].physicalReg = PhysicalReg_Null;
     }
     index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg-1);
@@ -2246,6 +2727,7 @@ void invalidateVRDueToConst(int reg, OpndSize size) {
         ALOGI("INVALIDATE virtual reg %d type %d", reg-1, LowOpndRegType_xmm);
 #endif
         dumpPartToMem(compileTable[index].physicalReg, reg-1, true); //dump low of xmm to memory
+
         compileTable[index].physicalReg = PhysicalReg_Null;
     }
     index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg);
@@ -2253,6 +2735,7 @@ void invalidateVRDueToConst(int reg, OpndSize size) {
 #ifdef DEBUG_INVALIDATE
         ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_gp);
 #endif
+
         compileTable[index].physicalReg = PhysicalReg_Null;
     }
     index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg);
@@ -2260,6 +2743,7 @@ void invalidateVRDueToConst(int reg, OpndSize size) {
 #ifdef DEBUG_INVALIDATE
         ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_ss);
 #endif
+
         compileTable[index].physicalReg = PhysicalReg_Null;
     }
     if(size == OpndSize_64) {
@@ -2269,6 +2753,7 @@ void invalidateVRDueToConst(int reg, OpndSize size) {
             ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_xmm);
 #endif
             dumpPartToMem(compileTable[index].physicalReg, reg+1, false); //dump high of xmm to memory
+
             compileTable[index].physicalReg = PhysicalReg_Null;
         }
         index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg+1);
@@ -2276,6 +2761,7 @@ void invalidateVRDueToConst(int reg, OpndSize size) {
 #ifdef DEBUG_INVALIDATE
             ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_gp);
 #endif
+
             compileTable[index].physicalReg = PhysicalReg_Null;
         }
         index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg+1);
@@ -2283,6 +2769,7 @@ void invalidateVRDueToConst(int reg, OpndSize size) {
 #ifdef DEBUG_INVALIDATE
             ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_ss);
 #endif
+
             compileTable[index].physicalReg = PhysicalReg_Null;
         }
     }
@@ -2306,6 +2793,7 @@ void invalidateVR(int reg, LowOpndRegType pType) {
 #endif
             if(getRegSize(pType) == OpndSize_32)
                 dumpPartToMem(compileTable[index].physicalReg, reg, false); //dump high of xmm to memory
+
             compileTable[index].physicalReg = PhysicalReg_Null;
         }
     }
@@ -2316,6 +2804,7 @@ void invalidateVR(int reg, LowOpndRegType pType) {
         ALOGI("INVALIDATE virtual reg %d type %d", reg-1, LowOpndRegType_xmm);
 #endif
         dumpPartToMem(compileTable[index].physicalReg, reg-1, true); //dump low of xmm to memory
+
         compileTable[index].physicalReg = PhysicalReg_Null;
     }
     //check misaligned xmm @ reg+1
@@ -2327,6 +2816,7 @@ void invalidateVR(int reg, LowOpndRegType pType) {
             ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_xmm);
 #endif
             dumpPartToMem(compileTable[index].physicalReg, reg+1, false); //dump high of xmm to memory
+
             compileTable[index].physicalReg = PhysicalReg_Null;
         }
     }
@@ -2337,6 +2827,7 @@ void invalidateVR(int reg, LowOpndRegType pType) {
 #ifdef DEBUG_INVALIDATE
             ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_gp);
 #endif
+
             compileTable[index].physicalReg = PhysicalReg_Null;
         }
     }
@@ -2347,6 +2838,7 @@ void invalidateVR(int reg, LowOpndRegType pType) {
 #ifdef DEBUG_INVALIDATE
             ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_gp);
 #endif
+
             compileTable[index].physicalReg = PhysicalReg_Null;
         }
     }
@@ -2357,6 +2849,7 @@ void invalidateVR(int reg, LowOpndRegType pType) {
 #ifdef DEBUG_INVALIDATE
             ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_ss);
 #endif
+
             compileTable[index].physicalReg = PhysicalReg_Null;
         }
     }
@@ -2367,6 +2860,7 @@ void invalidateVR(int reg, LowOpndRegType pType) {
 #ifdef DEBUG_INVALIDATE
             ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_ss);
 #endif
+
             compileTable[index].physicalReg = PhysicalReg_Null;
         }
     }
@@ -2376,7 +2870,6 @@ void invalidateVR(int reg, LowOpndRegType pType) {
 //! invalidate contents of some physical registers, clear nullCheckDone, and update inMemory;
 //! check whether there exist tranfer points for this bytecode, if yes, perform the transfer
 int updateVirtualReg(int reg, LowOpndRegType pType) {
-    int k;
     OpndSize size = getRegSize(pType);
     //WAS only invalidate xmm VRs for the following cases:
     //if def reaches a use of vA,xmm and (the def is not xmm or is misaligned xmm)
@@ -2389,7 +2882,9 @@ int updateVirtualReg(int reg, LowOpndRegType pType) {
     else {
         clearVRToMemory(reg, size);
     }
-    for(k = 0; k < currentBB->num_xfer_points; k++) {
+
+    unsigned int max = currentBB->xferPoints.size ();
+    for(unsigned int k = 0; k < max; k++) {
         if(currentBB->xferPoints[k].offsetPC == offsetPC &&
            currentBB->xferPoints[k].regNum == reg &&
            currentBB->xferPoints[k].physicalType == pType &&
@@ -2409,7 +2904,7 @@ int updateVirtualReg(int reg, LowOpndRegType pType) {
                     ALOGI("XFER set_virtual_reg to memory: xmm VR %d", reg);
 #endif
                     PhysicalReg regAll = (PhysicalReg)checkVirtualReg(reg, LowOpndRegType_xmm, 0 /* do not update*/);
-                    dumpToMem(reg, LowOpndRegType_xmm, regAll);
+                    writeBackVR(reg, LowOpndRegType_xmm, regAll);
                 }
                 if(currentBB->xferPoints[k].vr_gpl >= 0) { //
                 }
@@ -2422,7 +2917,7 @@ int updateVirtualReg(int reg, LowOpndRegType pType) {
                 //the defined gp VR already in register
                 //invalidateXmmVR(currentBB->xferPoints[k].tableIndex);
                 regAll = (PhysicalReg)checkVirtualReg(reg, pType, 0 /* do not update*/);
-                dumpToMem(reg, pType, regAll);
+                writeBackVR(reg, pType, regAll);
 #ifdef DEBUG_XFER_POINTS
                 ALOGI("XFER set_virtual_reg to memory: gp VR %d", reg);
 #endif
@@ -2452,12 +2947,15 @@ void dumpCompileTable();
 //!if no physical register is free, call spillForLogicalReg to free up a physical register;
 //!if the variable is a temporary and it was spilled, call unspillLogicalReg to load from spill location to the allocated physical register;
 //!if updateRefCount is true, reduce reference count of the variable by 1
-int registerAlloc(int type, int reg, bool isPhysical, bool updateRefCount) {
+//!if isDest is true, we inform the compileTable about it
+int registerAlloc(int type, int reg, bool isPhysical, bool updateRefCount, bool isDest) {
 #ifdef DEBUG_REGALLOC
     ALOGI("%p: try to allocate register %d type %d isPhysical %d", currentBB, reg, type, isPhysical);
 #endif
     if(currentBB == NULL) {
-        if(type & LowOpndRegType_virtual) return PhysicalReg_Null;
+        if(type & LowOpndRegType_virtual) {
+            return PhysicalReg_Null;
+        }
         if(isPhysical) return reg; //for helper functions
         return PhysicalReg_Null;
     }
@@ -2484,7 +2982,14 @@ int registerAlloc(int type, int reg, bool isPhysical, bool updateRefCount) {
 #ifdef DEBUG_REG_USED
         ALOGI("REGALLOC: allocate a reg %d", reg);
 #endif
+
+        //Update the physical register
         compileTable[tIndex].physicalReg = reg;
+        //Update the isWritten field, if isDest is true, set it to true
+        if (isDest == true) {
+            compileTable[tIndex].isWritten = true;
+        }
+
         if(updateRefCount)
             decreaseRefCount(tIndex);
 #ifdef DEBUG_REGALLOC
@@ -2498,6 +3003,10 @@ int registerAlloc(int type, int reg, bool isPhysical, bool updateRefCount) {
 #ifdef DEBUG_REGALLOC
         ALOGI("already allocated to physical register %d", compileTable[tIndex].physicalReg);
 #endif
+        //Update the isWritten field, if isDest is true, set it to true
+        if (isDest == true) {
+            compileTable[tIndex].isWritten = true;
+        }
         if(updateRefCount)
             decreaseRefCount(tIndex);
         return compileTable[tIndex].physicalReg;
@@ -2521,6 +3030,9 @@ int registerAlloc(int type, int reg, bool isPhysical, bool updateRefCount) {
     if(compileTable[tIndex].spill_loc_index >= 0) {
         unspillLogicalReg(tIndex, compileTable[tIndex].physicalReg);
     }
+
+    //In this case, it's a new register, set isWritten to isDest
+    compileTable[tIndex].isWritten = isDest;
     if(updateRefCount)
         decreaseRefCount(tIndex);
 #ifdef DEBUG_REGALLOC
@@ -2532,7 +3044,7 @@ int registerAlloc(int type, int reg, bool isPhysical, bool updateRefCount) {
 //!a variable will use a physical register allocated for another variable
 
 //!This is used when MOVE_OPT is on, it tries to alias a virtual register with a temporary to remove a move
-int registerAllocMove(int reg, int type, bool isPhysical, int srcReg) {
+int registerAllocMove(int reg, int type, bool isPhysical, int srcReg, bool isDest) {
     if(srcReg == PhysicalReg_EDI || srcReg == PhysicalReg_ESP || srcReg == PhysicalReg_EBP) {
         ALOGI("JIT_INFO: Cannot move from srcReg EDI or ESP or EBP");
         SET_JIT_ERROR(kJitErrorRegAllocFailed);
@@ -2550,8 +3062,13 @@ int registerAllocMove(int reg, int type, bool isPhysical, int srcReg) {
         return -1;
     }
 
+    //Update the isWritten field, if isDest is true, set it to true
+    if (isDest == true) {
+        compileTable[index].isWritten = true;
+    }
     decreaseRefCount(index);
     compileTable[index].physicalReg = srcReg;
+
 #ifdef DEBUG_REGALLOC
     ALOGI("REGALLOC: registerAllocMove %d for logical register %d %d",
            compileTable[index].physicalReg, reg, newType);
@@ -2589,17 +3106,10 @@ int getFreeReg(int type, int reg, int indexToCompileTable) {
 
     /* a VR is requesting a physical register */
     if(isVirtualReg(type)) { //find a callee-saved register
-        /* if VR is type GG, check the pre-allocated physical register first */
-        bool isGGVR = compileTable[indexToCompileTable].gType == GLOBALTYPE_GG;
-        if(isGGVR) {
-            int regCandidateT = compileTable[indexToCompileTable].physicalReg_prev;
-            if(!allRegs[regCandidateT].isUsed) return regCandidateT;
-        }
-
-        int index = searchVirtualInfoOfBB((LowOpndRegType)(type&MASK_FOR_TYPE), reg, currentBB);
+        int index = searchVirtualInfoOfBB((LowOpndRegType)(type & MASK_FOR_TYPE), reg, currentBB);
         if(index < 0) {
             ALOGI("JIT_INFO: VR %d %d not found in infoBasicBlock of currentBB %d (num of VRs %d)",
-                  reg, type, currentBB->bb_index, currentBB->num_regs);
+                  reg, type, currentBB->id, currentBB->infoBasicBlock.size ());
             SET_JIT_ERROR(kJitErrorRegAllocFailed);
             //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
             return -1;
@@ -2871,11 +3381,15 @@ PhysicalReg spillForLogicalReg(int type, int reg, int indexToCompileTable) {
 #endif
     return allocR;
 }
-//!spill a variable to memory, the variable is specified by an index to compileTable
 
-//!If the variable is a temporary, get a spill location that is not in use and spill the content to the spill location;
-//!If updateTable is true, set physicalReg to Null;
-//!Return the physical register that was allocated to the variable
+/**
+ * @brief Spill a variable to memory, the variable is specified by an index to compileTable
+ * @details If the variable is a temporary, get a spill location that is not in use and spill the content to the spill location;
+ *       If updateTable is true, set physicalReg to Null;
+ * @param spill_index the index into the compile table
+ * @param updateTable do we update the table?
+ * @return Return the physical register that was allocated to the variable
+ */
 int spillLogicalReg(int spill_index, bool updateTable) {
     if((compileTable[spill_index].physicalType & LowOpndRegType_hard) != 0) {
         ALOGI("JIT_INFO: can't spill a hard-coded register");
@@ -2885,18 +3399,26 @@ int spillLogicalReg(int spill_index, bool updateTable) {
     }
     int physicalReg = compileTable[spill_index].physicalReg;
     if(!canSpillReg[physicalReg]) {
+        // This scenario can occur whenever a VR is allocated to the
+        // same physical register as a hardcoded temporary
 #ifdef PRINT_WARNING
-        ALOGW("can't spill register %d", physicalReg);
+        ALOGW("Shouldn't spill register %s but going to do it anyway.",
+                physicalRegToString(static_cast<PhysicalReg>(physicalReg)));
 #endif
-        //dvmAbort(); //this happens in get_virtual_reg where VR is allocated to the same reg as the hardcoded temporary
     }
     if(isVirtualReg(compileTable[spill_index].physicalType)) {
         //spill back to memory
-        dumpToMem(compileTable[spill_index].regNum,
-                  (LowOpndRegType)(compileTable[spill_index].physicalType&MASK_FOR_TYPE),
+        writeBackVR(compileTable[spill_index].regNum,
+                  (LowOpndRegType)(compileTable[spill_index].physicalType & MASK_FOR_TYPE),
                   compileTable[spill_index].physicalReg);
     }
     else {
+        //If the gCompilationUnit has maximumRegisterization set
+        if (gCompilationUnit->maximumRegisterization > 0)
+        {
+            //Signal the error framework we spilled: this is a warning that can help recompile if possible
+            SET_JIT_ERROR (kJitErrorSpill);
+        }
         //update spill_loc_index
         int k = getSpillIndex(spill_index == indexForGlue,
                     getRegSize(compileTable[spill_index].physicalType));
@@ -2917,6 +3439,7 @@ int spillLogicalReg(int spill_index, bool updateTable) {
 
     int allocR = compileTable[spill_index].physicalReg;
     compileTable[spill_index].physicalReg = PhysicalReg_Null;
+
     return allocR;
 }
 //! load a varible from memory to physical register, the variable is specified with an index to compileTable
@@ -2959,10 +3482,10 @@ int spillVirtualReg(int vrNum, LowOpndRegType type, bool updateTable) {
     int value[2];
     int isConst = isVirtualRegConstant(vrNum, type, value, false); //do not update refCount
     if(isConst == 1 || isConst == 3) {
-        dumpImmToMem(vrNum, OpndSize_32, value[0]);
+        writeBackConstVR(vrNum, value[0]);
     }
     if(getRegSize(type) == OpndSize_64 && (isConst == 2 || isConst == 3)) {
-        dumpImmToMem(vrNum+1, OpndSize_32, value[1]);
+        writeBackConstVR(vrNum+1, value[1]);
     }
     if(isConst != 3 && compileTable[index].physicalReg != PhysicalReg_Null)
         spillLogicalReg(index, updateTable);
@@ -2970,6 +3493,37 @@ int spillVirtualReg(int vrNum, LowOpndRegType type, bool updateTable) {
     return -1;
 }
 
+/**
+ * @brief Writes virtual register back to memory if it holds a constant value
+ * @param vR virtual register number
+ * @param type the physical type register type that can be associated with
+ * this VR
+ * @return true if the entire VR was written back to memory
+ */
+bool writeBackVRIfConstant(int vR, LowOpndRegType type) {
+    int constantValue[2];
+    bool writtenBack = false;
+
+    // Check if the VR is a constant. This function returns 3 if VR
+    // is 32-bit and constant, or if VR is 64-bit and both high order
+    // bits and low order bits are constant
+    int isConst = isVirtualRegConstant(vR, type, constantValue, false);
+
+    // If the VR is a constant, then write it back to memory
+    if (isConst == 3) {
+        writeBackConstVR(vR, constantValue[0]);
+        writtenBack |= true;
+    }
+
+    // If VR is wide and high order bits are constant, then write them to memory
+    if (getRegSize(type) == OpndSize_64 && isConst == 3) {
+        writeBackConstVR(vR + 1, constantValue[1]);
+        writtenBack |= true;
+    }
+
+    return writtenBack;
+}
+
 //! spill variables that are mapped to physical register (regNum)
 
 //!
@@ -3309,12 +3863,22 @@ void printDefUseTable() {
         ptr = ptr->next;
     }
 }
-//! when a VR is used, check whether a transfer from memory to XMM is necessary
 
-//!
-int updateVRAtUse(int reg, LowOpndRegType pType, int regAll) {
-    int k;
-    for(k = 0; k < currentBB->num_xfer_points; k++) {
+/**
+ * @brief Update a VR use
+ * @param reg the register
+ * @param pType the type we want for the register
+ * @param regAll
+ */
+void updateVRAtUse(int reg, LowOpndRegType pType, int regAll) {
+
+    //Get a local size of xferPoints' size
+    unsigned int max = currentBB->xferPoints.size ();
+
+    //Go through each element
+    for(unsigned int k = 0; k < max; k++) {
+
+        //If the xferPoint matches and says we want memory to xmm
         if(currentBB->xferPoints[k].offsetPC == offsetPC &&
            currentBB->xferPoints[k].xtype == XFER_MEM_TO_XMM &&
            currentBB->xferPoints[k].regNum == reg &&
@@ -3322,13 +3886,45 @@ int updateVRAtUse(int reg, LowOpndRegType pType, int regAll) {
 #ifdef DEBUG_XFER_POINTS
             ALOGI("XFER from memory to xmm %d", reg);
 #endif
-            move_mem_to_reg_noalloc(OpndSize_64,
-                                    4*currentBB->xferPoints[k].regNum, PhysicalReg_FP, true,
-                                    MemoryAccess_VR, currentBB->xferPoints[k].regNum,
-                                    regAll, true);
+            // If we get to this point, it is possible that we believe we need
+            // to load the wide VR from memory, but in reality this VR might
+            // already be in a physical register.
+
+            // TODO Figure out why this transfer point is inserted even when we already
+            // have VR in xmm.
+
+            int xmmVRType = static_cast<int>(LowOpndRegType_virtual)
+                    | static_cast<int>(LowOpndRegType_xmm);
+            int ssVRType = static_cast<int>(LowOpndRegType_virtual)
+                    | static_cast<int>(LowOpndRegType_ss);
+            bool loadFromMemory = true;
+
+            // Look in compile table for this VR
+            int entry = searchCompileTable(xmmVRType, reg);
+
+            if (entry == -1) {
+                // Single FP VRs can also use xmm, so try looking for this
+                // as well if we haven't already found an entry
+                entry = searchCompileTable(ssVRType, reg);
+            }
+
+            if (entry != -1) {
+                // If we found an entry, check whether its physical register
+                // is not null. If we have a physical register, we shouldn't be
+                // loading from memory
+                if (compileTable[entry].physicalReg != PhysicalReg_Null)
+                    loadFromMemory = false;
+            }
+
+            // Load from memory into the physical register
+            if (loadFromMemory) {
+                move_mem_to_reg_noalloc(OpndSize_64,
+                        4 * currentBB->xferPoints[k].regNum, PhysicalReg_FP,
+                        true, MemoryAccess_VR, currentBB->xferPoints[k].regNum,
+                        regAll, true);
+            }
         }
     }
-    return 0;
 }
 ///////////////////////////////////////////////////////////////////////////////
 // DEAD/USELESS STATEMENT ELMINATION
@@ -3352,7 +3948,7 @@ static int getDeadStmts() {
     num_dead_pc = 0;
     //traverse each bytecode in the basic block
     //update offsetPC, rPC & inst
-    for(MIR * mir = bb->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
+    for(MIR * mir = bb->firstMIRInsn; mir; mir = mir->next) {
         offsetPC = mir->seqNum;
         if(mir->dalvikInsn.opcode >= kNumPackedOpcodes) continue;
 #ifdef DEBUG_DSE
@@ -3411,86 +4007,15 @@ static int getDeadStmts() {
 #endif
     return 0;
 }
-//! \brief entry point to remove dead statements
-//!
-//! \details recursively call getDeadStmts and remove uses in defUseTable that are from a dead PC
-//! until there is no change to number of dead PCs
-//! \return -1 if error happened, 0 otherwise
-static int removeDeadDefs() {
-    int k;
-    int deadPCs_2[MAX_NUM_DEAD_PC_IN_BB];
-    int num_dead_pc_2 = 0;
-    int retCode = 0;
-    retCode = getDeadStmts();
-    if (retCode < 0)
-        return retCode;
-    if(num_dead_pc == 0)
-        return 0;
-    DefUsePair* ptr = NULL;
-    DefOrUseLink* ptrUse = NULL;
-    DefOrUseLink* ptrUse_prev = NULL;
-    while(true) {
-        //check all the uses in defUseTable and remove any use that is from a dead PC
-        ptr = currentBB->defUseTable;
-        while(ptr != NULL) {
-            int k3;
-            ptrUse = ptr->uses;
-            ptrUse_prev = NULL;
-            while(ptrUse != NULL) {
-                bool isIn = false;
-                for(k3 = 0; k3 < num_dead_pc; k3++) {
-                    if(ptrUse->offsetPC == deadPCs[k3]) {
-                        isIn = true;
-                        break;
-                    }
-                }//k3
-                if(!isIn) {
-                    ptrUse_prev = ptrUse;
-                    ptrUse = ptrUse->next; //next use
-                }
-                else {
-                    //go to next use and remove ptrUse
-#ifdef DEBUG_DSE
-                    ALOGI("DSE: remove usage at offsetPC %d reached by def at %d", ptrUse->offsetPC,
-                           ptr->def.offsetPC);
-#endif
-                    DefOrUseLink* nextP = ptrUse->next;
-                    if(ptrUse == ptr->useTail) ptr->useTail = ptrUse_prev;
-                    free(ptrUse);
-                    if(ptrUse_prev == NULL) {
-                        ptr->uses = nextP;
-                    } else {
-                        ptrUse_prev->next = nextP;
-                    }
-                    ptrUse = nextP; //do not update ptrUse_prev
-                    ptr->num_uses--;
-                }
-            }//while ptrUse
-            ptr = ptr->next;
-        }//while ptr
-        //save deadPCs in deadPCs_2
-        num_dead_pc_2 = num_dead_pc;
-        for(k = 0; k < num_dead_pc_2; k++)
-            deadPCs_2[k] = deadPCs[k];
-        //update deadPCs
-        retCode = getDeadStmts();
-        if (retCode < 0)
-            return retCode;
-        //if no change to number of dead PCs, break out of the while loop
-        if(num_dead_pc_2 == num_dead_pc) break;
-    }//while
-#ifdef DEBUG_DSE
-    ALOGI("DSE: DEAD STMTS: ");
-    for(k = 0; k < num_dead_pc; k++) {
-        ALOGI("%d ", deadPCs[k]);
-    }
-#endif
-    return 0;
-}
+
 /////////////////////////////////////////////////////////////
 //!search memVRTable for a given virtual register
 
-//!
+/**
+ * @brief Search in the memory table for a register
+ * @param regNum the register we are looking for
+ * @return the index for the register, -1 if not found
+ */
 int searchMemTable(int regNum) {
     int k;
     for(k = 0; k < num_memory_vr; k++) {
@@ -3529,6 +4054,9 @@ void setVRToMemory(int regNum, OpndSize size) {
         return;
     }
     memVRTable[indexL].inMemory = true;
+    DEBUG_MEMORYVR(ALOGD("Setting state of v%d %sin memory",
+            memVRTable[indexL].regNum,
+            (memVRTable[indexL].inMemory ? "" : "NOT ")));
     if(size == OpndSize_64) {
         if(indexH < 0) {
             ALOGI("JIT_INFO: VR %d not in memVRTabl at setVRToMemory for upper 64-bits", regNum+1);
@@ -3536,6 +4064,9 @@ void setVRToMemory(int regNum, OpndSize size) {
             return;
         }
         memVRTable[indexH].inMemory = true;
+        DEBUG_MEMORYVR(ALOGD("Setting state of v%d %sin memory",
+                memVRTable[indexH].regNum,
+                (memVRTable[indexH].inMemory ? "" : "NOT ")));
     }
 }
 //! check whether null check for a VR is performed previously
@@ -3619,9 +4150,17 @@ void clearVRToMemory(int regNum, OpndSize size) {
     if(size == OpndSize_64) indexH = searchMemTable(regNum+1);
     if(indexL >= 0) {
         memVRTable[indexL].inMemory = false;
+
+        DEBUG_MEMORYVR(ALOGD("Setting state of v%d %sin memory",
+                memVRTable[indexL].regNum,
+                (memVRTable[indexL].inMemory ? "" : "NOT ")));
     }
     if(size == OpndSize_64 && indexH >= 0) {
         memVRTable[indexH].inMemory = false;
+
+        DEBUG_MEMORYVR(ALOGD("Setting state of v%d %sin memory",
+                memVRTable[indexH].regNum,
+                (memVRTable[indexH].inMemory ? "" : "NOT ")));
     }
 }
 //! set nullCheckDone of memVRTable to false
@@ -3689,7 +4228,7 @@ void cancelVRFreeDelayRequest(int regNum, u4 reason) {
         }
     }
     if(needCallToFreeReg)
-        freeReg(true);
+        freeReg(false);
 }
 
 //! Gets status of virtual register free delay request
@@ -3711,41 +4250,53 @@ bool getVRFreeDelayRequested(int regNum) {
     return false;
 }
 
-//! find the basic block that a bytecode is in
-
-//!
-BasicBlock_O1* findForOffset(int offset) {
-    int k;
-    for(k = 0; k < num_bbs_for_method; k++) {
-        if(method_bbs_sorted[k]->pc_start <= offset && method_bbs_sorted[k]->pc_end > offset)
-            return method_bbs_sorted[k];
-    }
-    return NULL;
-}
-void dump_CFG(Method* method);
-
 int current_bc_size = -1;
 
-//! check whether a virtual register is used in a basic block
-
-//!
+/**
+ * @brief Search the basic block information for a register number and type
+ * @param type the register type we are looking for (masked with MASK_FOR_TYPE)
+ * @param regNum the register number we are looking for
+ * @param bb the BasicBlock
+ * @return true if found
+ */
 bool isUsedInBB(int regNum, int type, BasicBlock_O1* bb) {
-    int k;
-    for(k = 0; k < bb->num_regs; k++) {
-        if(bb->infoBasicBlock[k].physicalType == (type&MASK_FOR_TYPE) && bb->infoBasicBlock[k].regNum == regNum)
+    unsigned int k, max;
+
+    //Get a local version of the infoBasicBlock's size
+    max = bb->infoBasicBlock.size ();
+
+    //Go through each element, if it matches, return true
+    for(k = 0; k < max; k++) {
+        if(bb->infoBasicBlock[k].physicalType == (type & MASK_FOR_TYPE) && bb->infoBasicBlock[k].regNum == regNum) {
             return true;
+        }
     }
+
+    //Report failure
     return false;
 }
-//! return the index to infoBasicBlock for a given virtual register
 
-//! return -1 if not found
+/**
+ * @brief Search the basic block information for a register number and type
+ * @param type the register type we are looking for
+ * @param regNum the register number we are looking for
+ * @param bb the BasicBlock
+ * @return the index in the infoBasicBlock table, -1 if not found
+ */
 int searchVirtualInfoOfBB(LowOpndRegType type, int regNum, BasicBlock_O1* bb) {
-    int k;
-    for(k = 0; k < bb->num_regs; k++) {
-        if(bb->infoBasicBlock[k].physicalType == type && bb->infoBasicBlock[k].regNum == regNum)
+    unsigned int k, max;
+
+    //Get a local version of the infoBasicBlock's size
+    max = bb->infoBasicBlock.size ();
+
+    //Go through each element, if it matches, return the index
+    for(k = 0; k < max; k++) {
+        if(bb->infoBasicBlock[k].physicalType == type && bb->infoBasicBlock[k].regNum == regNum) {
             return k;
+        }
     }
+
+    //Report failure
     return -1;
 }
 //! return the index to compileTable for a given virtual register
@@ -3759,6 +4310,34 @@ int searchCompileTable(int type, int regNum) { //returns the index
     }
     return -1;
 }
+
+/**
+ * @brief Update the compileTable entry with the new register
+ * @param vR what virtual register are we interested in
+ * @param oldReg the old register that used to be used
+ * @param newReg the new register that is now used
+ * @return whether the update was successful
+ */
+bool updatePhysicalRegForVR(int vR, PhysicalReg oldReg, PhysicalReg newReg) {
+    //Go through the entries in the compile table
+    for (int entry = 0; entry < num_compile_entries; entry++) {
+
+        //If it is a virtual register, the vR we are looking for, and is associated to the old register
+        if (isVirtualReg(compileTable[entry].physicalType)
+                && compileTable[entry].regNum == vR
+                && compileTable[entry].physicalReg == oldReg) {
+
+            //Update it and report success
+            compileTable[entry].physicalReg = newReg;
+
+            return true;
+        }
+    }
+
+    //We did not find it
+    return false;
+}
+
 //!check whether a physical register for a variable with typeA will work for another variable with typeB
 
 //!Type LowOpndRegType_ss is compatible with type LowOpndRegType_xmm
@@ -3771,21 +4350,6 @@ bool matchType(int typeA, int typeB) {
     return false;
 }
 
-#if 0 /* This code is dead. If reenabling, need to pass proper parameters
-         to getVirtualRegInfo */
-//!check whether a virtual register is used in the current bytecode
-
-//!
-bool isUsedInByteCode(int regNum, int type) {
-    getVirtualRegInfo(infoByteCode);
-    int k;
-    for(k = 0; k < num_regs_per_bytecode; k++) {
-        if(infoByteCode[k].physicalType == (type&MASK_FOR_TYPE) && infoByteCode[k].regNum == regNum)
-            return true;
-    }
-    return false;
-}
-#endif
 //! obsolete
 bool defineFirst(int atype) {
     if(atype == REGACCESS_D || atype == REGACCESS_L || atype == REGACCESS_H || atype == REGACCESS_DU)
@@ -3814,9 +4378,9 @@ bool hasExposedUsage2(BasicBlock_O1* bb, int index) {
 int getSpillIndex(bool isGLUE, OpndSize size) {
     if(isGLUE) return 0;
     int k;
-    for(k = 1; k <= MAX_SPILL_JIT_IA-1; k++) {
+    for(k = 1; k <= MAX_SPILL_JIT_IA - 1; k++) {
         if(size == OpndSize_64) {
-            if(k < MAX_SPILL_JIT_IA-1 && spillIndexUsed[k] == 0 && spillIndexUsed[k+1] == 0)
+            if(k < MAX_SPILL_JIT_IA - 1 && spillIndexUsed[k] == 0 && spillIndexUsed[k+1] == 0)
                 return k;
         }
         else if(spillIndexUsed[k] == 0) {
@@ -4360,7 +4924,7 @@ int convertOffsetPCtoBytecodeOffset(int offPC) {
         return currentBB->pc_start;
     if(offPC == PC_FOR_END_OF_BB)
         return currentBB->pc_end;
-    for(MIR * mir = currentBB->jitBasicBlock->firstMIRInsn; mir; mir = mir->next) {
+    for(MIR * mir = currentBB->firstMIRInsn; mir; mir = mir->next) {
        if(mir->seqNum == offPC)
          return mir->offset;
     }
@@ -4453,10 +5017,11 @@ int getNextAccess(int compileIndex) {
         }
         ptr = ptr->next;
     }
+    if(!found) {
 #ifdef PRINT_WARNING
-    if(!found)
         ALOGW("offsetPC %d not in live range of VR %d", offsetPC, compileTable[k].regNum);
 #endif
+    }
     if(tSize == OpndSize_32) return nextUse;
 
     /* check live ranges of the high half */
@@ -4483,108 +5048,63 @@ int getNextAccess(int compileIndex) {
         }
         ptr = ptr->next;
     }
+    if(!found) {
 #ifdef PRINT_WARNING
-    if(!found) ALOGW("offsetPC %d not in live range of VR %d", offsetPC, compileTable[k].regNum+1);
+        ALOGW("offsetPC %d not in live range of VR %d", offsetPC, compileTable[k].regNum+1);
 #endif
+    }
     /* return the earlier one */
     if(nextUse2 < nextUse) return nextUse2;
     return nextUse;
 }
 
-/** free variables that are no longer in use
-    free a temporary with reference count of zero
-    will dump content of a GL VR to memory if necessary
+/**
+ * @brief Free variables that are no longer in use.
+ * @param writeBackAllVRs When true, writes back all dirty VRs including
+ * constants
+ * @return Returns value >= 0 on success.
 */
-int freeReg(bool spillGL) {
-    if(currentBB == NULL) return 0;
-    int k;
-    for(k = 0; k < num_compile_entries; k++) {
-        if(compileTable[k].refCount == 0 && compileTable[k].physicalReg != PhysicalReg_Null) {
-            /* check entries with reference count of zero and is mapped to a physical register */
-            bool typeA = !isVirtualReg(compileTable[k].physicalType);
-            bool freeCrit = true, delayFreeing = false;
-            bool loopIndep64 = false;
-            OpndSize tSize = getRegSize(compileTable[k].physicalType);
-            bool typeC = false, typeB = false, reachEnd = false;
-            if(isVirtualReg(compileTable[k].physicalType)) {
-                /* VRs in the compile table */
-
-                /* Check if delay for freeing was requested for this VR */
-                delayFreeing = getVRFreeDelayRequested(compileTable[k].regNum);
-
-                freeCrit = isLastByteCodeOfLiveRange(k); /* last bytecode of a live range */
-                reachEnd = reachEndOfBB(k); /* in a live range that extends to end of a basic block */
-#ifdef DEBUG_LIVE_RANGE
-                ALOGI("IN freeReg: VR %d offsetPC %x freecrit %d reachEnd %d nextToLast %d", compileTable[k].regNum, offsetPC, freeCrit, reachEnd, isNextToLastAccess(k));
-#endif
-                /* Bug: spilling of VRs after edi(rFP) is updated in RETURN bytecode
-                        will cause variables for callee to be spilled to the caller stack frame and
-                                                        to overwrite varaibles for caller
-                */
-                /* last bytecode of a live range reaching end of BB if not counting the fake usage at end */
-                bool boolB = reachEnd && isNextToLastAccess(k);
-                /* Bug: when a GG VR is checked at end of a basic block,
-                        freeCrit will be true and physicalReg will be set to Null
-                   Fix: change free condition from freeCrit to (freeCrit && offsetPC != currentBB->pc_end)
-                */
-                /* conditions to free a GG VR:
-                       last bytecode of a live range reaching end of BB if not counting the fake usage at end && endsWithReturn
-                       or
-                       last bytecode of a live range && offsetPC != currentBB->pc_end
-                           -> last bytecode of a live range not reaching end
-                */
-                typeC = ((freeCrit && offsetPC != PC_FOR_END_OF_BB) ||
-                         (currentBB->endsWithReturn && boolB)) &&
-                        compileTable[k].gType == GLOBALTYPE_GG &&
-                        !delayFreeing;
-                /* conditions to free a L|GL VR:
-                       last bytecode of a live range
-                       or
-                       last bytecode of a live range reaching end of BB if not counting the fake usage at end
-                */
-                loopIndep64 = (traceMode == kJitLoop) && (tSize == OpndSize_64) && loopIndepUse(k) && !branchInLoop;
-                typeB = (freeCrit || boolB) &&
-                        (compileTable[k].gType != GLOBALTYPE_GG) && !loopIndep64 &&
-                        !delayFreeing;
+int freeReg(bool writeBackAllVRs) {
+    //If the current BasicBlock is 0, we have nothing to do
+    if(currentBB == NULL) {
+        return 0;
+    }
+
+    //If writeBackAllVRs is true, we also spill the constants
+    if (writeBackAllVRs == true) {
+        for (int k = 0; k < num_const_vr; k++) {
+            if (constVRTable[k].isConst) {
+                writeBackConstVR(constVRTable[k].regNum, constVRTable[k].value);
             }
-            if(typeA || typeB || typeC) {
+        }
+    }
+
+    for(int k = 0; k < num_compile_entries; k++) {
+        if (writeBackAllVRs && isVirtualReg(compileTable[k].physicalType)
+                && compileTable[k].physicalReg != PhysicalReg_Null) {
 #ifdef DEBUG_REGALLOC
-                if(typeA)
-                    ALOGI("FREE TEMP %d with type %d allocated to %d",
-                           compileTable[k].regNum, compileTable[k].physicalType,
-                           compileTable[k].physicalReg);
-                else if(typeB)
-                    ALOGI("FREE VR L|GL %d with type %d allocated to %d",
-                           compileTable[k].regNum, compileTable[k].physicalType,
-                           compileTable[k].physicalReg);
-                else if(typeC)
-                    ALOGI("FREE VR GG %d with type %d allocated to %d",
-                           compileTable[k].regNum, compileTable[k].physicalType,
-                           compileTable[k].physicalReg);
+            ALOGI("FREE v%d with type %d allocated to %s",
+                    compileTable[k].regNum, compileTable[k].physicalType,
+                    physicalRegToString(static_cast<PhysicalReg>(
+                            compileTable[k].physicalReg)));
 #endif
-                bool dumpGL = false;
-                if(compileTable[k].gType == GLOBALTYPE_GL && !reachEnd) {
-                    /* if the live range does not reach end of basic block
-                       and there exists a try block from offsetPC to the next live range
-                           dump VR to interpreted stack */
-                    int tmpPC = getNextLiveRange(k);
-                    if(existATryBlock(currentMethod, convertOffsetPCtoBytecodeOffset(offsetPC),
-                        convertOffsetPCtoBytecodeOffset(tmpPC))) dumpGL = true;
-                }
-                /* if the live range reach end of basic block, dump VR to interpreted stack */
-                if(compileTable[k].gType == GLOBALTYPE_GL && reachEnd) dumpGL = true;
-                if(dumpGL) {
-                    if(spillGL) {
+
+            spillLogicalReg(k, true);
+        }
+
+        if(compileTable[k].refCount == 0 && compileTable[k].physicalReg != PhysicalReg_Null) {
+            bool isTemp = (isVirtualReg(compileTable[k].physicalType) == false);
+
+            if (isTemp) {
 #ifdef DEBUG_REGALLOC
-                        ALOGI("SPILL VR GL %d %d", compileTable[k].regNum, compileTable[k].physicalType);
+                ALOGI("FREE temporary %d with type %d allocated to %s",
+                       compileTable[k].regNum, compileTable[k].physicalType,
+                       physicalRegToString(static_cast<PhysicalReg>(
+                               compileTable[k].physicalReg)));
 #endif
-                        spillLogicalReg(k, true); //will dump VR to memory & update physicalReg
-                    }
-                }
-                else
-                     compileTable[k].physicalReg = PhysicalReg_Null;
-            }
-            if(typeA) {
+
+                compileTable[k].physicalReg = PhysicalReg_Null;
+
                 if(compileTable[k].spill_loc_index >= 0) {
                     /* update spill info for temporaries */
                     spillIndexUsed[compileTable[k].spill_loc_index >> 2] = 0;
@@ -4756,34 +5276,38 @@ int checkVirtualReg(int reg, LowOpndRegType type, int updateRefCount) {
 //!This is called in get_virtual_reg
 //!If this function returns false, new register will be allocated for this temporary
 bool checkTempReg2(int reg, int type, bool isPhysical, int physicalRegForVR, u2 vB) {
-    if(currentBB == NULL) return false;
-    if(isPhysical) return false;
-    int index = -1;
-    for(int k = 0; k < num_compile_entries; k++) {
-        if (isVirtualReg(compileTable[k].physicalType) && compileTable[k].regNum == vB) {
-            if (isLastByteCodeOfLiveRange(k) || (reachEndOfBB(k) && isNextToLastAccess(k))) {
-                index = k;
-            }
-            break;
-        }
+    if (isPhysical) {
+        // If temporary is already physical, we cannot share with VR
+        return false;
     }
 
     int newType = convertType(type, reg, isPhysical);
     if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
-    int k;
-    for(k = 0; k < num_temp_regs_per_bytecode; k++) {
+
+    // Look through all of the temporaries used by this bytecode implementation
+    for(int k = 0; k < num_temp_regs_per_bytecode; k++) {
+
         if(infoByteCodeTemp[k].physicalType == newType &&
            infoByteCodeTemp[k].regNum == reg) {
-#ifdef DEBUG_MOVE_OPT
-            ALOGI("MOVE_OPT checkTempRegs for %d %d returns %d %d",
-                   reg, newType, infoByteCodeTemp[k].shareWithVR, infoByteCodeTemp[k].is8Bit);
-#endif
-            if(!infoByteCodeTemp[k].is8Bit || (physicalRegForVR >= PhysicalReg_EAX && physicalRegForVR <= PhysicalReg_EDX)) {
-                if (index >= 0 && infoByteCodeTemp[k].shareRegWithLivenessInfo) return true;
+            // We found a matching temporary
+
+            if (!infoByteCodeTemp[k].is8Bit
+                    || (physicalRegForVR >= PhysicalReg_EAX
+                            && physicalRegForVR <= PhysicalReg_EDX)) {
+                DEBUG_MOVE_OPT(ALOGD("Temp%d can%s share %s with v%d",
+                        reg, infoByteCodeTemp[k].shareWithVR ? "" : " NOT",
+                        physicalRegToString(static_cast<PhysicalReg>(
+                                physicalRegForVR)), vB));
+
                 return infoByteCodeTemp[k].shareWithVR;
-            }
-            else
+            } else {
+                DEBUG_MOVE_OPT(ALOGD("Temp%d can NOT share %s with v%d",
+                        reg, physicalRegToString(static_cast<PhysicalReg>(
+                                physicalRegForVR)), vB));
+
+                // We cannot share same physical register as VR
                 return false;
+            }
         }
     }
     ALOGI("JIT_INFO: in checkTempReg2 %d %d\n", reg, newType);
@@ -4827,6 +5351,7 @@ int checkTempReg(int reg, int type, bool isPhysical, int vrNum) {
 #ifdef DEBUG_REGALLOC
         ALOGW("in checkTempReg, the temporary register %d %d was spilled", reg, type);
 #endif
+        //No need for written, we don't write in it yet, just aliasing at worse
         int regAll = registerAlloc(type, reg, isPhysical, true/* updateRefCount */);
         return regAll;
     }
@@ -4842,150 +5367,38 @@ bool hasExposedUsage(LowOpndRegType type, int regNum, BasicBlock_O1* bb) {
     }
     return false;
 }
-//!check whether a variable has exposed usage in other basic blocks
-
-//!
-bool hasOtherExposedUsage(OpndSize size, int regNum, BasicBlock_O1* bb) {
-    return true; //assume the worst case
-}
-
-//! handles constant VRs at end of a basic block
-
-//!If a VR is constant at end of a basic block and (it has exposed usage in other basic blocks or reaches a GG VR), dump immediate to memory
-void constVREndOfBB() {
-    BasicBlock_O1* bb = currentBB;
-    int k, k2;
-    //go through GG VRs, update a bool array
-    int constUsedByGG[MAX_CONST_REG];
-    for(k = 0; k < num_const_vr; k++)
-        constUsedByGG[k] = 0;
-    for(k = 0; k < num_compile_entries; k++) {
-        if(isVirtualReg(compileTable[k].physicalType) && compileTable[k].gType == GLOBALTYPE_GG) {
-            OpndSize size = getRegSize(compileTable[k].physicalType);
-            int regNum = compileTable[k].regNum;
-            int indexL = -1;
-            int indexH = -1;
-            for(k2 = 0; k2 < num_const_vr; k2++) {
-                if(constVRTable[k2].regNum == regNum) {
-                    indexL = k2;
-                    continue;
-                }
-                if(constVRTable[k2].regNum == regNum + 1 && size == OpndSize_64) {
-                    indexH = k2;
-                    continue;
-                }
-            }
-            if(indexL >= 0) constUsedByGG[indexL] = 1;
-            if(indexH >= 0) constUsedByGG[indexH] = 1;
-        } //GG VR
-    }
-    for(k = 0; k < num_const_vr; k++) {
-        if(!constVRTable[k].isConst) continue;
-        bool hasExp = false;
-        if(constUsedByGG[k] == 0)
-            hasExp = hasOtherExposedUsage(OpndSize_32, constVRTable[k].regNum, bb);
-        if(constUsedByGG[k] != 0 || hasExp) {
-            dumpImmToMem(constVRTable[k].regNum, OpndSize_32, constVRTable[k].value);
-            setVRToMemory(constVRTable[k].regNum, OpndSize_32);
-#ifdef DEBUG_ENDOFBB
-            ALOGI("ENDOFBB: exposed VR %d is const %d (%x)",
-                  constVRTable[k].regNum, constVRTable[k].value, constVRTable[k].value);
-#endif
-        } else {
-#ifdef DEBUG_ENDOFBB
-            ALOGI("ENDOFBB: unexposed VR %d is const %d (%x)",
-                  constVRTable[k].regNum, constVRTable[k].value, constVRTable[k].value);
-#endif
-        }
-    }
-}
 
-// check if it's needed to store VR at the exit of the loop
-bool hasVRStoreExitOfLoop() {
-    int k;
-    for(k=0; k < num_compile_entries; k++)
-       if(isVirtualReg(compileTable[k].physicalType) &&
-          compileTable[k].refCount == 0 &&
-          compileTable[k].physicalReg != PhysicalReg_Null &&
-          compileTable[k].gType != GLOBALTYPE_GG &&
-          loopIndepUse(k))
-           return true;
-    return false;
-}
+/**
+ * @brief Handle the spilling of registers at the end of a BasicBlock
+ * @param lastByteCodeIsJump is the last bytecode is a jump
+ * @return -1 if error, 0 otherwise
+ */
+int handleVRsEndOfBB(bool lastByteCodeIsJump) {
+    // Call freeReg to free temporaries. However, we only spill VRs depending on
+    // association tables.
+    freeReg(false);
 
-/*
-  Dump VR back to memory at the loop exit branch
-*/
-void storeVRExitOfLoop() {
-    int k;
-    for(k=0; k < num_compile_entries; k++) {
-       if(isVirtualReg(compileTable[k].physicalType) &&
-          compileTable[k].refCount == 0 &&
-          compileTable[k].physicalReg != PhysicalReg_Null &&
-          compileTable[k].gType != GLOBALTYPE_GG &&
-          loopIndepUse(k)) {
-#ifdef DEBUG_ENDOFBB
-           ALOGI("EXITOFLOOP SPILL VR %d %d\n", compileTable[k].regNum, compileTable[k].physicalType);
-#endif
-           spillLogicalReg(k, true);
-       }
+    //If it's a jump, then we don't update the association tables
+    if (lastByteCodeIsJump == false)
+    {
+        // Update association tables of children
+        if (AssociationTable::createOrSyncTable(currentBB, true) == false)
+            return -1;
+        if (AssociationTable::createOrSyncTable(currentBB, false) == false)
+            return -1;
     }
-    syncAllRegs();
-}
 
-//! \brief handles GG VRs at end of a basic block
-//!
-//! \details make sure all GG VRs are in pre-defined physical registers
-//!
-//! \param method The enclosing method
-//!
-//! \return -1 on error, 0 otherwise
-int globalVREndOfBB(const Method* method) {
-    //fix: freeReg first to write LL VR back to memory to avoid it gets overwritten by GG VRs
-    freeReg(true);
-    int k;
     //spill GG VR first if it is not mapped to the specific reg
     //release GLUE regs
-    for(k = 0; k < num_compile_entries; k++) {
+    for(int k = 0; k < num_compile_entries; k++) {
         if(compileTable[k].regNum >= PhysicalReg_GLUE_DVMDEX &&
            compileTable[k].regNum != PhysicalReg_GLUE) {
             compileTable[k].physicalReg = PhysicalReg_Null;
             compileTable[k].spill_loc_index = -1;
         }
-        //if part of a GG VR is const, the physical reg is set to null
-        if(isVirtualReg(compileTable[k].physicalType) &&
-           compileTable[k].gType == GLOBALTYPE_GG && compileTable[k].physicalReg != PhysicalReg_Null &&
-           compileTable[k].physicalReg != compileTable[k].physicalReg_prev) {
-#ifdef DEBUG_ENDOFBB
-            ALOGW("end of BB GG VR is not mapped to the specific reg: %d %d %d",
-                  compileTable[k].regNum, compileTable[k].physicalType, compileTable[k].physicalReg);
-            ALOGW("ENDOFBB SPILL VR %d %d", compileTable[k].regNum, compileTable[k].physicalType);
-#endif
-            spillLogicalReg(k, true); //the next section will load VR from memory to the specific reg
-        }
     }
     syncAllRegs();
-    for(k = 0; k < num_compile_entries; k++) {
-        if(isVirtualReg(compileTable[k].physicalType)) {
-            if(compileTable[k].gType == GLOBALTYPE_GG &&
-               compileTable[k].physicalReg == PhysicalReg_Null && (!currentBB->endsWithReturn)) {
-#ifdef DEBUG_ENDOFBB
-                ALOGI("ENDOFBB GET GG VR %d %d to physical register %d", compileTable[k].regNum,
-                      compileTable[k].physicalType, compileTable[k].physicalReg_prev);
-#endif
-                compileTable[k].physicalReg = compileTable[k].physicalReg_prev;
-                if(allRegs[compileTable[k].physicalReg_prev].isUsed) {
-                    ALOGI("JIT_INFO: Physical register for GG VR is still used\n");
-                    SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                    return -1;
-                }
-                get_virtual_reg_noalloc(compileTable[k].regNum,
-                                        getRegSize(compileTable[k].physicalType),
-                                        compileTable[k].physicalReg_prev,
-                                        true);
-            }
-        }//not const
-    }
+
     if(indexForGlue >= 0 &&
         compileTable[indexForGlue].physicalReg == PhysicalReg_Null) {
         unspillLogicalReg(indexForGlue, PhysicalReg_EBP); //load %ebp
@@ -5025,8 +5438,6 @@ int insertFromVirtualInfo(BasicBlock_O1* bb, int k) {
         compileTable[num_compile_entries].physicalType = (LowOpndRegType_virtual | bb->infoBasicBlock[k].physicalType);
         compileTable[num_compile_entries].regNum = bb->infoBasicBlock[k].regNum;
         compileTable[num_compile_entries].physicalReg = PhysicalReg_Null;
-        compileTable[num_compile_entries].bb = bb;
-        compileTable[num_compile_entries].indexToInfoBB = k;
         compileTable[num_compile_entries].spill_loc_index = -1;
         compileTable[num_compile_entries].gType = bb->infoBasicBlock[k].gType;
         num_compile_entries++;
@@ -5038,9 +5449,7 @@ int insertFromVirtualInfo(BasicBlock_O1* bb, int k) {
     }
     /* re-set reference count of all VRs */
     compileTable[index].refCount = bb->infoBasicBlock[k].refCount;
-    compileTable[index].accessType = bb->infoBasicBlock[k].accessType;
-    if(compileTable[index].gType == GLOBALTYPE_GG)
-        compileTable[index].physicalReg_prev = bb->infoBasicBlock[k].physicalReg_GG;
+
     return 0;
 }
 
@@ -5077,9 +5486,7 @@ void insertGlueReg() {
     compileTable[num_compile_entries].regNum = PhysicalReg_GLUE_DVMDEX;
     compileTable[num_compile_entries].refCount = 2;
     compileTable[num_compile_entries].physicalReg = PhysicalReg_Null;
-    compileTable[num_compile_entries].bb = NULL;
     compileTable[num_compile_entries].spill_loc_index = -1;
-    compileTable[num_compile_entries].accessType = REGACCESS_N;
     compileTable[num_compile_entries].linkageToVR = -1;
     compileTable[num_compile_entries].gType = GLOBALTYPE_L;
 
@@ -5094,9 +5501,10 @@ void insertGlueReg() {
 /** print infoBasicBlock of the given basic block
 */
 void dumpVirtualInfoOfBasicBlock(BasicBlock_O1* bb) {
-    int jj;
-    ALOGI("Virtual Info for BB%d --------", bb->bb_index);
-    for(jj = 0; jj < bb->num_regs; jj++) {
+    unsigned int jj, max;
+    ALOGI("Virtual Info for BB%d --------", bb->id);
+    max = bb->infoBasicBlock.size ();
+    for(jj = 0; jj < max; jj++) {
         ALOGI("regNum %d physicalType %d accessType %d refCount %d def ",
                bb->infoBasicBlock[jj].regNum, bb->infoBasicBlock[jj].physicalType,
                bb->infoBasicBlock[jj].accessType, bb->infoBasicBlock[jj].refCount);
@@ -5112,63 +5520,27 @@ void dumpVirtualInfoOfBasicBlock(BasicBlock_O1* bb) {
 /** print compileTable
 */
 void dumpCompileTable() {
-    int jj;
-    ALOGI("Compile Table for method ----------");
-    for(jj = 0; jj < num_compile_entries; jj++) {
-        ALOGI("regNum %d physicalType %d refCount %d isConst %d physicalReg %d type %d",
-               compileTable[jj].regNum, compileTable[jj].physicalType,
-               compileTable[jj].refCount, compileTable[jj].isConst, compileTable[jj].physicalReg, compileTable[jj].gType);
+    ALOGD("+++++++++++++++++++++ Compile Table +++++++++++++++++++++");
+    ALOGD("%d entries\t%d memory_vr\t%d const_vr", num_compile_entries,
+            num_memory_vr, num_const_vr);
+    for(int entry = 0; entry < num_compile_entries; entry++) {
+        ALOGD("regNum %d physicalType %d refCount %d type %d physicalReg %s",
+               compileTable[entry].regNum, compileTable[entry].physicalType,
+               compileTable[entry].refCount, compileTable[entry].gType,
+               physicalRegToString(static_cast<PhysicalReg>(
+                       compileTable[entry].physicalReg)));
     }
-}
-
-//!check whether a basic block is the start of an exception handler
-
-//!
-bool isFirstOfHandler(BasicBlock_O1* bb) {
-    int i;
-    for(i = 0; i < num_exception_handlers; i++) {
-        if(bb->pc_start == exceptionHandlers[i]) return true;
-    }
-    return false;
-}
-
-//! create a basic block that starts at src_pc and ends at end_pc
-
-//!
-BasicBlock_O1* createBasicBlock(int src_pc, int end_pc) {
-    BasicBlock_O1* bb = (BasicBlock_O1*)malloc(sizeof(BasicBlock_O1));
-    if(bb == NULL) {
-        ALOGI("JIT_INFO: Out of memory when trying to alloc basic block for pc %d\n", src_pc);
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return NULL;
+    for(int entry = 0; entry < num_memory_vr; entry++) {
+        ALOGD("v%d inMemory:%s", memVRTable[entry].regNum,
+                memVRTable[entry].inMemory ? "yes" : "no");
     }
-    bb->pc_start = src_pc;
-    bb->bb_index = num_bbs_for_method;
-    if(bb_entry == NULL) bb_entry = bb;
 
-    /* insert the basic block to method_bbs_sorted in ascending order of pc_start */
-    int k;
-    int index = -1;
-    for(k = 0; k < num_bbs_for_method; k++)
-        if(method_bbs_sorted[k]->pc_start > src_pc) {
-            index = k;
-            break;
-        }
-    if(index == -1)
-        method_bbs_sorted[num_bbs_for_method] = bb;
-    else {
-        /* push the elements from index by 1 */
-        for(k = num_bbs_for_method-1; k >= index; k--)
-            method_bbs_sorted[k+1] = method_bbs_sorted[k];
-        method_bbs_sorted[index] = bb;
-    }
-    num_bbs_for_method++;
-    if(num_bbs_for_method >= MAX_NUM_BBS_PER_METHOD) {
-        ALOGI("JIT_INFO: Exceeded maximum number of basic blocks\n");
-        SET_JIT_ERROR(kJitErrorTraceFormation);
-        return NULL;
+    for(int entry = 0; entry < num_const_vr; entry++) {
+        ALOGD("v%d isConst:%s value:%d", constVRTable[entry].regNum,
+                constVRTable[entry].isConst ? "yes" : "no",
+                constVRTable[entry].value);
     }
-    return bb;
+    ALOGD("---------------------------------------------------------");
 }
 
 /* BEGIN code to handle state transfers */
@@ -5294,6 +5666,8 @@ void goToState(int stateNum) {
             SET_JIT_ERROR(kJitErrorRegAllocFailed);
             return;
         }
+        DEBUG_MEMORYVR(ALOGD("Updating state of v%d %sin memory",
+                memVRTable[k].regNum, (memVRTable[k].inMemory ? "" : "NOT ")));
     }
 }
 typedef struct TransferOrder {
@@ -5652,7 +6026,7 @@ void transferToState(int stateNum) {
                                             targetReg, true);
                 }
                 if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg == PhysicalReg_Null) {
-                    dumpToMem(compileTable[k].regNum, (LowOpndRegType)(compileTable[k].physicalType & MASK_FOR_TYPE),
+                    writeBackVR(compileTable[k].regNum, (LowOpndRegType)(compileTable[k].physicalType & MASK_FOR_TYPE),
                               compileTable[k].physicalReg);
                 }
             } //VR
@@ -5704,33 +6078,51 @@ void transferToState(int stateNum) {
             ALOGW("inMemory mismatch for VR %d in transferToState", targetReg);
 #endif
             bool doneXfer = false;
+
             int index = searchCompileTable(LowOpndRegType_xmm | LowOpndRegType_virtual, targetReg);
             if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-                dumpToMem(targetReg, LowOpndRegType_xmm, compileTable[index].physicalReg);
+                writeBackVR(targetReg, LowOpndRegType_xmm, compileTable[index].physicalReg);
+                doneXfer = true;
+            } else if (index >= 0 && writeBackVRIfConstant(targetReg, LowOpndRegType_xmm) == true) {
                 doneXfer = true;
             }
+
             if(!doneXfer) { //vA-1, xmm
                 index = searchCompileTable(LowOpndRegType_xmm | LowOpndRegType_virtual, targetReg-1);
                 if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-                    dumpToMem(targetReg-1, LowOpndRegType_xmm, compileTable[index].physicalReg);
+                    writeBackVR(targetReg-1, LowOpndRegType_xmm, compileTable[index].physicalReg);
+                    doneXfer = true;
+                }
+                else if (index >= 0 && writeBackVRIfConstant(targetReg - 1, LowOpndRegType_xmm) == true) {
                     doneXfer = true;
                 }
             }
             if(!doneXfer) { //vA gp
                 index = searchCompileTable(LowOpndRegType_gp | LowOpndRegType_virtual, targetReg);
                 if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-                    dumpToMem(targetReg, LowOpndRegType_gp, compileTable[index].physicalReg);
+                    writeBackVR(targetReg, LowOpndRegType_gp, compileTable[index].physicalReg);
+                    doneXfer = true;
+                }
+                else if (index >= 0 && writeBackVRIfConstant(targetReg, LowOpndRegType_gp) == true) {
                     doneXfer = true;
                 }
             }
             if(!doneXfer) { //vA, ss
                 index = searchCompileTable(LowOpndRegType_ss | LowOpndRegType_virtual, targetReg);
                 if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-                    dumpToMem(targetReg, LowOpndRegType_ss, compileTable[index].physicalReg);
+                    writeBackVR(targetReg, LowOpndRegType_ss, compileTable[index].physicalReg);
+                    doneXfer = true;
+                }
+                else if (index >= 0 && writeBackVRIfConstant(targetReg, LowOpndRegType_ss) == true) {
                     doneXfer = true;
                 }
             }
-            if(!doneXfer) ALOGW("can't match inMemory of VR %d in transferToState", targetReg);
+            if(!doneXfer) {
+                ALOGE("JIT_INFO: Can't match inMemory state of v%d in "
+                        "transferToState.", targetReg);
+                SET_JIT_ERROR(kJitErrorStateTransfer);
+                return;
+            }
         }
         if((!targetBool) && memVRTable[k].inMemory) {
             //do nothing
diff --git a/vm/compiler/codegen/x86/AnalysisO1.h b/vm/compiler/codegen/x86/AnalysisO1.h
index ae0aed0..634e336 100644
--- a/vm/compiler/codegen/x86/AnalysisO1.h
+++ b/vm/compiler/codegen/x86/AnalysisO1.h
@@ -21,12 +21,14 @@
 #ifndef _DALVIK_NCG_ANALYSISO1_H
 #define _DALVIK_NCG_ANALYSISO1_H
 
+#include <set>
 #include "Dalvik.h"
 #include "enc_wrapper.h"
 #include "Lower.h"
 #ifdef WITH_JIT
 #include "compiler/CompilerIR.h"
 #endif
+#include "RegisterizationBE.h"
 
 //! maximal number of edges per basic block
 #define MAX_NUM_EDGE_PER_BB 300
@@ -40,11 +42,9 @@
 #define MAX_REG_PER_METHOD 200
 //! maximal number of temporaries per bytecode
 #define MAX_TEMP_REG_PER_BYTECODE 30
-//! maximal number of GG GPR VRs in a method
-#define MAX_GLOBAL_VR      2
-//! maximal number of GG XMM VRs in a method
-#define MAX_GLOBAL_VR_XMM  4
+
 #define MAX_CONST_REG 150
+#define NUM_MEM_VR_ENTRY 140
 
 #define MASK_FOR_TYPE 7 //last 3 bits 111
 
@@ -182,7 +182,6 @@ typedef struct VirtualRegInfo {
   int refCount;
   RegAccessType accessType;
   GlobalType gType;
-  int physicalReg_GG;
   RegAllocConstraint allocConstraints[8];
   RegAllocConstraint allocConstraintsSorted[8];
 
@@ -210,16 +209,21 @@ typedef struct BoundCheckIndex {
   int indexVR;
   bool checkDone;
 } BoundCheckIndex;
-//!information for a virtual register such as live ranges, in memory
+
+/**
+ * @class MemoryVRInfo
+ * @brief information for a virtual register such as live ranges, in memory
+ */
 typedef struct MemoryVRInfo {
-  int regNum;
-  bool inMemory;
-  bool nullCheckDone;
-  BoundCheckIndex boundCheck;
-  int num_ranges;
-  LiveRange* ranges;
-  u4 delayFreeFlags; //! for use with flags defined by VRDelayFreeFlags enum
+  int regNum;                   /**< @brief The register number */
+  bool inMemory;                /**< @brief Is it in memory or not */
+  bool nullCheckDone;           /**< @brief Has a null check been done for it? */
+  BoundCheckIndex boundCheck;   /**< @brief Bound check information for the VR */
+  int num_ranges;               /**< @brief Number of ranges, used as a size for ranges */
+  LiveRange* ranges;            /**< @brief Live range information for the entry */
+  u4 delayFreeFlags;            /**< @brief Used with flags defined by VRDelayFreeFlags enum to delay freeing */
 } MemoryVRInfo;
+
 //!information of a temporary
 //!the pair <regNum, physicalType> uniquely determines a variable
 typedef struct TempRegInfo {
@@ -229,33 +233,42 @@ typedef struct TempRegInfo {
   int linkageToVR;
   int versionNum;
   bool shareWithVR; //for temp. regs updated by get_virtual_reg
-  //! Set to TRUE to allow using VR liveness information to decide register sharing in regardless of the shareWithVR setting
-  bool shareRegWithLivenessInfo;
   bool is8Bit;
 } TempRegInfo;
 struct BasicBlock_O1;
 //!all variables accessed
-//!the pair <regNum, physicalType> uniquely determines a variable
-typedef struct compileTableEntry {
-  int regNum;
-  int physicalType; //gp, xmm or scratch, virtual
-  int physicalReg;
-  int physicalReg_prev; //for spilled GG VR
-  RegAccessType accessType;
-
-  bool isConst;
-  int value[2]; //[0]: lower [1]: higher
-  int refCount;
 
-  int linkageToVR; //for temporary registers only
-  GlobalType gType;
-  struct BasicBlock_O1* bb; //bb VR belongs to
-  int indexToInfoBB;
+/**
+ * @class compileTableEntry
+ * @brief compileTableEntry represents an entry to the compilation table, helping the compiler follow what register is where
+ * @details the pair <regNum, physicalType> uniquely determines a variable
+ */
+struct compileTableEntry {
+    int regNum;               /**< @brief The register number */
+    int physicalType;         /**< @brief The physical type: gp, xmm or scratch, virtual */
+    int physicalReg;          /**< @brief Which physical register was chosen */
+
+  // TODO isConst and value[2] should be removed because we keep track of them
+  // via the constVRTable
+    bool isConst;             /**< @brief Is the entry a constant */
+    int value[2];             /**< @brief The value: [0]: lower [1]: higher */
+    int refCount;             /**< @brief Number of reference counts for the entry */
+
+    int linkageToVR;          /**< @brief Linked to which VR, for temporary registers only */
+    GlobalType gType;         /**< @brief What is the GlobalType for the entry */
+
+    int spill_loc_index;      /**< @brief what is the spill location index (for temporary registers only) */
+    bool isWritten;           /**< @brief is the entry written */
+
+    PhysicalReg getPhysicalReg() {
+        return static_cast<PhysicalReg>(physicalReg);
+    }
+
+    void setPhysicalReg(PhysicalReg newReg) {
+        physicalReg = newReg;
+    }
+};
 
-  VRState regState;
-  TRState trState; //for temporary registers only
-  int spill_loc_index; //for temporary registers only
-} compileTableEntry;
 //!to save the state of register allocator
 typedef struct regAllocStateEntry1 {
   int spill_loc_index;
@@ -270,38 +283,43 @@ typedef struct Edge_O1 {
   struct BasicBlock_O1* src;
   struct BasicBlock_O1* dst;
 } Edge_O1;
+
+//Forward declaration
+struct LowOpBlockLabel;
+
 //!information associated with a basic block
-typedef struct BasicBlock_O1 {
-  int bb_index;
-  int bb_index2;
+struct BasicBlock_O1 : BasicBlock {
   int pc_start;       //!inclusive
-#ifndef WITH_JIT
-  int pc_end;         //!exclusive
-  Edge_O1* in_edges[MAX_NUM_EDGE_PER_BB]; //array of Edge*
-  int num_in_edges;
-  Edge_O1* out_edges[MAX_NUM_EDGE_PER_BB];
-  int num_out_edges;
-#else
-  char *streamStart;        //Where the code generation started for the BasicBlock
   int pc_end;
-  BasicBlock* jitBasicBlock;
-#endif
-  VirtualRegInfo infoBasicBlock[MAX_REG_PER_BASICBLOCK];
-  int num_regs;
+  char *streamStart;        //Where the code generation started for the BasicBlock
 
-  RegAllocConstraint allocConstraints[8]; //# of times a hardcoded register is used in this basic block
+  std::vector<VirtualRegInfo> infoBasicBlock;
+
+  RegAllocConstraint allocConstraints[PhysicalReg_EndOfGPMarker+1]; //# of times a hardcoded register is used in this basic block
   //a physical register that is used many times has a lower priority to get picked in getFreeReg
-  RegAllocConstraint allocConstraintsSorted[8]; //count from low to high
+  RegAllocConstraint allocConstraintsSorted[PhysicalReg_EndOfGPMarker+1]; //count from low to high
 
   DefUsePair* defUseTable;
   DefUsePair* defUseTail;
   int num_defs;
-  XferPoint xferPoints[MAX_XFER_PER_BB]; //program points where the transfer is required
-  int num_xfer_points;
+
+  std::vector <XferPoint> xferPoints; //program points where the transfer is required
 
   bool endsWithReturn;
   bool hasAccessToGlue;
-} BasicBlock_O1;
+
+  AssociationTable associationTable;    //Association table to keep track of physical registers beyond a BasicBlock
+
+  LowOpBlockLabel *label;               //Label for the BasicBlock
+
+  //Constructor
+  BasicBlock_O1 (void);
+  //Clear function: do we allocate the label (default: false)
+  void clear (bool allocateLabel = false);
+
+  //Clear and free everything
+  void freeIt (void);
+};
 typedef struct CFG_O1 {
   BasicBlock_O1* head;
 } CFG_O1;
@@ -313,6 +331,10 @@ typedef struct CFGWork {
 } CFGWork;
 
 /////////////////////////////////////////
+//! array of MemoryVRInfo to store whether a VR is in memory
+
+extern MemoryVRInfo memVRTable[NUM_MEM_VR_ENTRY];
+extern int num_memory_vr;
 extern compileTableEntry compileTable[COMPILE_TABLE_SIZE];
 extern int num_compile_entries;
 extern VirtualRegInfo infoByteCode[MAX_REG_PER_BYTECODE];
@@ -323,34 +345,14 @@ extern VirtualRegInfo infoMethod[MAX_REG_PER_METHOD];
 extern int num_regs_per_method;
 extern BasicBlock_O1* currentBB;
 
-extern BasicBlock_O1* method_bbs[MAX_NUM_BBS_PER_METHOD];
+// TODO The usage of method_bbs_sorted should be eliminated since
+// we automatically get all of the BasicBlock_O1 as part of the cUnit
 extern int num_bbs_for_method;
 extern BasicBlock_O1* method_bbs_sorted[MAX_NUM_BBS_PER_METHOD];
-extern BasicBlock_O1* bb_entry;
-extern int pc_start;
-extern int pc_end;
-extern int current_bc_size;
-extern int num_exception_handlers;
-extern int exceptionHandlers[10];
 
 extern int num_const_vr;
 extern ConstVRInfo constVRTable[MAX_CONST_REG];
 
-extern int genSet[MAX_REG_PER_BYTECODE];
-extern int killSet[MAX_REG_PER_BYTECODE];
-extern int num_regs_gen; //per bytecode
-extern int num_regs_kill; //per bytecode
-
-extern int genSetBB[MAX_NUM_BBS_PER_METHOD][40];
-extern int killSetBB[MAX_NUM_BBS_PER_METHOD][40]; //same as size of memVRTable
-extern int num_gen_bb[MAX_NUM_BBS_PER_METHOD];
-extern int num_kill_bb[MAX_NUM_BBS_PER_METHOD];
-
-extern int nullCheck_inB[MAX_NUM_BBS_PER_METHOD][40];
-extern int nullCheck_inSize[MAX_NUM_BBS_PER_METHOD];
-extern int nullCheck_outB[MAX_NUM_BBS_PER_METHOD][40];
-extern int nullCheck_outSize[MAX_NUM_BBS_PER_METHOD];
-
 typedef enum GlueVarType {
   RES_CLASS = 0,
   RES_METHOD,
@@ -371,7 +373,6 @@ int createCFGHandler(Method* method);
 
 int findVirtualRegInTable(u2 vA, LowOpndRegType type);
 int searchCompileTable(int type, int regNum);
-BasicBlock_O1* createBasicBlock(int src_pc, int end_pc);
 void handleJump(BasicBlock_O1* bb_prev, int relOff);
 void connectBasicBlock(BasicBlock_O1* src, BasicBlock_O1* dst);
 int insertWorklist(BasicBlock_O1* bb_prev, int targetOff);
@@ -382,5 +383,33 @@ void updateCurrentBBWithConstraints(PhysicalReg reg);
 void updateConstInfo(BasicBlock_O1*);
 OpndSize getRegSize(int type);
 void invalidateVRDueToConst(int reg, OpndSize size);
+
+//Set a VR to a constant value
+void setVRToConst(int regNum, OpndSize size, int* tmpValue);
+
+//Find free registers and update the set
+void findFreeRegisters(std::set<PhysicalReg> & outFreeRegisters);
+
+//Get a scratch register of a given type
+PhysicalReg getScratch(std::set<PhysicalReg> & scratchCandidates,
+        LowOpndRegType type);
+
+//Synchronize all registers in the compileTable
+void syncAllRegs(void);
+
+//Get a type for a given register
+LowOpndRegType getTypeOfRegister(PhysicalReg reg);
+
+//Update the physical register in the compile table from oldReg to newReg
+bool updatePhysicalRegForVR(int vR, PhysicalReg oldReg, PhysicalReg newReg);
+
+//Check whether a type is a virtual register type
+bool isVirtualReg(int type);
+
+//Spill a logical register using a index in the compileTable
+int spillLogicalReg(int spill_index, bool updateTable);
+
+// Search in the memory table for a register
+int searchMemTable(int regNum);
 #endif
 
diff --git a/vm/compiler/codegen/x86/BytecodeVisitor.cpp b/vm/compiler/codegen/x86/BytecodeVisitor.cpp
index 30bf10f..cbce454 100644
--- a/vm/compiler/codegen/x86/BytecodeVisitor.cpp
+++ b/vm/compiler/codegen/x86/BytecodeVisitor.cpp
@@ -439,30 +439,6 @@ static int touchOneVR(u2 vA, LowOpndRegType type) {
     return 0;
 }
 
-//! \brief reduces refCount of two virtual registers
-//!
-//! \param vA
-//! \param vB
-//! \param type
-//!
-//! \return -1 if error, 0 otherwise
-static int touchTwoVRs(u2 vA, u2 vB, LowOpndRegType type) {
-    int index = searchCompileTable(LowOpndRegType_virtual | type, vA);
-    if(index < 0) {
-        ALOGI("JIT_INFO: virtual reg %d type %d not found in touchTwoVRs\n", vA, type);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-    compileTable[index].refCount--;
-    index = searchCompileTable(LowOpndRegType_virtual | type, vB);
-    if(index < 0) {
-        ALOGI("JIT_INFO: virtual reg %d type %d not found in touchTwoVRs\n", vB, type);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-    compileTable[index].refCount--;
-    return 0;
-}
 int num_const_worklist;
 //! worklist to update constVRTable later
 int constWorklist[10];
@@ -582,6 +558,16 @@ int getConstInfo(BasicBlock_O1* bb, const MIR * currentMIR) {
         return 0; // does NOT generate a constant
 #endif
 
+    // Check if we need to handle an extended MIR
+    if (currentMIR->dalvikInsn.opcode >= static_cast<Opcode> (kMirOpFirst)) {
+        //Currently no extended MIR generates constants
+        switch (static_cast<ExtendedMIROpcode>(currentMIR->dalvikInsn.opcode)) {
+            default:
+                // No constant is generated
+                return 0;
+        }
+    }
+
     switch(inst_op) {
         //for other opcode, if update the register, set isConst to false
     case OP_MOVE:
@@ -1309,6 +1295,43 @@ int getVirtualRegInfo(VirtualRegInfo* infoArray, const MIR * currentMIR) {
         return currentMIR->width;
 #endif
 
+    // Check if we need to handle an extended MIR
+    if (currentMIR->dalvikInsn.opcode >= static_cast<Opcode> (kMirOpFirst)) {
+        switch (static_cast<ExtendedMIROpcode>(currentMIR->dalvikInsn.opcode)) {
+            case kMirOpRegisterize:
+                {
+                    infoArray[0].regNum = currentMIR->dalvikInsn.vA;
+                    infoArray[0].refCount = 2;
+                    infoArray[0].accessType = REGACCESS_DU;
+                    //Ok the type really depends on vB
+                    RegisterClass regClass = static_cast<RegisterClass> (currentMIR->dalvikInsn.vB);
+
+                    //Decide the type depending on the register class
+                    switch (regClass) {
+                        case kCoreReg:
+                            infoArray[0].physicalType = LowOpndRegType_gp;
+                            break;
+                        case kSFPReg:
+                            infoArray[0].physicalType = LowOpndRegType_ss;
+                            break;
+                        case kDFPReg:
+                            infoArray[0].physicalType = LowOpndRegType_xmm;
+                            break;
+                        default:
+                            ALOGE("JIT_ERROR: kMirOpRegisterize does not support regClass %d", regClass);
+                            SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+                            break;
+                    }
+                    num_regs_per_bytecode = 1;
+                }
+                return currentMIR->width;
+            default:
+                ALOGE("JIT ERROR! Extended MIR not supported in getVirtualRegInfo");
+                dvmAbort();
+                break;
+        }
+    }
+
     switch (inst_op) {
     case OP_NOP:
         codeSize = 1;
@@ -3367,7 +3390,6 @@ int getTempRegInfo(TempRegInfo* infoArray, const MIR * currentMIR) { //returns a
         infoArray[k].linkageToVR = -1;
         infoArray[k].versionNum = 0;
         infoArray[k].shareWithVR = true;
-        infoArray[k].shareRegWithLivenessInfo = true;
         infoArray[k].is8Bit = false;
     }
     u2 vA, vB, v1, v2, length, num, tmp;
@@ -3382,6 +3404,31 @@ int getTempRegInfo(TempRegInfo* infoArray, const MIR * currentMIR) { //returns a
         return 0; // No temporaries accessed
 #endif
 
+    // Check if we need to handle an extended MIR
+    if (currentMIR->dalvikInsn.opcode >= static_cast<Opcode> (kMirOpFirst)) {
+        switch (static_cast<ExtendedMIROpcode>(currentMIR->dalvikInsn.opcode)) {
+            case kMirOpPhi:
+                return 0;
+            case kMirOpRegisterize:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 2; //UD
+                //Ok the type really depends on vB
+                if (currentMIR->dalvikInsn.vB == static_cast<int> (kCoreReg))
+                {
+                    infoArray[0].physicalType = LowOpndRegType_gp;
+                }
+                else
+                {
+                    infoArray[0].physicalType = LowOpndRegType_xmm;
+                }
+                return 1;
+            default:
+                ALOGE("JIT ERROR! Extended MIR not supported in getTempRegInfo");
+                dvmAbort();
+                break;
+        }
+    }
+
     switch (inst_op) {
     case OP_NOP:
         return 0;
diff --git a/vm/compiler/codegen/x86/CodegenErrors.cpp b/vm/compiler/codegen/x86/CodegenErrors.cpp
index c088121..f9a8567 100644
--- a/vm/compiler/codegen/x86/CodegenErrors.cpp
+++ b/vm/compiler/codegen/x86/CodegenErrors.cpp
@@ -38,6 +38,9 @@ static const char* jitErrorMessages[kJitErrorMaxDefined] = {
     "Problem while merging live ranges (mergeLiveRange)",
     "Global data not defined",
     "Problem while scheduling instructions"
+    "Issue registerizing the trace in the backend",
+    "The trace provoked a spill",
+    "The backend decided it cannot safely handle the Basic Block",
     //Add error messages here when adding error codes
 };
 
@@ -57,7 +60,6 @@ bool isErrorFatal(JitCompilationErrors jitError) {
         default:
             return false;
     }
-
 }
 
 /**
@@ -126,6 +128,44 @@ bool dvmCanFixErrorsAndRetry(CompilationUnit *cUnit){
         CLEAR_JIT_ERROR(kJitErrorShortJumpOffset);
     }
 
+    //Handle error due to spilling
+    if (IS_JIT_ERROR_SET(kJitErrorSpill)) {
+        //Clear the error:
+        CLEAR_JIT_ERROR(kJitErrorSpill);
+
+        //Ok we are going to see if we are registerizing something
+        int max = cUnit->maximumRegisterization;
+
+        // We should only get this error if maximum registerization is > 0
+        assert (max > 0);
+
+        //Divide it by 2, fastest way to get to 0 if we have issues across the board
+        cUnit->maximumRegisterization = max / 2;
+        ALOGI("Trying less registerization from %d to %d", max, (max / 2));
+    }
+
+    //Handle error due to backend
+    if (IS_JIT_ERROR_SET(kJitErrorBERegisterization)) {
+        //If registerization in the Backend is on
+        if (gDvmJit.backEndRegisterization == true) {
+            //Clear the error:
+            CLEAR_JIT_ERROR(kJitErrorBERegisterization);
+
+            //Turn off registerization
+            gDvmJit.backEndRegisterization = false;
+
+            //Set maximum registerization for this cUnit to 0
+            //since we disabled registerization
+            cUnit->maximumRegisterization = 0;
+
+            ALOGE("Trying to turn backend registerization off");
+            ALOGE("++++++++++++++++++++++++++++++++++++++++++++");
+
+            //Return true to try again
+            return true;
+        }
+    }
+
     /* If at this point, we again have no errors set
      * we have successfully resolved the errors. We can
      * retry the trace now.
@@ -145,9 +185,10 @@ bool dvmCanFixErrorsAndRetry(CompilationUnit *cUnit){
 
 void dvmSaveOptimizationState(SErrorCompilationState &info) {
     info.disableOpt = gDvmJit.disableOpt;
+    info.backEndRegisterization = gDvmJit.backEndRegisterization;
 }
 
 void dvmRestoreCompilationState(SErrorCompilationState &info) {
     gDvmJit.disableOpt = info.disableOpt;
+    gDvmJit.backEndRegisterization = info.backEndRegisterization;
 }
-
diff --git a/vm/compiler/codegen/x86/CodegenErrors.h b/vm/compiler/codegen/x86/CodegenErrors.h
index e3383d3..a5fa55f 100644
--- a/vm/compiler/codegen/x86/CodegenErrors.h
+++ b/vm/compiler/codegen/x86/CodegenErrors.h
@@ -28,13 +28,16 @@
 #include "../../CompilerIR.h"
 
 /**
- * @class ErrorFlags
  * @brief Keep any information that can be changed by the error framework
  */
 typedef struct sErrorCompilationState {
     int disableOpt;                 /**< @brief Disable the optimizations */
+    bool backEndRegisterization;    /**< @brief Backend registerization */
 }SErrorCompilationState;
 
+/**
+ * @class ErrorFlags
+ */
 enum JitCompilationErrors {
     /** @brief Exceeded maximum allowed VRs in a basic block */
     kJitErrorMaxVR = 0,
@@ -70,6 +73,12 @@ enum JitCompilationErrors {
     kJitErrorGlobalData,
     /** @brief Errors while scheduling instructions */
     kJitErrorInsScheduling,
+    /** @brief Errors due to backend registerization */
+    kJitErrorBERegisterization,
+    /** @brief Errors due to spilling logical registers */
+    kJitErrorSpill,
+    /** @brief Set when a basic block is reject by backend */
+    kJitErrorBBCannotBeHandled,
 
     /* ----- Add more errors above ---------------------------*/
     /* ----- Don't add new errors beyond this point ----------*/
@@ -109,7 +118,8 @@ enum JitCompilationErrors {
 /* Checks if ANY error is set */
 #define IS_ANY_JIT_ERROR_SET() (gDvmJit.jitErrorFlags != 0)
 
-#define MAX_RETRIES 1
+/* Max retries limits the number of retries for a given trace, used in CodegenInterface.cpp */
+#define MAX_RETRIES 2
 
 /**
  * @brief Checks whether a compilation should be re-attempted
diff --git a/vm/compiler/codegen/x86/CodegenInterface.cpp b/vm/compiler/codegen/x86/CodegenInterface.cpp
index f22e8a3..96737c3 100644
--- a/vm/compiler/codegen/x86/CodegenInterface.cpp
+++ b/vm/compiler/codegen/x86/CodegenInterface.cpp
@@ -245,6 +245,19 @@ bool dvmCompilerArchInit() {
     //Disable Method-JIT
     gDvmJit.disableOpt |= (1 << kMethodJit);
 
+#ifdef HAVE_ANDROID_OS
+    // If JIT verbose has not been enabled, check the global property dalvik.jit.verbose
+    if (!gDvmJit.printMe) {
+        memset(propertyBuffer, 0, PROPERTY_VALUE_MAX); // zero out buffer so we don't use junk
+        property_get("dalvik.jit.verbose", propertyBuffer, NULL);
+        // Look for text ". We could enable finer control by checking application
+        // name, but the VM would need to know which application it is running
+        if (strncmp("true", propertyBuffer, PROPERTY_VALUE_MAX) == 0) {
+            gDvmJit.printMe = true;
+        }
+    }
+#endif
+
     // Now determine machine model
     asm volatile (
             "movl $1, %%eax\n\t"
@@ -713,7 +726,7 @@ done:
 }
 
 #define BYTES_OF_NORMAL_CHAINING 13
-#define BYTES_OF_BACKWARDBRANCH_CHAINING 21
+#define BYTES_OF_BACKWARDBRANCH_CHAINING 25
 #define BYTES_OF_HOT_CHAINING 17
 #define BYTES_OF_SINGLETON_CHAINING 13
 #define BYTES_OF_PREDICTED_CHAINING 20
@@ -923,8 +936,7 @@ void dvmJitUnchainAll()
                   rPC
                   codePtr
 */
-static int handleNormalChainingCell(CompilationUnit *cUnit,
-                                     unsigned int offset, int blockId, LowOpBlockLabel* labelList)
+static int handleNormalChainingCell(CompilationUnit *cUnit, unsigned int offset, int blockId)
 {
     ALOGV("In handleNormalChainingCell for method %s block %d BC offset %x NCG offset %x",
           cUnit->method->name, blockId, offset, stream - streamMethodStart);
@@ -953,8 +965,7 @@ static int handleNormalChainingCell(CompilationUnit *cUnit,
                   codePtr
                   ismove_flag
  */
-static int handleHotChainingCell(CompilationUnit *cUnit,
-                                  unsigned int offset, int blockId, LowOpBlockLabel* labelList)
+static int handleHotChainingCell(CompilationUnit *cUnit, unsigned int offset, int blockId)
 {
     ALOGV("In handleHotChainingCell for method %s block %d BC offset %x NCG offset %x",
           cUnit->method->name, blockId, offset, stream - streamMethodStart);
@@ -989,9 +1000,11 @@ static int handleHotChainingCell(CompilationUnit *cUnit,
                   codePtr
                   loop header address
                   vrStoreCodePtr
+                  loop preheader address
 */
 static int handleBackwardBranchChainingCell(CompilationUnit *cUnit,
-                                     unsigned int offset, int blockId, LowOpBlockLabel* labelList, char *loopHeaderAddr)
+                                     unsigned int offset, int blockId, char *loopHeaderAddr,
+                                     char *preLoopHeaderAddr)
 
 {
     ALOGV("In handleBackwardBranchChainingCell for method %s block %d BC offset %x NCG offset %x",
@@ -1003,6 +1016,7 @@ static int handleBackwardBranchChainingCell(CompilationUnit *cUnit,
     char* vrStoreCodePtr = NULL;
     char* chainingCellHead = stream;
     call_dvmJitToInterpBackwardBranch();
+
     unsigned int *ptr = (unsigned int*)stream;
     *ptr++ = (unsigned int)(cUnit->method->insns + offset);
 
@@ -1023,7 +1037,9 @@ static int handleBackwardBranchChainingCell(CompilationUnit *cUnit,
         *ptr++ = (int)vrStoreCodePtr;
     else
         *ptr++ = (int)chainingCellHead;
-    stream = (char*)ptr;
+    *ptr++ = (unsigned int) preLoopHeaderAddr;
+    //Save back stream
+    stream = (char *) ptr;
     return 0;
 }
 
@@ -1034,7 +1050,7 @@ static int handleBackwardBranchChainingCell(CompilationUnit *cUnit,
                   codePtr
 */
 static int handleInvokeSingletonChainingCell(CompilationUnit *cUnit,
-                                              const Method *callee, int blockId, LowOpBlockLabel* labelList)
+                                              const Method *callee, int blockId)
 {
     ALOGV("In handleInvokeSingletonChainingCell for method %s block %d callee %s NCG offset %x",
           cUnit->method->name, blockId, callee->name, stream - streamMethodStart);
@@ -1095,6 +1111,12 @@ static void handleInvokePredictedChainingCell(CompilationUnit *cUnit, int blockI
 /* Extended MIR instructions like PHI */
 void handleExtendedMIR(CompilationUnit *cUnit, MIR *mir)
 {
+    if (dump_x86_inst) {
+        char * decodedString = dvmCompilerGetDalvikDisassembly(&mir->dalvikInsn,
+                NULL);
+        ALOGI("LOWER %s @%p\n", decodedString, stream);
+    }
+
     ExecutionMode origMode = gDvm.executionMode;
     gDvm.executionMode = kExecutionModeNcgO0;
     switch ((ExtendedMIROpcode)mir->dalvikInsn.opcode) {
@@ -1124,6 +1146,11 @@ void handleExtendedMIR(CompilationUnit *cUnit, MIR *mir)
         case kMirOpPunt: {
             break;
         }
+        case kMirOpRegisterize: {
+            gDvm.executionMode = origMode;
+            genRegisterize (cUnit, mir);
+            break;
+        }
 #ifdef WITH_JIT_INLINING_PHASE2
         case kMirOpCheckInlinePrediction: { //handled in ncg_o1_data.c
             genValidationForPredictedInline(cUnit, mir);
@@ -1200,17 +1227,6 @@ void printEmittedCodeBlock(unsigned char *startAddr, unsigned char *endAddr)
     }
 }
 
-// Return true if there are branch inside loop
-bool hasBranchInLoop(CompilationUnit *cUnit)
-{
-    BasicBlock *firstBB = cUnit->entryBlock->fallThrough;
-    if(firstBB->taken && firstBB->taken == cUnit->backChainBlock)
-       return false;
-    if(firstBB->fallThrough && firstBB->fallThrough == cUnit->backChainBlock)
-       return false;
-    return true;
-}
-
 /**
  * @brief Handle fallthrough branch: determine whether we need one or not
  * @param cUnit the CompilationUnit
@@ -1235,6 +1251,53 @@ static void handleFallThroughBranch (CompilationUnit *cUnit, BasicBlock *bb, Bas
 }
 
 /**
+ * @brief Handle the if back-end case where the taken branch has a table and we must merge
+ * @param cUnit the CompilationUnit
+ * @param bb the current BasicBlock
+ */
+static void handleIfTaken (CompilationUnit *cUnit, BasicBlock *bb)
+{
+    BasicBlock *ft = bb->fallThrough;
+    BasicBlock *taken = bb->taken;
+
+    //Only care if we have a taken and a fallthrough
+    if (ft != 0 && taken != 0)
+    {
+        //Paranoid, we should have an if at the end
+        assert (bb->lastMIRInsn != 0 &&
+                bb->lastMIRInsn->dalvikInsn.opcode >= OP_IF_EQ &&
+                bb->lastMIRInsn->dalvikInsn.opcode <= OP_IF_LEZ);
+
+        //Now only care about it if the taken has a table
+        BasicBlock_O1 *takenO1 = reinterpret_cast<BasicBlock_O1 *> (taken);
+        assert (takenO1 != 0);
+
+        if (takenO1->associationTable.hasBeenFinalized () == true)
+        {
+            //Ok we need a BB between us and the taken
+            BasicBlock *inter = dvmCompilerNewBB (kDalvikByteCode, cUnit->numBlocks++);
+            dvmInsertGrowableList(&cUnit->blockList, (intptr_t) inter);
+
+            //Now update links
+            inter->fallThrough = taken;
+            bb->taken = inter;
+
+            //Update predecessors, the back-end must do it itself
+            if (taken->predecessors != 0)
+            {
+                dvmClearBit (taken->predecessors, bb->id);
+                dvmSetBit (taken->predecessors, inter->id);
+            }
+
+            if (inter->predecessors != 0)
+            {
+                dvmSetBit (inter->predecessors, bb->id);
+            }
+        }
+    }
+}
+
+/**
  * @brief Generate the code for the BasicBlock
  * @param cUnit the CompilationUnit
  * @param bb the BasicBlock
@@ -1243,25 +1306,17 @@ static void handleFallThroughBranch (CompilationUnit *cUnit, BasicBlock *bb, Bas
  */
 static bool generateCode (CompilationUnit *cUnit, BasicBlock *bb, BasicBlock **nextFallThrough)
 {
-    ALOGV("Get ready to handle JIT bb %d type %d hidden %d",
-            bb->id, bb->blockType, bb->hidden);
+    ALOGV("Get ready to handle JIT bb %d type %d hidden %d @%p",
+            bb->id, bb->blockType, bb->hidden, stream);
 
     /* We want to update the stream start to remember it for future backward chaining cells */
-    for (int k = 0; k < num_bbs_for_method; k++)
-    {
-        if(method_bbs_sorted[k]->jitBasicBlock == bb)
-        {
-            BasicBlock_O1 *currentBB = method_bbs_sorted[k];
-
-            //Now we can update the stream for it
-            currentBB->streamStart = stream;
-        }
-    }
+    BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (bb);
+    assert (bbO1 != 0);
+    bbO1->streamStart = stream;
 
     //If in O1, not the entry block, and actually have an instruction
     if(gDvm.executionMode == kExecutionModeNcgO1 &&
-            bb->blockType != kEntryBlock &&
-            bb->firstMIRInsn != NULL) {
+            bb->blockType != kEntryBlock) {
 
         //Generate the code
         startOfBasicBlock(bb);
@@ -1353,75 +1408,13 @@ static bool generateCode (CompilationUnit *cUnit, BasicBlock *bb, BasicBlock **n
         } // end for
     } // end else //JIT + O0 code generator
 
-    //Ok we are going to skip this if the last instruction is an if
-    //This could be skipped if the if was not automatically generating the fallthrough jump
-    MIR *lastInsn = bb->lastMIRInsn;
-    bool shouldRegister = (lastInsn == 0);
-
-    if (lastInsn != 0)
-    {
-        Opcode opcode = lastInsn->dalvikInsn.opcode;
-
-        //First off, we always register it if it's an extended MIR because dexGetFlagsFromOpcode can't handle it
-        shouldRegister = (opcode >= static_cast<Opcode> (kMirOpFirst));
-        if (shouldRegister == false)
-        {
-            //We don't care about instructions that can branch in general
-            //It's safe to call dexGetFlagsFromOpcode, opcode can't be extended anymore
-            shouldRegister = ((dexGetFlagsFromOpcode(opcode) & kInstrCanBranch) == 0);
-        }
-    }
-
-    //If need be, register it: handleFallThroughBranch will do the rest
-    if (shouldRegister == true)
-    {
-        *nextFallThrough = bb->fallThrough;
-    }
-    else
-    {
-        //Otherwise, reset nextFallThrough
-        *nextFallThrough = 0;
-    }
+    //Register next fall through
+    *nextFallThrough = bb->fallThrough;
 
     //Everything went fine
     return true;
 }
 
-static char* searchStreamStart(BasicBlock* bb)
-{
-    char* streamStart = NULL;
-
-    assert(bb != 0);
-
-    /* We want to update the stream start to remember it for future backward chaining cells */
-    for (int k = 0; k < num_bbs_for_method; k++) {
-        if(method_bbs_sorted[k]->jitBasicBlock == bb) {
-            BasicBlock_O1 *currentBB = method_bbs_sorted[k];
-
-            //Now we can update the stream for it
-            streamStart = currentBB->streamStart;
-        }
-    }
-    return streamStart;
-}
-
-/*
- * Get loop header block from backward branch chaining cell. StartOffset of backward chaining BB
-   is set to be the same as the loop header BB. This property is assumed to be maintained by middle
-   end in MIR CFG
-*/
-static BasicBlock * getLoopHeaderFromBBChaining(CompilationUnit *cUnit, BasicBlock* chainingBB) {
-    BasicBlock * bb;
-    GrowableList *blockList = &cUnit->blockList;
-
-    for (unsigned int i = 0; i < blockList->numUsed; i++) {
-        bb = (BasicBlock *) blockList->elemList[i];
-        if(bb->blockType == kDalvikByteCode && bb->startOffset == chainingBB->startOffset)
-            return bb;
-    }
-    return NULL;
-}
-
 /* 4 is the number  f additional bytes needed for chaining information for trace:
  * 2 bytes for chaining cell count offset and 2 bytes for chaining cell offset */
 #define EXTRA_BYTES_FOR_CHAINING 4
@@ -1439,10 +1432,8 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
 
     dump_x86_inst = cUnit->printMe;
 
-    /* Used to hold the labels of each block */
-    LowOpBlockLabel *labelList =
-        (LowOpBlockLabel *)dvmCompilerNew(sizeof(LowOpBlockLabel) * cUnit->numBlocks, true); //Utility.c
     GrowableList chainingListByType[kChainingCellLast];
+
     unsigned int i, padding;
 
     traceMode = cUnit->jitMode;
@@ -1455,7 +1446,6 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
     }
 
     GrowableListIterator iterator;
-    dvmGrowableListIteratorInit(&cUnit->blockList, &iterator);
 
     /* Traces start with a profiling entry point.  Generate it here */
     cUnit->profileCodeSize = genTraceProfileEntry(cUnit);
@@ -1478,29 +1468,42 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
     stream = (char*)(((unsigned int)stream + 0xF) & ~0xF); /* Align trace to 16-bytes */
     streamMethodStart = stream; /* code start */
 
-    for (i = 0; i < ((unsigned int) cUnit->numBlocks); i++) {
-        labelList[i].lop.generic.offset = -1;
-    }
     cUnit->exceptionBlockId = -1;
     for (i = 0; i < blockList->numUsed; i++) {
         bb = (BasicBlock *) blockList->elemList[i];
         if(bb->blockType == kExceptionHandling)
             cUnit->exceptionBlockId = i;
     }
-    startOfTrace(cUnit->method, labelList, cUnit->exceptionBlockId, cUnit);
+    startOfTrace(cUnit->method, cUnit->exceptionBlockId, cUnit);
+
     if(gDvm.executionMode == kExecutionModeNcgO1) {
-        for (i = 0; i < blockList->numUsed; i++) {
-            bb = (BasicBlock *) blockList->elemList[i];
-            if(bb->blockType == kDalvikByteCode) {
-                int retCode = preprocessingBB(bb);
-                if (retCode < 0) {
-                    endOfTrace(true/*freeOnly*/);
-                    cUnit->baseAddr = NULL;
-                    SET_JIT_ERROR(kJitErrorCodegen);
-                    return;
-                }
+
+        dvmGrowableListIteratorInit(&cUnit->blockList, &iterator);
+
+        //We do a first pass of the basic blocks before preprocessing because handleIfTaken might add some
+        for (bb = (BasicBlock *) (dvmGrowableListIteratorNext(&iterator));
+                bb != NULL;
+                bb = (BasicBlock *) (dvmGrowableListIteratorNext(&iterator))) {
+            //Handle the case of the taken if, in case an association table is already there
+            handleIfTaken (cUnit, bb);
+        }
+
+        //Start over the iterator
+        dvmGrowableListIteratorInit(&cUnit->blockList, &iterator);
+        for (bb = (BasicBlock *) (dvmGrowableListIteratorNext(&iterator));
+                bb != NULL;
+                bb = (BasicBlock *) (dvmGrowableListIteratorNext(&iterator))) {
+
+            int retCode = preprocessingBB(cUnit, bb);
+
+            if (retCode < 0) {
+                endOfTrace(true/*freeOnly*/);
+                cUnit->baseAddr = NULL;
+                SET_JIT_ERROR(kJitErrorCodegen);
+                return;
             }
         }
+
         if (preprocessingTrace() == -1) {
             endOfTrace(true/*freeOnly*/);
             cUnit->baseAddr = NULL;
@@ -1509,7 +1512,7 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
         }
     }
 
-    branchInLoop = cUnit->jitMode == kJitLoop && hasBranchInLoop(cUnit);
+    dvmGrowableListIteratorInit(&cUnit->blockList, &iterator);
 
     /* Handle the content in each basic block */
     for (bb = (BasicBlock *) (dvmGrowableListIteratorNext (&iterator)),
@@ -1520,30 +1523,25 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
          bb = (BasicBlock *) (dvmGrowableListIteratorNext (&iterator)),
          i++) {
 
-        //Set label information
-        labelList[i].immOpnd.value = bb->startOffset;
+        //Get O1 version
+        BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (bb);
 
-        if (bb->blockType >= kChainingCellLast) {
-            /*
-             * Append the label pseudo LIR first. Chaining cells will be handled
-             * separately afterwards.
-             */
-            dvmCompilerAppendLIR(cUnit, (LIR *) &labelList[i]);
+        //Paranoid
+        if (bbO1 == 0) {
+            continue;
         }
 
         //Switch depending on the BasicBlock type
-        switch (bb->blockType)
+        switch (bbO1->blockType)
         {
             case kEntryBlock:
                 //First handle fallthrough branch
-                handleFallThroughBranch (cUnit, bb, &nextFallThrough);
-
-                labelList[i].lop.opCode2 = ATOM_PSEUDO_ENTRY_BLOCK;
+                handleFallThroughBranch (cUnit, bbO1, &nextFallThrough);
 
                 //Set label offset
-                labelList[i].lop.generic.offset = (stream - streamMethodStart);
+                bbO1->label->lop.generic.offset = (stream - streamMethodStart);
 
-                if (generateCode (cUnit, bb, &nextFallThrough) == false)
+                if (generateCode (cUnit, bbO1, &nextFallThrough) == false)
                 {
                     //Generate code set an error for the jit, we can just return
                     return;
@@ -1551,17 +1549,16 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
                 break;
             case kExitBlock:
                 //Only do the handle through if there is an instruction in the exit block
-                if (bb->firstMIRInsn != 0)
+                if (bbO1->firstMIRInsn != 0)
                 {
                     //First handle fallthrough branch
-                    handleFallThroughBranch (cUnit, bb, &nextFallThrough);
+                    handleFallThroughBranch (cUnit, bbO1, &nextFallThrough);
                 }
 
-                labelList[i].lop.opCode2 = ATOM_PSEUDO_EXIT_BLOCK;
                 //Set label offset
-                labelList[i].lop.generic.offset = (stream - streamMethodStart);
+                bbO1->label->lop.generic.offset = (stream - streamMethodStart);
 
-                if (generateCode (cUnit, bb, &nextFallThrough) == false)
+                if (generateCode (cUnit, bbO1, &nextFallThrough) == false)
                 {
                     //Generate code set an error for the jit, we can just return
                     return;
@@ -1569,16 +1566,15 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
                 break;
             case kDalvikByteCode:
                 //If hidden, we don't generate code
-                if (bb->hidden == false)
+                if (bbO1->hidden == false)
                 {
                     //First handle fallthrough branch
-                    handleFallThroughBranch (cUnit, bb, &nextFallThrough);
+                    handleFallThroughBranch (cUnit, bbO1, &nextFallThrough);
 
-                    labelList[i].lop.opCode2 = ATOM_PSEUDO_NORMAL_BLOCK_LABEL;
                     //Set label offset
-                    labelList[i].lop.generic.offset = (stream - streamMethodStart);
+                    bbO1->label->lop.generic.offset = (stream - streamMethodStart);
 
-                    if (generateCode (cUnit, bb, &nextFallThrough) == false)
+                    if (generateCode (cUnit, bbO1, &nextFallThrough) == false)
                     {
                         //Generate code set an error for the jit, we can just return
                         return;
@@ -1586,40 +1582,32 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
                 }
                 break;
             case kChainingCellNormal:
-                labelList[i].lop.opCode2 = ATOM_PSEUDO_CHAINING_CELL_NORMAL;
                 /* Handle the codegen later */
                 dvmInsertGrowableList(&chainingListByType[kChainingCellNormal], i);
                 break;
             case kChainingCellInvokeSingleton:
                     if (!cUnit->singletonInlined) {
-                        labelList[i].lop.opCode2 =
-                            ATOM_PSEUDO_CHAINING_CELL_INVOKE_SINGLETON;
-                        labelList[i].immOpnd.value = (int) bb->containingMethod;
                         /* Handle the codegen later */
                         dvmInsertGrowableList(
                             &chainingListByType[kChainingCellInvokeSingleton], i);
                     }
                 break;
             case kChainingCellInvokePredicted:
-                labelList[i].lop.opCode2 = ATOM_PSEUDO_CHAINING_CELL_INVOKE_PREDICTED;
                 /* Handle the codegen later */
                 dvmInsertGrowableList(&chainingListByType[kChainingCellInvokePredicted], i);
                 break;
             case kChainingCellHot:
-                labelList[i].lop.opCode2 = ATOM_PSEUDO_CHAINING_CELL_HOT;
                 /* Handle the codegen later */
                 dvmInsertGrowableList(&chainingListByType[kChainingCellHot], i);
                 break;
             case kExceptionHandling:
                 //First handle fallthrough branch
-                handleFallThroughBranch (cUnit, bb, &nextFallThrough);
-                labelList[i].lop.opCode2 = ATOM_PSEUDO_EH_BLOCK_LABEL;
-                labelList[i].lop.generic.offset = (stream - streamMethodStart);
+                handleFallThroughBranch (cUnit, bbO1, &nextFallThrough);
+                bbO1->label->lop.generic.offset = (stream - streamMethodStart);
                 scratchRegs[0] = PhysicalReg_EAX;
                 jumpToInterpPunt();
                 break;
             case kChainingCellBackwardBranch:
-                labelList[i].lop.opCode2 = ATOM_PSEUDO_CHAINING_CELL_BACKWARD_BRANCH;
                 /* Handle the codegen later */
                 dvmInsertGrowableList(&chainingListByType[kChainingCellBackwardBranch], i);
                 break;
@@ -1629,13 +1617,10 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
         }
 
     char* streamChainingStart = (char*)stream;
-    char* loopHeaderAddr = NULL;
-
     /* Handle the chaining cells in predefined order */
+
     for (i = 0; i < kChainingCellGap; i++) {
         size_t j;
-        int *blockIdList = (int *) chainingListByType[i].elemList;
-
         cUnit->numChainingCells[i] = chainingListByType[i].numUsed;
 
         /* No chaining cells of this type */
@@ -1646,49 +1631,97 @@ static void compilerMIR2LIRJit(CompilationUnit *cUnit, JitTranslationInfo *info)
         handleFallThroughBranch (cUnit, 0, &nextFallThrough);
 
         /* Record the first LIR for a new type of chaining cell */
-        cUnit->firstChainingLIR[i] = (LIR *) &labelList[blockIdList[0]];
         for (j = 0; j < chainingListByType[i].numUsed; j++) {
-            int blockId = blockIdList[j];
+            int blockId = (int) dvmGrowableListGetElement (& (chainingListByType[i]), j);
+
             BasicBlock *chainingBlock =
                 (BasicBlock *) dvmGrowableListGetElement(&cUnit->blockList,
                                                          blockId);
 
-            labelList[blockId].lop.generic.offset = (stream - streamMethodStart);
+            //Get O1 version
+            BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (chainingBlock);
+
+            //Paranoid
+            if (bbO1 == 0) {
+                continue;
+            }
 
-            /* Insert the pseudo chaining instruction */
-            dvmCompilerAppendLIR(cUnit, (LIR *) &labelList[blockId]);
+            //Set offset
+            bbO1->label->lop.generic.offset = (stream - streamMethodStart);
 
             int nop_size;
             switch (chainingBlock->blockType) {
                 case kChainingCellNormal:
                     nop_size = handleNormalChainingCell(cUnit,
-                     chainingBlock->startOffset, blockId, labelList);
-                    labelList[blockId].lop.generic.offset += nop_size; //skip over nop
+                     chainingBlock->startOffset, blockId);
+                    bbO1->label->lop.generic.offset += nop_size; //skip over nop
                     break;
                 case kChainingCellInvokeSingleton:
                     nop_size = handleInvokeSingletonChainingCell(cUnit,
-                        chainingBlock->containingMethod, blockId, labelList);
-                    labelList[blockId].lop.generic.offset += nop_size; //skip over nop
+                        chainingBlock->containingMethod, blockId);
+                    bbO1->label->lop.generic.offset += nop_size; //skip over nop
                     break;
                 case kChainingCellInvokePredicted:
                     handleInvokePredictedChainingCell(cUnit, blockId);
                     break;
                 case kChainingCellHot:
                     nop_size = handleHotChainingCell(cUnit,
-                        chainingBlock->startOffset, blockId, labelList);
-                    labelList[blockId].lop.generic.offset += nop_size; //skip over nop
+                        chainingBlock->startOffset, blockId);
+                    bbO1->label->lop.generic.offset += nop_size; //skip over nop
                     break;
                 case kChainingCellBackwardBranch:
-                    loopHeaderAddr = searchStreamStart(getLoopHeaderFromBBChaining(cUnit, chainingBlock));
-                    if (loopHeaderAddr == NULL){
-                        SET_JIT_ERROR(kJitErrorCodegen);
-                        endOfTrace(true/*freeOnly*/);
-                        cUnit->baseAddr = NULL;
-                        return;
+                    {
+                        //Get the loop entry
+                        BasicBlock *loopEntry = chainingBlock->fallThrough;
+
+                        //Paranoid
+                        assert (cUnit->loopInformation != 0);
+
+                        //We want the loop header and preloop header
+                        char *loopHeaderAddr = 0;
+                        char *preLoopHeaderAddr = 0;
+
+                        BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (loopEntry);
+                        assert (bbO1 != 0);
+
+                        //Set the loop header address
+                        loopHeaderAddr = bbO1->streamStart;
+
+                        //Get the associated loop information
+                        LoopInformation *info = cUnit->loopInformation;
+
+                        //But if info is 0, we might not have that and should just use the fallThrough's information
+                        //This can happen if the user has used the old loop system, and should only happen then
+                        if (info == 0)
+                        {
+                            //Then request the interpreter jump back to where the loop is
+                            preLoopHeaderAddr = loopHeaderAddr;
+                        }
+                        else
+                        {
+                            //Get the right loop
+                            info = info->getLoopInformationByEntry (loopEntry);
+
+                            //We have a preLoop
+                            BasicBlock *preLoop = info->getPreHeader ();
+
+                            bbO1 = reinterpret_cast<BasicBlock_O1 *> (preLoop);
+                            assert (bbO1 != 0);
+
+                            preLoopHeaderAddr = bbO1->streamStart;
+                        }
+
+                        if (loopHeaderAddr == 0 || preLoopHeaderAddr == 0){
+                            SET_JIT_ERROR(kJitErrorCodegen);
+                            endOfTrace(true/*freeOnly*/);
+                            cUnit->baseAddr = NULL;
+                            return;
+                        }
+
+                        nop_size = handleBackwardBranchChainingCell(cUnit,
+                                chainingBlock->startOffset, blockId, loopHeaderAddr, preLoopHeaderAddr);
+                        bbO1->label->lop.generic.offset += nop_size; //skip over nop
                     }
-                    nop_size = handleBackwardBranchChainingCell(cUnit,
-                        chainingBlock->startOffset, blockId, labelList, loopHeaderAddr);
-                    labelList[blockId].lop.generic.offset += nop_size; //skip over nop
                     break;
                 default:
                     ALOGI("JIT_INFO: Bad blocktype %d", chainingBlock->blockType);
@@ -1795,7 +1828,7 @@ void dvmCompilerMIR2LIR(CompilationUnit *cUnit, JitTranslationInfo *info) {
    int numTries = 0;
 
    //Try to lower MIR
-    do {
+   do {
         //See if we have been here too many times:
         if (numTries > MAX_RETRIES) {
             ALOGI("Too many retries while compiling trace  %s%s, offset %d", cUnit->method->clazz->descriptor,
diff --git a/vm/compiler/codegen/x86/InstructionGeneration.cpp b/vm/compiler/codegen/x86/InstructionGeneration.cpp
index 2c704d3..6649b07 100644
--- a/vm/compiler/codegen/x86/InstructionGeneration.cpp
+++ b/vm/compiler/codegen/x86/InstructionGeneration.cpp
@@ -204,3 +204,35 @@ void genValidationForPredictedInline(CompilationUnit *cUnit, MIR *mir)
     callsiteInfo->misPredBranchOver = (LIR*)conditional_jump_int(Condition_NE, 0, OpndSize_8);
 }
 #endif
+
+void genRegisterize (CompilationUnit *cUnit, MIR *mir)
+{
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+
+    //Get the class from vB, it determines which instruction to use for the move
+    RegisterClass regClass = static_cast<RegisterClass> (vB);
+
+    switch (regClass)
+    {
+        case kCoreReg:
+            //We just want vA in a register
+            get_virtual_reg (vA, OpndSize_32, 1, false);
+            set_virtual_reg (vA, OpndSize_32, 1, false);
+            break;
+        case kSFPReg:
+            //We just want vA in a register
+            get_VR_ss (vA, 1, false);
+            set_VR_ss (vA, 1, false);
+            break;
+        case kDFPReg:
+            //We just want vA in a register
+            get_VR_sd (vA, 1, false);
+            set_VR_sd (vA, 1, false);
+            break;
+        default:
+            ALOGE("JIT_ERROR: genRegisterize is requesting an unsupported regClass %d", regClass);
+            SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+            break;
+    }
+}
diff --git a/vm/compiler/codegen/x86/InstructionGeneration.h b/vm/compiler/codegen/x86/InstructionGeneration.h
index a569742..206804e 100644
--- a/vm/compiler/codegen/x86/InstructionGeneration.h
+++ b/vm/compiler/codegen/x86/InstructionGeneration.h
@@ -81,4 +81,12 @@ void genHoistedLowerBoundCheck(CompilationUnit *cUnit, MIR *mir);
  * @param mir the MIR instruction
  */
 void genValidationForPredictedInline(CompilationUnit *cUnit, MIR *mir);
+
+/*
+ * @brief Generate the registerize instruction
+ * vA = the register to set in a physical register
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR instruction
+ */
+void genRegisterize (CompilationUnit *cUnit, MIR *mir);
 #endif
diff --git a/vm/compiler/codegen/x86/Lower.cpp b/vm/compiler/codegen/x86/Lower.cpp
index 8fc0126..a0f2d01 100644
--- a/vm/compiler/codegen/x86/Lower.cpp
+++ b/vm/compiler/codegen/x86/Lower.cpp
@@ -57,10 +57,9 @@ char* stream; //current stream pointer
 int lowOpTimeStamp = 0;
 Method* currentMethod = NULL;
 int currentExceptionBlockIdx = -1;
-LowOpBlockLabel* traceLabelList = NULL;
 BasicBlock* traceCurrentBB = NULL;
 JitMode traceMode = kJitTrace;
-bool branchInLoop = false;
+CompilationUnit *gCompilationUnit;
 
 int common_invokeArgsDone(ArgsDoneType);
 
@@ -457,8 +456,12 @@ void startOfBasicBlock(BasicBlock* bb) {
     }
 }
 
-void startOfTrace(const Method* method, LowOpBlockLabel* labelList, int exceptionBlockId,
+void startOfTrace(const Method* method, int exceptionBlockId,
                   CompilationUnit *cUnit) {
+
+    //Set the global compilation
+    gCompilationUnit = cUnit;
+
     origMode = gDvm.executionMode;
     gDvm.executionMode = kExecutionModeNcgO1;
     if(gDvm.executionMode == kExecutionModeNcgO0) {
@@ -476,9 +479,8 @@ void startOfTrace(const Method* method, LowOpBlockLabel* labelList, int exceptio
     streamMethodStart = stream;
     //initialize mapFromBCtoNCG
     memset(&mapFromBCtoNCG[0], -1, BYTECODE_SIZE_PER_METHOD * sizeof(mapFromBCtoNCG[0]));
-    traceLabelList = labelList;
     if(gDvm.executionMode == kExecutionModeNcgO1)
-        startOfTraceO1(method, labelList, exceptionBlockId, cUnit);
+        startOfTraceO1(method, exceptionBlockId, cUnit);
 }
 
 void endOfTrace(bool freeOnly) {
@@ -498,6 +500,9 @@ void endOfTrace(bool freeOnly) {
         endOfTraceO1();
     }
     gDvm.executionMode = origMode;
+
+    //Reset the global compilation unit
+    gCompilationUnit = 0;
 }
 
 /**
@@ -1078,3 +1083,24 @@ void sendLabelInfoToVTune(int startStreamPtr, int endStreamPtr, const char* labe
     }
 }
 #endif
+
+int getLabelOffset (unsigned int blockId) {
+    //Paranoid
+    if (gCompilationUnit == 0) {
+        return -1;
+    }
+
+    //Get the BasicBlock
+    BasicBlock *bb = (BasicBlock *) dvmGrowableListGetElement(&gCompilationUnit->blockList, blockId);
+
+    //Transform into a BasicBlock_O1
+    BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (bb);
+
+    //Paranoid
+    if (bbO1 == 0 || bbO1->label == 0) {
+        return -1;
+    }
+
+    //Now return the label
+    return bbO1->label->lop.generic.offset;
+}
diff --git a/vm/compiler/codegen/x86/Lower.h b/vm/compiler/codegen/x86/Lower.h
index 9a359f1..2a83e76 100644
--- a/vm/compiler/codegen/x86/Lower.h
+++ b/vm/compiler/codegen/x86/Lower.h
@@ -507,8 +507,6 @@ struct LowOpBlockLabel {
     //! data structures because of a git merge issue. In future,
     //! this can be safely updated.
     LowOp lop;
-    //! \brief Holds offset information.
-    LowOpndImm immOpnd;
 };
 
 //! \brief Specialized LowOp with an immediate operand.
@@ -693,12 +691,18 @@ extern LabelMap* VMAPIWorklist;
 extern int ncgClassNum;
 extern int ncgMethodNum;
 
+// Global pointer to the current CompilationUnit
+extern CompilationUnit *gCompilationUnit;
+
 bool existATryBlock(Method* method, int startPC, int endPC);
 // interface between register allocator & lowering
 extern int num_removed_nullCheck;
 
-int registerAlloc(int type, int reg, bool isPhysical, bool updateRef);
-int registerAllocMove(int reg, int type, bool isPhysical, int srcReg);
+//Allocate a register
+int registerAlloc(int type, int reg, bool isPhysical, bool updateRef, bool isDest = false);
+//Allocate a register trying to alias a virtual register with a temporary
+int registerAllocMove(int reg, int type, bool isPhysical, int srcReg, bool isDest = false);
+
 int checkVirtualReg(int reg, LowOpndRegType type, int updateRef); //returns the physical register
 int updateRefCount(int reg, LowOpndRegType type);
 int updateRefCount2(int reg, int type, bool isPhysical);
@@ -706,7 +710,7 @@ int spillVirtualReg(int vrNum, LowOpndRegType type, bool updateTable);
 int isVirtualRegConstant(int regNum, LowOpndRegType type, int* valuePtr, bool updateRef);
 int checkTempReg(int reg, int type, bool isPhysical, int vA);
 bool checkTempReg2(int reg, int type, bool isPhysical, int physicalRegForVR, u2 vB);
-int freeReg(bool spillGL);
+int freeReg(bool writeBackAllVRs);
 int nextVersionOfHardReg(PhysicalReg pReg, int refCount);
 int updateVirtualReg(int reg, LowOpndRegType type);
 int setVRNullCheck(int regNum, OpndSize size);
@@ -719,7 +723,9 @@ bool getVRFreeDelayRequested(int regNum);
 bool isGlueHandled(int glue_reg);
 int resetGlue(int glue_reg);
 int updateGlue(int reg, bool isPhysical, int glue_reg);
-int updateVRAtUse(int reg, LowOpndRegType pType, int regAll);
+
+//Update the virtual register use information
+void updateVRAtUse(int reg, LowOpndRegType pType, int regAll);
 int touchEcx();
 int touchEax();
 int touchEdx();
@@ -730,10 +736,10 @@ void endBranch();
 void rememberState(int);
 void goToState(int);
 void transferToState(int);
-int globalVREndOfBB(const Method*);
-void constVREndOfBB();
-bool hasVRStoreExitOfLoop();
-void storeVRExitOfLoop();
+
+//Handle virtual register writebacks
+int handleVRsEndOfBB(bool lastBytecodeIsJump);
+
 void startNativeCode(int num, int type);
 void endNativeCode();
 void donotSpillReg(int physicalReg);
@@ -1058,10 +1064,10 @@ void performChainingWorklist();
 void freeNCGWorklist();
 void freeDataWorklist();
 void freeLabelWorklist();
+/** @brief search chainingWorklist to return instruction offset address in move instruction */
+char* searchChainingWorklist(unsigned int blockId);
 /** @brief search globalNCGWorklist to find the jmp/jcc offset address */
 char* searchNCGWorklist(int blockId);
-/** @brief search chainingWorklist to return instruction offset address in move instruction */
-char* searchChainingWorklist(int blockId);
 /** @brief search globalWorklist to find the jmp/jcc offset address */
 char* searchLabelWorklist(char* label);
 void freeChainingWorklist();
@@ -1388,11 +1394,9 @@ bool lowerByteCodeJit(const Method* method, const u2* codePtr, MIR* mir);
 #if defined(WITH_JIT)
 bool lowerByteCodeJit(const Method* method, const MIR * mir, const u2 * dalvikPC);
 void startOfBasicBlock(struct BasicBlock* bb);
-extern LowOpBlockLabel* traceLabelList;
 extern struct BasicBlock* traceCurrentBB;
 extern JitMode traceMode;
-extern bool branchInLoop;
-void startOfTrace(const Method* method, LowOpBlockLabel* labelList, int, CompilationUnit*);
+void startOfTrace(const Method* method, int, CompilationUnit*);
 void endOfTrace(bool freeOnly);
 LowOp* jumpToBasicBlock(char* instAddr, int targetId, bool targetIsChainingCell = false);
 LowOp* condJumpToBasicBlock(char* instAddr, ConditionCode cc, int targetId, bool targetIsChainingCell = false);
@@ -1401,17 +1405,19 @@ int codeGenBasicBlockJit(const Method* method, BasicBlock* bb);
 void endOfBasicBlock(struct BasicBlock* bb);
 void handleExtendedMIR(CompilationUnit *cUnit, MIR *mir);
 int insertChainingWorklist(int bbId, char * codeStart);
-void startOfTraceO1(const Method* method, LowOpBlockLabel* labelList, int exceptionBlockId, CompilationUnit *cUnit);
+void startOfTraceO1(const Method* method, int exceptionBlockId, CompilationUnit *cUnit);
 void endOfTraceO1();
 /** @brief search globalMap to find the entry for the given label */
 char* findCodeForLabel(const char* label);
+/* Find a label offset given a BasicBlock index */
+int getLabelOffset (unsigned int bbIdx);
 #endif
 int isPowerOfTwo(int imm);
 void move_chain_to_mem(OpndSize size, int imm,
                         int disp, int base_reg, bool isBasePhysical);
 void move_chain_to_reg(OpndSize size, int imm, int reg, bool isPhysical);
 
-void dumpImmToMem(int vrNum, OpndSize size, int value);
+void writeBackConstVR(int vR, int value);
 bool isInMemory(int regNum, OpndSize size);
 int touchEbx();
 int boundCheck(int vr_array, int reg_array, bool isPhysical_array,
@@ -1423,7 +1429,8 @@ int getRelativeNCG(s4 tmp, JmpCall_type type, bool* unknown, OpndSize* size);
 void freeAtomMem();
 OpndSize estOpndSizeFromImm(int target);
 
-int preprocessingBB(BasicBlock* bb);
+//Preprocess a BasicBlock before being lowered
+int preprocessingBB(CompilationUnit *cUnit, BasicBlock* bb);
 int preprocessingTrace();
 /** @brief align the relative offset of jmp/jcc and movl within 16B */
 void alignOffset(int cond);
@@ -1432,3 +1439,4 @@ bool isBasicBlockAChainingCell(BasicBlock * bb);
 
 void pushCallerSavedRegs(void);
 void popCallerSavedRegs(void);
+
diff --git a/vm/compiler/codegen/x86/LowerAlu.cpp b/vm/compiler/codegen/x86/LowerAlu.cpp
index 6834759..ce79a60 100644
--- a/vm/compiler/codegen/x86/LowerAlu.cpp
+++ b/vm/compiler/codegen/x86/LowerAlu.cpp
@@ -1260,7 +1260,6 @@ int common_div_rem_int_lit(bool isRem, u2 vA, u2 vB, s2 imm) {
 #ifdef DEBUG_EXCEPTION
         ALOGI("EXTRA code to handle exception");
 #endif
-        constVREndOfBB();
         beforeCall("exception"); //dump GG, GL VRs
         unconditional_jump_global_API(
                           "common_errDivideByZero", false);
diff --git a/vm/compiler/codegen/x86/LowerHelper.cpp b/vm/compiler/codegen/x86/LowerHelper.cpp
index 9234324..83b7062 100644
--- a/vm/compiler/codegen/x86/LowerHelper.cpp
+++ b/vm/compiler/codegen/x86/LowerHelper.cpp
@@ -285,10 +285,10 @@ LowOpMem* lower_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
 LowOpMem* dump_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
         int base_reg, bool isBasePhysical) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(true);
+        freeReg(false);
         //type of the base is gpr
         int regAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true);
+                true, true);
         return lower_mem(m, m2, size, disp, regAll, true /*isBasePhysical*/);
     } else {
         return lower_mem(m, m2, size, disp, base_reg, isBasePhysical);
@@ -324,7 +324,7 @@ LowOpReg* lower_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
 LowOpReg* dump_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
         bool isPhysical, LowOpndRegType type) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(true);
+        freeReg(false);
         if (m == Mnemonic_MUL || m == Mnemonic_IMUL || m == Mnemonic_DIV
                 || m == Mnemonic_IDIV) {
             //these four instructions use eax & edx implicitly
@@ -442,6 +442,7 @@ inline bool isMoveOptimizable(Mnemonic m) {
 LowOpRegReg* dump_reg_reg_noalloc_dst(Mnemonic m, OpndSize size, int reg,
         bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
+        //TODO should mark reg2 as written
         int regAll = registerAlloc(type, reg, isPhysical, true);
         /* remove move from one register to the same register */
         if (isMoveOptimizable(m) && regAll == reg2)
@@ -464,9 +465,9 @@ LowOpRegReg* dump_reg_reg_noalloc_src(Mnemonic m, AtomOpCode m2, OpndSize size,
         int regAll2;
         if(isMoveOptimizable(m) && checkTempReg2(reg2, type, isPhysical2, reg, -1)) { //dst reg is logical
             //only from get_virtual_reg_all
-            regAll2 = registerAllocMove(reg2, type, isPhysical2, reg);
+            regAll2 = registerAllocMove(reg2, type, isPhysical2, reg, true);
         } else {
-            regAll2 = registerAlloc(type, reg2, isPhysical2, true);
+            regAll2 = registerAlloc(type, reg2, isPhysical2, true, true);
             return lower_reg_to_reg(m, m2, size, reg, true /*isPhysical*/, regAll2,
                     true /*isPhysical2*/, type);
         }
@@ -497,7 +498,7 @@ LowOpRegReg* dump_reg_reg_diff_types(Mnemonic m, AtomOpCode m2, OpndSize srcSize
     if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
         //reg is source if m is MOV
-        freeReg(true);
+        freeReg(false);
         int regAll = registerAlloc(srcType, srcReg, isSrcPhysical, true);
         int regAll2;
         LowOpRegReg* op = NULL;
@@ -506,11 +507,11 @@ LowOpRegReg* dump_reg_reg_diff_types(Mnemonic m, AtomOpCode m2, OpndSize srcSize
                 ((reg != PhysicalReg_EDI && srcReg != PhysicalReg_ESP && srcReg != PhysicalReg_EBP) || (!isSrcPhysical)) &&
                 isDestPhysical == false) { //dst reg is logical
             //called from move_reg_to_reg
-            regAll2 = registerAllocMove(regDest, destType, isDestPhysical, regAll);
+            regAll2 = registerAllocMove(regDest, destType, isDestPhysical, regAll, true);
         } else {
 #endif
         donotSpillReg(regAll);
-        regAll2 = registerAlloc(destType, destReg, isDestPhysical, true);
+        regAll2 = registerAlloc(destType, destReg, isDestPhysical, true, true);
 
         // NOTE: The use of (destSize, destType) as THE (size, type) can be confusing. In most
         // cases, we are using this function through dump_reg_reg, so the (size, type) doesn't
@@ -600,7 +601,7 @@ LowOpMemReg* dump_mem_reg_noalloc_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
         int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
         int mIndex, int reg, bool isPhysical, LowOpndRegType type) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
-        int regAll = registerAlloc(type, reg, isPhysical, true);
+        int regAll = registerAlloc(type, reg, isPhysical, true, true);
         return lower_mem_to_reg(m, m2, size, disp, base_reg,
                 true /*isBasePhysical*/, mType, mIndex, regAll,
                 true /*isPhysical*/, type);
@@ -619,16 +620,16 @@ LowOpMemReg* dump_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
         int reg, bool isPhysical, LowOpndRegType type) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
-        freeReg(true);
+        freeReg(false);
         int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
                 true);
         //it is okay to use the same physical register
         if (isMoveOptimizable(m)) {
-            freeReg(true);
+            freeReg(false);
         } else {
             donotSpillReg(baseAll);
         }
-        int regAll = registerAlloc(type, reg, isPhysical, true);
+        int regAll = registerAlloc(type, reg, isPhysical, true, true);
         endNativeCode();
         return lower_mem_to_reg(m, m2, size, disp, baseAll,
                 true /*isBasePhysical*/, mType, mIndex, regAll,
@@ -672,11 +673,11 @@ LowOpMemReg* dump_movez_mem_reg(Mnemonic m, OpndSize size, int disp,
         int base_reg, bool isBasePhysical, int reg, bool isPhysical) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
-        freeReg(true);
+        freeReg(false);
         int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
                 true);
         donotSpillReg(baseAll);
-        int regAll = registerAlloc(LowOpndRegType_gp, reg, isPhysical, true);
+        int regAll = registerAlloc(LowOpndRegType_gp, reg, isPhysical, true, true);
         endNativeCode();
         return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, baseAll,
                 true /*isBasePhysical*/, MemoryAccess_Unknown, -1, regAll,
@@ -769,21 +770,21 @@ LowOpMemReg* dump_mem_scale_reg(Mnemonic m, OpndSize size, int base_reg,
         int scale, int reg, bool isPhysical, LowOpndRegType type) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
-        freeReg(true);
+        freeReg(false);
         int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
                 true);
         donotSpillReg(baseAll); //make sure index will not use the same physical reg
         int indexAll = registerAlloc(LowOpndRegType_gp, index_reg,
                 isIndexPhysical, true);
         if (isMoveOptimizable(m)) {
-            freeReg(true);
+            freeReg(false);
             doSpillReg(baseAll); //base can be used now
         } else {
             donotSpillReg(indexAll);
         }
         bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
         int regAll = registerAlloc(isMovzs ? LowOpndRegType_gp : type, reg,
-                isPhysical, true);
+                isPhysical, true, true);
         endNativeCode();
         return lower_mem_scale_to_reg(m, size, baseAll, true /*isBasePhysical*/,
                 disp, indexAll, true /*isIndexPhysical*/, scale, regAll,
@@ -836,14 +837,14 @@ LowOpRegMem* dump_reg_mem_scale(Mnemonic m, OpndSize size, int reg,
         int index_reg, bool isIndexPhysical, int scale, LowOpndRegType type) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
-        freeReg(true);
+        freeReg(false);
         int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
                 true);
         donotSpillReg(baseAll);
         int indexAll = registerAlloc(LowOpndRegType_gp, index_reg,
                 isIndexPhysical, true);
         donotSpillReg(indexAll);
-        int regAll = registerAlloc(type, reg, isPhysical, true);
+        int regAll = registerAlloc(type, reg, isPhysical, true, true);
         endNativeCode();
         return lower_reg_to_mem_scale(m, size, regAll, true /*isPhysical*/,
                 baseAll, true /*isBasePhysical*/, disp, indexAll,
@@ -904,7 +905,7 @@ LowOpRegMem* dump_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
         MemoryAccessType mType, int mIndex, LowOpndRegType type) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
-        freeReg(true);
+        freeReg(false);
         int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
                 true);
         donotSpillReg(baseAll);
@@ -997,8 +998,8 @@ LowOpImmReg* dump_imm_reg_noalloc(Mnemonic m, OpndSize size, int imm, int reg,
 LowOpImmReg* dump_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
         int reg, bool isPhysical, LowOpndRegType type, bool chaining) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(true);
-        int regAll = registerAlloc(type, reg, isPhysical, true);
+        freeReg(false);
+        int regAll = registerAlloc(type, reg, isPhysical, true, true);
         return lower_imm_to_reg(m, m2, size, imm, regAll, true /*isPhysical*/,
                 type, chaining);
     } else {
@@ -1059,7 +1060,7 @@ LowOpImmMem* dump_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
         if (!isBasePhysical
                 || (base_reg != PhysicalReg_EDI && base_reg != PhysicalReg_ESP
                         && base_reg != PhysicalReg_EBP)) {
-            freeReg(true);
+            freeReg(false);
         }
         int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
                 true);
@@ -1111,7 +1112,7 @@ LowOpRegMem* dump_fp_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
         int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
         int mIndex) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(true);
+        freeReg(false);
         int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
                 true);
         return lower_fp_to_mem(m, m2, size, reg, disp, baseAll,
@@ -1163,7 +1164,7 @@ LowOpMemReg* dump_mem_fp(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
         int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
         int reg) {
     if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(true);
+        freeReg(false);
         int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
                 true);
         return lower_mem_to_fp(m, m2, size, disp, baseAll,
@@ -1322,7 +1323,7 @@ void compare_VR_reg_all(OpndSize size,
 #ifdef DEBUG_NCG_O1
                 ALOGI("VR is const and SS in compare_VR_reg");
 #endif
-                dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
+                writeBackConstVR(vA, tmpValue[0]);
                 //dumpImmToMem(vA+1, OpndSize_32, 0); //CHECK necessary? will overwrite vA+1!!!
                 dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, reg, isPhysical, pType);
                 return;
@@ -1338,16 +1339,16 @@ void compare_VR_reg_all(OpndSize size,
 #ifdef DEBUG_NCG_O1
                 ALOGI("VR is const and 64 bits in compare_VR_reg");
 #endif
-                dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
-                dumpImmToMem(vA+1, OpndSize_32, tmpValue[1]);
+                writeBackConstVR(vA, tmpValue[0]);
+                writeBackConstVR(vA+1, tmpValue[1]);
                 dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
                     MemoryAccess_VR, vA, reg, isPhysical, pType);
                 return;
             }
         }
-        if(isConst == 1) dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
-        if(isConst == 2) dumpImmToMem(vA+1, OpndSize_32, tmpValue[1]);
-        freeReg(true);
+        if(isConst == 1) writeBackConstVR(vA, tmpValue[0]);
+        if(isConst == 2) writeBackConstVR(vA+1, tmpValue[1]);
+        freeReg(false);
         int regAll = checkVirtualReg(vA, type, 0/*do not update*/);
         if(regAll != PhysicalReg_Null) { //do not spill regAll when allocating register for dst
             startNativeCode(-1, -1);
@@ -1395,14 +1396,14 @@ void load_fp_stack_VR_all(OpndSize size, int vB, Mnemonic m) {
 #ifdef DEBUG_NCG_O1
                 ALOGI("VR is const and 32 bits in load_fp_stack");
 #endif
-                dumpImmToMem(vB, OpndSize_32, tmpValue[0]);
+                writeBackConstVR(vB, tmpValue[0]);
             }
             else {
 #ifdef DEBUG_NCG_O1
                 ALOGI("VR is const and 64 bits in load_fp_stack_VR");
 #endif
-                if(isConst == 1 || isConst == 3) dumpImmToMem(vB, OpndSize_32, tmpValue[0]);
-                if(isConst == 2 || isConst == 3) dumpImmToMem(vB+1, OpndSize_32, tmpValue[1]);
+                if(isConst == 1 || isConst == 3) writeBackConstVR(vB, tmpValue[0]);
+                if(isConst == 2 || isConst == 3) writeBackConstVR(vB+1, tmpValue[1]);
             }
         }
         else { //if VR was updated by a def of gp, a xfer point was inserted
@@ -1470,14 +1471,14 @@ void fpu_VR(ALU_Opcode opc, OpndSize size, int vA) {
         if(isConst > 0) {
             if(size != OpndSize_64) {
                 //allocate a register for dst
-                dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
+                writeBackConstVR(vA, tmpValue[0]);
             }
             else {
                 if((isConst == 1 || isConst == 3) && size == OpndSize_64) {
-                    dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
+                    writeBackConstVR(vA, tmpValue[0]);
                 }
                 if((isConst == 2 || isConst == 3) && size == OpndSize_64) {
-                    dumpImmToMem(vA+1, OpndSize_32, tmpValue[1]);
+                    writeBackConstVR(vA+1, tmpValue[1]);
                 }
             }
         }
@@ -1500,7 +1501,7 @@ void compare_imm_reg(OpndSize size, int imm,
         LowOpndRegType type = getTypeFromIntSize(size);
         Mnemonic m = Mnemonic_TEST;
         if(gDvm.executionMode == kExecutionModeNcgO1) {
-            freeReg(true);
+            freeReg(false);
             int regAll = registerAlloc(type, reg, isPhysical, true);
             lower_reg_to_reg(m, ATOM_NORMAL, size, regAll, true /*isPhysical*/, regAll, true /*isPhysical2*/, type);
         } else {
@@ -1535,7 +1536,7 @@ void compare_imm_VR(OpndSize size, int imm,
         int tmpValue[2];
         int isConst = isVirtualRegConstant(vA, getTypeFromIntSize(size), tmpValue, false/*updateRefCount*/);
         if(isConst > 0) {
-            dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
+            writeBackConstVR(vA, tmpValue[0]);
         }
         int regAll = checkVirtualReg(vA, getTypeFromIntSize(size), 0);
         if(regAll != PhysicalReg_Null)
@@ -1708,22 +1709,22 @@ void alu_sd_binary_VR_reg(ALU_Opcode opc, int vA, int reg, bool isPhysical, bool
                           true/*updateRefCount*/);
         if(isConst == 3 && !isSD) {
             //isConst can be 0 or 3, mem32, use xmm
-            dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
+            writeBackConstVR(vA, tmpValue[0]);
             dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_32, 4*vA, PhysicalReg_FP, true,
                        MemoryAccess_VR, vA, reg, isPhysical,
                        LowOpndRegType_xmm);
             return;
         }
         if(isConst == 3 && isSD) {
-            dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
-            dumpImmToMem(vA+1, OpndSize_32, tmpValue[1]);
+            writeBackConstVR(vA, tmpValue[0]);
+            writeBackConstVR(vA+1, tmpValue[1]);
             dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, 4*vA, PhysicalReg_FP, true,
                        MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm);
             return;
         }
-        if(isConst == 1) dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
-        if(isConst == 2) dumpImmToMem(vA+1, OpndSize_32, tmpValue[1]);
-        freeReg(true);
+        if(isConst == 1) writeBackConstVR(vA, tmpValue[0]);
+        if(isConst == 2) writeBackConstVR(vA+1, tmpValue[1]);
+        freeReg(false);
 
         int regAll = checkVirtualReg(vA, type, 0/*do not update refCount*/);
         if(regAll != PhysicalReg_Null) {
@@ -1766,16 +1767,16 @@ void alu_binary_VR_reg(OpndSize size, ALU_Opcode opc, int vA, int reg, bool isPh
             return;
         }
         if(isConst == 3 && size == OpndSize_64) {
-            dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
-            dumpImmToMem(vA+1, OpndSize_32, tmpValue[1]);
+            writeBackConstVR(vA, tmpValue[0]);
+            writeBackConstVR(vA+1, tmpValue[1]);
             dump_mem_reg(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
                 MemoryAccess_VR, vA, reg, isPhysical, getTypeFromIntSize(size));
             return;
         }
-        if(isConst == 1) dumpImmToMem(vA, OpndSize_32, tmpValue[0]);
-        if(isConst == 2) dumpImmToMem(vA+1, OpndSize_32, tmpValue[1]);
+        if(isConst == 1) writeBackConstVR(vA, tmpValue[0]);
+        if(isConst == 2) writeBackConstVR(vA+1, tmpValue[1]);
 
-        freeReg(true);
+        freeReg(false);
         int regAll = checkVirtualReg(vA, getTypeFromIntSize(size), 0);
         if(regAll != PhysicalReg_Null) {
             startNativeCode(-1, -1);
@@ -2057,12 +2058,13 @@ void set_VR_to_imm(u2 vA, OpndSize size, int imm) {
             return;
         }
         //will call freeReg
-        freeReg(true);
-        regAll = registerAlloc(LowOpndRegType_virtual | getTypeFromIntSize(size), vA, false/*dummy*/, true);
+        freeReg(false);
+        regAll = registerAlloc(LowOpndRegType_virtual | getTypeFromIntSize(size), vA, false/*dummy*/, true, true);
         if(regAll == PhysicalReg_Null) {
             dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
             return;
         }
+
         dump_imm_reg_noalloc(m, size, imm, regAll, true, LowOpndRegType_gp);
         updateVirtualReg(vA, getTypeFromIntSize(size));
     }
@@ -2158,7 +2160,7 @@ void move_sd_reg_to_mem(LowOp* op, int reg, bool isPhysical,
 //!load from VR to a temporary
 
 //!
-void get_virtual_reg_all(u2 vB, OpndSize size, int reg, bool isPhysical, Mnemonic m) {
+void get_virtual_reg_all(u2 vR, OpndSize size, int reg, bool isPhysical, Mnemonic m) {
     LowOpndRegType type = getTypeFromIntSize(size);
     LowOpndRegType pType = type;//gp or xmm
     OpndSize size2 = size;
@@ -2173,22 +2175,22 @@ void get_virtual_reg_all(u2 vB, OpndSize size, int reg, bool isPhysical, Mnemoni
     if(gDvm.executionMode == kExecutionModeNcgO1) {
         int tmpValue[2];
         int isConst;
-        isConst = isVirtualRegConstant(vB, type, tmpValue, true/*updateRefCount*/);
+        isConst = isVirtualRegConstant(vR, type, tmpValue, true/*updateRefCount*/);
         if(isConst == 3) {
             if(m == Mnemonic_MOVSS) { //load 32 bits from VR
                 //VR is not mapped to a register but in memory
-                dumpImmToMem(vB, OpndSize_32, tmpValue[0]);
+                writeBackConstVR(vR, tmpValue[0]);
                 //temporary reg has "pType" (which is xmm)
-                dump_mem_reg(m, ATOM_NORMAL, size, 4*vB, PhysicalReg_FP, true,
-                    MemoryAccess_VR, vB, reg, isPhysical, pType);
+                dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
+                    MemoryAccess_VR, vR, reg, isPhysical, pType);
                 return;
             }
             else if(m == Mnemonic_MOVSD || size == OpndSize_64) {
                 //VR is not mapped to a register but in memory
-                dumpImmToMem(vB, OpndSize_32, tmpValue[0]);
-                dumpImmToMem(vB+1, OpndSize_32, tmpValue[1]);
-                dump_mem_reg(m, ATOM_NORMAL, size, 4*vB, PhysicalReg_FP, true,
-                    MemoryAccess_VR, vB, reg, isPhysical, pType);
+                writeBackConstVR(vR, tmpValue[0]);
+                writeBackConstVR(vR+1, tmpValue[1]);
+                dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
+                    MemoryAccess_VR, vR, reg, isPhysical, pType);
                 return;
             }
             else if(size != OpndSize_64) {
@@ -2197,52 +2199,65 @@ void get_virtual_reg_all(u2 vB, OpndSize size, int reg, bool isPhysical, Mnemoni
                 return;
             }
         }
-        if(isConst == 1) dumpImmToMem(vB, OpndSize_32, tmpValue[0]);
-        if(isConst == 2) dumpImmToMem(vB+1, OpndSize_32, tmpValue[1]);
-        freeReg(true);
-        int regAll = checkVirtualReg(vB, type, 0);
-        if(regAll != PhysicalReg_Null) {
-            startNativeCode(vB, type);
-            donotSpillReg(regAll);
+        if(isConst == 1) writeBackConstVR(vR, tmpValue[0]);
+        if(isConst == 2) writeBackConstVR(vR+1, tmpValue[1]);
+
+        // We want to free any variables no longer in use
+        freeReg(false);
+
+        // Do we have a physical register associated for this VR?
+        int physRegForVR = checkVirtualReg(vR, type, 0);
+
+        // If we do, then let register allocator decide if a new physical
+        // register needs allocated for the temp
+        if(physRegForVR != PhysicalReg_Null) {
+            startNativeCode(vR, type);
+            donotSpillReg(physRegForVR);
             //check XFER_MEM_TO_XMM
-            updateVRAtUse(vB, type, regAll);
+            updateVRAtUse(vR, type, physRegForVR);
             //temporary reg has "pType"
-            dump_reg_reg_noalloc_src(m2, ATOM_NORMAL, size2, regAll, true, reg, isPhysical, pType); //register allocator handles assembly move
+            dump_reg_reg_noalloc_src(m2, ATOM_NORMAL, size2, physRegForVR, true, reg, isPhysical, pType);
             endNativeCode();
-            updateRefCount(vB, type);
+            updateRefCount(vR, type);
             return;
         }
-        //not allocated to a register yet, no need to check XFER_MEM_TO_XMM
-        regAll = registerAlloc(LowOpndRegType_virtual | type, vB, false/*dummy*/, false);
-        if(regAll == PhysicalReg_Null) {
-            dump_mem_reg_noalloc(m, size, 4*vB, PhysicalReg_FP, true,
-                MemoryAccess_VR, vB, reg, isPhysical, pType);
+
+        // When we get to this point, we know that we have no physical register
+        // associated with the VR
+        physRegForVR = registerAlloc(LowOpndRegType_virtual | type, vR, false/*dummy*/, false);
+
+        // If we still have no physical register for the VR, then use it as
+        // a memory operand
+        if(physRegForVR == PhysicalReg_Null) {
+            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
+                MemoryAccess_VR, vR, reg, isPhysical, pType);
             return;
         }
 
-        //temporary reg has pType
-        if(checkTempReg2(reg, pType, isPhysical, regAll, vB)) {
-            registerAllocMove(reg, pType, isPhysical, regAll);
-            dump_mem_reg_noalloc(m, size, 4*vB, PhysicalReg_FP, true,
-                MemoryAccess_VR, vB, regAll, true, pType);
-            updateRefCount(vB, type);
+        // At this point we definitely have a physical register for the VR.
+        // Check to see if the temp can share same physical register.
+        if(checkTempReg2(reg, pType, isPhysical, physRegForVR, vR)) {
+            registerAllocMove(reg, pType, isPhysical, physRegForVR);
+            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
+                MemoryAccess_VR, vR, physRegForVR, true, pType);
+            updateRefCount(vR, type);
             return;
         }
         else {
-            dump_mem_reg_noalloc(m, size, 4*vB, PhysicalReg_FP, true,
-                MemoryAccess_VR, vB, regAll, true, pType);
+            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
+                MemoryAccess_VR, vR, physRegForVR, true, pType);
             //xmm with 32 bits
-            startNativeCode(vB, type);
-            donotSpillReg(regAll);
-            dump_reg_reg_noalloc_src(m2, ATOM_NORMAL, size2, regAll, true, reg, isPhysical, pType);
+            startNativeCode(vR, type);
+            donotSpillReg(physRegForVR);
+            dump_reg_reg_noalloc_src(m2, ATOM_NORMAL, size2, physRegForVR, true, reg, isPhysical, pType);
             endNativeCode();
-            updateRefCount(vB, type);
+            updateRefCount(vR, type);
             return;
         }
     }
     else {
-        dump_mem_reg(m, ATOM_NORMAL, size, 4*vB, PhysicalReg_FP, true,
-            MemoryAccess_VR, vB, reg, isPhysical, pType);
+        dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
+            MemoryAccess_VR, vR, reg, isPhysical, pType);
     }
 }
 void get_virtual_reg(u2 vB, OpndSize size, int reg, bool isPhysical) {
@@ -2282,7 +2297,7 @@ void set_virtual_reg_all(u2 vA, OpndSize size, int reg, bool isPhysical, Mnemoni
         //   allocate a physical register for the VR
         //   then call dump_reg_reg_noalloc_dst
         //may need to convert from gp to xmm or the other way
-        freeReg(true);
+        freeReg(false);
         int regAll = checkVirtualReg(vA, type, 0);
         if(regAll != PhysicalReg_Null)  { //case 1
             startNativeCode(-1, -1);
@@ -2295,17 +2310,18 @@ void set_virtual_reg_all(u2 vA, OpndSize size, int reg, bool isPhysical, Mnemoni
         }
         regAll = checkTempReg(reg, pType, isPhysical, vA); //vA is not used inside
         if(regAll != PhysicalReg_Null) { //case 2
-            registerAllocMove(vA, LowOpndRegType_virtual | type, false, regAll);
+            registerAllocMove(vA, LowOpndRegType_virtual | type, false, regAll, true);
             updateVirtualReg(vA, type); //will dump VR to memory, should happen afterwards
             return; //next native instruction starts at op
         }
         //case 3
-        regAll = registerAlloc(LowOpndRegType_virtual | type, vA, false/*dummy*/, false);
+        regAll = registerAlloc(LowOpndRegType_virtual | type, vA, false/*dummy*/, false, true);
         if(regAll == PhysicalReg_Null) {
             dump_reg_mem_noalloc(m, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
                 MemoryAccess_VR, vA, pType);
             return;
         }
+
         startNativeCode(-1, -1);
         donotSpillReg(regAll);
         dump_reg_reg_noalloc_dst(m2, size2, reg, isPhysical, regAll, true, pType);
@@ -2444,7 +2460,6 @@ int nullCheck(int reg, bool isPhysical, int exceptionNum, int vr) {
     export_pc(); //use %edx
 
     if(gDvm.executionMode == kExecutionModeNcgO1) {
-        constVREndOfBB();
         beforeCall("exception"); //dump GG, GL VRs
     }
 
@@ -2500,7 +2515,6 @@ int handlePotentialException(
     export_pc(); //use %edx
 
     if(gDvm.executionMode == kExecutionModeNcgO1) {
-        constVREndOfBB();
         beforeCall("exception"); //dump GG, GL VRs
     }
 
@@ -2547,7 +2561,7 @@ int get_res_strings(int reg, bool isPhysical) {
 #endif
         startNativeCode(-1, -1);
         freeReg(true);
-        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/);
+        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/, true);
         donotSpillReg(regAll);
         dump_mem_reg_noalloc_mem(Mnemonic_MOV, ATOM_NORMAL, OpndSize_32, offDvmDex_pResStrings, regAll, true, MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_gp);
         endNativeCode();
@@ -2572,7 +2586,7 @@ int get_res_classes(int reg, bool isPhysical) {
         //  load from spilled location, updte spill_loc_index & physicalReg
         startNativeCode(-1, -1);
         freeReg(true);
-        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/);
+        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/, true);
         donotSpillReg(regAll);
         dump_mem_reg_noalloc_mem(Mnemonic_MOV, ATOM_NORMAL, OpndSize_32, offDvmDex_pResClasses, regAll, true, MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_gp);
         endNativeCode();
@@ -2600,7 +2614,7 @@ int get_res_fields(int reg, bool isPhysical) {
         //  load from spilled location, updte spill_loc_index & physicalReg
         startNativeCode(-1, -1);
         freeReg(true);
-        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/);
+        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/, true);
         donotSpillReg(regAll);
         dump_mem_reg_noalloc_mem(Mnemonic_MOV, ATOM_NORMAL, OpndSize_32, offDvmDex_pResFields, regAll, true, MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_gp);
         endNativeCode();
@@ -2628,7 +2642,7 @@ int get_res_methods(int reg, bool isPhysical) {
         //  load from spilled location, updte spill_loc_index & physicalReg
         startNativeCode(-1, -1);
         freeReg(true);
-        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/);
+        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/, true);
         donotSpillReg(regAll);
         dump_mem_reg_noalloc_mem(Mnemonic_MOV, ATOM_NORMAL, OpndSize_32, offDvmDex_pResMethods, regAll, true, MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_gp);
         endNativeCode();
@@ -2682,7 +2696,7 @@ int get_glue_dvmdex(int reg, bool isPhysical) {
         //  load from spilled location, updte spill_loc_index & physicalReg
         startNativeCode(-1, -1);
         freeReg(true);
-        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/);
+        int regAll = registerAlloc(LowOpndRegType_gp, PhysicalReg_GLUE_DVMDEX, false, false/*updateRefCount*/, true);
         donotSpillReg(regAll);
         dump_reg_reg_noalloc_src(Mnemonic_MOV, ATOM_NORMAL, OpndSize_32, regAll, true,
                                           reg, isPhysical, LowOpndRegType_gp);
diff --git a/vm/compiler/codegen/x86/LowerInvoke.cpp b/vm/compiler/codegen/x86/LowerInvoke.cpp
index ec3892d..801917f 100644
--- a/vm/compiler/codegen/x86/LowerInvoke.cpp
+++ b/vm/compiler/codegen/x86/LowerInvoke.cpp
@@ -98,7 +98,6 @@ int common_invoke_virtual_nohelper(bool isRange, u2 tmp, u2 vD, const MIR *mir)
 #endif
     scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
     export_pc();
-    constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
 
     get_virtual_reg(vD, OpndSize_32, 5, false);
@@ -160,7 +159,6 @@ int common_invoke_virtual(bool isRange, u2 tmp, u2 vD) {
 int common_invoke_super(bool isRange, u2 tmp,
         const DecodedInstruction &decodedInst) {
     export_pc();
-    constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
         ///////////////////////
         scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
@@ -241,7 +239,6 @@ int common_invoke_direct(bool isRange, u2 tmp, u2 vD, const MIR *mir)
     const DecodedInstruction &decodedInst = mir->dalvikInsn;
     //%ecx can be used as scratch when calling export_pc, get_res_methods and resolve_method
     export_pc();
-    constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
 #if !defined(WITH_JIT)
         scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
@@ -305,7 +302,6 @@ int common_invoke_static(bool isRange, u2 tmp,
         const DecodedInstruction &decodedInst) {
     //%ecx can be used as scratch when calling export_pc, get_res_methods and resolve_method
     export_pc();
-    constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
 #if !defined(WITH_JIT)
         scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
@@ -373,7 +369,6 @@ int common_invoke_interface(bool isRange, u2 tmp, u2 vD, const MIR *mir) {
     }
 #endif
     export_pc(); //use %edx
-    constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
     ///////////////////////
     scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
@@ -696,7 +691,7 @@ int common_invokeMethodNoRange_noJmp(const DecodedInstruction &decodedInst) {
                                 PhysicalReg_FP, true);
                 offsetFromSaveArea += 4;
                 numMov++;
-            } 
+            }
             if (numMov == 0){
                 get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 23, false);
                 move_reg_to_mem(OpndSize_32, 23, false, offsetFromSaveArea - sizeofStackSaveArea,
@@ -1262,7 +1257,6 @@ int op_execute_inline(const MIR * mir, bool isRange) {
             conditional_jump(Condition_NE, ".do_inlined_string_length", true);
             scratchRegs[0] = PhysicalReg_SCRATCH_1;
             rememberState(1);
-            constVREndOfBB();
             beforeCall("exception"); //dump GG, GL VRs
             unconditional_jump("common_errNullObject", false);
             goToState(1);
@@ -1278,7 +1272,6 @@ int op_execute_inline(const MIR * mir, bool isRange) {
             conditional_jump(Condition_NE, ".do_inlined_string_length", true);
             scratchRegs[0] = PhysicalReg_SCRATCH_1;
             rememberState(1);
-            constVREndOfBB();
             beforeCall("exception"); //dump GG, GL VRs
             unconditional_jump("common_errNullObject", false);
             goToState(1);
@@ -1358,7 +1351,6 @@ int op_execute_inline(const MIR * mir, bool isRange) {
             compare_imm_reg(OpndSize_32, 0, 1, false);
             conditional_jump(Condition_NE, ".inlined_string_CharAt_arg_validate_1", true);
             rememberState(1);
-            constVREndOfBB();
             beforeCall("exception");
             unconditional_jump("common_errNullObject", false);
             goToState(1);
@@ -1368,7 +1360,6 @@ int op_execute_inline(const MIR * mir, bool isRange) {
             compare_mem_reg(OpndSize_32, 0x14, 1, false, 2, false);
             conditional_jump(Condition_L, ".inlined_string_CharAt_arg_validate_2", true);
             rememberState(2);
-            constVREndOfBB();
             beforeCall("exception");
             unconditional_jump("common_errStringIndexOutOfBounds", false);
             goToState(2);
@@ -1377,7 +1368,6 @@ int op_execute_inline(const MIR * mir, bool isRange) {
             compare_imm_reg(OpndSize_32, 0, 2, false);
             conditional_jump(Condition_NS, ".do_inlined_string_CharAt", true);
             rememberState(3);
-            constVREndOfBB();
             beforeCall("exception");
             unconditional_jump("common_errStringIndexOutOfBounds", false);
             goToState(3);
@@ -1401,7 +1391,6 @@ int op_execute_inline(const MIR * mir, bool isRange) {
                              true);
             scratchRegs[0] = PhysicalReg_SCRATCH_1;
             rememberState(1);
-            constVREndOfBB();
             beforeCall("exception"); //dump GG, GL VRs
             unconditional_jump("common_errNullObject", false);
             goToState(1);
@@ -1551,7 +1540,6 @@ int common_invoke_virtual_quick(bool hasRange, u2 vD, u2 IMMC, const MIR *mir) {
     }
 #endif
     export_pc();
-    constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
     /////////////////////////////////////////////////
     get_virtual_reg(vD, OpndSize_32, 1, false);
@@ -1628,7 +1616,6 @@ int op_invoke_virtual_quick_range(const MIR * mir) {
 int common_invoke_super_quick(bool hasRange, u2 vD, u2 IMMC,
         const DecodedInstruction &decodedInst) {
     export_pc();
-    constVREndOfBB();
     beforeCall("exception"); //dump GG, GL VRs
     compare_imm_VR(OpndSize_32, 0, vD);
 
diff --git a/vm/compiler/codegen/x86/LowerJump.cpp b/vm/compiler/codegen/x86/LowerJump.cpp
index 53ec08c..642fca0 100644
--- a/vm/compiler/codegen/x86/LowerJump.cpp
+++ b/vm/compiler/codegen/x86/LowerJump.cpp
@@ -26,6 +26,7 @@
 #include "enc_wrapper.h"
 #include "interp/InterpDefs.h"
 #include "NcgHelper.h"
+#include "RegisterizationBE.h"
 #include "Scheduler.h"
 #include "Singleton.h"
 
@@ -304,7 +305,7 @@ int insertGlobalPCWorklist(char * offset, char * codeStart)
 /*
  * search chainingWorklist to return instruction offset address in move instruction
  */
-char* searchChainingWorklist(int blockId) {
+char* searchChainingWorklist(unsigned int blockId) {
     LabelMap* ptr = chainingWorklist;
     unsigned instSize;
 
@@ -394,7 +395,7 @@ int updateImmRMInst(char* moveInst, const char* label, int relativeNCG); //forwa
 void performChainingWorklist() {
     LabelMap* ptr = chainingWorklist;
     while(ptr != NULL) {
-        int tmpNCG = traceLabelList[ptr->addend].lop.generic.offset;
+        int tmpNCG = getLabelOffset (ptr->addend);
         char* NCGaddr = streamMethodStart + tmpNCG;
         updateImmRMInst(ptr->codePtr, "", (int)NCGaddr);
         chainingWorklist = ptr->nextItem;
@@ -892,9 +893,9 @@ int insertDataWorklist(s4 relativePC, char* codePtr1) {
 int performNCGWorklist() {
     NCGWorklist* ptr = globalNCGWorklist;
     while(ptr != NULL) {
+        int tmpNCG = getLabelOffset (ptr->relativePC);
         ALOGV("Perform NCG worklist: @ %p target block %d target NCG %x",
-             ptr->codePtr, ptr->relativePC, traceLabelList[ptr->relativePC].lop.generic.offset);
-        int tmpNCG = traceLabelList[ptr->relativePC].lop.generic.offset;
+             ptr->codePtr, ptr->relativePC, tmpNCG);
         assert(tmpNCG >= 0);
         int relativeNCG = tmpNCG - ptr->offsetNCG;
         unsigned instSize = encoder_get_inst_size(ptr->codePtr);
@@ -1057,7 +1058,7 @@ If the branch target is not handled, call insertNCGWorklist, unknown is set to t
 If the branch target is handled, call estOpndSizeFromImm to set immSize for jump instruction, returns the value of the immediate
 */
 int getRelativeNCG(s4 tmp, JmpCall_type type, bool* unknown, OpndSize* size) {//tmp: relativePC
-    int tmpNCG = traceLabelList[tmp].lop.generic.offset;
+    int tmpNCG = getLabelOffset (tmp);
 
     *unknown = false;
     if(tmpNCG <0) {
@@ -1109,7 +1110,7 @@ int common_backwardBranch() {
 \brief common code to handle GOTO
 
 If it is a backward branch, call common_periodicChecks4 to handle GC request.
-Since this is the end of a basic block, constVREndOfBB and globalVREndOfBB are called right before the jump instruction.
+Since this is the end of a basic block, globalVREndOfBB are called right before the jump instruction.
 */
 int common_goto(s4 tmp) { //tmp: relativePC
     int retCode = 0;
@@ -1125,8 +1126,7 @@ int common_goto(s4 tmp) { //tmp: relativePC
         //potential garbage collection will work as designed
         call_helper_API("common_periodicChecks4");
     }
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
+    retCode = handleVRsEndOfBB(true);
     if (retCode < 0)
         return retCode;
     bool unknown;
@@ -1162,8 +1162,8 @@ int common_if(s4 tmp, ConditionCode cc_next, ConditionCode cc_taken) {
 //!
 //! \details If it is a backward branch, call common_periodicChecks4
 //! to handle GC request.
-//! Since this is the end of a basic block, constVREndOfBB and
-//! globalVREndOfBB are called right before the jump instruction.
+//! Since this is the end of a basic block,
+//! globalVREndOfBB is called right before the jump instruction.
 //! when this is called from JIT, there is no need to check GC
 //!
 //! \param targetBlockId
@@ -1173,8 +1173,10 @@ int common_goto(s4 targetBlockId) {
     bool unknown;
     int retCode = 0;
     OpndSize size;
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
+
+    // We call it with false because we want to actually want to update
+    // association tables of children and handle ME spill requests
+    retCode = handleVRsEndOfBB(false);
     if (retCode < 0)
         return retCode;
 
@@ -1194,97 +1196,132 @@ int common_if(s4 tmp, ConditionCode cc_next, ConditionCode cc) {
     OpndSize size = OpndSize_Null;
     int relativeNCG;
 
-    if (traceCurrentBB->taken && traceCurrentBB->taken->blockType == kChainingCellBackwardBranch) {
-        char buffer[80];
-        snprintf (buffer, sizeof(buffer), ".vr_store_at_loop_back_%d", traceCurrentBB->taken->id);
+    // A basic block whose last bytecode is "if" must have two children
+    assert (traceCurrentBB->taken != NULL);
+    assert (traceCurrentBB->fallThrough != NULL);
 
-        if(gDvmJit.scheduling)
-            singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
-        rememberState(1);
-        alignOffset(2); // 2 is (instruction size of "jcc rel32" - sizeof(rel32))
-        conditional_jump(cc, buffer, false);
-        storeVRExitOfLoop();
+    BasicBlock_O1 * takenBB =
+            reinterpret_cast<BasicBlock_O1 *>(traceCurrentBB->taken);
+    BasicBlock_O1 * fallThroughBB =
+            reinterpret_cast<BasicBlock_O1 *>(traceCurrentBB->fallThrough);
 
-        if(gDvmJit.scheduling && traceCurrentBB->fallThrough) {
-            unconditional_jump_block(traceCurrentBB->fallThrough->id,
-                    isBasicBlockAChainingCell(traceCurrentBB->fallThrough));
-        } else {
-            alignOffset(1); // 1 is (instruction size of "jmp rel32" - sizeof(rel32))
-            relativeNCG = traceCurrentBB->fallThrough ? traceCurrentBB->fallThrough->id : 0;
-            if(traceCurrentBB->fallThrough)
-                relativeNCG = getRelativeNCG(traceCurrentBB->fallThrough->id, JmpCall_uncond, &unknown, &size);
-            unconditional_jump_int(relativeNCG, size);
+    // Test if taken BB is a backward branch chaining cell
+    BasicBlock_O1 * backwardBranchChainingCell =
+            (takenBB->blockType == kChainingCellBackwardBranch) ?
+                    takenBB : NULL;
+    if (backwardBranchChainingCell == NULL) {
+        // Try to see if fallThrough BB is a backward branch chaining cell
+        backwardBranchChainingCell =
+                (fallThroughBB->blockType == kChainingCellBackwardBranch) ?
+                        fallThroughBB : NULL;
+    }
+
+    if (backwardBranchChainingCell != NULL) {
+        // We have a backward branch and thus we must deal with it specially
+
+        bool isTaken = (backwardBranchChainingCell == takenBB);
+
+        // Figure out which one the other BB is
+        BasicBlock_O1 * otherBB = (isTaken == true) ? fallThroughBB : takenBB;
+
+        //Figure the condition code: we basically will make the system make the taken be the BWCC
+        ConditionCode bwcc = (backwardBranchChainingCell == takenBB) ? cc : cc_next;
+
+        if(gDvmJit.scheduling == true) {
+            singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
         }
 
-        if (insertLabel(buffer, false) == -1)
+        //Now handle the registerization requested by the Backward Chaining Cell here
+        //Compared to normally this is done here because the chaining cell can actually
+        //Redirect the jump below to directly the right BB, bypassing the backward chaining cell
+
+        // First: update the BWCC's fallthrough table: the head of the loop
+        BasicBlock_O1 *loopEntry =
+                reinterpret_cast<BasicBlock_O1 *>(backwardBranchChainingCell->fallThrough);
+
+        //We want to satisfy our backward branch dependencies, it contains which registers we care about
+        if (AssociationTable::satisfyBBAssociations(backwardBranchChainingCell, loopEntry) == false) {
             return -1;
-        goToState(1);
-        storeVRExitOfLoop();
+        }
 
-        if(gDvmJit.scheduling && traceCurrentBB->taken) {
-            unconditional_jump_block(traceCurrentBB->taken->id, false);
-        } else {
-            relativeNCG = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
-            if(traceCurrentBB->taken)
-                relativeNCG = getRelativeNCG(traceCurrentBB->taken->id, JmpCall_uncond, &unknown, &size);
-            unconditional_jump_int(relativeNCG, size);
+        // Second: update the BWCC's table: because BWCC actually is empty this
+        // will just update its table as what its fallthrough just got
+        // It's not isTaken because the function wants to know if we are concerned with the fallthrough
+        //    Here we care about the BWCC, so it is wherever not isTaken says it is
+        if (AssociationTable::createOrSyncTable(currentBB, !isTaken) == false) {
+            return -1;
         }
-    }
-    else if (traceCurrentBB->fallThrough && traceCurrentBB->fallThrough->blockType == kChainingCellBackwardBranch) {
-        char buffer[80];
-        snprintf (buffer, sizeof(buffer), ".vr_store_at_loop_back_%d", traceCurrentBB->fallThrough->id);
 
-        if(gDvmJit.scheduling)
-            singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
         rememberState(1);
         alignOffset(2);
-        conditional_jump(cc_next, buffer, false);
-        storeVRExitOfLoop();
 
-        if(gDvmJit.scheduling && traceCurrentBB->taken) {
-            unconditional_jump_block(traceCurrentBB->taken->id,
-                    isBasicBlockAChainingCell(traceCurrentBB->taken));
+        char storeBackLabel[LABEL_SIZE];
+        snprintf(storeBackLabel, LABEL_SIZE, ".vr_store_at_loop_back_%d",
+                backwardBranchChainingCell->id);
+
+
+        // After chaining, the following conditional jump will no longer
+        // be a jump to the BWCC but will go to loop start. However, for
+        // now we are jumping to store back VRs before going back to
+        // interpreter via the BWCC.
+        conditional_jump(bwcc, storeBackLabel, false);
+
+        //Sync with the child's association table
+        // We use isTaken here to take care of the other child
+        if (AssociationTable::createOrSyncTable(currentBB, isTaken) == false) {
+            return -1;
+        }
+
+        if (gDvmJit.scheduling) {
+            unconditional_jump_block(otherBB->id,
+                    isBasicBlockAChainingCell(otherBB));
         } else {
             alignOffset(1);
-            relativeNCG = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
-            if(traceCurrentBB->taken)
-                relativeNCG = getRelativeNCG(traceCurrentBB->taken->id, JmpCall_uncond, &unknown, &size);
+            relativeNCG = getRelativeNCG(otherBB->id, JmpCall_uncond, &unknown,
+                    &size);
             unconditional_jump_int(relativeNCG, size);
         }
 
-        if (insertLabel(buffer, false) == -1)
+        // At this point we need to
+        if (insertLabel(storeBackLabel, false) == -1)
             return -1;
         goToState(1);
-        storeVRExitOfLoop();
 
-        if(gDvmJit.scheduling && traceCurrentBB->fallThrough) {
-            unconditional_jump_block(traceCurrentBB->fallThrough->id, false);
-        } else {
-            relativeNCG = traceCurrentBB->fallThrough ? traceCurrentBB->fallThrough->id : 0;
-            if(traceCurrentBB->fallThrough)
-                relativeNCG = getRelativeNCG(traceCurrentBB->fallThrough->id, JmpCall_uncond, &unknown, &size);
-            unconditional_jump_int(relativeNCG, size);
+        // Since we are still processing the parent, we still have its compile
+        // table. However, let's satisfy spill requests of children
+        if (AssociationTable::handleSpillRequestsFromME(backwardBranchChainingCell) == false)
+            return -1;
+
+        // Let the fallthrough system automatically handle this branch if indeed
+        // the BWCC is the fallthrough
+        if (backwardBranchChainingCell != fallThroughBB) {
+            if (gDvmJit.scheduling) {
+                unconditional_jump_block(backwardBranchChainingCell->id, false);
+            } else {
+                relativeNCG = getRelativeNCG(backwardBranchChainingCell->id,
+                        JmpCall_uncond, &unknown, &size);
+                unconditional_jump_int(relativeNCG, size);
+            }
         }
-    }
-    else {
+    } else {
+        // First sync with the taken child
+        if (AssociationTable::createOrSyncTable(currentBB, false) == false) {
+            return -1;
+        }
+
         if(gDvmJit.scheduling) {
-            if(traceCurrentBB->taken)
-                conditional_jump_block(cc, traceCurrentBB->taken->id,
-                        isBasicBlockAChainingCell(traceCurrentBB->taken));
-            if(traceCurrentBB->fallThrough)
-                unconditional_jump_block(traceCurrentBB->fallThrough->id,
-                        isBasicBlockAChainingCell(traceCurrentBB->fallThrough));
+            conditional_jump_block(cc, takenBB->id,
+                    isBasicBlockAChainingCell(takenBB));
         } else {
             alignOffset(2);
-            relativeNCG = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
-            if(traceCurrentBB->taken)
-                relativeNCG = getRelativeNCG(traceCurrentBB->taken->id, JmpCall_cond, &unknown, &size);
+            relativeNCG = getRelativeNCG(takenBB->id, JmpCall_cond, &unknown,
+                    &size);
             conditional_jump_int(cc, relativeNCG, size);
-            alignOffset(1);
-            relativeNCG = traceCurrentBB->fallThrough ? traceCurrentBB->fallThrough->id : 0;
-            if(traceCurrentBB->fallThrough)
-                relativeNCG = getRelativeNCG(traceCurrentBB->fallThrough->id, JmpCall_uncond, &unknown, &size);
-            unconditional_jump_int(relativeNCG, size);
+        }
+
+        // Now sync with the fallthrough child
+        if (AssociationTable::createOrSyncTable(currentBB, true) == false) {
+            return -1;
         }
     }
     return 2;
@@ -1655,10 +1692,8 @@ int op_packed_switch(const MIR * mir, const u2 * dalvikPC) {
     scratchRegs[0] = PhysicalReg_SCRATCH_1;
     call_dvmJitHandlePackedSwitch();
     load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    //TODO: eax should be absolute address, call globalVREndOfBB, constVREndOfBB
-    //conditional_jump_global_API(Condition_LE, "common_backwardBranch", false);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod); //update GG VRs
+    //TODO: eax should be absolute address, call handleVRsEndOfBB 
+    retCode = handleVRsEndOfBB(true); //update GG VRs
     if (retCode < 0)
         return retCode;
     //get rPC, %eax has the relative PC offset
@@ -1706,8 +1741,7 @@ int op_sparse_switch(const MIR * mir, const u2 * dalvikPC) {
     assert(tSize > 0);
     const s4* keys = (const s4*) switchData;
     assert(((u4)keys & 0x3) == 0);
-    s4* entries = (s4*)switchData + tSize;
-    assert(((u4)entries & 0x3) == 0);
+    assert((((u4) ((s4*) switchData + tSize)) & 0x3) == 0);
 #endif
 
     get_virtual_reg(vA, OpndSize_32, 1, false);
@@ -1725,10 +1759,8 @@ int op_sparse_switch(const MIR * mir, const u2 * dalvikPC) {
     //otherwise, fall through (no_op)
     call_dvmJitHandleSparseSwitch();
     load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    //TODO: eax should be absolute address, call globalVREndOfBB constVREndOfBB
-    //conditional_jump_global_API(Condition_LE, "common_backwardBranch", false);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
+    //TODO: eax should be absolute address, call handleVRsEndOfBB
+    retCode = handleVRsEndOfBB(true);
     if (retCode < 0)
         return retCode;
     //get rPC, %eax has the relative PC offset
@@ -1755,19 +1787,13 @@ int op_sparse_switch(const MIR * mir, const u2 * dalvikPC) {
  * @return value >= 0 when handled
  */
 int op_if_eq(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_EQ);
     u2 vA = mir->dalvikInsn.vA;
     u2 vB = mir->dalvikInsn.vB;
     s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_NE, Condition_E);
-    return 0;
+    return common_if(tmp, Condition_NE, Condition_E);
 }
 
 /**
@@ -1776,19 +1802,13 @@ int op_if_eq(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_ne(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_NE);
     u2 vA = mir->dalvikInsn.vA;
     u2 vB = mir->dalvikInsn.vB;
     s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_E, Condition_NE);
-    return 0;
+    return common_if(tmp, Condition_E, Condition_NE);
 }
 
 /**
@@ -1797,19 +1817,13 @@ int op_if_ne(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_lt(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_LT);
     u2 vA = mir->dalvikInsn.vA;
     u2 vB = mir->dalvikInsn.vB;
     s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_GE, Condition_L);
-    return 0;
+    return common_if(tmp, Condition_GE, Condition_L);
 }
 
 /**
@@ -1818,19 +1832,13 @@ int op_if_lt(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_ge(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_GE);
     u2 vA = mir->dalvikInsn.vA;
     u2 vB = mir->dalvikInsn.vB;
     s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_L, Condition_GE);
-    return 0;
+    return common_if(tmp, Condition_L, Condition_GE);
 }
 
 /**
@@ -1839,19 +1847,13 @@ int op_if_ge(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_gt(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_GT);
     u2 vA = mir->dalvikInsn.vA;
     u2 vB = mir->dalvikInsn.vB;
     s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_LE, Condition_G);
-    return 0;
+    return common_if(tmp, Condition_LE, Condition_G);
 }
 
 /**
@@ -1860,19 +1862,13 @@ int op_if_gt(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_le(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_LE);
     u2 vA = mir->dalvikInsn.vA;
     u2 vB = mir->dalvikInsn.vB;
     s2 tmp = mir->dalvikInsn.vC;
     get_virtual_reg(vA, OpndSize_32, 1, false);
     compare_VR_reg(OpndSize_32, vB, 1, false);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_G, Condition_LE);
-    return 0;
+    return common_if(tmp, Condition_G, Condition_LE);
 }
 #undef P_GPR_1
 
@@ -1882,17 +1878,11 @@ int op_if_le(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_eqz(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_EQZ);
     u2 vA = mir->dalvikInsn.vA;
     s2 tmp = mir->dalvikInsn.vB;
     compare_imm_VR(OpndSize_32, 0, vA);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_NE, Condition_E);
-    return 0;
+    return common_if(tmp, Condition_NE, Condition_E);
 }
 
 /**
@@ -1901,17 +1891,11 @@ int op_if_eqz(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_nez(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_NEZ);
     u2 vA = mir->dalvikInsn.vA;
     s2 tmp = mir->dalvikInsn.vB;
     compare_imm_VR(OpndSize_32, 0, vA);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_E, Condition_NE);
-    return 0;
+    return common_if(tmp, Condition_E, Condition_NE);
 }
 
 /**
@@ -1920,17 +1904,11 @@ int op_if_nez(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_ltz(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_LTZ);
     u2 vA = mir->dalvikInsn.vA;
     s2 tmp = mir->dalvikInsn.vB;
     compare_imm_VR(OpndSize_32, 0, vA);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_GE, Condition_L);
-    return 0;
+    return common_if(tmp, Condition_GE, Condition_L);
 }
 
 /**
@@ -1939,17 +1917,11 @@ int op_if_ltz(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_gez(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_GEZ);
     u2 vA = mir->dalvikInsn.vA;
     s2 tmp = mir->dalvikInsn.vB;
     compare_imm_VR(OpndSize_32, 0, vA);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_L, Condition_GE);
-    return 0;
+    return common_if(tmp, Condition_L, Condition_GE);
 }
 
 /**
@@ -1958,17 +1930,11 @@ int op_if_gez(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_gtz(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_GTZ);
     u2 vA = mir->dalvikInsn.vA;
     s2 tmp = mir->dalvikInsn.vB;
     compare_imm_VR(OpndSize_32, 0, vA);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_LE, Condition_G);
-    return 0;
+    return common_if(tmp, Condition_LE, Condition_G);
 }
 
 /**
@@ -1977,17 +1943,11 @@ int op_if_gtz(const MIR * mir) {
  * @return value >= 0 when handled
  */
 int op_if_lez(const MIR * mir) {
-    int retCode = 0;
     assert(mir->dalvikInsn.opcode == OP_IF_LEZ);
     u2 vA = mir->dalvikInsn.vA;
     s2 tmp = mir->dalvikInsn.vB;
     compare_imm_VR(OpndSize_32, 0, vA);
-    constVREndOfBB();
-    retCode = globalVREndOfBB(currentMethod);
-    if (retCode < 0)
-        return retCode;
-    common_if(tmp, Condition_G, Condition_LE);
-    return 0;
+    return common_if(tmp, Condition_G, Condition_LE);
 }
 
 #define P_GPR_1 PhysicalReg_ECX
diff --git a/vm/compiler/codegen/x86/NcgAot.cpp b/vm/compiler/codegen/x86/NcgAot.cpp
index c7112f1..b68e92f 100644
--- a/vm/compiler/codegen/x86/NcgAot.cpp
+++ b/vm/compiler/codegen/x86/NcgAot.cpp
@@ -66,7 +66,6 @@ int jumpToExceptionThrown(int exceptionNum) {
     if(gDvm.executionMode == kExecutionModeNcgO1) {
         rememberState(exceptionNum);
         export_pc();
-        constVREndOfBB();
         beforeCall("exception"); //dump GG, GL VRs
     }
 
diff --git a/vm/compiler/codegen/x86/RegisterizationBE.cpp b/vm/compiler/codegen/x86/RegisterizationBE.cpp
new file mode 100644
index 0000000..40ed57c
--- /dev/null
+++ b/vm/compiler/codegen/x86/RegisterizationBE.cpp
@@ -0,0 +1,999 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <map>
+#include <set>
+#include <algorithm>
+#include "Dalvik.h"
+#include "Lower.h"
+#include "AnalysisO1.h"
+#include "CodegenErrors.h"
+#include "RegisterizationBE.h"
+
+//#define DEBUG_REGISTERIZATION
+#define DEBUG_ASSOCIATION(X)
+#define DEBUG_SPILLING(X)
+#define DEBUG_ASSOCIATION_MERGE(X)
+#define DEBUG_COMPILETABLE_UPDATE(X)
+
+AssociationTable::AssociationTable(void) {
+    //Call clear function, it will reset everything
+    clear();
+}
+
+AssociationTable::~AssociationTable(void) {
+    //Call clear function, it will reset everything
+    clear();
+}
+
+void AssociationTable::clear(void) {
+    DEBUG_ASSOCIATION (ALOGD ("Clearing association table\n"));
+
+    //Clear maps
+    associations.clear();
+    inMemoryTracker.clear();
+    constTracker.clear();
+
+    //Have we finalized the table
+    isFinal = false;
+}
+
+bool AssociationTable::copy(AssociationTable & source) {
+    //We cannot copy anything if we are finalized
+    if (hasBeenFinalized()) {
+        //Print out an error and set it before returning false
+        ALOGW("Association table has been finalized but we want to update it.");
+        SET_JIT_ERROR(kJitErrorBERegisterization);
+        return false;
+    }
+
+    //Insert all associations from source
+    associations.insert(source.associations.begin(), source.associations.end());
+
+    //Insert all memory trackers
+    inMemoryTracker.insert(source.inMemoryTracker.begin(),
+            source.inMemoryTracker.end());
+
+    //Insert all constants
+    constTracker.insert(source.constTracker.begin(), source.constTracker.end());
+
+    //Finalize the current table and return success
+    finalize();
+    return true;
+}
+
+bool AssociationTable::associate(compileTableEntry & compileEntry) {
+    // We cannot update once the association table has been finalized
+    if (hasBeenFinalized()) {
+        //Print out error, set error, and return false
+        ALOGW("Association table has been finalized but we want to update it.");
+        SET_JIT_ERROR(kJitErrorBERegisterization);
+        return false;
+    }
+
+    // Paranoid: this must be a virtual register
+    assert(isVirtualReg(compileEntry.physicalType));
+
+    //Get local versions of the compileEntry
+    int VR = compileEntry.regNum;
+    int physicalReg = compileEntry.physicalReg;
+
+    // Check if we are overwriting an existing association
+    const_iterator assocEntry = associations.find(VR);
+    if (assocEntry != associations.end()) {
+        int oldPhysicalReg = assocEntry->second.physicalReg;
+
+        // We might be saving VRs even when they don't have physical register
+        // associated and thus we don't care for overwriting unless one has
+        // physical register
+        if (oldPhysicalReg != PhysicalReg_Null || physicalReg != PhysicalReg_Null) {
+            // Overwriting an association must mean that we are reading from a source
+            // that has the duplicate entries for the same VR. Most likely this can
+            // happen when a VR is associated with XMM and GP in same trace
+            ALOGE ("JIT_INFO: Overwriting association of v%d:%s with %s\n", VR,
+                    physicalRegToString(static_cast<PhysicalReg>(oldPhysicalReg)),
+                    physicalRegToString(static_cast<PhysicalReg>(physicalReg)));
+            SET_JIT_ERROR(kJitErrorBERegisterization);
+            return false;
+        }
+    }
+
+    // Make a copy of the compileTableEntry
+    associations[VR] = compileEntry;
+
+    DEBUG_ASSOCIATION( ALOGD ("Associating v%d with %s\n",
+                    VR, physicalRegToString(physicalReg)));
+
+    //Report success
+    return true;
+}
+
+bool AssociationTable::associate(MemoryVRInfo & memVRInfo) {
+    // We cannot update once the association table has been finalized
+    if (hasBeenFinalized()) {
+        ALOGW("Association table has been finalized but we want to update it.");
+        SET_JIT_ERROR(kJitErrorBERegisterization);
+        return false;
+    }
+
+    int VR = memVRInfo.regNum;
+
+    // Make a copy of the in memory information
+    inMemoryTracker[VR] = memVRInfo;
+
+    return true;
+}
+
+bool AssociationTable::associate(ConstVRInfo & constVRInfo) {
+    // We cannot update once the association table has been finalized
+    if (hasBeenFinalized()) {
+        ALOGW("Association table has been finalized but we want to update it.");
+        SET_JIT_ERROR(kJitErrorBERegisterization);
+        return false;
+    }
+
+    int VR = constVRInfo.regNum;
+
+    // Make a copy of the in memory information
+    constTracker[VR] = constVRInfo;
+
+    return true;
+}
+
+bool AssociationTable::wasVRInMemory(int VR) {
+    //Find the VR in the inMemoryTracker map
+    inMemoryTrackerConstIterator entry = inMemoryTracker.find(VR);
+
+    //If we can't find it, it isn't in memory
+    if (entry == inMemoryTracker.end()) {
+        return false;
+    }
+    else {
+        //Return what the entry tells us
+        return entry->second.inMemory;
+    }
+}
+
+bool AssociationTable::wasVRConstant(int VR) {
+    //Find the VR in the constTracker map
+    constantTrackerConstIterator entry = constTracker.find(VR);
+
+    //Return whether we found it
+    return (entry != constTracker.end());
+}
+
+int AssociationTable::getVRConstValue(int VR) {
+    //Find the VR in the constTracker map
+    constantTrackerConstIterator entry = constTracker.find(VR);
+
+    //Paranoid: this function should not be called if wasVRConstant returns false
+    assert(entry != constTracker.end());
+
+    //Return value
+    return entry->second.value;
+}
+
+void AssociationTable::finalize() {
+    //Set to final
+    isFinal = true;
+}
+
+void AssociationTable::findUsedRegisters(
+        std::set<PhysicalReg> & outUsedRegisters) {
+
+    // Go through all association table entries and find used registers
+    for (const_iterator iter = begin(); iter != end(); iter++) {
+        //Get a local version of the register used
+        PhysicalReg regUsed = static_cast<PhysicalReg>(iter->second.physicalReg);
+
+        //If not the Null register, insert it
+        if (regUsed != PhysicalReg_Null) {
+            outUsedRegisters.insert(regUsed);
+        }
+    }
+}
+
+PhysicalReg AssociationTable::getVRAssociation(int vR) {
+    //Traverse the table
+    for (const_iterator iter = begin(); iter != end(); iter++) {
+
+        //Get local values
+        int curVR = iter->first;
+        PhysicalReg regUsed = static_cast<PhysicalReg>(iter->second.physicalReg);
+
+        //If it is the vR we want and it isn't Null, return it
+        //Allows us to search for a next association using vR
+        if (curVR == vR && regUsed != PhysicalReg_Null) {
+            return regUsed;
+        }
+
+    }
+
+    //Report we did not find the association
+    return PhysicalReg_Null;
+}
+
+void AssociationTable::printToDot(FILE * file) {
+    DEBUG_ASSOCIATION(ALOGD("Printing association table to dot file"));
+
+    if (associations.size() == 0) {
+        fprintf(file, " {Association table is empty} |\\\n");
+    }
+    else
+    {
+        fprintf(file, " {Association table at entry:}|\\\n");
+
+        //Now go through the iteration
+        for (const_iterator iter = associations.begin(); iter != associations.end(); iter++) {
+            //If it's a constant, print it out using %d
+            if (wasVRConstant(iter->second.regNum)) {
+                fprintf(file, "{v%d : %d} | \\\n", iter->first,
+                        getVRConstValue(iter->second.regNum));
+            } else {
+                //Otherwise, use the physicalRegToString function
+                fprintf(file, "{v%d : %s} | \\\n", iter->first,
+                        physicalRegToString(
+                            static_cast<PhysicalReg>(iter->second.physicalReg)));
+            }
+        }
+    }
+}
+
+//Helper static functions for the class
+static bool shouldSaveAssociation(compileTableEntry & compileEntry) {
+    // Make a buffer for constant value even if we won't use it
+    int constantValue[2];
+
+    // We want to save association if the VR is either in a physical
+    // register or is a constant
+    bool res = (compileEntry.physicalReg != PhysicalReg_Null
+            || isVirtualRegConstant(compileEntry.regNum,
+                    static_cast<LowOpndRegType>(compileEntry.physicalType
+                            & MASK_FOR_TYPE), constantValue, false) == 3);
+
+    return res;
+}
+
+//Static functions of the class
+bool AssociationTable::syncAssociationsWithCompileTable(AssociationTable & associationsToUpdate) {
+    if (associationsToUpdate.hasBeenFinalized()) {
+        ALOGW("Association table has been finalized but we want to update it.");
+        SET_JIT_ERROR(kJitErrorBERegisterization);
+        return false;
+    }
+
+    // Go through each entry of the compile table
+    for (int entry = 0; entry < num_compile_entries; entry++) {
+        // Update associations for every VR entry we find
+        if (isVirtualReg(compileTable[entry].physicalType) == true
+                && shouldSaveAssociation(compileTable[entry]) == true) {
+            if (associationsToUpdate.associate(compileTable[entry]) == false) {
+                return false;
+            }
+        }
+    }
+
+    // Go through each entry in memVRTable to save whether VR is in memory
+    for (int entry = 0; entry < num_memory_vr; entry++) {
+        if (associationsToUpdate.associate(memVRTable[entry]) == false) {
+            return false;
+        }
+    }
+
+    // Go through each entry in constVRTable
+    for (int entry = 0; entry < num_const_vr; entry++) {
+        // Only save entry if it is actually a constant
+        if (constVRTable[entry].isConst == true) {
+            if (associationsToUpdate.associate(constVRTable[entry]) == false) {
+                return false;
+            }
+        }
+    }
+
+    //Finalize the table and report success
+    associationsToUpdate.finalize();
+
+    return true;
+}
+
+bool AssociationTable::syncCompileTableWithAssociations(AssociationTable & associationsToUse) {
+    DEBUG_COMPILETABLE_UPDATE(ALOGD("There are %d associations to merge",
+            associationsToUse.size()));
+
+    // Go through every association we saved
+    for (AssociationTable::const_iterator assocIter = associationsToUse.begin();
+            assocIter != associationsToUse.end(); assocIter++) {
+
+        //Suppose we will not find the entry
+        bool foundCompileTableEntry = false;
+
+        DEBUG_COMPILETABLE_UPDATE(ALOGD("Starting to search through compile "
+                "table that has %d entries", num_compile_entries));
+
+        // Now search the compile table for an appropriate entry
+        for (int entry = 0; entry < num_compile_entries; entry++) {
+
+            //If it is a virtual register and the right register
+            if (isVirtualReg(compileTable[entry].physicalType) == true
+                    && compileTable[entry].regNum == assocIter->first) {
+                DEBUG_COMPILETABLE_UPDATE(ALOGD("Found that v%d is in compile "
+                        "table already.", assocIter->first));
+
+                // Overwrite current compile table entry
+                compileTable[entry] = assocIter->second;
+                //Mark that we found it
+                foundCompileTableEntry = true;
+                break;
+            }
+        }
+
+        // If we did not find an entry, we must insert it
+        if (foundCompileTableEntry == false) {
+            DEBUG_COMPILETABLE_UPDATE(ALOGD("We have not found v%d in compile "
+                        "table so we will make a new entry.", assocIter->first));
+
+            //Add it to the global compileTable
+            compileTable[num_compile_entries] = assocIter->second;
+            //Augment number of entries
+            num_compile_entries++;
+
+            //Paranoid
+            if (num_compile_entries >= COMPILE_TABLE_SIZE) {
+                ALOGE("JIT_INFO: compileTable overflow at "
+                        "syncCompileTableWithAssociations");
+                SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                return false;
+            }
+        }
+    }
+
+    // In case we have updated the compile table, we must also update the
+    // state of registers to match what compile table believes
+    if (associationsToUse.size() > 0) {
+        syncAllRegs();
+    }
+
+    DEBUG_COMPILETABLE_UPDATE(ALOGD("Finished merging associations into "
+            "compile table"));
+    DEBUG_COMPILETABLE_UPDATE(dumpCompileTable());
+
+    //Report success
+    return true;
+}
+
+/**
+ * @brief Initialization for satisfyBBAssociations helper: sets up most parameters
+ * @param associationToUse the association to compare ourselves with
+ * @param otherVRToPhysicalReg what is the associationsToUse having as associations (updated by the function)
+ * @param otherVRs what VRs is the associationsToUse using (updated by the function)
+ * @param currentVRToPhysicalReg what is the current association state between VRs to Physical ((updated by the function))
+ * @param currentVRs what VRs is the current association using as VRs (updated by the function)
+ */
+static void initAssociationHelperTables (AssociationTable &associationsToUse,
+                  std::map<int, PhysicalReg> &otherVRToPhysicalReg,
+                  std::set<int> &otherVRs,
+                  std::map<int, PhysicalReg> &currentVRToPhysicalReg,
+                  std::set<int> &currentVRs) {
+
+    // First we need to go through each of the child's association entries
+    // to figure out each VR's association with physical register
+    for (AssociationTable::const_iterator assocIter = associationsToUse.begin();
+            assocIter != associationsToUse.end(); assocIter++) {
+
+        if (assocIter->second.physicalReg != PhysicalReg_Null) {
+            // Save the mapping to physical register
+            otherVRToPhysicalReg[assocIter->first] =
+                    static_cast<PhysicalReg>(assocIter->second.physicalReg);
+            // Keep track of VRs that have physical reg mapping
+            otherVRs.insert(assocIter->first);
+        }
+
+    }
+
+    // Now go through current compile table to figure out what VRs are in
+    // physical registers
+    for (int entry = 0; entry < num_compile_entries; entry++) {
+
+        if (isVirtualReg(compileTable[entry].physicalType)
+                && compileTable[entry].physicalReg != PhysicalReg_Null) {
+
+            // Save the mapping to physical register
+            currentVRToPhysicalReg[compileTable[entry].regNum] =
+                    static_cast<PhysicalReg>(compileTable[entry].physicalReg);
+            // Keep track of VRs that have physical reg mapping
+            currentVRs.insert(compileTable[entry].regNum);
+
+        }
+
+    }
+}
+
+/**
+ * @brief Spill Virtual Registers to memory
+ * @param parent the parent BasicBlock, contains the write back requests
+ * @param childUsedReg what registers are being used at entry for the child
+ * @param currentVRs the current Virtual Registers used by the parent at the end of code generation
+ * @param childVRs the child's Virtual Register usage at entrance of code generation
+ */
+static void spillVRToMemory (const BasicBlock_O1 *parent,
+                             const std::set<PhysicalReg> &childUsedReg,
+                             const std::set<int> &currentVRs,
+                             const std::set<int> &childVRs) {
+    std::set<int> setOperationResult;
+
+    // Do we have any VRs that are currently in physical registers
+    // but the child is not expecting them to be in registers? To get this we
+    // do a set difference (currentVRs - childVRs)
+    std::set_difference(currentVRs.begin(), currentVRs.end(), childVRs.begin(),
+            childVRs.end(),
+            std::inserter(setOperationResult, setOperationResult.end()));
+
+    //Spill anything that is in this difference set
+    for (std::set<int>::const_iterator setOpIter = setOperationResult.begin();
+            setOpIter != setOperationResult.end(); setOpIter++) {
+        // TODO This spilling should reuse some common code for spilling just VRs
+
+        int VR = *setOpIter;
+
+        // So we spill if parent requested it for spill or if child plans on reusing
+        // the physical register.
+        for (int entry = 0; entry < num_compile_entries; entry++) {
+            if (isVirtualReg(compileTable[entry].physicalType)
+                    && compileTable[entry].physicalReg != PhysicalReg_Null
+                    && compileTable[entry].regNum == VR
+                    && (dvmIsBitSet(parent->requestWriteBack, VR)
+                            || childUsedReg.find(
+                                    static_cast<PhysicalReg>(compileTable[entry].physicalReg))
+                                    != childUsedReg.end())) {
+                DEBUG_ASSOCIATION_MERGE(ALOGD("Spilling v%d back to memory because "
+                        "child does not expect it in a physical register", VR));
+                spillLogicalReg(entry, true);
+            }
+        }
+    }
+
+    // Since we have spilled VRs, lets make sure we properly keep track
+    // of which physical registers are currently being used
+    syncAllRegs();
+}
+
+/**
+ * @brief Find scratch registers and fill the scratchReg set
+ * @param childUsedReg registers used by the child at entrance
+ * @param scratchRegs any scratch register at the end of parent's code generation (updated by the function)
+ */
+static void findScratchRegisters (std::set<PhysicalReg> &childUsedReg,
+                                  std::set<PhysicalReg> &scratchRegs) {
+
+    // All free registers are candidates for use as scratch
+    std::set<PhysicalReg> parentFreeReg;
+    findFreeRegisters(parentFreeReg);
+
+    // Subtract child used registers from parent free registers
+    // so we can figure out what we can use as scratch
+    std::set_difference(parentFreeReg.begin(), parentFreeReg.end(),
+            childUsedReg.begin(), childUsedReg.end(),
+            std::inserter(scratchRegs, scratchRegs.end()));
+
+#ifdef DEBUG_REGISTERIZATION
+    //Debuging purposes
+    std::set<PhysicalReg>::const_iterator scratchRegIter;
+    for (scratchRegIter = scratchRegs.begin();
+            scratchRegIter != scratchRegs.end(); scratchRegIter++) {
+        DEBUG_ASSOCIATION_MERGE(ALOGD("%s is free for use as scratch\n",
+                physicalRegToString(*scratchRegIter)));
+    }
+#endif
+}
+
+/**
+ * @brief Find the registers to be moved and fill the regToRegMoves map
+ * @param childVRs the child usage of Virtual Registers at entrance
+ * @param currentVRs the current usage of Virtual Registers at end of code generation for the current BasicBlock
+ * @param childVRToPhysicalReg child association between VRs and physical registers at child code generation entrance
+ * @param currentVRToPhysicalReg child association between VRs and physical registers at current code generation exit
+ * @param regToRegMoves register to register moves to be done (updated by function)
+ */
+static bool findRegistersToMove (std::set<int> &childVRs,
+                          std::set<int> &currentVRs,
+                          std::map<int, PhysicalReg> &childVRToPhysicalReg,
+                          std::map<int, PhysicalReg> &currentVRToPhysicalReg,
+                          std::map<PhysicalReg, PhysicalReg> &regToRegMoves) {
+    // What VRs do we have in common? Lets make sure they are in the
+    // right physical registers
+
+    // Lets do an intersection to find the common VRs that are in physical
+    // registers.
+    std::set<int> setOperationResult;
+    setOperationResult.clear();
+    std::set_intersection(childVRs.begin(), childVRs.end(), currentVRs.begin(),
+            currentVRs.end(),
+            std::inserter(setOperationResult, setOperationResult.end()));
+
+    std::set<int>::const_iterator setOpIter;
+    for (setOpIter = setOperationResult.begin();
+            setOpIter != setOperationResult.end(); setOpIter++) {
+        int VR = *setOpIter;
+        PhysicalReg childReg = childVRToPhysicalReg[VR];
+        PhysicalReg currentReg = currentVRToPhysicalReg[VR];
+
+        // Check whether they are in the same physical register
+        if(childReg != currentReg) {
+            DEBUG_ASSOCIATION_MERGE(ALOGD("We are moving %s to %s",
+                    physicalRegToString(currentReg),
+                    physicalRegToString(childReg)));
+
+            if (regToRegMoves.find(currentReg) != regToRegMoves.end()) {
+                ALOGE("JIT_INFO: We are overwriting the reg to reg move from %s",
+                        physicalRegToString(currentReg));
+                return false;
+            }
+
+            // We want to generate a move that takes value from current physical
+            // register and puts it in the physical register child expects it
+            regToRegMoves[currentReg] = childReg;
+        }
+    }
+
+    //Report success
+    return true;
+}
+
+/**
+ * @brief Move registers following the regToReg map
+ * @param regToRegMoves the register to register move order
+ * @param scratchRegs the registers that are scratch and can be safely used for moving registers
+ * @param currentVRToPhysicalReg current VR to physical registers at end of current BasicBlock generation
+ * @return whether or not the move registers succeeds
+ */
+static bool moveRegisters (std::map<PhysicalReg, PhysicalReg> &regToRegMoves,
+                           std::set<PhysicalReg> &scratchRegs,
+                           std::map<int, PhysicalReg> &currentVRToPhysicalReg) {
+
+    std::map<PhysicalReg, PhysicalReg>::const_iterator regToRegIter;
+    //Go through each register to register request, inverse order
+    for (regToRegIter = regToRegMoves.begin();
+            regToRegIter != regToRegMoves.end(); regToRegIter++) {
+
+        std::vector<PhysicalReg> toBeMoved;
+        std::vector<PhysicalReg>::reverse_iterator moveIter;
+        bool cycleFound = false;
+        bool useMemoryForSwap = false;
+        PhysicalReg source = regToRegIter->first;
+        PhysicalReg dest = regToRegIter->second;
+
+        if (dest == PhysicalReg_Null)
+        {
+            continue;
+        }
+
+        if (regToRegMoves.count(source) > 1) {
+            ALOGE("JIT_INFO: We have the same physical register as source "
+                    "multiple times.");
+            SET_JIT_ERROR(kJitErrorBERegisterization);
+            return false;
+        }
+
+        DEBUG_ASSOCIATION_MERGE(ALOGD("We want to move from %s to %s",
+                physicalRegToString(source), physicalRegToString(dest)));
+
+        toBeMoved.push_back(source);
+        toBeMoved.push_back(dest);
+
+        // Now look through the rest of the moves to see if anyone
+        // is going to replace source register
+        std::map<PhysicalReg, PhysicalReg>::const_iterator regToRegFinder;
+        for (regToRegFinder = regToRegMoves.find(dest);
+                regToRegFinder != regToRegMoves.end(); regToRegFinder =
+                        regToRegMoves.find(regToRegFinder->second)) {
+
+
+            // If we already have this register in the toBeMoved list,
+            // it means we found a cycle. Instead of doing a find,
+            // keeping track of this information in a bit vector would be
+            // better. However, PhysicalReg enum contains invalid
+            // physical registers so it is hard to decide which ones we
+            // need to keep track of.
+            if (std::find(toBeMoved.begin(), toBeMoved.end(),
+                    regToRegFinder->second) != toBeMoved.end()) {
+                cycleFound = true;
+                // Save this because we will need to move value into it
+                toBeMoved.push_back(regToRegFinder->second);
+                break;
+            }
+
+            // Save this because we will need to move value into it
+            toBeMoved.push_back(regToRegFinder->second);
+        }
+
+        if (cycleFound) {
+            // If we find a cycle, the last value in the toBeMoved list
+            // is the register that caused cycle. Lets pop it off
+            // for now so we can use std::replace below but we will
+            // reinsert it
+            PhysicalReg cycleCause = toBeMoved.back();
+            toBeMoved.pop_back();
+
+            // Lets hope we have a scratch register to break the cycle
+            PhysicalReg scratch = getScratch(scratchRegs,
+                    getTypeOfRegister(source));
+
+            if (scratch != PhysicalReg_Null) {
+                // We found a scratch register
+                // If we had C->A B->C A->B
+                // Now we will have A->T C->A B->C T->B
+
+                // Which means that toBeMoved contained A B C and now we
+                // want it to have T B C A T
+
+                std::replace(toBeMoved.begin(), toBeMoved.end(), cycleCause,
+                        scratch);
+                toBeMoved.push_back(cycleCause);
+                toBeMoved.push_back(scratch);
+            } else {
+                useMemoryForSwap = true;
+            }
+        }
+
+        if (useMemoryForSwap) {
+            ALOGE("JIT_INFO: We have no scratch registers so we must use memory "
+                    "for swap");
+            SET_JIT_ERROR(kJitErrorBERegisterization);
+            return false;
+        }
+
+        // Now handle the actual reg to reg moves
+        PhysicalReg tmpIter = PhysicalReg_Null;
+        for (moveIter = toBeMoved.rbegin();
+                moveIter != toBeMoved.rend()
+                        ; moveIter++) {
+            PhysicalReg dest = tmpIter;
+            PhysicalReg source = *(moveIter);
+
+            //Remember source
+            tmpIter = source;
+
+            //If destination is null, next iteration
+            if (dest == PhysicalReg_Null) {
+                continue;
+            }
+
+            if ((getTypeOfRegister(source) != LowOpndRegType_gp
+                    && getTypeOfRegister(source) != LowOpndRegType_xmm)
+                    || (getTypeOfRegister(dest) != LowOpndRegType_gp
+                            && getTypeOfRegister(dest) != LowOpndRegType_xmm)) {
+                ALOGE("JIT_INFO: We cannot handle association table merge for "
+                        "anything but xmm and gp registers");
+                SET_JIT_ERROR(kJitErrorBERegisterization);
+                return false;
+            }
+
+            if (getTypeOfRegister(source) != getTypeOfRegister(dest)) {
+                // They are different types of registers and we currently
+                // cannot handle moves between different types
+                ALOGE("JIT_INFO: We cannot transfer from GP reg to XMM and "
+                        "vice versa");
+                SET_JIT_ERROR(kJitErrorBERegisterization);
+                return false;
+            }
+
+            DEBUG_ASSOCIATION_MERGE(ALOGD("Moving %s to %s",
+                    physicalRegToString(source),
+                    physicalRegToString(dest)));
+
+            OpndSize regSize = OpndSize_32;
+            if (source >= PhysicalReg_StartOfXmmMarker
+                    && source <= PhysicalReg_StartOfXmmMarker)
+                regSize = OpndSize_64;
+
+            // Do the actual reg to reg move
+            move_reg_to_reg_noalloc(regSize, source, true, dest,
+                    true);
+
+            // We have moved from reg to reg, but we must also update the
+            // entry in the compile table
+            std::map<int, PhysicalReg>::const_iterator currentVRToRegIter;
+            for (currentVRToRegIter = currentVRToPhysicalReg.begin();
+                    currentVRToRegIter != currentVRToPhysicalReg.end();
+                    currentVRToRegIter++) {
+                int VR = currentVRToRegIter->first;
+                int oldReg = currentVRToRegIter->second;
+
+                if (oldReg == source) {
+                    updatePhysicalRegForVR(VR, source, dest);
+                }
+            }
+
+            // Now remove entry from map
+            regToRegMoves[source] = PhysicalReg_Null;
+        }
+    }
+
+    // Since we update the physical registers for some of the VRs
+    // lets sync up register usage with compile table
+    syncAllRegs();
+
+    //Report success
+    return true;
+}
+
+/**
+ * @brief Move to child's physical registers requests
+ * @param associationsToUse the association table for the child at code generation entrance
+ * @param childVRs the child Virtual Register usage at start of the code generation
+ * @param currentVRs the current BasicBlock's Virtual Register usage at end of the code generation
+ * @param childVRToPhysicalReg the child map of Virtual Register to physical register at start of code generation
+ * @return whether or not the function succeeds
+ */
+static bool moveToChildPhysical (AssociationTable &associationsToUse,
+                                 std::set<int> &childVRs,
+                                 std::set<int> &currentVRs,
+                                 std::map<int, PhysicalReg> &childVRToPhysicalReg
+                                 ) {
+
+    //Ge the difference between child and current virtual register usage
+    std::set<int> setOperationResult;
+    std::set_difference(childVRs.begin(), childVRs.end(), currentVRs.begin(),
+            currentVRs.end(),
+            std::inserter(setOperationResult, setOperationResult.end()));
+
+    //Now iterate on the difference
+    std::set<int>::const_iterator setOpIter;
+    for (setOpIter = setOperationResult.begin();
+            setOpIter != setOperationResult.end(); setOpIter++) {
+        int VR = *setOpIter;
+        PhysicalReg targetReg = childVRToPhysicalReg[VR];
+
+        LowOpndRegType type = LowOpndRegType_invalid;
+        // Look through child's association entries to find the type of the VR
+        // so we can load it properly into the physical register
+        for (AssociationTable::const_iterator assocIter = associationsToUse.begin();
+                assocIter != associationsToUse.end(); assocIter++) {
+            if (assocIter->second.physicalReg == targetReg) {
+                type =
+                        static_cast<LowOpndRegType>(assocIter->second.physicalType
+                                & MASK_FOR_TYPE);
+            }
+        }
+
+        assert (type !=  LowOpndRegType_invalid);
+        OpndSize size = getRegSize(type);
+
+        // If we are loading wide VR, lets do a sanity check that high order
+        // bits are in memory
+        if (type == LowOpndRegType_xmm) {
+            // Lets look in the table for the high order VR. If we cannot
+            // find it, then lets assume it is in memory
+            if (searchMemTable(VR + 1) >= 0) {
+                // We only care about the 32-bit that is high order. It must be
+                // in memory before we load it in xmm
+                if (isInMemory(VR + 1, OpndSize_32) == false) {
+                    ALOGE("JIT_INFO: We are loading wide v%d but high order "
+                            "bits are not in memory!", VR);
+                    SET_JIT_ERROR(kJitErrorBERegisterization);
+                    return false;
+                }
+            }
+        }
+
+        DEBUG_ASSOCIATION_MERGE(ALOGD("Loading v%d to %s",
+                VR, physicalRegToString(targetReg)));
+
+        // Load VR into the target physical register
+        if (type == LowOpndRegType_ss) {
+            move_ss_mem_to_reg_noalloc(4 * VR, PhysicalReg_FP, true,
+                    MemoryAccess_VR, VR, targetReg, true);
+        } else {
+            get_virtual_reg_noalloc(VR, size, targetReg, true);
+        }
+
+        // TODO The following code needs to be refactored out of here. There
+        // should be a helper that can do this
+        int entry;
+        for (entry = 0; entry < num_compile_entries; entry++) {
+            if (isVirtualReg(compileTable[entry].physicalType)
+                    && compileTable[entry].regNum == VR) {
+                compileTable[entry].physicalReg = targetReg;
+                break;
+            }
+        }
+
+        // TODO We must handle case when we don't have entry in the compile table
+        if (entry == num_compile_entries) {
+            ALOGE("JIT_INFO: We are not handling properly when we don't have "
+                    "entry in compile table");
+            SET_JIT_ERROR(kJitErrorBERegisterization);
+            return false;
+        }
+    }
+
+    // We loaded some VRs into physical registers. Lets keep registers synced
+    syncAllRegs();
+
+    //Report success
+    return true;
+}
+
+bool AssociationTable::satisfyBBAssociations(BasicBlock_O1 * parent, BasicBlock_O1 * child) {
+    // To get here, it must be the case that this child's associations have
+    // already been finalized
+    assert (child->associationTable.hasBeenFinalized() == true);
+
+    /**
+     * This function merges associations, therefore it needs to know:
+     *   - The child's associations
+     *   - The parent's associations
+     *   - How both associations can be synchronized
+     */
+
+    //First part: get the information about the current associations and the child's
+    AssociationTable & associationsToUse = child->associationTable;
+    std::map<int, PhysicalReg> childVRToPhysicalReg, currentVRToPhysicalReg;
+    std::set<int> childVRs, currentVRs;
+
+    //Initialize helper maps in regards to parent and child associations
+    initAssociationHelperTables (associationsToUse, childVRToPhysicalReg, childVRs,
+                                                    currentVRToPhysicalReg, currentVRs);
+
+    // Look at child to see what physical registers it is using
+    std::set<PhysicalReg> childUsedReg;
+    associationsToUse.findUsedRegisters(childUsedReg);
+
+    //Handle VR difference to spill anything the child wants: frees registers
+    spillVRToMemory (parent, childUsedReg, currentVRs, childVRs);
+
+    //Third step: find scratch registers
+    std::set<PhysicalReg> scratchRegs;
+    findScratchRegisters (childUsedReg, scratchRegs);
+
+    //Fourth step: find the registers that should be moved
+    std::map<PhysicalReg, PhysicalReg> regToRegMoves;
+
+    if (findRegistersToMove (childVRs, currentVRs, childVRToPhysicalReg,
+                             currentVRToPhysicalReg, regToRegMoves) == false) {
+        //If findRegistersToMove fails, we bail too
+        return false;
+    }
+
+    //Fifth step: move registers to the correct physical register
+    if (moveRegisters (regToRegMoves, scratchRegs, currentVRToPhysicalReg) == false) {
+        //If moveRegisters fails, we bail too
+        return false;
+    }
+
+    //Sixth step: Do we have any VRs that child is expecting to be in physical
+    // register but currently is in memory?
+    if (moveToChildPhysical (associationsToUse, childVRs, currentVRs, childVRToPhysicalReg) == false) {
+        //If moveToChildPhysical fails, we bail too
+        return false;
+    }
+
+    // TODO What happens when child thinks that a VR is constant when we
+    // have no idea if it is?!?!
+
+    return true;
+}
+
+bool AssociationTable::handleSpillRequestsFromME(BasicBlock_O1 * bb) {
+    BitVector * spillRequests = bb->requestWriteBack;
+
+    // TODO All this spilling code should either be reused or we must
+    // have something like spillVirtualReg instead of explicitly calling
+    // spillLogicalReg and writeBackConstVR
+
+    // TODO Check whether VR is in memory before spilling
+
+    for (int entry = 0; entry < num_compile_entries; entry++) {
+        if (isVirtualReg(compileTable[entry].physicalType)
+                && compileTable[entry].physicalReg != PhysicalReg_Null
+                && dvmIsBitSet(spillRequests, compileTable[entry].regNum)) {
+            DEBUG_SPILLING(ALOGD("Spilling registerized v%d",
+                    compileTable[entry].regNum));
+            spillLogicalReg(entry, true);
+        }
+    }
+    syncAllRegs();
+
+    std::set<int> spilledConstVRs;
+
+    // Now handle spill requests for constants
+    for (int entry = 0; entry < num_const_vr; entry++) {
+        // TODO Probably a better idea is to call isVirtualRegConst instead
+        // of checking the flag in the table
+        if (constVRTable[entry].isConst
+                && dvmIsBitSet(spillRequests, constVRTable[entry].regNum)) {
+            DEBUG_SPILLING(ALOGD("Spilling CONSTANT v%d",
+                    constVRTable[entry].regNum));
+            // TODO We must be able to spill wide constants!
+            writeBackConstVR(constVRTable[entry].regNum, constVRTable[entry].value);
+            // TODO Since we know for sure this is the end of BB, lets unmark the
+            // constant so we don't pass its information via association table.
+            // Technically it should be job of writeBackConstVR to do that.
+            constVRTable[entry].isConst = false;
+            spilledConstVRs.insert(constVRTable[entry].regNum);
+        }
+    }
+
+    // The following stub checks for unhandled spilling of high order bits.
+    // Should be removed when we can handle spilling of wide constants.
+    for (int entry = 0; entry < num_const_vr; entry++) {
+        if (constVRTable[entry].isConst) {
+            if (spilledConstVRs.find(constVRTable[entry].regNum - 1)
+                    != spilledConstVRs.end()) {
+                ALOGE("JIT_INFO: We have spilled low order bits of constant "
+                        "v%d but not the higher order bits.",
+                        constVRTable[entry].regNum - 1);
+                SET_JIT_ERROR(kJitErrorBERegisterization);
+                return false;
+            }
+        }
+    }
+
+    return true;
+}
+
+/**
+ * @details First we handle any spill requests for the current basic block
+ * so we do not pass useless associations to child. Then if child already
+ * has an existing association table, we generate instructions to match
+ * our state to that. If the child does not, then we tell it what our
+ * current associations are. If the child is a chaining cell or exit block,
+ * we spill everything because those BBs are handled specially and are exit
+ * points.
+ */
+bool AssociationTable::createOrSyncTable(BasicBlock_O1 * bb, bool forFallthrough) {
+    // Before we pass association tables, lets handle spill requests from ME
+    // so we don't pass anything useless for associations
+    if (handleSpillRequestsFromME(bb) == false) {
+        return false;
+    }
+
+    //Get child depending on the forFallthrough boolean
+    BasicBlock_O1 * child = reinterpret_cast<BasicBlock_O1 *>(
+            forFallthrough ? bb->fallThrough : bb->taken);
+
+    //If there is a child
+    if (child != NULL) {
+
+        //If it is not a dalvik code and it's not the backward chaining cell
+        if (child->blockType != kDalvikByteCode
+                && child->blockType != kChainingCellBackwardBranch) {
+            //Simply free the registers
+            freeReg(true);
+        }
+        else {
+            if (child->associationTable.hasBeenFinalized() == false) {
+                // If the child's association table has not been finalized then we can
+                // update it now. However, if we don't have any MIRs in this BB,
+                // it means the compile table has not been updated and thus we can
+                // just copy associations
+                if (syncAssociationsWithCompileTable(
+                            child->associationTable) == false) {
+                    return false;
+                }
+            }
+            else {
+                //Otherwise, let's satisfy the associations for the child
+                if (satisfyBBAssociations(bb, child) == false) {
+                    return false;
+                }
+            }
+        }
+    }
+
+    //Report success
+    return true;
+}
diff --git a/vm/compiler/codegen/x86/RegisterizationBE.h b/vm/compiler/codegen/x86/RegisterizationBE.h
new file mode 100644
index 0000000..46efdb1
--- /dev/null
+++ b/vm/compiler/codegen/x86/RegisterizationBE.h
@@ -0,0 +1,269 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef REGISTERIZATIONBE_H_
+#define REGISTERIZATIONBE_H_
+
+#include <map>
+
+// Forward declarations
+struct BasicBlock_O1;
+struct compileTableEntry;
+struct ConstVRInfo;
+struct MemoryVRInfo;
+
+/**
+ * @brief Used to keep track of virtual registers and their various associations.
+ * @details Keeps track of compile table information associated with VR including
+ * the physical register, the in memory state of a VR, and the constantness of VR.
+ */
+class AssociationTable {
+public:
+    /**
+     * @brief Looks for the physical register associated with a VR
+     * @param vR virtual register to look up
+     * @return Returns physical register associated with VR.
+     * PhysicalReg_Null if association for the VR cannot be found or if
+     * really there is no association
+     */
+    PhysicalReg getVRAssociation(int vR);
+
+    /**
+     * @brief Looks through all associations and finds used physical registers
+     * @param outUsedRegisters is a set that is updated with the used physical
+     * registers
+     */
+    void findUsedRegisters(std::set<PhysicalReg> & outUsedRegisters);
+
+    /**
+     * @brief Once association table is been finalized, this can be called to
+     * find out if the virtual register was in memory.
+     * @param vR virtual register
+     * @return Returns whether or not VR was in memory
+     */
+    bool wasVRInMemory(int vR);
+
+    /**
+     * @brief Once association table is been finalized, this can be called to
+     * find out if the virtual register was a constant.
+     * @details For wide VRs, this should be called twice to find out if both
+     * low order bits and high order bits were constant.
+     * @param vR virtual register
+     * @return Returns whether or not virtual register was a constant.
+     */
+    bool wasVRConstant(int vR);
+
+    /**
+     * @brief Returns the 32 bit constant value associated with VR
+     * @pre wasVRConstant should return true
+     * @param vR virtual register
+     * @return Returns the constant value of virtual register.
+     */
+    int getVRConstValue(int vR);
+
+    /**
+     * @brief Updates association table given a compile entry from the compile table.
+     * @param compileEntry compilation entry
+     * @return Returns whether adding compile entry to associations was successful
+     */
+    bool associate(compileTableEntry & compileEntry);
+
+    /**
+     * @brief Updates association table given a memory VR information
+     * @param memVRInfo the memory virtual register information entry
+     * @return Returns whether adding compile entry to associations was successful
+     */
+    bool associate(MemoryVRInfo & memVRInfo);
+
+    /**
+     * @brief Updates association table given a constant VR information
+     * @param constVRInfo the constant virtual register entry
+     * @return Returns whether adding compile entry to associations was successful
+     */
+    bool associate(ConstVRInfo & constVRInfo);
+
+    /**
+     * @brief Used to determine whether the association table can be
+     * updated anymore.
+     * @return Returns whether the associations are final and cannot be updated.
+     */
+    bool hasBeenFinalized(void) const {
+        return isFinal;
+    }
+
+    /**
+     * @brief Used to tell association table that it cannot accept anymore
+     * updates.
+     */
+    void finalize();
+
+    /**
+     * @brief Clears the association table.
+     */
+    void clear(void);
+
+    /**
+     * @brief Used to copy another association table into current one
+     * @param source association table to read from
+     * @return whether the copy was successful
+     */
+    bool copy(AssociationTable & source);
+
+    /**
+     * Returns number of entries in association table
+     * @return size of association table
+     */
+    size_t size(void) const {
+        return this->associations.size();
+    }
+
+    /**
+     * @brief Prints the association table to a file separating entries with
+     * vertical bar.
+     * @param file File to print to.
+     */
+    void printToDot(FILE * file);
+
+    /**
+     * @typedef Random access const iterator. This does not modify
+     * structure it is iterating.
+     */
+    typedef std::map<int, compileTableEntry>::const_iterator const_iterator;
+
+    /**
+     * @typedef Random access iterator. This may modify structure it
+     * is iterating.
+     */
+    typedef std::map<int, compileTableEntry>::iterator iterator;
+
+    /**
+     * @brief Returns an iterator pointing to the first association.
+     * @return iterator to beginning
+     */
+    iterator begin() {
+        return associations.begin();
+    }
+
+    /**
+     * @brief Returns a const iterator pointing to the first association.
+     * @return iterator to beginning
+     */
+    const_iterator begin() const {
+        return associations.begin();
+    }
+
+    /**
+     * @brief Returns an iterator referring to the past-the-end association.
+     * @return iterator past end
+     */
+    iterator end() {
+        return associations.end();
+    }
+
+    /**
+     * @brief Returns a const iterator referring to the past-the-end
+     * association.
+     * @return iterator past end
+     */
+    const_iterator end() const {
+        return associations.end();
+    }
+
+    AssociationTable(void); /**< @brief Constructor */
+
+    ~AssociationTable(void); /**< @brief Destructor */
+
+    //Static functions of the RegisterizationBE class
+
+    /**
+     * @brief Updates a given association table using the current state of the
+     * compile table.
+     * @param associationsToUpdate Association table to update.
+     * @return Returns whether the association table was updated successfully.
+     */
+    static bool syncAssociationsWithCompileTable(AssociationTable & associationsToUpdate);
+
+    /**
+     * @brief Updates the current state of the compile table to all VR entries
+     * in the association table
+     * @param associationsToUse Association table to use for compile table updated.
+     * @return Returns whether the compile table update was successful.
+     */
+    static bool syncCompileTableWithAssociations(AssociationTable & associationsToUse);
+
+    /**
+     * @brief Creates association table for child or generates instructions to
+     * match it.
+     * @param bb Parent basic block
+     * @param forFallthrough Flag on whether should update fallthrough child. Else
+     * we update taken child.
+     * @return Whether the sync was successful.
+     */
+    static bool createOrSyncTable(BasicBlock_O1 * bb, bool forFallthrough = true);
+
+    /**
+     * @brief Generates instructions to match current state of parent basic block
+     * to the association table state of child.
+     * @param parent Parent basic block.
+     * @param child Child basic block.
+     * @return Returns whether the state of parent successfully matches state of
+     * child.
+     */
+    static bool satisfyBBAssociations(BasicBlock_O1 * parent, BasicBlock_O1 * child);
+
+    /**
+     * @brief Spills virtual registers marked for spilling by the middle end.
+     * @param bb Basic block whose spill requests we need to handle.
+     * @return Returns whether we successfully handled spill requests.
+     */
+    static bool handleSpillRequestsFromME(BasicBlock_O1 * bb);
+
+private:
+    /**
+     * @brief Map for every VR to its corresponding compile table entry
+     * when association occurred.
+     */
+    std::map<int, compileTableEntry> associations;
+
+    /**
+     * @brief Map for every VR to its state in memory when the association
+     * occurred.
+     */
+    std::map<int, MemoryVRInfo> inMemoryTracker;
+
+    /**
+     * @typedef Iterator for use with inMemoryTracker.
+     */
+    typedef std::map<int, MemoryVRInfo>::const_iterator inMemoryTrackerConstIterator;
+
+    /**
+     * @brief Map for every VR to its constant value (if it had any) when the
+     * association occurred.
+     */
+    std::map<int, ConstVRInfo> constTracker;
+
+    /**
+     * @typedef Iterator for use with constTracker
+     */
+    typedef std::map<int, ConstVRInfo>::const_iterator constantTrackerConstIterator;
+
+    /**
+     * @brief Keeps track whether association table has been finalized.
+     */
+    bool isFinal;
+};
+
+#endif /* REGISTERIZATIONBE_H_ */
diff --git a/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp b/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp
index b519595..a43980e 100644
--- a/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp
+++ b/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp
@@ -613,3 +613,45 @@ char* decoder_disassemble_instr(char* stream, char* strbuf, unsigned int len)
     DisassembleInstToBuf(decInst, strbuf, len);
     return (stream + numBytes);
 }
+
+/**
+ * @brief Physical register char* counterparts
+ */
+static const char * PhysicalRegString[] = { "eax", "ebx", "ecx", "edx", "edi",
+        "esi", "esp", "ebp", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5",
+        "xmm6", "xmm7", "st0", "st1", "st2", "st3", "st4", "st5", "st6", "st7",
+        "null"
+        };
+
+/**
+ * @brief Scratch register char* counterparts
+ */
+static const char * ScratchRegString[] = { "scratch1", "scratch2", "scratch3",
+        "scratch4", "scratch5", "scratch6", "scratch7", "scratch8", "scratch9",
+        "scratch10" };
+
+/**
+ * @brief Glue register char* counterparts
+ */
+static const char * GlueRegString[] = { "glue-dvmdex", "glue" };
+
+extern "C" ENCODER_DECLARE_EXPORT
+/**
+ * @brief Transform a physical register into its char* counterpart
+ * @param reg the PhysicalReg we want to have a char* equivalent
+ * @return the register reg in char* form
+ */
+const char * physicalRegToString(PhysicalReg reg)
+{
+    if (reg < PhysicalReg_Null) {
+        return PhysicalRegString[reg];
+    } else if (reg >= PhysicalReg_SCRATCH_1 && reg <= PhysicalReg_SCRATCH_10) {
+        return ScratchRegString[reg - PhysicalReg_SCRATCH_1];
+    } else if (reg >= PhysicalReg_GLUE_DVMDEX && reg <= PhysicalReg_GLUE) {
+        return GlueRegString[reg - PhysicalReg_GLUE_DVMDEX];
+    } else if (reg == PhysicalReg_Null) {
+        return "null";
+    } else {
+        return "corrupted-data";
+    }
+}
diff --git a/vm/compiler/codegen/x86/libenc/enc_wrapper.h b/vm/compiler/codegen/x86/libenc/enc_wrapper.h
index eca38ea..47d6117 100644
--- a/vm/compiler/codegen/x86/libenc/enc_wrapper.h
+++ b/vm/compiler/codegen/x86/libenc/enc_wrapper.h
@@ -21,12 +21,27 @@
 
 extern bool dump_x86_inst;
 typedef enum PhysicalReg {
-  PhysicalReg_EAX = 0, PhysicalReg_EBX, PhysicalReg_ECX, PhysicalReg_EDX,
+  // Currently initializing StartOfGPMarker to be 0 in order to match
+  // register index in Reg_No. However, ideally PhysicalReg_Null should
+  // be 0 and the rest moved over.
+  PhysicalReg_StartOfGPMarker = 0,
+  PhysicalReg_EAX = PhysicalReg_StartOfGPMarker,
+  PhysicalReg_EBX, PhysicalReg_ECX, PhysicalReg_EDX,
   PhysicalReg_EDI, PhysicalReg_ESI, PhysicalReg_ESP, PhysicalReg_EBP,
-  PhysicalReg_XMM0, PhysicalReg_XMM1, PhysicalReg_XMM2, PhysicalReg_XMM3,
+  PhysicalReg_EndOfGPMarker = PhysicalReg_EBP,
+
+  PhysicalReg_StartOfXmmMarker,
+  PhysicalReg_XMM0 = PhysicalReg_StartOfXmmMarker,
+  PhysicalReg_XMM1, PhysicalReg_XMM2, PhysicalReg_XMM3,
   PhysicalReg_XMM4, PhysicalReg_XMM5, PhysicalReg_XMM6, PhysicalReg_XMM7,
-  PhysicalReg_ST0,  PhysicalReg_ST1, PhysicalReg_ST2,  PhysicalReg_ST3,
-  PhysicalReg_ST4, PhysicalReg_ST5, PhysicalReg_ST6, PhysicalReg_ST7,
+  PhysicalReg_EndOfXmmMarker = PhysicalReg_XMM7,
+
+  PhysicalReg_StartOfX87Marker,
+  PhysicalReg_ST0 = PhysicalReg_StartOfX87Marker,  PhysicalReg_ST1,
+  PhysicalReg_ST2, PhysicalReg_ST3, PhysicalReg_ST4, PhysicalReg_ST5,
+  PhysicalReg_ST6, PhysicalReg_ST7,
+  PhysicalReg_EndOfX87Marker = PhysicalReg_ST7,
+
   PhysicalReg_Null,
   //used as scratch logical register in NCG O1
   //should not overlap with regular logical register, start from 100
@@ -157,7 +172,8 @@ typedef enum LowOpndRegType {
   LowOpndRegType_temp = 16,
   LowOpndRegType_hard = 32,   //NCG O1
   LowOpndRegType_virtual = 64, //used by NCG O1
-  LowOpndRegType_glue = 128
+  LowOpndRegType_glue = 128,
+  LowOpndRegType_invalid = 256, //Invalid type, used by NCG O1
 } LowOpndRegType;
 
 //if inline, separte enc_wrapper.cpp into two files, one of them is .inl
@@ -249,6 +265,9 @@ ENCODER_DECLARE_EXPORT char * encoder_moves_reg_to_reg(OpndSize size,
 ENCODER_DECLARE_EXPORT char * encoder_nops(unsigned numBytes, char * stream);
 ENCODER_DECLARE_EXPORT int decodeThenPrint(char* stream_start);
 ENCODER_DECLARE_EXPORT char* decoder_disassemble_instr(char* stream, char* strbuf, unsigned int len);
+
+//Provide a char* equivalent to a PhysicalReg type
+ENCODER_DECLARE_EXPORT const char * physicalRegToString(PhysicalReg reg);
 #ifdef __cplusplus
 }
 #endif
diff --git a/vm/mterp/out/InterpAsm-x86.S b/vm/mterp/out/InterpAsm-x86.S
index 90c63ca..5404b16 100644
--- a/vm/mterp/out/InterpAsm-x86.S
+++ b/vm/mterp/out/InterpAsm-x86.S
@@ -16756,18 +16756,21 @@ dvmJitToInterpTraceSelect:
     .global dvmJitToInterpBackwardBranch
 dvmJitToInterpBackwardBranch:
     movl        0(%esp), %eax          # get return address
-    movl        (%eax), rPC              # get first argument (target rPC)
+    movl        (%eax), rPC            # get first argument (target rPC)
     lea         4(%esp), %esp
     movl        rSELF, %ecx
     cmpb        $0, offThread_breakFlags(%ecx)
     jne         1f
-    movl        8(%eax), %ebx          #loop header address
+    movl        8(%eax), %ebx          # loop header address
+    movl        16(%eax), %edx         # preloop address
+    SPILL_TMP1(%edx) //save preloop
     movl        %ebx, offThread_inJitCodeCache(%ecx)  # set inJitCodeCache flag
     movl        4(%eax), %eax           # patch address
     movl        %eax, OUT_ARG1(%esp)    # %ebx live through dvmJitGetTraceAddrThread, address waiting to be patched
     movl        %ebx, OUT_ARG0(%esp)    # first argument, target address
     call        dvmJitChain
-    jmp         *%eax                   #to native address
+    UNSPILL_TMP1(%eax)                  # jump to preloop
+    jmp         *%eax                   # native address
 1:
     movl        $0, offThread_inJitCodeCache(%ecx)  # set inJitCodeCache flag
     jmp         toInterpreter
diff --git a/vm/mterp/x86/footer.S b/vm/mterp/x86/footer.S
index 9d9f5d8..8718364 100644
--- a/vm/mterp/x86/footer.S
+++ b/vm/mterp/x86/footer.S
@@ -278,18 +278,21 @@ dvmJitToInterpTraceSelect:
     .global dvmJitToInterpBackwardBranch
 dvmJitToInterpBackwardBranch:
     movl        0(%esp), %eax          # get return address
-    movl        (%eax), rPC              # get first argument (target rPC)
+    movl        (%eax), rPC            # get first argument (target rPC)
     lea         4(%esp), %esp
     movl        rSELF, %ecx
     cmpb        $$0, offThread_breakFlags(%ecx)
     jne         1f
-    movl        8(%eax), %ebx          #loop header address
+    movl        8(%eax), %ebx          # loop header address
+    movl        16(%eax), %edx         # preloop address
+    SPILL_TMP1(%edx) //save preloop
     movl        %ebx, offThread_inJitCodeCache(%ecx)  # set inJitCodeCache flag
     movl        4(%eax), %eax           # patch address
     movl        %eax, OUT_ARG1(%esp)    # %ebx live through dvmJitGetTraceAddrThread, address waiting to be patched
     movl        %ebx, OUT_ARG0(%esp)    # first argument, target address
     call        dvmJitChain
-    jmp         *%eax                   #to native address
+    UNSPILL_TMP1(%eax)                  # jump to preloop
+    jmp         *%eax                   # native address
 1:
     movl        $$0, offThread_inJitCodeCache(%ecx)  # set inJitCodeCache flag
     jmp         toInterpreter
-- 
1.7.4.1

