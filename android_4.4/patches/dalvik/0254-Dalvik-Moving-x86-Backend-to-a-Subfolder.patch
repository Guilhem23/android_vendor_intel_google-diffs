From d4f33be9f9bc7bb0fac3df541ac80bf884f27081 Mon Sep 17 00:00:00 2001
From: Jean Christophe Beyler <jean.christophe.beyler@intel.com>
Date: Tue, 9 Jul 2013 10:40:17 -0700
Subject: Dalvik: Moving x86 Backend to a Subfolder

BZ: 122335

Moving the current X86 backend to a subfolder

Category: device-enablement
Domain: AOSP-Dalvik-Compiler-CG; AOSP-Dalvik-Compiler-ME; AOSP-Dalvik-Runtime
Origin: internal
Upstream-Candidate: no, needs rework

Change-Id: Ib190e7adc8caf619c6e139dd8cd511cbc00b25db
Orig-MCG-Change-Id: I61971fd117788f7dff717d0bffec825886c563c2
Signed-off-by: Jean Christophe Beyler <jean.christophe.beyler@intel.com>
Signed-off-by: Qiming Shi <qiming.shi@intel.com>
Signed-off-by: Serguei Katkov <serguei.i.katkov@intel.com>
---
 vm/Dvm.mk                                          |   60 +-
 vm/Init.cpp                                        |    2 +-
 vm/compiler/Compiler.cpp                           |    4 +-
 vm/compiler/Frontend.cpp                           |    2 +-
 vm/compiler/StackExtension.h                       |    2 +-
 vm/compiler/codegen/x86/AnalysisO1.cpp             | 6310 -------------------
 vm/compiler/codegen/x86/AnalysisO1.h               |  445 --
 vm/compiler/codegen/x86/BytecodeVisitor.cpp        | 6421 --------------------
 vm/compiler/codegen/x86/CodegenErrors.cpp          |  259 -
 vm/compiler/codegen/x86/CodegenErrors.h            |  161 -
 vm/compiler/codegen/x86/CompilationUnit.cpp        |   45 -
 vm/compiler/codegen/x86/CompilationUnit.h          |   53 -
 vm/compiler/codegen/x86/CompileTable.cpp           |   99 -
 vm/compiler/codegen/x86/CompileTable.h             |  502 --
 vm/compiler/codegen/x86/ExceptionHandling.cpp      |  149 -
 vm/compiler/codegen/x86/ExceptionHandling.h        |  107 -
 vm/compiler/codegen/x86/InstructionGeneration.cpp  |  344 --
 vm/compiler/codegen/x86/InstructionGeneration.h    |   97 -
 vm/compiler/codegen/x86/Lower.cpp                  | 1255 ----
 vm/compiler/codegen/x86/Lower.h                    | 1453 -----
 vm/compiler/codegen/x86/LowerAlu.cpp               | 2557 --------
 vm/compiler/codegen/x86/LowerConst.cpp             |  240 -
 vm/compiler/codegen/x86/LowerGetPut.cpp            | 1873 ------
 vm/compiler/codegen/x86/LowerHelper.cpp            | 4277 -------------
 vm/compiler/codegen/x86/LowerInvoke.cpp            | 2090 -------
 vm/compiler/codegen/x86/LowerJump.cpp              | 2208 -------
 vm/compiler/codegen/x86/LowerMove.cpp              |  182 -
 vm/compiler/codegen/x86/LowerObject.cpp            | 1035 ----
 vm/compiler/codegen/x86/LowerReturn.cpp            |   98 -
 vm/compiler/codegen/x86/NcgAot.cpp                 |  226 -
 vm/compiler/codegen/x86/NcgAot.h                   |   48 -
 vm/compiler/codegen/x86/NcgHelper.cpp              |  104 -
 vm/compiler/codegen/x86/NcgHelper.h                |   45 -
 vm/compiler/codegen/x86/Profile.cpp                |  492 --
 vm/compiler/codegen/x86/Profile.h                  |   61 -
 vm/compiler/codegen/x86/RegisterizationBE.cpp      | 1569 -----
 vm/compiler/codegen/x86/RegisterizationBE.h        |  271 -
 vm/compiler/codegen/x86/Scheduler.cpp              | 2073 -------
 vm/compiler/codegen/x86/Scheduler.h                |  171 -
 vm/compiler/codegen/x86/Singleton.h                |   53 -
 vm/compiler/codegen/x86/StackExtensionX86.cpp      |   58 -
 vm/compiler/codegen/x86/StackExtensionX86.h        |   68 -
 vm/compiler/codegen/x86/Translator.h               |   28 -
 vm/compiler/codegen/x86/doxygen-config-x86-jit     |    9 -
 vm/compiler/codegen/x86/libenc/Android.mk          |   65 -
 vm/compiler/codegen/x86/libenc/README.txt          |   26 -
 vm/compiler/codegen/x86/libenc/dec_base.cpp        |  542 --
 vm/compiler/codegen/x86/libenc/dec_base.h          |  136 -
 vm/compiler/codegen/x86/libenc/enc_base.cpp        | 1137 ----
 vm/compiler/codegen/x86/libenc/enc_base.h          |  745 ---
 vm/compiler/codegen/x86/libenc/enc_defs.h          |  786 ---
 vm/compiler/codegen/x86/libenc/enc_defs_ext.h      |  346 --
 vm/compiler/codegen/x86/libenc/enc_prvt.h          |  382 --
 vm/compiler/codegen/x86/libenc/enc_tabl.cpp        | 1969 ------
 vm/compiler/codegen/x86/libenc/enc_wrapper.cpp     |  757 ---
 vm/compiler/codegen/x86/libenc/enc_wrapper.h       |  279 -
 vm/compiler/codegen/x86/libenc/encoder.cpp         |  155 -
 vm/compiler/codegen/x86/libenc/encoder.h           |  717 ---
 vm/compiler/codegen/x86/libenc/encoder.inl         |  863 ---
 vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp     | 6310 +++++++++++++++++++
 vm/compiler/codegen/x86/lightcg/AnalysisO1.h       |  445 ++
 .../codegen/x86/lightcg/BytecodeVisitor.cpp        | 6421 ++++++++++++++++++++
 vm/compiler/codegen/x86/lightcg/CodegenErrors.cpp  |  259 +
 vm/compiler/codegen/x86/lightcg/CodegenErrors.h    |  161 +
 .../codegen/x86/lightcg/CodegenInterface.cpp       | 2580 ++++++++
 .../codegen/x86/lightcg/CompilationUnit.cpp        |   45 +
 vm/compiler/codegen/x86/lightcg/CompilationUnit.h  |   53 +
 vm/compiler/codegen/x86/lightcg/CompileTable.cpp   |   99 +
 vm/compiler/codegen/x86/lightcg/CompileTable.h     |  502 ++
 .../codegen/x86/lightcg/ExceptionHandling.cpp      |  149 +
 .../codegen/x86/lightcg/ExceptionHandling.h        |  107 +
 .../codegen/x86/lightcg/InstructionGeneration.cpp  |  344 ++
 .../codegen/x86/lightcg/InstructionGeneration.h    |   97 +
 vm/compiler/codegen/x86/lightcg/Lower.cpp          | 1255 ++++
 vm/compiler/codegen/x86/lightcg/Lower.h            | 1453 +++++
 vm/compiler/codegen/x86/lightcg/LowerAlu.cpp       | 2557 ++++++++
 vm/compiler/codegen/x86/lightcg/LowerConst.cpp     |  240 +
 vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp    | 1873 ++++++
 vm/compiler/codegen/x86/lightcg/LowerHelper.cpp    | 4277 +++++++++++++
 vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp    | 2090 +++++++
 vm/compiler/codegen/x86/lightcg/LowerJump.cpp      | 2208 +++++++
 vm/compiler/codegen/x86/lightcg/LowerMove.cpp      |  182 +
 vm/compiler/codegen/x86/lightcg/LowerObject.cpp    | 1035 ++++
 vm/compiler/codegen/x86/lightcg/LowerReturn.cpp    |   98 +
 vm/compiler/codegen/x86/lightcg/NcgAot.cpp         |  226 +
 vm/compiler/codegen/x86/lightcg/NcgAot.h           |   48 +
 vm/compiler/codegen/x86/lightcg/NcgHelper.cpp      |  104 +
 vm/compiler/codegen/x86/lightcg/NcgHelper.h        |   45 +
 vm/compiler/codegen/x86/lightcg/Profile.cpp        |  492 ++
 vm/compiler/codegen/x86/lightcg/Profile.h          |   61 +
 .../codegen/x86/lightcg/RegisterizationBE.cpp      | 1569 +++++
 .../codegen/x86/lightcg/RegisterizationBE.h        |  271 +
 vm/compiler/codegen/x86/lightcg/Scheduler.cpp      | 2073 +++++++
 vm/compiler/codegen/x86/lightcg/Scheduler.h        |  171 +
 vm/compiler/codegen/x86/lightcg/Singleton.h        |   53 +
 .../codegen/x86/lightcg/StackExtensionX86.cpp      |   58 +
 .../codegen/x86/lightcg/StackExtensionX86.h        |   68 +
 vm/compiler/codegen/x86/lightcg/Translator.h       |   28 +
 .../codegen/x86/lightcg/doxygen-config-x86-jit     |    9 +
 vm/compiler/codegen/x86/lightcg/libenc/Android.mk  |   65 +
 vm/compiler/codegen/x86/lightcg/libenc/README.txt  |   26 +
 .../codegen/x86/lightcg/libenc/dec_base.cpp        |  542 ++
 vm/compiler/codegen/x86/lightcg/libenc/dec_base.h  |  136 +
 .../codegen/x86/lightcg/libenc/enc_base.cpp        | 1137 ++++
 vm/compiler/codegen/x86/lightcg/libenc/enc_base.h  |  745 +++
 vm/compiler/codegen/x86/lightcg/libenc/enc_defs.h  |  786 +++
 .../codegen/x86/lightcg/libenc/enc_defs_ext.h      |  346 ++
 vm/compiler/codegen/x86/lightcg/libenc/enc_prvt.h  |  382 ++
 .../codegen/x86/lightcg/libenc/enc_tabl.cpp        | 1969 ++++++
 .../codegen/x86/lightcg/libenc/enc_wrapper.cpp     |  757 +++
 .../codegen/x86/lightcg/libenc/enc_wrapper.h       |  279 +
 vm/compiler/codegen/x86/lightcg/libenc/encoder.cpp |  155 +
 vm/compiler/codegen/x86/lightcg/libenc/encoder.h   |  717 +++
 vm/compiler/codegen/x86/lightcg/libenc/encoder.inl |  863 +++
 vm/compiler/codegen/x86/pcg/Android.mk             |    6 +-
 115 files changed, 49061 insertions(+), 46477 deletions(-)
 delete mode 100644 vm/compiler/codegen/x86/AnalysisO1.cpp
 delete mode 100644 vm/compiler/codegen/x86/AnalysisO1.h
 delete mode 100644 vm/compiler/codegen/x86/BytecodeVisitor.cpp
 delete mode 100644 vm/compiler/codegen/x86/CodegenErrors.cpp
 delete mode 100644 vm/compiler/codegen/x86/CodegenErrors.h
 delete mode 100644 vm/compiler/codegen/x86/CompilationUnit.cpp
 delete mode 100644 vm/compiler/codegen/x86/CompilationUnit.h
 delete mode 100644 vm/compiler/codegen/x86/CompileTable.cpp
 delete mode 100644 vm/compiler/codegen/x86/CompileTable.h
 delete mode 100644 vm/compiler/codegen/x86/ExceptionHandling.cpp
 delete mode 100644 vm/compiler/codegen/x86/ExceptionHandling.h
 delete mode 100644 vm/compiler/codegen/x86/InstructionGeneration.cpp
 delete mode 100644 vm/compiler/codegen/x86/InstructionGeneration.h
 delete mode 100644 vm/compiler/codegen/x86/Lower.cpp
 delete mode 100644 vm/compiler/codegen/x86/Lower.h
 delete mode 100644 vm/compiler/codegen/x86/LowerAlu.cpp
 delete mode 100644 vm/compiler/codegen/x86/LowerConst.cpp
 delete mode 100644 vm/compiler/codegen/x86/LowerGetPut.cpp
 delete mode 100644 vm/compiler/codegen/x86/LowerHelper.cpp
 delete mode 100644 vm/compiler/codegen/x86/LowerInvoke.cpp
 delete mode 100644 vm/compiler/codegen/x86/LowerJump.cpp
 delete mode 100644 vm/compiler/codegen/x86/LowerMove.cpp
 delete mode 100644 vm/compiler/codegen/x86/LowerObject.cpp
 delete mode 100644 vm/compiler/codegen/x86/LowerReturn.cpp
 delete mode 100644 vm/compiler/codegen/x86/NcgAot.cpp
 delete mode 100644 vm/compiler/codegen/x86/NcgAot.h
 delete mode 100644 vm/compiler/codegen/x86/NcgHelper.cpp
 delete mode 100644 vm/compiler/codegen/x86/NcgHelper.h
 delete mode 100644 vm/compiler/codegen/x86/Profile.cpp
 delete mode 100644 vm/compiler/codegen/x86/Profile.h
 delete mode 100644 vm/compiler/codegen/x86/RegisterizationBE.cpp
 delete mode 100644 vm/compiler/codegen/x86/RegisterizationBE.h
 delete mode 100644 vm/compiler/codegen/x86/Scheduler.cpp
 delete mode 100644 vm/compiler/codegen/x86/Scheduler.h
 delete mode 100644 vm/compiler/codegen/x86/Singleton.h
 delete mode 100644 vm/compiler/codegen/x86/StackExtensionX86.cpp
 delete mode 100644 vm/compiler/codegen/x86/StackExtensionX86.h
 delete mode 100644 vm/compiler/codegen/x86/Translator.h
 delete mode 100644 vm/compiler/codegen/x86/doxygen-config-x86-jit
 delete mode 100644 vm/compiler/codegen/x86/libenc/Android.mk
 delete mode 100644 vm/compiler/codegen/x86/libenc/README.txt
 delete mode 100644 vm/compiler/codegen/x86/libenc/dec_base.cpp
 delete mode 100644 vm/compiler/codegen/x86/libenc/dec_base.h
 delete mode 100644 vm/compiler/codegen/x86/libenc/enc_base.cpp
 delete mode 100644 vm/compiler/codegen/x86/libenc/enc_base.h
 delete mode 100644 vm/compiler/codegen/x86/libenc/enc_defs.h
 delete mode 100644 vm/compiler/codegen/x86/libenc/enc_defs_ext.h
 delete mode 100644 vm/compiler/codegen/x86/libenc/enc_prvt.h
 delete mode 100644 vm/compiler/codegen/x86/libenc/enc_tabl.cpp
 delete mode 100644 vm/compiler/codegen/x86/libenc/enc_wrapper.cpp
 delete mode 100644 vm/compiler/codegen/x86/libenc/enc_wrapper.h
 delete mode 100644 vm/compiler/codegen/x86/libenc/encoder.cpp
 delete mode 100644 vm/compiler/codegen/x86/libenc/encoder.h
 delete mode 100644 vm/compiler/codegen/x86/libenc/encoder.inl
 create mode 100644 vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/AnalysisO1.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/BytecodeVisitor.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/CodegenErrors.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/CodegenErrors.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/CodegenInterface.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/CompilationUnit.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/CompilationUnit.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/CompileTable.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/CompileTable.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/ExceptionHandling.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/ExceptionHandling.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/InstructionGeneration.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/InstructionGeneration.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/Lower.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/Lower.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/LowerAlu.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/LowerConst.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/LowerHelper.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/LowerJump.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/LowerMove.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/LowerObject.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/LowerReturn.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/NcgAot.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/NcgAot.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/NcgHelper.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/NcgHelper.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/Profile.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/Profile.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/RegisterizationBE.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/RegisterizationBE.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/Scheduler.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/Scheduler.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/Singleton.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/StackExtensionX86.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/StackExtensionX86.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/Translator.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/doxygen-config-x86-jit
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/Android.mk
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/README.txt
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/dec_base.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/dec_base.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/enc_base.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/enc_base.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/enc_defs.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/enc_defs_ext.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/enc_prvt.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/enc_tabl.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/encoder.cpp
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/encoder.h
 create mode 100644 vm/compiler/codegen/x86/lightcg/libenc/encoder.inl

diff --git a/vm/Dvm.mk b/vm/Dvm.mk
index 4ef96dc..807b35c 100644
--- a/vm/Dvm.mk
+++ b/vm/Dvm.mk
@@ -343,31 +343,31 @@ ifeq ($(dvm_arch),x86)
     ifeq ($(WITH_JIT),true)
       LOCAL_CFLAGS += -DARCH_IA32
       LOCAL_SRC_FILES += \
-	      compiler/codegen/$(dvm_arch_variant)/LowerAlu.cpp \
-              compiler/codegen/$(dvm_arch_variant)/LowerConst.cpp \
-              compiler/codegen/$(dvm_arch_variant)/LowerMove.cpp \
-              compiler/codegen/$(dvm_arch_variant)/Lower.cpp \
-              compiler/codegen/$(dvm_arch_variant)/LowerHelper.cpp \
-              compiler/codegen/$(dvm_arch_variant)/LowerJump.cpp \
-              compiler/codegen/$(dvm_arch_variant)/LowerObject.cpp \
-              compiler/codegen/$(dvm_arch_variant)/AnalysisO1.cpp \
-              compiler/codegen/$(dvm_arch_variant)/CompilationUnit.cpp \
-              compiler/codegen/$(dvm_arch_variant)/BytecodeVisitor.cpp \
-              compiler/codegen/$(dvm_arch_variant)/NcgAot.cpp \
-              compiler/codegen/$(dvm_arch_variant)/CodegenInterface.cpp \
-              compiler/codegen/$(dvm_arch_variant)/LowerInvoke.cpp \
-              compiler/codegen/$(dvm_arch_variant)/LowerReturn.cpp \
-              compiler/codegen/$(dvm_arch_variant)/NcgHelper.cpp \
-              compiler/codegen/$(dvm_arch_variant)/LowerGetPut.cpp \
-              compiler/codegen/$(dvm_arch_variant)/Scheduler.cpp \
-              compiler/codegen/$(dvm_arch_variant)/InstructionGeneration.cpp \
-              compiler/codegen/$(dvm_arch_variant)/ExceptionHandling.cpp \
-              compiler/codegen/$(dvm_arch_variant)/CodegenErrors.cpp \
-              compiler/codegen/$(dvm_arch_variant)/RegisterizationBE.cpp \
-              compiler/codegen/$(dvm_arch_variant)/StackExtensionX86.cpp \
-              compiler/codegen/$(dvm_arch_variant)/CompileTable.cpp \
+	      compiler/codegen/$(dvm_arch_variant)/lightcg/LowerAlu.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/LowerConst.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/LowerMove.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/Lower.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/LowerHelper.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/LowerJump.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/LowerObject.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/AnalysisO1.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/CompilationUnit.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/BytecodeVisitor.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/NcgAot.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/CodegenInterface.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/LowerInvoke.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/LowerReturn.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/NcgHelper.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/LowerGetPut.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/Scheduler.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/InstructionGeneration.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/ExceptionHandling.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/CodegenErrors.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/RegisterizationBE.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/StackExtensionX86.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/CompileTable.cpp \
+              compiler/codegen/$(dvm_arch_variant)/lightcg/Profile.cpp \
               compiler/codegen/$(dvm_arch_variant)/x86Specific.cpp \
-              compiler/codegen/$(dvm_arch_variant)/Profile.cpp \
               compiler/LoopOpt.cpp \
               compiler/Checks.cpp \
               compiler/LoopRegisterUsage.cpp \
@@ -382,14 +382,16 @@ ifeq ($(dvm_arch),x86)
        # need apache harmony x86 encoder/decoder
        LOCAL_C_INCLUDES += \
               dalvik/vm/compiler \
+              dalvik/vm/compiler/codegen \
               dalvik/vm/compiler/codegen/x86 \
-              dalvik/vm/compiler/codegen/x86/libenc \
+              dalvik/vm/compiler/codegen/x86/lightcg \
+              dalvik/vm/compiler/codegen/x86/lightcg/libenc \
 
        LOCAL_SRC_FILES += \
-              compiler/codegen/x86/libenc/enc_base.cpp \
-              compiler/codegen/x86/libenc/dec_base.cpp \
-              compiler/codegen/x86/libenc/enc_wrapper.cpp \
-              compiler/codegen/x86/libenc/enc_tabl.cpp
+              compiler/codegen/x86/lightcg/libenc/enc_base.cpp \
+              compiler/codegen/x86/lightcg/libenc/dec_base.cpp \
+              compiler/codegen/x86/lightcg/libenc/enc_wrapper.cpp \
+              compiler/codegen/x86/lightcg/libenc/enc_tabl.cpp
     endif
   endif
 endif
diff --git a/vm/Init.cpp b/vm/Init.cpp
index 8af9de7..58e059b 100644
--- a/vm/Init.cpp
+++ b/vm/Init.cpp
@@ -40,7 +40,7 @@
 #include "JniConstants.h"
 
 #ifdef ARCH_IA32
-#include "compiler/codegen/x86/Lower.h"
+#include "compiler/codegen/x86/lightcg/Lower.h"
 #include "compiler/codegen/x86/x86Specific.h"
 #include "compiler/CompilerUtility.h"
 #include "compiler/LoopOpt.h"
diff --git a/vm/compiler/Compiler.cpp b/vm/compiler/Compiler.cpp
index 0670649..7c42fbb 100644
--- a/vm/compiler/Compiler.cpp
+++ b/vm/compiler/Compiler.cpp
@@ -22,8 +22,8 @@
 #include "interp/Jit.h"
 #include "CompilerInternals.h"
 #ifdef ARCH_IA32
-#include "codegen/x86/Translator.h"
-#include "codegen/x86/Lower.h"
+#include "codegen/x86/lightcg/Translator.h"
+#include "codegen/x86/lightcg/Lower.h"
 #endif
 
 extern "C" void dvmCompilerTemplateStart(void);
diff --git a/vm/compiler/Frontend.cpp b/vm/compiler/Frontend.cpp
index ba936cb..cd87bfb 100644
--- a/vm/compiler/Frontend.cpp
+++ b/vm/compiler/Frontend.cpp
@@ -33,7 +33,7 @@ extern int opcodeJit[kNumPackedOpcodes];
 #endif
 
 #ifdef ARCH_IA32
-#include "codegen/x86/CompilationUnit.h"
+#include "codegen/x86/lightcg/CompilationUnit.h"
 #endif
 
 #if defined(VTUNE_DALVIK)
diff --git a/vm/compiler/StackExtension.h b/vm/compiler/StackExtension.h
index 519b267..177142f 100644
--- a/vm/compiler/StackExtension.h
+++ b/vm/compiler/StackExtension.h
@@ -35,7 +35,7 @@ int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx);
 
 #ifdef ARCH_IA32
 
-#include "vm/compiler/codegen/x86/StackExtensionX86.h"
+#include "codegen/x86/lightcg/StackExtensionX86.h"
 
 #else
 
diff --git a/vm/compiler/codegen/x86/AnalysisO1.cpp b/vm/compiler/codegen/x86/AnalysisO1.cpp
deleted file mode 100644
index bbd420f..0000000
--- a/vm/compiler/codegen/x86/AnalysisO1.cpp
+++ /dev/null
@@ -1,6310 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file AnalysisO1.cpp
-  \brief This file implements register allocator, constant folding
-*/
-#include "CompilationUnit.h"
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "interp/InterpState.h"
-#include "interp/InterpDefs.h"
-#include "libdex/Leb128.h"
-#include "../../RegisterizationME.h"
-#include "Scheduler.h"
-#include "Singleton.h"
-#include <set>
-
-/* compilation flags to turn on debug printout */
-//#define DEBUG_COMPILE_TABLE
-//#define DEBUG_ALLOC_CONSTRAINT
-//#define DEBUG_REGALLOC
-//#define DEBUG_REG_USED
-//#define DEBUG_REFCOUNT
-//#define DEBUG_REACHING_DEF2
-//#define DEBUG_REACHING_DEF
-//#define DEBUG_LIVE_RANGE
-//#define DEBUG_ENDOFBB
-//#define DEBUG_CONST
-//#define DEBUG_XFER_POINTS
-//#define DEBUG_DSE
-//#define DEBUG_CFG
-//#define DEBUG_GLOBALTYPE
-//#define DEBUG_STATE
-//#define DEBUG_COMPILE_TABLE
-//#define DEBUG_VIRTUAL_INFO
-//#define DEBUG_MERGE_ENTRY
-//#define DEBUG_INVALIDATE
-#define DEBUG_MEMORYVR(X)
-#define DEBUG_MOVE_OPT(X)
-#define DEBUG_SPILL(X)
-
-#include "AnalysisO1.h"
-#include "CompileTable.h"
-
-void dumpCompileTable();
-
-/**
- * @brief Check whether a type is a virtual register type
- * @param type the logical type of the register
- * @return whether type is a virtual register or not
- */
-bool isVirtualReg(int type) {
-    return ((type & LowOpndRegType_virtual) != 0);
-}
-
-/**
- * @brief Check whether the logical type represents a temporary
- * @param type the logical type of the register
- * @param regNum the register number
- * @return Returns true if we are looking at a temporary, scratch, or hardcoded register.
- */
-bool isTemporary (int type, int regNum)
-{
-    return (isVirtualReg (type) == false);
-}
-
-/**
- * @brief Given a physical register it determines if it is scratch type.
- * @param reg The physical register.
- * @return Returns whether the register is scratch
- */
-static bool isScratchReg (int reg)
-{
-    //A register is a scratch register if its physical type is one of the scratch ones
-    bool isScratch = (reg >= static_cast<int> (PhysicalReg_SCRATCH_1)
-            && reg <= static_cast<int> (PhysicalReg_SCRATCH_10));
-
-    return isScratch;
-}
-
-/** convert type defined in lowering module to type defined in register allocator
-    in lowering module <type, isPhysical>
-    in register allocator: LowOpndRegType_hard LowOpndRegType_virtual LowOpndRegType_scratch
-*/
-int convertType(int type, int reg, bool isPhysical) {
-    int newType = type;
-    if(isPhysical) newType |= LowOpndRegType_hard;
-    if(isVirtualReg(type)) newType |= LowOpndRegType_virtual;
-    else {
-        /* reg number for a VR can exceed PhysicalReg_SCRATCH_1 */
-        if(isScratchReg (reg) == true)
-        {
-            newType |= LowOpndRegType_scratch;
-        }
-    }
-    return newType;
-}
-
-/** return the size of a variable
- */
-OpndSize getRegSize(int type) {
-    if((type & MASK_FOR_TYPE) == LowOpndRegType_xmm) return OpndSize_64;
-    if((type & MASK_FOR_TYPE) == LowOpndRegType_fs) return OpndSize_64;
-    /* for type _gp, _fs_s, _ss */
-    return OpndSize_32;
-}
-
-/*
-   Overlapping cases between two variables A and B
-   layout for A,B   isAPartiallyOverlapB  isBPartiallyOverlapA
-   1> |__|  |____|         OVERLAP_ALIGN        OVERLAP_B_COVER_A
-      |__|  |____|
-   2> |____|           OVERLAP_B_IS_LOW_OF_A    OVERLAP_B_COVER_LOW_OF_A
-        |__|
-   3> |____|           OVERLAP_B_IS_HIGH_OF_A   OVERLAP_B_COVER_HIGH_OF_A
-      |__|
-   4> |____|      OVERLAP_LOW_OF_A_IS_HIGH_OF_B OVERLAP_B_COVER_LOW_OF_A
-         |____|
-   5>    |____|   OVERLAP_HIGH_OF_A_IS_LOW_OF_B OVERLAP_B_COVER_HIGH_OF_A
-      |____|
-   6>   |__|           OVERLAP_A_IS_LOW_OF_B    OVERLAP_B_COVER_A
-      |____|
-   7> |__|             OVERLAP_A_IS_HIGH_OF_B   OVERLAP_B_COVER_A
-      |____|
-*/
-/** determine the overlapping between variable B and A
-*/
-OverlapCase getBPartiallyOverlapA(int regB, LowOpndRegType tB, int regA, LowOpndRegType tA) {
-    if(getRegSize(tA) == getRegSize(tB) && regA == regB) return OVERLAP_B_COVER_A;
-    if(getRegSize(tA) == OpndSize_64 && getRegSize(tB) == OpndSize_32 && regA == regB) return OVERLAP_B_COVER_LOW_OF_A;
-    if(getRegSize(tA) == OpndSize_64 && getRegSize(tB) == OpndSize_32 && regB == regA + 1) return OVERLAP_B_COVER_HIGH_OF_A;
-    if(getRegSize(tA) == OpndSize_32 && getRegSize(tB) == OpndSize_64 && (regA == regB || regA == regB+1)) return OVERLAP_B_COVER_A;
-    if(getRegSize(tB) == OpndSize_64 && getRegSize(tA) == OpndSize_64 && regA == regB+1) return OVERLAP_B_COVER_LOW_OF_A;
-    if(getRegSize(tB) == OpndSize_64 && getRegSize(tA) == OpndSize_64 && regB == regA+1) return OVERLAP_B_COVER_HIGH_OF_A;
-    return OVERLAP_NO;
-}
-
-/** determine overlapping between variable A and B
-*/
-OverlapCase getAPartiallyOverlapB(int regA, LowOpndRegType tA, int regB, LowOpndRegType tB) {
-    if(getRegSize(tA) == getRegSize(tB) && regA == regB) return OVERLAP_ALIGN;
-    if(getRegSize(tA) == OpndSize_64 && getRegSize(tB) == OpndSize_32 && regA == regB)
-        return OVERLAP_B_IS_LOW_OF_A;
-    if(getRegSize(tA) == OpndSize_64 && getRegSize(tB) == OpndSize_32 && regB == regA+1)
-        return OVERLAP_B_IS_HIGH_OF_A;
-    if(getRegSize(tB) == OpndSize_64 && getRegSize(tA) == OpndSize_64 && regA == regB+1)
-        return OVERLAP_LOW_OF_A_IS_HIGH_OF_B;
-    if(getRegSize(tB) == OpndSize_64 && getRegSize(tA) == OpndSize_64 && regB == regA+1)
-        return OVERLAP_HIGH_OF_A_IS_LOW_OF_B;
-    if(getRegSize(tA) == OpndSize_32 && getRegSize(tB) == OpndSize_64 && regA == regB)
-        return OVERLAP_A_IS_LOW_OF_B;
-    if(getRegSize(tA) == OpndSize_32 && getRegSize(tB) == OpndSize_64 && regA == regB+1)
-        return OVERLAP_A_IS_HIGH_OF_B;
-    return OVERLAP_NO;
-}
-
-/** determine whether variable A fully covers B
- */
-bool isAFullyCoverB(int regA, LowOpndRegType tA, int regB, LowOpndRegType tB) {
-    if(getRegSize(tB) == OpndSize_32) return true;
-    if(getRegSize(tA) == getRegSize(tB) && regA == regB) return true;
-    return false;
-}
-
-/*
-   RegAccessType accessType
-   1> DefOrUse.accessType
-      can only be D(VR), L(low part of VR), H(high part of VR), N(none)
-      for def, it means which part of the VR is live
-      for use, it means which part of the VR comes from the def
-   2> VirtualRegInfo.accessType
-      for currentInfo, it can only be a combination of U & D
-      for entries in infoBasicBlock, it can be a combination of U & D|L|H
-*/
-
-/*
-   Key data structures used:
-   1> BasicBlock_O1
-      VirtualRegInfo infoBasicBlock[]
-      DefUsePair* defUseTable
-      XferPoint xferPoints[]
-   2> MemoryVRInfo memVRTable[]
-      LiveRange* ranges
-   3> CompileTableEntry compileTable[]
-   4> VirtualRegInfo
-      DefOrUse reachingDefs[3]
-   5> DefUsePair, LiveRange
-*/
-
-//! one entry for each variable used
-
-//! a variable can be virtual register, or a temporary (can be hard-coded)
-CompileTable compileTable;
-//! tables to save the states of register allocation
-regAllocStateEntry2 stateTable2_1[NUM_MEM_VR_ENTRY];
-regAllocStateEntry2 stateTable2_2[NUM_MEM_VR_ENTRY];
-regAllocStateEntry2 stateTable2_3[NUM_MEM_VR_ENTRY];
-regAllocStateEntry2 stateTable2_4[NUM_MEM_VR_ENTRY];
-
-//! array of TempRegInfo to store temporaries accessed by a single bytecode
-TempRegInfo infoByteCodeTemp[MAX_TEMP_REG_PER_BYTECODE];
-int num_temp_regs_per_bytecode;
-//! array of MemoryVRInfo to store whether a VR is in memory
-MemoryVRInfo memVRTable[NUM_MEM_VR_ENTRY];
-int num_memory_vr;
-
-CompilationUnit* currentUnit = NULL;
-
-//! the current basic block
-BasicBlock_O1* currentBB = NULL;
-
-/**
- * @brief Information for each physical register
- * @details Initialized during code generation
- */
-RegisterInfo allRegs[PhysicalReg_Last+1];
-
-//! this array says whether a spill location is used (0 means not used, 1 means used)
-int spillIndexUsed[MAX_SPILL_JIT_IA];
-
-int inGetVR_num = -1;
-int inGetVR_type;
-
-///////////////////////////////////////////////////////////////////////////////
-// FORWARD FUNCTION DECLARATION
-void addExceptionHandler(s4 tmp);
-
-int createCFG(Method* method);
-void dumpVirtualInfoOfBasicBlock(BasicBlock_O1* bb);
-void setTypeOfVR();
-void dumpVirtualInfoOfMethod();
-int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb);
-
-//used in collectInfoOfBasicBlock: getVirtualRegInfo
-int mergeEntry2 (BasicBlock_O1* bb, VirtualRegInfo &currentInfo);
-int sortAllocConstraint(RegAllocConstraint* allocConstraints,
-                        RegAllocConstraint* allocConstraintsSorted, bool fromHighToLow);
-
-//Updates compile table with information about virtual register usage
-static void insertFromVirtualInfo (const VirtualRegInfo &regInfo);
-
-//Updates compile table with information about temporary usage
-static void insertFromTempInfo (const TempRegInfo &tempRegInfo);
-
-static int updateXferPoints (BasicBlock_O1 *bb);
-static int updateLiveTable (BasicBlock_O1 *bb);
-static void handleStartOfBBXferPoints (BasicBlock_O1 *bb);
-void printDefUseTable();
-bool isFirstOfHandler(BasicBlock_O1* bb);
-
-//used in mergeEntry2
-//following functions will not update global data structure
-RegAccessType mergeAccess2(RegAccessType A, RegAccessType B, OverlapCase isBPartiallyOverlapA);
-RegAccessType updateAccess1(RegAccessType A, OverlapCase isAPartiallyOverlapB); //will not update global data structure
-RegAccessType updateAccess2(RegAccessType C1, RegAccessType C2);
-RegAccessType updateAccess3(RegAccessType C, RegAccessType B);
-
-void updateDefUseTable (VirtualRegInfo &currentInfo);
-static int updateReachingDefA (VirtualRegInfo &currentInfo, int indexToA, OverlapCase isBPartiallyOverlapA);
-static int updateReachingDefB1 (VirtualRegInfo &currentInfo, VirtualRegInfo &tmpInfo, int indexToA);
-static int updateReachingDefB2 (VirtualRegInfo &currentInfo, VirtualRegInfo &tmpInfo);
-void updateReachingDefB3 (VirtualRegInfo &currentInfo);
-
-RegAccessType insertAUse(DefUsePair* ptr, int offsetPC, int regNum, LowOpndRegType physicalType);
-DefUsePair* insertADef(BasicBlock_O1 *bb, int offsetPC, int regNum, LowOpndRegType pType, RegAccessType rType);
-RegAccessType insertDefUsePair (VirtualRegInfo &currentInfo, int reachingDefIndex);
-
-//used in updateXferPoints
-int fakeUsageAtEndOfBB (BasicBlock_O1* bb, int vR, int physicalAndLogicalType);
-static int insertLoadXfer(int offset, int regNum, LowOpndRegType pType);
-int searchMemTable(int regNum);
-static int mergeLiveRange(int tableIndex, int rangeStart, int rangeEnd);
-//used in updateLiveTable
-RegAccessType setAccessTypeOfUse(OverlapCase isDefPartiallyOverlapUse, RegAccessType reachingDefLive);
-DefUsePair* searchDefUseTable(int offsetPC, int regNum, LowOpndRegType pType);
-void insertAccess(int tableIndex, LiveRange* startP, int rangeStart);
-
-/**
- * @brief Checks whether the opcode can branch or switch
- * @param opcode the Dalvik mnemonic
- * @return true if opcode can branch or switch
- */
-static inline bool isCurrentByteCodeJump(Opcode opcode)
-{
-    //Get the opcode flags
-    int flags = dvmCompilerGetOpcodeFlags (opcode);
-
-    //Check whether it can branch or switch
-    return (flags & (kInstrCanBranch | kInstrCanSwitch)) != 0;
-}
-
-/* this function is called before code generation of basic blocks
-   initialize data structure allRegs, which stores information for each physical register,
-   whether it is used, when it was last freed, whether it is callee-saved */
-void initializeAllRegs() {
-    //Initialize entire array
-    memset (allRegs, PhysicalReg_Null, sizeof (allRegs));
-
-    int k;
-    for(k = PhysicalReg_EAX; k <= PhysicalReg_EBP; k++) {
-        allRegs[k].physicalReg = (PhysicalReg) k;
-        if(k == PhysicalReg_EDI || k == PhysicalReg_ESP || k == PhysicalReg_EBP)
-            allRegs[k].isUsed = true;
-        else {
-            allRegs[k].isUsed = false;
-            allRegs[k].freeTimeStamp = -1;
-        }
-        if(k == PhysicalReg_EBX || k == PhysicalReg_EBP || k == PhysicalReg_ESI || k == PhysicalReg_EDI)
-            allRegs[k].isCalleeSaved = true;
-        else
-            allRegs[k].isCalleeSaved = false;
-    }
-    for(k = PhysicalReg_XMM0; k <= PhysicalReg_XMM7; k++) {
-        allRegs[k].physicalReg = (PhysicalReg) k;
-        allRegs[k].isUsed = false;
-        allRegs[k].freeTimeStamp = -1;
-        allRegs[k].isCalleeSaved = false;
-    }
-}
-
-/** sync up allRegs (isUsed & freeTimeStamp) with compileTable
-    global data: RegisterInfo allRegs[PhysicalReg_Null]
-    update allRegs[EAX to XMM7] except EDI,ESP,EBP
-    update RegisterInfo.isUsed & RegisterInfo.freeTimeStamp
-        if the physical register was used and is not used now
-*/
-void syncAllRegs() {
-    int k, k2;
-    for(k = PhysicalReg_EAX; k <= PhysicalReg_XMM7; k++) {
-        if(k == PhysicalReg_EDI || k == PhysicalReg_ESP || k == PhysicalReg_EBP)
-            continue;
-        //check whether the physical register is used by any logical register
-        bool stillUsed = false;
-        for(k2 = 0; k2 < compileTable.size (); k2++) {
-            if(compileTable[k2].physicalReg == k) {
-                stillUsed = true;
-                break;
-            }
-        }
-        if(stillUsed && !allRegs[k].isUsed) {
-            allRegs[k].isUsed = true;
-        }
-        if(!stillUsed && allRegs[k].isUsed) {
-            allRegs[k].isUsed = false;
-        }
-    }
-    return;
-}
-
-/**
- * @brief Looks through all physical registers and determines what is used
- * @param outFreeRegisters is a set that is updated with the unused physical registers
- * @param includeGPs Whether or not to include general purpose registers
- * @param includeXMMs Whether or not to include XMM registers
- */
-void findFreeRegisters (std::set<PhysicalReg> &outFreeRegisters, bool includeGPs, bool includeXMMs)
-{
-    if (includeGPs == true)
-    {
-        // Go through all GPs
-        for (int reg = PhysicalReg_StartOfGPMarker; reg <= PhysicalReg_EndOfGPMarker; reg++)
-        {
-            //If it is not used, then we can add it to the list of free registers
-            if (allRegs[reg].isUsed == false)
-            {
-                outFreeRegisters.insert (static_cast<PhysicalReg> (reg));
-            }
-        }
-    }
-
-    if (includeXMMs == true)
-    {
-        // Go through all XMMs
-        for (int reg = PhysicalReg_StartOfXmmMarker; reg <= PhysicalReg_EndOfXmmMarker; reg++)
-        {
-            if (allRegs[reg].isUsed == false)
-            {
-                outFreeRegisters.insert (static_cast<PhysicalReg> (reg));
-            }
-        }
-    }
-}
-
-/**
- * @brief Given a list of scratch register candidates and a register type,
- * it returns a scratch register of that type
- * @param scratchCandidates registers that can be used for scratch
- * @param type xmm or gp
- * @return physical register which can be used as scratch
- */
-PhysicalReg getScratch(const std::set<PhysicalReg> &scratchCandidates, LowOpndRegType type) {
-    if (type != LowOpndRegType_gp && type != LowOpndRegType_xmm) {
-        return PhysicalReg_Null;
-    }
-
-    int start = (
-            type == LowOpndRegType_gp ?
-                    PhysicalReg_StartOfGPMarker : PhysicalReg_StartOfXmmMarker);
-    int end = (
-            type == LowOpndRegType_gp ?
-                    PhysicalReg_EndOfGPMarker : PhysicalReg_EndOfXmmMarker);
-
-    PhysicalReg candidate = PhysicalReg_Null;
-    std::set<PhysicalReg>::const_iterator iter;
-    for (iter = scratchCandidates.begin(); iter != scratchCandidates.end();
-            iter++) {
-        PhysicalReg scratch = *iter;
-        if (scratch >= start && scratch <= end) {
-            candidate = scratch;
-            break;
-        }
-    }
-
-    return candidate;
-}
-
-/**
- * @brief Given a physical register, it returns its physical type
- * @param reg physical register to check
- * @return Returns LowOpndRegType_gp if register general purpose.
- * Returns LowOpndRegType_xmm if register is XMM. Returns
- * LowOpndRegType_fs is register is stack register for x87.
- * Otherwise, returns LowOpndRegType_invalid for anything else.
- */
-LowOpndRegType getTypeOfRegister(PhysicalReg reg) {
-    if (reg >= PhysicalReg_StartOfGPMarker && reg <= PhysicalReg_EndOfGPMarker)
-        return LowOpndRegType_gp;
-    else if (reg >= PhysicalReg_StartOfXmmMarker
-            && reg <= PhysicalReg_EndOfXmmMarker)
-        return LowOpndRegType_xmm;
-    else if (reg >= PhysicalReg_StartOfX87Marker
-            && reg <= PhysicalReg_EndOfX87Marker)
-        return LowOpndRegType_fs;
-
-    return LowOpndRegType_invalid;
-}
-
-/**
- * @brief Synchronize the spillIndexUsed table with the informatino from compileTable
- * @return 0 on success, -1 on error
- */
-static int updateSpillIndexUsed(void) {
-    int k;
-
-    /* First: for each entry, set the index used to 0, in order to reset it */
-    for(k = 0; k <= MAX_SPILL_JIT_IA - 1; k++) {
-        spillIndexUsed[k] = 0;
-    }
-
-    /* Second: go through each compile entry */
-    for(k = 0; k < compileTable.size (); k++) {
-        /** If it is a virtual register, we skip it, we don't need a special spill region for VRs */
-        if(isVirtualReg(compileTable[k].physicalType)) {
-            continue;
-        }
-
-        /** It might have been spilled, let's see if we have the spill_loc_index filled */
-        if(compileTable[k].spill_loc_index >= 0) {
-
-            /* We do have it, but is the index correct (and will fit in our table) */
-            if(compileTable[k].spill_loc_index > 4 * (MAX_SPILL_JIT_IA - 1)) {
-                ALOGI("JIT_INFO: spill_loc_index is wrong for entry %d: %d\n",
-                      k, compileTable[k].spill_loc_index);
-                SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                return -1;
-            }
-
-            /* The spill index is correct, we use the higher bits as a hash for it and set it to 1 */
-            spillIndexUsed[compileTable[k].spill_loc_index >> 2] = 1;
-        }
-    }
-    return 0;
-}
-
-/**
- * @brief Inserts high VR entries into compile table.
- * @details Looks through compile table and for all wide VRs it finds, it ensures that an entry exists for the
- * high part of the VR. Namely, if it finds that v1 is wide, it ensures that there is entry in compile table for v2.
- * @return Returns true if it succeed insert high VR for all wide VRs.
- */
-static bool addHighOfWideVRToCompileTable (void)
-{
-    for (int entry = 0; entry < compileTable.size (); entry++)
-    {
-        //We only need to do the correction for wide VRs
-        if (compileTable[entry].isVirtualReg () == false)
-        {
-            continue;
-        }
-
-        //If we have a 64-bit VR, we should also insert the high bits into the compile table
-        if (compileTable[entry].getSize () == OpndSize_64)
-        {
-            //The high bits of a wide virtual register are available in following register number
-            //For example: wide v0 occupies space on stack for v0 and v1.
-            int highVR = compileTable[entry].getRegisterNumber () + 1;
-
-            int indexHigh = searchCompileTable (LowOpndRegType_virtual | LowOpndRegType_gp, highVR);
-
-            //If we don't have an entry for the high bits, we insert it now.
-            if (indexHigh < 0)
-            {
-                //Create a new entry for the high VR. Since we just care about 32-bits we make it GP type
-                CompileTableEntry newEntry (highVR, LowOpndRegType_virtual | LowOpndRegType_gp);
-
-                //We now copy it to the table
-                compileTable.insert (newEntry);
-            }
-        }
-    }
-
-    //If we make it here we were successful at inserting the high VR
-    return true;
-}
-
-void MemoryVRInfo::reset (void)
-{
-    //Set this entry to invalid register
-    regNum = -1;
-
-    //Now set its state to being in memory because it's certainly not in a
-    //location we are keeping track of
-    inMemory = true;
-
-    //Set null check and bound check to false
-    nullCheckDone = false;
-    boundCheck.checkDone = false;
-
-    //Use invalid register for index VR for bound check
-    boundCheck.indexVR = -1;
-
-    //Zero out information about live ranges
-    num_ranges = 0;
-    ranges = 0;
-
-    //Initalize all delay free requests
-    for (int c = 0; c < VRDELAY_COUNT; c++)
-    {
-        delayFreeCounters[c] = 0;
-    }
-}
-
-/**
- * @brief Updates the table that keeps the in memory state of VRs to contain new VR.
- * @param vR The virtual register
- * @param inMemory The initial inMemory state
- * @return Returns whether the VR's addition was successful.
- */
-bool addToMemVRTable (int vR, bool inMemory)
-{
-    //We want to keep track of index in memory table
-    int index = 0;
-
-    //Search mem table for the virtual register we are interested in adding
-    for (; index < num_memory_vr; index++)
-    {
-        if (memVRTable[index].regNum == vR)
-        {
-            break;
-        }
-    }
-
-    //If the index is not the size of table, then it means we have found an entry
-    if (index != num_memory_vr)
-    {
-        //We already have entry for this VR so simply update its memory state
-        memVRTable[index].setInMemoryState (inMemory);
-    }
-    else
-    {
-        //Let's make sure we won't overflow the table if we make this insertion
-        if (num_memory_vr >= NUM_MEM_VR_ENTRY)
-        {
-            ALOGI("JIT_INFO: Index %d exceeds size of memVRTable during addToMemVRTable\n", num_memory_vr);
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return false;
-        }
-
-        //Create the new entry
-        MemoryVRInfo memInfo (vR);
-
-        //Now set the in memory state for our new entry
-        memInfo.setInMemoryState (inMemory);
-
-        //We access the index at end of the table
-        index = num_memory_vr;
-
-        //We are adding an entry so increment the number of entries
-        num_memory_vr++;
-
-        //Finally add it to the table
-        memVRTable[index] = memInfo;
-    }
-
-    //If we made it here everything went well
-    return true;
-}
-
-/**
- * @brief Initializes the in memory tracking table for virtual registers.
- * @param bb The basic block which we are looking at for initialization.
- * @return Returns true if all initialization completed successfully. Otherwise it returns false.
- */
-static bool initializeMemVRTable (BasicBlock_O1 *bb)
-{
-    //Reset number of entries in table
-    num_memory_vr = 0;
-
-    //Now walk through compile entries so we can track in memory state for every VR.
-    //For wide VRs, the compile table must guarantee an entry for both the low and high VR.
-    for(int entry = 0; entry < compileTable.size (); entry++)
-    {
-        //We can skip any entry that is not a virtual register
-        if (compileTable[entry].isVirtualReg () == false)
-        {
-            continue;
-        }
-
-        //Make it easier to refer to the VR number
-        int vR = compileTable[entry].getRegisterNumber ();
-
-        //Determine if parent said that the VR was in memory
-        bool setToInMemory = bb->associationTable.wasVRInMemory (vR);
-
-        //Now let's add it to the table
-        bool result = addToMemVRTable (vR, setToInMemory);
-
-        if (result == false)
-        {
-            //We simply pass along failure since addToMemVRTable has already set error code
-            return false;
-        }
-
-        DEBUG_MEMORYVR(ALOGD("Initializing state of v%d %sin memory",
-                vR, (setToInMemory ? "" : "NOT ")));
-    }
-
-    //If we made it here we were successful
-    return true;
-}
-
-/**
- * @brief Initializes constant table.
- * @param bb The basic block which we are looking at for initialization.
- * @return Returns true if all initialization completed successfully. Otherwise it returns false.
- */
-static bool initializeConstVRTable (BasicBlock_O1 *bb)
-{
-    //Reset the number of entries to zero since we are initializing
-    num_const_vr = 0;
-
-    //Now walk through compile entries so we can track the constantness for every VR.
-    //For wide VRs, the compile table must guarantee an entry for both the low and high VR.
-    for (int entry = 0; entry < compileTable.size (); entry++)
-    {
-        //We can skip any entry that is not a virtual register
-        if (compileTable[entry].isVirtualReg () == false)
-        {
-            continue;
-        }
-
-        //Make it easier to refer to the VR number
-        int vR = compileTable[entry].getRegisterNumber ();
-
-        //Determine if the virtual register was constant
-        if (bb->associationTable.wasVRConstant (vR) == true)
-        {
-            //We make space for two value because setVRToConst might access high bits
-            int constValue[2];
-
-            //It was constant so let's get its value
-            constValue[0] = bb->associationTable.getVRConstValue (vR);
-
-            //Paranoid so we set high bits to 0
-            constValue[1] = 0;
-
-            //Set it to constant
-            bool result = setVRToConst (vR, OpndSize_32, constValue);
-
-            //We bail out if we failed to set VR to constant. If setVRToConst failed, it already
-            //set an error message.
-            if (result == false)
-            {
-                return false;
-            }
-        }
-    }
-
-    return true;
-}
-
-/**
- * @brief Used to add registerized virtual registers as defined at entry into basic block
- */
-static bool initializeRegisterizeDefs (BasicBlock_O1* bb)
-{
-    //Walk through the compile entries
-    for (CompileTable::const_iterator it = compileTable.begin (); it != compileTable.end (); it++)
-    {
-        const CompileTableEntry &compileEntry = *it;
-
-        //Did we find a virtual register that is in physical register? If yes then we must add a def for it
-        if (compileEntry.isVirtualReg () == true && compileEntry.inPhysicalRegister() == true)
-        {
-            //Add a def for this virtual register coming into the BB
-            VirtualRegInfo regDefineInfo;
-            regDefineInfo.regNum = compileEntry.getRegisterNumber ();
-            regDefineInfo.physicalType = compileEntry.getPhysicalType ();
-            regDefineInfo.accessType = REGACCESS_D;
-            offsetPC = PC_FOR_START_OF_BB;
-
-            //Now add it to the defuse tables
-            int res = mergeEntry2 (bb, regDefineInfo);
-
-            if (res < 0)
-            {
-                //We just pass along the error information
-                return false;
-            }
-        }
-    }
-
-    //If we made it here everything went okay
-    return true;
-}
-
-/**
- * @brief Initializes entries in the compile table at start of BB.
- * @details It ensures that it updates compile table based on the given associations from its parent.
- * @param bb The basic block whose virtual register state should be initialized.
- * @return Returns true if all initialization completed successfully. Otherwise it returns false.
- */
-static bool initializeRegStateOfBB (BasicBlock_O1* bb)
-{
-    assert (bb != 0);
-
-    //First we clear the compile table
-    compileTable.clear ();
-
-    //Load associations into compile table
-    if (AssociationTable::syncCompileTableWithAssociations (bb->associationTable) == false)
-    {
-        return false;
-    }
-
-    //Since we loaded associations into compile table now we may have virtual registers that are
-    //in physical registers. Thus we set up defines of all those VRs at entry to the BB.
-    if (initializeRegisterizeDefs (bb) == false)
-    {
-        //Just pass along error information
-        return false;
-    }
-
-    //Collect information about the virtual registers in current BB
-    collectInfoOfBasicBlock (bb);
-
-    //Update compileTable with virtual register information from current BB
-    for (std::vector<VirtualRegInfo>::const_iterator vrInfoIter = bb->infoBasicBlock.begin ();
-            vrInfoIter != bb->infoBasicBlock.end (); vrInfoIter++)
-    {
-        insertFromVirtualInfo (*vrInfoIter);
-    }
-
-    //For each virtual register, we insert fake usage at end of basic block to keep it live
-    for (CompileTable::const_iterator tableIter = compileTable.begin (); tableIter != compileTable.end (); tableIter++)
-    {
-        //Get the compile entry
-        const CompileTableEntry &compileEntry = *tableIter;
-
-        if (compileEntry.isVirtualReg ())
-        {
-            //Calling fakeUsageAtEndOfBB uses offsetPC so we switch it to point to end of basic block
-            offsetPC = PC_FOR_END_OF_BB;
-
-            //Update the defUseTable by assuming a fake usage at end of basic block
-            fakeUsageAtEndOfBB (bb, compileEntry.getRegisterNumber (), compileEntry.getLogicalAndPhysicalTypes ());
-        }
-    }
-
-    //Ensure that we also have an entry in compile table for high bits of a VR
-    if (addHighOfWideVRToCompileTable () == false)
-    {
-        return false;
-    }
-
-    //We are ready to initialize the MemVRTable since compile table has been updated
-    if (initializeMemVRTable (bb) == false)
-    {
-        return false;
-    }
-
-    //We are ready to initialize the constant table since compile table has been updated
-    if (initializeConstVRTable (bb) == false)
-    {
-        return false;
-    }
-
-    //Now let's make sure we synchronize all registers being used
-    syncAllRegs ();
-
-    return true;
-}
-
-
-/**
- * @brief Constructor for BasicBlock_O1
- */
-BasicBlock_O1::BasicBlock_O1 (void)
-{
-    //We set the defUseTable to 0 to make sure it doesn't try to free it in the clear function
-    defUseTable = 0;
-    clear (true);
-}
-
-/**
- * @brief Clear function for BasicBlock_O1
- * @param allocateLabel do we allocate the label
- */
-void BasicBlock_O1::clear (bool allocateLabel)
-{
-    // Free defUseTable
-    DefUsePair* ptr = defUseTable;
-    defUseTable = 0;
-
-    //Go through each entry
-    while(ptr != NULL) {
-
-        //Get next
-        DefUsePair* tmp = ptr->next;
-
-        //Free its uses
-        DefOrUseLink* ptrUse = ptr->uses;
-
-        while(ptrUse != NULL) {
-            DefOrUseLink* tmp2 = ptrUse->next;
-            free(ptrUse), ptrUse = 0;
-            ptrUse = tmp2;
-        }
-
-        free(ptr), ptr = 0;
-        ptr = tmp;
-    }
-
-    //Reset variables
-    pc_start = 0;
-    pc_end = 0;
-    streamStart = 0;
-    defUseTable = 0;
-    defUseTail = 0;
-    defUseTail = 0;
-
-    //Clear the vectors
-    xferPoints.clear ();
-    associationTable.clear ();
-    infoBasicBlock.clear ();
-
-    //Allocate label
-    if (allocateLabel == true)
-    {
-        label = static_cast<LowOpBlockLabel *> (dvmCompilerNew (sizeof (*label), true));
-    }
-
-    //Paranoid
-    assert (label != NULL);
-
-    //Default value for the offset
-    label->lop.generic.offset = -1;
-
-    //! The logic below assumes that PhysicalReg_EAX is first entry in
-    //! PhysicalReg enum so let's assert it
-    assert(static_cast<int>(PhysicalReg_EAX) == 0);
-
-    // Initialize allocation constraints
-    for (PhysicalReg reg = PhysicalReg_StartOfGPMarker;
-            reg <= PhysicalReg_EndOfGPMarker;
-            reg = static_cast<PhysicalReg>(reg + 1)) {
-        allocConstraints[static_cast<int>(reg)].physicalReg = reg;
-        allocConstraints[static_cast<int>(reg)].count = 0;
-    }
-}
-
-/**
- * @brief Free everything in the BasicBlock that requires it
- */
-void BasicBlock_O1::freeIt (void) {
-    //First call clear
-    clear (false);
-
-    //Now free anything that is C++ oriented
-    std::vector<XferPoint> emptyXfer;
-    xferPoints.swap(emptyXfer);
-
-    std::vector<VirtualRegInfo> emptyVRI;
-    infoBasicBlock.swap(emptyVRI);
-}
-
-/**
- * @brief Do we have enough of a given class of registers to registerize
- * @param cUnit the BasicBlock
- * @param reg the RegisterClass to consider
- * @param cnt the number of this type we have right now
- */
-static bool isEnoughRegisterization (CompilationUnit *cUnit, RegisterClass reg, int cnt)
-{
-    // Get the max set in the cUnit
-    const int max = cUnit->maximumRegisterization;
-
-    return cnt > max;
-}
-
-/**
- * @brief Backend specific checker for possible bail out to VM
- * @details Returns true if bail out from JIT code is possible
- *          A bit complex logic:
- *              If someone proved that bail out is not possible we trust him
- *              Handle not special cases
- *              Handle special cases: null check elimination, bound check elimination.
- * @param cUnit the CompilationUnit
- * @param mir the MIR to check
- * @return true if bail out from JIT code is possible
- */
-bool backendCanBailOut(CompilationUnit *cUnit, MIR *mir)
-{
-    if ((mir->OptimizationFlags & MIR_IGNORE_BAIL_OUT_CHECK) != 0) {
-        return false;
-    }
-
-    /* We miss here some entries which makes sense only for method JIT
-     * TODO: update table if method JIT is enabled
-     */
-    switch (mir->dalvikInsn.opcode) {
-
-    /* Monitor enter/exit - there is a call to dvmLockObject */
-    case OP_MONITOR_ENTER:
-    case OP_MONITOR_EXIT:
-        return true;
-
-    /* possible call to class resolution */
-    case OP_CHECK_CAST:
-    case OP_INSTANCE_OF:
-    case OP_SGET:
-    case OP_SGET_WIDE:
-    case OP_SGET_OBJECT:
-    case OP_SGET_BOOLEAN:
-    case OP_SGET_BYTE:
-    case OP_SGET_CHAR:
-    case OP_SGET_SHORT:
-    case OP_SPUT:
-    case OP_SPUT_WIDE:
-    case OP_SPUT_OBJECT:
-    case OP_SPUT_BOOLEAN:
-    case OP_SPUT_BYTE:
-    case OP_SPUT_CHAR:
-    case OP_SPUT_SHORT:
-    case OP_SGET_VOLATILE:
-    case OP_SPUT_VOLATILE:
-    case OP_SGET_WIDE_VOLATILE:
-    case OP_SPUT_WIDE_VOLATILE:
-    case OP_SGET_OBJECT_VOLATILE:
-    case OP_SPUT_OBJECT_VOLATILE:
-        return true;
-
-    /* memory allocation */
-    case OP_NEW_INSTANCE:
-    case OP_NEW_ARRAY:
-    case OP_FILLED_NEW_ARRAY:
-    case OP_FILLED_NEW_ARRAY_RANGE:
-        return true;
-
-    /* implicit throw */
-    case OP_THROW:
-    case OP_THROW_VERIFICATION_ERROR:
-        return true;
-
-    /* invocation */
-    case OP_INVOKE_VIRTUAL:
-    case OP_INVOKE_SUPER:
-    case OP_INVOKE_DIRECT:
-    case OP_INVOKE_STATIC:
-    case OP_INVOKE_INTERFACE:
-    case OP_INVOKE_VIRTUAL_RANGE:
-    case OP_INVOKE_SUPER_RANGE:
-    case OP_INVOKE_DIRECT_RANGE:
-    case OP_INVOKE_STATIC_RANGE:
-    case OP_INVOKE_INTERFACE_RANGE:
-    case OP_EXECUTE_INLINE:
-    case OP_EXECUTE_INLINE_RANGE:
-    case OP_INVOKE_OBJECT_INIT_RANGE:
-    case OP_INVOKE_VIRTUAL_QUICK:
-    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
-    case OP_INVOKE_SUPER_QUICK:
-    case OP_INVOKE_SUPER_QUICK_RANGE:
-        return true;
-
-    /* Division By Zero */
-    case OP_DIV_INT:
-    case OP_REM_INT:
-    case OP_DIV_LONG:
-    case OP_REM_LONG:
-    case OP_DIV_INT_2ADDR:
-    case OP_REM_INT_2ADDR:
-    case OP_DIV_LONG_2ADDR:
-    case OP_REM_LONG_2ADDR:
-        return true;
-
-    case OP_DIV_INT_LIT16:
-    case OP_REM_INT_LIT16:
-    case OP_DIV_INT_LIT8:
-    case OP_REM_INT_LIT8:
-        return mir->dalvikInsn.vC == 0;
-
-    /* Access an Array index */
-    case OP_AGET:
-    case OP_AGET_WIDE:
-    case OP_AGET_OBJECT:
-    case OP_AGET_BOOLEAN:
-    case OP_AGET_BYTE:
-    case OP_AGET_CHAR:
-    case OP_AGET_SHORT:
-    case OP_APUT:
-    case OP_APUT_WIDE:
-    case OP_APUT_OBJECT:
-    case OP_APUT_BOOLEAN:
-    case OP_APUT_BYTE:
-    case OP_APUT_CHAR:
-    case OP_APUT_SHORT:
-        return ((mir->OptimizationFlags & MIR_IGNORE_RANGE_CHECK) == 0) || ((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0);
-
-    /* Access Object field */
-    case OP_ARRAY_LENGTH:
-    case OP_IGET:
-    case OP_IGET_WIDE:
-    case OP_IGET_OBJECT:
-    case OP_IGET_BOOLEAN:
-    case OP_IGET_BYTE:
-    case OP_IGET_CHAR:
-    case OP_IGET_SHORT:
-    case OP_IPUT:
-    case OP_IPUT_WIDE:
-    case OP_IPUT_OBJECT:
-    case OP_IPUT_BOOLEAN:
-    case OP_IPUT_BYTE:
-    case OP_IPUT_CHAR:
-    case OP_IPUT_SHORT:
-    case OP_IGET_VOLATILE:
-    case OP_IPUT_VOLATILE:
-    case OP_IGET_OBJECT_VOLATILE:
-    case OP_IGET_WIDE_VOLATILE:
-    case OP_IPUT_WIDE_VOLATILE:
-    case OP_IGET_QUICK:
-    case OP_IGET_WIDE_QUICK:
-    case OP_IGET_OBJECT_QUICK:
-    case OP_IPUT_QUICK:
-    case OP_IPUT_WIDE_QUICK:
-    case OP_IPUT_OBJECT_QUICK:
-    case OP_IPUT_OBJECT_VOLATILE:
-        return (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0;
-
-    default:
-        /* All other cases do not bail out
-         * For non-trace JIT the following opcodes should be checked:
-         * OP_CONST_STRING, OP_CONST_STRING_JUMBO, OP_CONST_CLASS.
-         */
-        break;
-    }
-
-    return false;
-}
-
-/**
- * @brief Handles registerization decision before lowering.
- * @details If registerization is disabled, set the writeBack vector to all 1s.
- * Registerization extended MIRs are then removed in order to only registerize
- * maximum set by the cUnit.
- * @param cUnit the BasicBlock
- * @param bb the BasicBlock
- */
-static void handleRegisterizationPrework (CompilationUnit *cUnit, BasicBlock *bb)
-{
-    //Handle no registerization option first
-    if (gDvmJit.backEndRegisterization == false)
-    {
-        //In this case, we are going to rewrite the requestWriteBack to spilling everything
-        dvmCompilerWriteBackAll (cUnit, bb);
-    }
-
-    //A counter for the kOpRegisterize requests
-    MIR *mir = bb->firstMIRInsn;
-    std::map<RegisterClass, int> counters;
-
-    //Go through the instructions
-    while (mir != 0)
-    {
-        //Did we remove an instruction? Suppose no
-        bool removed = false;
-
-        //If it's a registerize request, we might have to ignore it
-        if (mir->dalvikInsn.opcode == static_cast<Opcode> (kMirOpRegisterize))
-        {
-            //Get the class for it
-            RegisterClass reg = static_cast<RegisterClass> (mir->dalvikInsn.vB);
-
-            //Increment counter
-            counters[reg]++;
-
-            //If we've had enough
-            if (isEnoughRegisterization (cUnit, reg, counters[reg]) == true)
-            {
-                //We are going to remove it, remember it
-                MIR *toRemove = mir;
-                //Remove the instruction but first remember the next one
-                //Note: we are removing this registerization request knowing that the system might request a recompile
-                //TODO: Most likely a flag to ignore it would be better
-                mir = mir->next;
-                //Call the helper function
-                dvmCompilerRemoveMIR (bb, toRemove);
-                //Set removed
-                removed = true;
-            }
-        }
-
-        if (removed == false)
-        {
-            //Go to next
-            mir = mir->next;
-        }
-    }
-}
-
-/**
- * @brief Parse the BasicBlock and perform pre-lowering work
- * @param cUnit the BasicBlock
- * @param bb the BasicBlock
- */
-static void parseBlock (CompilationUnit *cUnit, BasicBlock *bb)
-{
-    //Always handle registerization request
-    handleRegisterizationPrework (cUnit, bb);
-}
-
-/**
- * @brief Pre-process BasicBlocks
- * @details This parses the block to perform some pre-code generation tasks
- * @param cUnit the Compilation Unit
- * @param bb the BasicBlock
- * @return -1 if error happened, 0 otherwise
- */
-int preprocessingBB (CompilationUnit *cUnit, BasicBlock* bb)
-{
-    //Parse the BasicBlock, we might have some pre work to do
-    parseBlock (cUnit, bb);
-
-    //Everything went well
-    return 0;
-}
-
-void printJitTraceInfoAtRunTime(const Method* method, int offset) {
-    ALOGI("execute trace for %s%s at offset %x", method->clazz->descriptor, method->name, offset);
-}
-
-void startOfTraceO1(const Method* method, int exceptionBlockId, CompilationUnit *cUnit) {
-    compileTable.clear ();
-    currentBB = NULL;
-    currentUnit = cUnit;
-
-    /* initialize data structure allRegs */
-    initializeAllRegs();
-
-// dumpDebuggingInfo is gone in CompilationUnit struct
-#if 0
-    /* add code to dump debugging information */
-    if(cUnit->dumpDebuggingInfo) {
-        move_imm_to_mem(OpndSize_32, cUnit->startOffset, -4, PhysicalReg_ESP, true); //2nd argument: offset
-        move_imm_to_mem(OpndSize_32, (int)currentMethod, -8, PhysicalReg_ESP, true); //1st argument: method
-        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-        typedef void (*vmHelper)(const Method*, int);
-        vmHelper funcPtr = printJitTraceInfoAtRunTime;
-        move_imm_to_reg(OpndSize_32, (int)funcPtr, PhysicalReg_ECX, true);
-        call_reg(PhysicalReg_ECX, true);
-
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    }
-#endif
-}
-
-
-/* Code generation for a basic block defined for JIT
-   We have two data structures for a basic block:
-       BasicBlock defined in vm/compiler by JIT
-       BasicBlock_O1 defined in o1 */
-int codeGenBasicBlockJit(const Method* method, BasicBlock* bb) {
-    // For x86, the BasicBlock should be the specialized one
-    currentBB = reinterpret_cast<BasicBlock_O1 *> (bb);
-
-    // Basic block here also means new native basic block
-    if (gDvmJit.scheduling)
-    {
-        singletonPtr<Scheduler> ()->signalEndOfNativeBasicBlock ();
-    }
-
-    // Finalize this block's association table because we are generating
-    // it and thus any parent of it that hasn't been generated yet must
-    // be aware of this fact.
-    currentBB->associationTable.finalize ();
-
-    // Generate code for this basic block
-    int result = codeGenBasicBlock (method, currentBB);
-
-    // End of managed basic block means end of native basic block
-    if (gDvmJit.scheduling)
-    {
-        singletonPtr<Scheduler> ()->signalEndOfNativeBasicBlock ();
-    }
-
-    currentBB = NULL;
-
-    return result;
-}
-void endOfBasicBlock(BasicBlock* bb) {
-    isScratchPhysical = true;
-    currentBB = NULL;
-}
-
-/**
- * @brief decide if skip the extended Op whose implementation uses NcgO0 mode
- * @param opc opcode of the extended MIR
- * @return return false if the opcode doesn't use NcgO0, otherwise return true
- */
-bool skipExtendedMir(Opcode opc) {
-    ExtendedMIROpcode extendedOpCode = static_cast<ExtendedMIROpcode> (opc);
-
-    switch (extendedOpCode) {
-        case kMirOpRegisterize:
-            return false;
-        default:
-            return true;
-    }
-}
-
-/** entry point to collect information about virtual registers used in a basic block
-    Initialize data structure BasicBlock_O1
-    The usage information of virtual registers is stoerd in bb->infoBasicBlock
-
-    Global variables accessed: offsetPC, rPC
-*/
-int collectInfoOfBasicBlock (BasicBlock_O1* bb)
-{
-    int seqNum = 0;
-    /* traverse the MIR in basic block
-       sequence number is used to make sure next bytecode will have a larger sequence number */
-    for(MIR * mir = bb->firstMIRInsn; mir; mir = mir->next) {
-        offsetPC = seqNum;
-        mir->seqNum = seqNum++;
-
-        // Skip extended MIRs whose implementation uses NcgO0 mode
-        if ( isExtendedMir(mir->dalvikInsn.opcode) == true &&
-             skipExtendedMir(mir->dalvikInsn.opcode) == true) {
-            continue;
-        }
-
-        //Get information about the VRs in current bytecode
-        VirtualRegInfo infoByteCode[MAX_REG_PER_BYTECODE];
-        int numVRs = getVirtualRegInfo (infoByteCode, mir, true);
-
-        for(int kk = 0; kk < numVRs; kk++) {
-            //Make a copy of current bytecode
-            VirtualRegInfo currentInfo = infoByteCode[kk];
-#ifdef DEBUG_MERGE_ENTRY
-            ALOGI("Call mergeEntry2 at offsetPC %x kk %d VR %d %d\n", offsetPC, kk,
-                  currentInfo.regNum, currentInfo.physicalType);
-#endif
-            int retCode = mergeEntry2(bb, currentInfo); //update defUseTable of the basic block
-            if (retCode < 0)
-                return retCode;
-        }
-
-        //dumpVirtualInfoOfBasicBlock(bb);
-    }//for each bytecode
-
-    bb->pc_end = seqNum;
-
-    //sort allocConstraints of each basic block
-    unsigned int max = bb->infoBasicBlock.size ();
-    for(unsigned int kk = 0; kk < max; kk++) {
-#ifdef DEBUG_ALLOC_CONSTRAINT
-        ALOGI("Sort virtual reg %d type %d -------", bb->infoBasicBlock[kk].regNum,
-              bb->infoBasicBlock[kk].physicalType);
-#endif
-        sortAllocConstraint(bb->infoBasicBlock[kk].allocConstraints,
-                            bb->infoBasicBlock[kk].allocConstraintsSorted, true);
-    }
-#ifdef DEBUG_ALLOC_CONSTRAINT
-    ALOGI("Sort constraints for BB %d --------", bb->bb_index);
-#endif
-    sortAllocConstraint(bb->allocConstraints, bb->allocConstraintsSorted, false);
-    return 0;
-}
-
-/**
- * @brief Looks through a basic block for any reasons to reject it
- * @param bb Basic Block to look at
- * @return true if Basic Block cannot be handled safely by Backend
- */
-static bool shouldRejectBasicBlock(BasicBlock_O1* bb) {
-    // Assume that we do not want to reject the BB
-    bool shouldReject = false;
-
-    // Set a generic error message in case someone forgets to set a proper one
-    JitCompilationErrors errorIfRejected = kJitErrorCodegen;
-
-    /**
-     * Rejection Scenario 1:
-     * If the basic block has incoming virtual registers that are in physical
-     * registers but we have a usage of that VR in x87, we should reject trace
-     * until we properly handle the Xfer point.
-     */
-
-    // We will need to call getVirtualRegInfo but we do not want to update
-    // register constraints so temporarily set the global currentBB to null
-    BasicBlock_O1 * savedCurrentBB = currentBB;
-    currentBB = NULL;
-
-    std::set<int> registerizedVRs;
-
-    // Find all of the VRs that have been registerized at entry into this BB
-    for (AssociationTable::const_iterator iter = bb->associationTable.begin();
-            iter != bb->associationTable.end(); iter++) {
-        // If there is a physical register for this VR, then it has been
-        // registerized
-        if (iter->second.physicalReg != PhysicalReg_Null) {
-            registerizedVRs.insert(iter->first);
-        }
-    }
-
-    for (MIR * mir = bb->firstMIRInsn; mir != NULL; mir = mir->next) {
-
-        if ( isExtendedMir(mir->dalvikInsn.opcode) == true &&
-             skipExtendedMir(mir->dalvikInsn.opcode) == true) {
-            continue;
-        }
-
-        //Get information about the VRs in current bytecode
-        VirtualRegInfo infoByteCode[MAX_REG_PER_BYTECODE];
-        int numVRs = getVirtualRegInfo (infoByteCode, mir);
-
-        // Go through each VR of the MIR
-        for (int vrIter = 0; vrIter < numVRs; vrIter++) {
-            int VR = infoByteCode[vrIter].regNum;
-            LowOpndRegType type = infoByteCode[vrIter].physicalType;
-
-            // Has this VR been registerized?
-            if (registerizedVRs.find(VR) != registerizedVRs.end()) {
-                // If we will be using x87 for this registerized VR, we cannot
-                // handle
-                if (type == LowOpndRegType_fs || type == LowOpndRegType_fs_s) {
-                    ALOGI("JIT_INFO: Found x87 usage for VR that has been registerized.");
-                    errorIfRejected = kJitErrorBERegisterization;
-                    shouldReject = true;
-                    break;
-                }
-            }
-        }
-
-        // We already know we need to reject, break out of loop
-        if (shouldReject == true) {
-            break;
-        }
-    }
-
-    // Restore currentBB
-    currentBB = savedCurrentBB;
-
-    if (shouldReject) {
-        SET_JIT_ERROR (errorIfRejected);
-    }
-
-    return shouldReject;
-}
-
-/** entry point to generate native code for a O1 basic block
-    There are 3 kinds of virtual registers in a O1 basic block:
-    1> L VR: local within the basic block
-    2> GG VR: is live in other basic blocks,
-              its content is in a pre-defined GPR at the beginning of a basic block
-    3> GL VR: is live in other basic blocks,
-              its content is in the interpreted stack at the beginning of a basic block
-    compileTable is updated with infoBasicBlock at the start of the basic block;
-    Before lowering each bytecode, compileTable is updated with infoByteCodeTemp;
-    At end of the basic block, right before the jump instruction, handles constant VRs and GG VRs
-*/
-int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb)
-{
-    //Eagerly set retCode to 0 since most likely everything will be okay
-    int retCode = 0;
-
-    //We have no MIRs if first MIR pointer is null
-    bool noMirs = (currentBB->firstMIRInsn == NULL);
-
-    // If we should reject the BB, return that it has not been handled
-    if (shouldRejectBasicBlock (bb) == true)
-    {
-        //If rejected, an error message will have been set so we just pass along the error
-        return -1;
-    }
-
-    //We have already loaded the information about VR from each bytecode in this basic block.
-    //Thus we are now ready to finish initializing virtual register state.
-    if (initializeRegStateOfBB (bb) == false)
-    {
-        return -1;
-    }
-
-    //If we do have MIRs, we must update the transfer points and the live table
-    if (noMirs == false)
-    {
-        //Now we update any transfer points between virtual registers that are represented by different types
-        //throughout this same BB
-        retCode = updateXferPoints (bb);
-
-        if (retCode < 0)
-        {
-            //Someone else has set the error so we just pass it along
-            return retCode;
-        }
-
-        //Since we have set up the transfer points, check to see if there are any points at the start of the BB
-        //so that we can handle them right now.
-        handleStartOfBBXferPoints (bb);
-
-        retCode = updateLiveTable (bb);
-
-        if (retCode < 0)
-        {
-            //Someone else has set the error so we just pass it along
-            return retCode;
-        }
-    }
-
-#ifdef DEBUG_REACHING_DEF
-    printDefUseTable();
-#endif
-
-#ifdef DEBUG_COMPILE_TABLE
-    ALOGI("At start of basic block %d (num of VRs %d) -------", bb->bb_index, bb->infoBasicBlock.size ());
-    dumpCompileTable();
-#endif
-
-    //Assume that the last bytecode in this block is not a jump unless proven otherwise
-    bool lastByteCodeIsJump = false;
-
-    //Now walk through the bytecodes to generate code for each
-    for (MIR * mir = bb->firstMIRInsn; mir; mir = mir->next)
-    {
-        int k;
-        offsetPC = mir->seqNum;
-        rPC = const_cast<u2 *>(method->insns) + mir->offset;
-
-        //Skip mirs tagged as being no-ops
-        if ((mir->OptimizationFlags & MIR_INLINED) != 0)
-        {
-            continue;
-        }
-
-        // Handle extended MIRs whose implementation uses NcgO0 mode
-        if ( isExtendedMir(mir->dalvikInsn.opcode) == true &&
-             skipExtendedMir(mir->dalvikInsn.opcode) == true) {
-            handleExtendedMIR (currentUnit, bb, mir);
-            // The rest of logic is for handling mirs that use NCG01 so
-            // we can safely skip
-            continue;
-        }
-
-        //before handling a bytecode, import info of temporary registers to compileTable including refCount
-        num_temp_regs_per_bytecode = getTempRegInfo(infoByteCodeTemp, mir);
-        for(k = 0; k < num_temp_regs_per_bytecode; k++) {
-            if(infoByteCodeTemp[k].versionNum > 0) continue;
-            insertFromTempInfo (infoByteCodeTemp[k]);
-        }
-        startNativeCode(-1, -1);
-        for(k = 0; k <= MAX_SPILL_JIT_IA - 1; k++) spillIndexUsed[k] = 0;
-
-#ifdef DEBUG_COMPILE_TABLE
-        ALOGI("compile table size after importing temporary info %d", num_compile_entries);
-        ALOGI("before one bytecode %d (num of VRs %d) -------", bb->bb_index, bb->infoBasicBlock.size ());
-#endif
-        //set isConst to true for CONST & MOVE MOVE_OBJ?
-        //clear isConst to true for MOVE, MOVE_OBJ, MOVE_RESULT, MOVE_EXCEPTION ...
-        bool isConst = false;
-        int retCode = getConstInfo(bb, mir); //will return 0 if a VR is updated by the bytecode
-        //if the bytecode generates a constant
-        if (retCode == 1)
-            isConst = true;
-        //if something went wrong at getConstInfo. getConstInfo has logged it
-        else if (retCode == -1)
-            return retCode;
-        //otherwise, bytecode does not generate a constant
-
-        //Get information about the VRs in current bytecode
-        VirtualRegInfo infoByteCode[MAX_REG_PER_BYTECODE];
-        int numVRs = getVirtualRegInfo (infoByteCode, mir);
-
-        //call something similar to mergeEntry2, but only update refCount
-        //clear refCount
-        for(k = 0; k < numVRs; k++) {
-            int indexT = searchCompileTable(LowOpndRegType_virtual | infoByteCode[k].physicalType,
-                                            infoByteCode[k].regNum);
-            if(indexT >= 0)
-                compileTable[indexT].refCount = 0;
-        }
-        for(k = 0; k < numVRs; k++) {
-            int indexT = searchCompileTable(LowOpndRegType_virtual | infoByteCode[k].physicalType,
-                                            infoByteCode[k].regNum);
-            if(indexT >= 0)
-                compileTable[indexT].refCount += infoByteCode[k].refCount;
-        } //for k
-        lastByteCodeIsJump = false;
-        if(isConst == false)
-        {
-#ifdef DEBUG_COMPILE_TABLE
-            dumpCompileTable();
-#endif
-            freeShortMap();
-            if (isCurrentByteCodeJump(mir->dalvikInsn.opcode))
-                lastByteCodeIsJump = true;
-
-            //We eagerly assume we don't handle unless proven otherwise.
-            bool notHandled = true;
-
-            if ((int) mir->dalvikInsn.opcode >= (int) kMirOpFirst)
-            {
-                notHandled = (handleExtendedMIR (currentUnit, bb, mir) == false);
-            }
-            else
-            {
-                notHandled = lowerByteCodeJit(method, mir, rPC);
-            }
-
-            if(gDvmJit.codeCacheByteUsed + (stream - streamStart) +
-                 CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
-                 ALOGI("JIT_INFO: Code cache full while lowering bytecode %s", dexGetOpcodeName(mir->dalvikInsn.opcode));
-                 gDvmJit.codeCacheFull = true;
-                 SET_JIT_ERROR(kJitErrorCodeCacheFull);
-                 return -1;
-            }
-
-            if (notHandled){
-                SET_JIT_ERROR(kJitErrorCodegen);
-                return -1;
-            }
-
-            //Check if an error happened while in the bytecode
-            if (IS_ANY_JIT_ERROR_SET()) {
-                SET_JIT_ERROR(kJitErrorCodegen);
-                return -1;
-            }
-
-            updateConstInfo(bb);
-            freeShortMap();
-        } else { //isConst
-            //if this bytecode is the target of a jump, the mapFromBCtoNCG should be updated
-            offsetNCG = stream - streamMethodStart;
-            mapFromBCtoNCG[mir->offset] = offsetNCG;
-#ifdef DEBUG_COMPILE_TABLE
-            ALOGI("Bytecode %s generates a constant and has no side effect\n", dexGetOpcodeName(mir->dalvikInsn.opcode));
-#endif
-        }
-
-        //After each bytecode, make sure the temporaries have refCount of zero.
-        for(k = 0; k < compileTable.size (); k++)
-        {
-            if(compileTable[k].isTemporary ())
-            {
-#ifdef PRINT_WARNING
-                //If warnings are enabled, we need to print a message because remaining ref count greater
-                //than zero means that bytecode visitor reference counts are not correct
-                if (compileTable[k].refCount > 0)
-                {
-                    ALOGD ("JIT_INFO: refCount for a temporary reg %d %d is %d after a bytecode", compileTable[k].regNum,
-                            compileTable[k].physicalType, compileTable[k].refCount);
-                }
-#endif
-                compileTable[k].updateRefCount (0);
-            }
-        }
-
-        //Now that we updated reference counts, let's clear the physical register associations
-        freeReg (false);
-
-#ifdef DEBUG_COMPILE_TABLE
-        ALOGI("After one bytecode BB %d (num of VRs %d)", bb->bb_index, bb->infoBasicBlock.size ());
-#endif
-    }//for each bytecode
-
-#ifdef DEBUG_COMPILE_TABLE
-    dumpCompileTable();
-#endif
-
-    //At the end of a basic block we want to handle VR information. If the BB ended with
-    //jump or switch, then we have nothing to handle because it has already been handled
-    //in the corresponding jumping bytecode.
-    retCode = handleRegistersEndOfBB (lastByteCodeIsJump == false);
-
-    if (retCode < 0)
-    {
-        //Just pass along error information
-        return retCode;
-    }
-
-    // We are done with compile table so clear it now
-    compileTable.clear ();
-
-    //Free live table
-    for (int k = 0; k < num_memory_vr; k++)
-    {
-        LiveRange* ptr2 = memVRTable[k].ranges;
-        while (ptr2 != NULL)
-        {
-            LiveRange* tmpP = ptr2->next;
-            free (ptr2->accessPC);
-            free (ptr2);
-            ptr2 = tmpP;
-        }
-    }
-
-    return 0;
-}
-
-/** update infoBasicBlock & defUseTable
-    input: currentInfo
-    side effect: update currentInfo.reachingDefs
-
-    update entries in infoBasicBlock by calling updateReachingDefA
-    if there is no entry in infoBasicBlock for B, an entry will be created and inserted to infoBasicBlock
-
-    defUseTable is updated to account for the access at currentInfo
-    if accessType of B is U or UD, we call updateReachingDefB to update currentInfo.reachingDefs
-        in order to correctly insert the usage to defUseTable
-*/
-int mergeEntry2 (BasicBlock_O1* bb, VirtualRegInfo &currentInfo)
-{
-    LowOpndRegType typeB = currentInfo.physicalType;
-    int regB = currentInfo.regNum;
-    int jj, k;
-    int jjend = bb->infoBasicBlock.size ();
-    bool isMerged = false;
-    bool hasAlias = false;
-    OverlapCase isBPartiallyOverlapA, isAPartiallyOverlapB;
-    RegAccessType tmpType = REGACCESS_N;
-    currentInfo.num_reaching_defs = 0;
-
-    /* traverse variable A in infoBasicBlock */
-    for(jj = 0; jj < jjend; jj++) {
-        int regA = bb->infoBasicBlock[jj].regNum;
-        LowOpndRegType typeA = bb->infoBasicBlock[jj].physicalType;
-        isBPartiallyOverlapA = getBPartiallyOverlapA(regB, typeB, regA, typeA);
-        isAPartiallyOverlapB = getAPartiallyOverlapB(regA, typeA, regB, typeB);
-        if(regA == regB && typeA == typeB) {
-            /* variable A and B are aligned */
-            bb->infoBasicBlock[jj].accessType = mergeAccess2(bb->infoBasicBlock[jj].accessType, currentInfo.accessType,
-                                                             OVERLAP_B_COVER_A);
-            bb->infoBasicBlock[jj].refCount += currentInfo.refCount;
-            /* copy reaching defs of variable B from variable A */
-            currentInfo.num_reaching_defs = bb->infoBasicBlock[jj].num_reaching_defs;
-            for(k = 0; k < currentInfo.num_reaching_defs; k++)
-                currentInfo.reachingDefs[k] = bb->infoBasicBlock[jj].reachingDefs[k];
-            updateDefUseTable (currentInfo); //use currentInfo to update defUseTable
-            int retCode = updateReachingDefA (currentInfo, jj, OVERLAP_B_COVER_A); //update reachingDefs of A
-            if (retCode < 0)
-                return -1;
-            isMerged = true;
-            hasAlias = true;
-            if(typeB == LowOpndRegType_gp) {
-                //merge allocConstraints
-                for(k = 0; k < 8; k++) {
-                    bb->infoBasicBlock[jj].allocConstraints[k].count += currentInfo.allocConstraints[k].count;
-                }
-            }
-        }
-        else if(isBPartiallyOverlapA != OVERLAP_NO) {
-            tmpType = updateAccess2(tmpType, updateAccess1(bb->infoBasicBlock[jj].accessType, isAPartiallyOverlapB));
-            bb->infoBasicBlock[jj].accessType = mergeAccess2(bb->infoBasicBlock[jj].accessType, currentInfo.accessType,
-                                                             isBPartiallyOverlapA);
-#ifdef DEBUG_MERGE_ENTRY
-            ALOGI("Update accessType in case 2: VR %d %d accessType %d", regA, typeA, bb->infoBasicBlock[jj].accessType);
-#endif
-            hasAlias = true;
-            if(currentInfo.accessType == REGACCESS_U || currentInfo.accessType == REGACCESS_UD) {
-                VirtualRegInfo tmpInfo;
-
-                /* update currentInfo.reachingDefs */
-                int retCode = updateReachingDefB1 (currentInfo, tmpInfo, jj);
-                if (retCode < 0)
-                    return retCode;
-                retCode = updateReachingDefB2 (currentInfo, tmpInfo);
-                if (retCode < 0)
-                    return retCode;
-            }
-            int retCode = updateReachingDefA (currentInfo, jj, isBPartiallyOverlapA);
-            if (retCode < 0)
-                return retCode;
-        }
-        else {
-            //even if B does not overlap with A, B can affect the reaching defs of A
-            //for example, B is a def of "v0", A is "v1"
-            //  B can kill some reaching defs of A or affect the accessType of a reaching def
-            int retCode = updateReachingDefA (currentInfo, jj, OVERLAP_NO); //update reachingDefs of A
-            if (retCode < 0)
-                return -1;
-        }
-    }//for each variable A in infoBasicBlock
-    if(!isMerged) {
-        /* create a new entry in infoBasicBlock */
-        VirtualRegInfo info;
-        info.refCount = currentInfo.refCount;
-        info.physicalType = typeB;
-        if(hasAlias)
-            info.accessType = updateAccess3(tmpType, currentInfo.accessType);
-        else
-            info.accessType = currentInfo.accessType;
-#ifdef DEBUG_MERGE_ENTRY
-        ALOGI("Update accessType in case 3: VR %d %d accessType %d", regB, typeB, info.accessType);
-#endif
-        info.regNum = regB;
-        for(k = 0; k < 8; k++)
-            info.allocConstraints[k] = currentInfo.allocConstraints[k];
-#ifdef DEBUG_MERGE_ENTRY
-        ALOGI("isMerged is false, call updateDefUseTable");
-#endif
-        updateDefUseTable (currentInfo); //use currentInfo to update defUseTable
-        updateReachingDefB3 (currentInfo); //update currentInfo.reachingDefs if currentInfo defines variable B
-
-        //copy from currentInfo.reachingDefs to info
-        info.num_reaching_defs = currentInfo.num_reaching_defs;
-        for(k = 0; k < currentInfo.num_reaching_defs; k++)
-            info.reachingDefs[k] = currentInfo.reachingDefs[k];
-#ifdef DEBUG_MERGE_ENTRY
-        ALOGI("Try to update reaching defs for VR %d %d", regB, typeB);
-        for(k = 0; k < info.num_reaching_defs; k++)
-            ALOGI("reaching def %d @ %d for VR %d %d access %d", k, currentInfo.reachingDefs[k].offsetPC,
-                  currentInfo.reachingDefs[k].regNum, currentInfo.reachingDefs[k].physicalType,
-                  currentInfo.reachingDefs[k].accessType);
-#endif
-        //Push it in the vector
-        bb->infoBasicBlock.push_back (info);
-
-        if(bb->infoBasicBlock.size () >= MAX_REG_PER_BASICBLOCK) {
-            ALOGI("JIT_INFO: Number of VRs (%d) in a basic block, exceed maximum (%d)\n", bb->infoBasicBlock.size (), MAX_REG_PER_BASICBLOCK);
-            SET_JIT_ERROR(kJitErrorMaxVR);
-            return -1;
-        }
-    }
-    return 0;
-}
-
-/**
- * @brief update reaching defs for infoBasicBlock[indexToA]
- * @details use currentInfo.reachingDefs to update reaching defs for variable A
- * @param currentInfo the current considered VirtualRegInfo
- * @param indexToA Index of variable A
- * @param isBPartiallyOverlapA the type of overlap
- * @return -1 if error, 0 otherwise
- */
-static int updateReachingDefA (VirtualRegInfo &currentInfo, int indexToA, OverlapCase isBPartiallyOverlapA)
-{
-    if(indexToA < 0) return 0;
-    int k, k2;
-    OverlapCase isBPartiallyOverlapDef;
-    if(currentInfo.accessType == REGACCESS_U) {
-        return 0; //no update to reachingDefs of the VR
-    }
-    /* access in currentInfo is DU, D, or UD */
-    if(isBPartiallyOverlapA == OVERLAP_B_COVER_A) {
-        /* from this point on, the reachingDefs for variable A is a single def to currentInfo at offsetPC */
-        currentBB->infoBasicBlock[indexToA].num_reaching_defs = 1;
-        currentBB->infoBasicBlock[indexToA].reachingDefs[0].offsetPC = offsetPC;
-        currentBB->infoBasicBlock[indexToA].reachingDefs[0].regNum = currentInfo.regNum;
-        currentBB->infoBasicBlock[indexToA].reachingDefs[0].physicalType = currentInfo.physicalType;
-        currentBB->infoBasicBlock[indexToA].reachingDefs[0].accessType = REGACCESS_D;
-#ifdef DEBUG_REACHING_DEF
-        ALOGI("Single reaching def @ %d for VR %d %d", offsetPC, currentInfo.regNum, currentInfo.physicalType);
-#endif
-        return 0;
-    }
-    /* update reachingDefs for variable A to get rid of dead defs */
-    /* Bug fix: it is possible that more than one reaching defs need to be removed
-                after one reaching def is removed, num_reaching_defs--, but k should not change
-    */
-    for(k = 0; k < currentBB->infoBasicBlock[indexToA].num_reaching_defs; ) {
-        /* remove one reaching def in one interation of the loop */
-        //check overlapping between def & B
-        isBPartiallyOverlapDef = getBPartiallyOverlapA(currentInfo.regNum, currentInfo.physicalType,
-                                                       currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum,
-                                                       currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType);
-#ifdef DEBUG_REACHING_DEF
-        ALOGI("DEBUG B %d %d def %d %d %d", currentInfo.regNum, currentInfo.physicalType,
-              currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum,
-              currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType,
-              currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType);
-#endif
-        /* cases where one def nees to be removed:
-           if B fully covers def, def is removed
-           if B overlaps high half of def & def's accessType is H, def is removed
-           if B overlaps low half of def & def's accessType is L, def is removed
-        */
-        if((isBPartiallyOverlapDef == OVERLAP_B_COVER_HIGH_OF_A &&
-            currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType == REGACCESS_H) ||
-           (isBPartiallyOverlapDef == OVERLAP_B_COVER_LOW_OF_A &&
-            currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType == REGACCESS_L) ||
-           isBPartiallyOverlapDef == OVERLAP_B_COVER_A
-           ) { //remove def
-            //shift from k+1 to end
-            for(k2 = k+1; k2 < currentBB->infoBasicBlock[indexToA].num_reaching_defs; k2++)
-                currentBB->infoBasicBlock[indexToA].reachingDefs[k2-1] = currentBB->infoBasicBlock[indexToA].reachingDefs[k2];
-            currentBB->infoBasicBlock[indexToA].num_reaching_defs--;
-        }
-        /*
-           if B overlaps high half of def & def's accessType is not H --> update accessType of def
-        */
-        else if(isBPartiallyOverlapDef == OVERLAP_B_COVER_HIGH_OF_A &&
-                currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType != REGACCESS_H) {
-            //low half is still valid
-            if(getRegSize(currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType) == OpndSize_32)
-                currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType = REGACCESS_D;
-            else
-                currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType = REGACCESS_L;
-#ifdef DEBUG_REACHING_DEF
-            ALOGI("DEBUG: set accessType of def to L");
-#endif
-            k++;
-        }
-        /*
-           if B overlaps low half of def & def's accessType is not L --> update accessType of def
-        */
-        else if(isBPartiallyOverlapDef == OVERLAP_B_COVER_LOW_OF_A &&
-                currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType != REGACCESS_L) {
-            //high half of def is still valid
-            currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType = REGACCESS_H;
-#ifdef DEBUG_REACHING_DEF
-            ALOGI("DEBUG: set accessType of def to H");
-#endif
-            k++;
-        }
-        else {
-            k++;
-        }
-    }//for k
-    if(isBPartiallyOverlapA != OVERLAP_NO) {
-        //insert the def to variable @ currentInfo
-        k = currentBB->infoBasicBlock[indexToA].num_reaching_defs;
-        if(k >= 3) {
-            ALOGI("JIT_INFO: more than 3 reaching defs at updateReachingDefA");
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return -1;
-        }
-        currentBB->infoBasicBlock[indexToA].reachingDefs[k].offsetPC = offsetPC;
-        currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum = currentInfo.regNum;
-        currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType = currentInfo.physicalType;
-        currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType = REGACCESS_D;
-        currentBB->infoBasicBlock[indexToA].num_reaching_defs++;
-    }
-#ifdef DEBUG_REACHING_DEF2
-    ALOGI("IN updateReachingDefA for VR %d %d", currentBB->infoBasicBlock[indexToA].regNum,
-          currentBB->infoBasicBlock[indexToA].physicalType);
-    for(k = 0; k < currentBB->infoBasicBlock[indexToA].num_reaching_defs; k++)
-        ALOGI("Reaching def %d @ %d for VR %d %d access %d", k,
-              currentBB->infoBasicBlock[indexToA].reachingDefs[k].offsetPC,
-              currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum,
-              currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType,
-              currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType);
-#endif
-    return 0;
-}
-
-/**
- * @brief updateReachingDefB1
- * @details Given a variable B in currentInfo, updates its reaching defs
- * by checking reaching defs of variable A in currentBB->infoBasicBlock[indexToA]
- * The result is stored in tmpInfo.reachingDefs
- * @param currentInfo the current considered VirtualRegInfo
- * @param tmpInfo the temporary information
- * @param indexToA Index of variable A
- * @return -1 if error, 0 otherwise
- */
-static int updateReachingDefB1 (VirtualRegInfo &currentInfo, VirtualRegInfo &tmpInfo, int indexToA)
-{
-    if(indexToA < 0) return 0;
-    int k;
-    tmpInfo.num_reaching_defs = 0;
-    for(k = 0; k < currentBB->infoBasicBlock[indexToA].num_reaching_defs; k++) {
-        /* go through reachingDefs of variable A @currentBB->infoBasicBlock[indexToA]
-           for each def, check whether it overlaps with variable B @currentInfo
-               if the def overlaps with variable B, insert it to tmpInfo.reachingDefs
-        */
-        OverlapCase isDefPartiallyOverlapB = getAPartiallyOverlapB(
-                                                 currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum,
-                                                 currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType,
-                                                 currentInfo.regNum, currentInfo.physicalType
-                                                 );
-        bool insert1 = false; //whether to insert the def to tmpInfo.reachingDefs
-        if(isDefPartiallyOverlapB == OVERLAP_ALIGN ||
-           isDefPartiallyOverlapB == OVERLAP_A_IS_LOW_OF_B ||
-           isDefPartiallyOverlapB == OVERLAP_A_IS_HIGH_OF_B) {
-            /* B aligns with def */
-            /* def is low half of B, def is high half of B
-               in these two cases, def is 32 bits */
-            insert1 = true;
-        }
-        RegAccessType deftype = currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType;
-        if(isDefPartiallyOverlapB == OVERLAP_B_IS_LOW_OF_A ||
-           isDefPartiallyOverlapB == OVERLAP_LOW_OF_A_IS_HIGH_OF_B) {
-            /* B is the low half of def */
-            /* the low half of def is the high half of B */
-            if(deftype != REGACCESS_H) insert1 = true;
-        }
-        if(isDefPartiallyOverlapB == OVERLAP_B_IS_HIGH_OF_A ||
-           isDefPartiallyOverlapB == OVERLAP_HIGH_OF_A_IS_LOW_OF_B) {
-            /* B is the high half of def */
-            /* the high half of def is the low half of B */
-            if(deftype != REGACCESS_L) insert1 = true;
-        }
-        if(insert1) {
-            if(tmpInfo.num_reaching_defs >= 3) {
-                ALOGI("JIT_INFO: more than 3 reaching defs for tmpInfo at updateReachingDefB1");
-                SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                return -1;
-            }
-            tmpInfo.reachingDefs[tmpInfo.num_reaching_defs] = currentBB->infoBasicBlock[indexToA].reachingDefs[k];
-            tmpInfo.num_reaching_defs++;
-#ifdef DEBUG_REACHING_DEF2
-            ALOGI("Insert from entry %d %d: index %d", currentBB->infoBasicBlock[indexToA].regNum,
-                  currentBB->infoBasicBlock[indexToA].physicalType, k);
-#endif
-        }
-    }
-    return 0;
-}
-
-//! \brief updateReachingDefB2
-//! \details update currentInfo.reachingDefs by merging
-//! currentInfo.reachingDefs with tmpInfo.reachingDefs
-//! \return -1 if error, 0 otherwise
-static int updateReachingDefB2 (VirtualRegInfo &currentInfo, VirtualRegInfo &tmpInfo)
-{
-    int k, k2;
-    for(k2 = 0; k2 < tmpInfo.num_reaching_defs; k2++ ) {
-        bool merged = false;
-        for(k = 0; k < currentInfo.num_reaching_defs; k++) {
-            /* check whether it is the same def, if yes, do nothing */
-            if(currentInfo.reachingDefs[k].regNum == tmpInfo.reachingDefs[k2].regNum &&
-               currentInfo.reachingDefs[k].physicalType == tmpInfo.reachingDefs[k2].physicalType) {
-                merged = true;
-                if(currentInfo.reachingDefs[k].offsetPC != tmpInfo.reachingDefs[k2].offsetPC) {
-                    ALOGI("JIT_INFO: defs on the same VR %d %d with different offsetPC %d vs %d",
-                          currentInfo.reachingDefs[k].regNum, currentInfo.reachingDefs[k].physicalType,
-                          currentInfo.reachingDefs[k].offsetPC, tmpInfo.reachingDefs[k2].offsetPC);
-                    SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                    return -1;
-                }
-                if(currentInfo.reachingDefs[k].accessType != tmpInfo.reachingDefs[k2].accessType) {
-                    ALOGI("JIT_INFO: defs on the same VR %d %d with different accessType\n",
-                          currentInfo.reachingDefs[k].regNum, currentInfo.reachingDefs[k].physicalType);
-                    SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                    return -1;
-                }
-                break;
-            }
-        }
-        if(!merged) {
-            if(currentInfo.num_reaching_defs >= 3) {
-                ALOGI("JIT_INFO: more than 3 reaching defs for currentInfo at updateReachingDefB2\n");
-                SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                return -1;
-            }
-            currentInfo.reachingDefs[currentInfo.num_reaching_defs] = tmpInfo.reachingDefs[k2];
-            currentInfo.num_reaching_defs++;
-        }
-    }
-    return 0;
-}
-
-//!update currentInfo.reachingDefs with currentInfo if variable is defined in currentInfo
-
-//!
-void updateReachingDefB3 (VirtualRegInfo &currentInfo)
-{
-    if(currentInfo.accessType == REGACCESS_U) {
-        return; //no need to update currentInfo.reachingDefs
-    }
-    currentInfo.num_reaching_defs = 1;
-    currentInfo.reachingDefs[0].regNum = currentInfo.regNum;
-    currentInfo.reachingDefs[0].physicalType = currentInfo.physicalType;
-    currentInfo.reachingDefs[0].offsetPC = offsetPC;
-    currentInfo.reachingDefs[0].accessType = REGACCESS_D;
-}
-
-/** update defUseTable by checking currentInfo
-*/
-void updateDefUseTable (VirtualRegInfo &currentInfo)
-{
-    /* no access */
-    if(currentInfo.accessType == REGACCESS_N) return;
-    /* define then use, or define only */
-    if(currentInfo.accessType == REGACCESS_DU || currentInfo.accessType == REGACCESS_D) {
-        /* insert a definition at offsetPC to variable @ currentInfo */
-        DefUsePair* ptr = insertADef(currentBB, offsetPC, currentInfo.regNum, currentInfo.physicalType, REGACCESS_D);
-        if(currentInfo.accessType != REGACCESS_D) {
-             /* if access is define then use, insert a use at offsetPC */
-            insertAUse(ptr, offsetPC, currentInfo.regNum, currentInfo.physicalType);
-        }
-        return;
-    }
-    /* use only or use then define
-       check the reaching defs for the usage */
-    int k;
-    bool isLCovered = false, isHCovered = false, isDCovered = false;
-    for(k = 0; k < currentInfo.num_reaching_defs; k++) {
-        /* insert a def currentInfo.reachingDefs[k] and a use of variable at offsetPC */
-        RegAccessType useType = insertDefUsePair (currentInfo, k);
-        if(useType == REGACCESS_D) isDCovered = true;
-        if(useType == REGACCESS_L) isLCovered = true;
-        if(useType == REGACCESS_H) isHCovered = true;
-    }
-    OpndSize useSize = getRegSize(currentInfo.physicalType);
-    if((!isDCovered) && (!isLCovered)) {
-        /* the low half of variable is not defined in the basic block
-           so insert a def to the low half at START of the basic block */
-        insertDefUsePair(currentInfo, -1);
-    }
-    if(useSize == OpndSize_64 && (!isDCovered) && (!isHCovered)) {
-        /* the high half of variable is not defined in the basic block
-           so insert a def to the high half at START of the basic block */
-        insertDefUsePair(currentInfo, -2);
-    }
-    if(currentInfo.accessType == REGACCESS_UD) {
-        /* insert a def at offsetPC to variable @ currentInfo */
-        insertADef(currentBB, offsetPC, currentInfo.regNum, currentInfo.physicalType, REGACCESS_D);
-        return;
-    }
-}
-
-//! \brief insertAUse
-//! \details Insert a use at offsetPC of given variable at end of DefUsePair
-//! \param ptr The DefUsePair
-//! \param offsetPC
-//! \param regNum
-//! \param physicalType
-//! \return useType
-RegAccessType insertAUse(DefUsePair* ptr, int offsetPC, int regNum, LowOpndRegType physicalType) {
-    DefOrUseLink* tLink = (DefOrUseLink*)malloc(sizeof(DefOrUseLink));
-    if(tLink == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertAUse");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return REGACCESS_UNKNOWN;
-    }
-    tLink->offsetPC = offsetPC;
-    tLink->regNum = regNum;
-    tLink->physicalType = physicalType;
-    tLink->next = NULL;
-    if(ptr->useTail != NULL)
-        ptr->useTail->next = tLink;
-    ptr->useTail = tLink;
-    if(ptr->uses == NULL)
-        ptr->uses = tLink;
-    ptr->num_uses++;
-
-    //check whether the def is partially overlapping with the variable
-    OverlapCase isDefPartiallyOverlapB = getBPartiallyOverlapA(ptr->def.regNum,
-                                                       ptr->def.physicalType,
-                                                       regNum, physicalType);
-    RegAccessType useType = setAccessTypeOfUse(isDefPartiallyOverlapB, ptr->def.accessType);
-    tLink->accessType = useType;
-    return useType;
-}
-
-/**
- * @brief Insert a definition
- * @details insert a def to currentBB->defUseTable, update currentBB->defUseTail if necessary
- * @param bb the BasicBlock_O1
- * @param offsetPC the PC offset
- * @param regNum the register number
- * @param pType Physical type
- * @param rType Register access type
- * @return the new inserted DefUsePair
- */
-DefUsePair* insertADef (BasicBlock_O1 *bb, int offsetPC, int regNum, LowOpndRegType pType, RegAccessType rType)
-{
-    DefUsePair* ptr = (DefUsePair*)malloc(sizeof(DefUsePair));
-    if(ptr == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertADef");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return NULL;
-    }
-
-    //First initialize the information we keep for defuse
-    ptr->next = NULL;
-    ptr->def.offsetPC = offsetPC;
-    ptr->def.regNum = regNum;
-    ptr->def.physicalType = pType;
-    ptr->def.accessType = rType;
-    ptr->num_uses = 0;
-    ptr->useTail = NULL;
-    ptr->uses = NULL;
-
-    //Now add this to the end of our defUse chain
-    if (bb->defUseTail != NULL)
-    {
-        bb->defUseTail->next = ptr;
-    }
-
-    bb->defUseTail = ptr;
-
-    //If this is the first entry, we must make the start point to it
-    if (bb->defUseTable == NULL)
-    {
-        bb->defUseTable = ptr;
-    }
-
-#ifdef DEBUG_REACHING_DEF
-    ALOGI("Insert a def at %d to defUseTable for VR %d %d", offsetPC,
-          regNum, pType);
-#endif
-    return ptr;
-}
-
-/** insert a def to defUseTable, then insert a use of variable @ currentInfo
-    if reachingDefIndex >= 0, the def is currentInfo.reachingDefs[index]
-    if reachingDefIndex is -1, the low half is defined at START of the basic block
-    if reachingDefIndex is -2, the high half is defined at START of the basic block
-*/
-RegAccessType insertDefUsePair (VirtualRegInfo &currentInfo, int reachingDefIndex)
-{
-    int k = reachingDefIndex;
-    DefUsePair* tableIndex = NULL;
-    DefOrUse theDef;
-    theDef.regNum = 0;
-    if(k < 0) {
-        /* def at start of the basic blcok */
-        theDef.offsetPC = PC_FOR_START_OF_BB;
-        theDef.accessType = REGACCESS_D;
-        if(k == -1) //low half of variable
-            theDef.regNum = currentInfo.regNum;
-        if(k == -2) //high half of variable
-            theDef.regNum = currentInfo.regNum+1;
-        theDef.physicalType = LowOpndRegType_gp;
-    }
-    else {
-        theDef = currentInfo.reachingDefs[k];
-    }
-    tableIndex = searchDefUseTable(theDef.offsetPC, theDef.regNum, theDef.physicalType);
-    if(tableIndex == NULL) //insert an entry
-        tableIndex = insertADef(currentBB, theDef.offsetPC, theDef.regNum, theDef.physicalType, theDef.accessType);
-    else
-        tableIndex->def.accessType = theDef.accessType;
-    RegAccessType useType = insertAUse(tableIndex, offsetPC, currentInfo.regNum, currentInfo.physicalType);
-    return useType;
-}
-
-/* @brief insert a XFER_MEM_TO_XMM to currentBB->xferPoints
- *
- * @params offset offsetPC of the transfer location
- * @params regNum Register number
- * @params pType Physical type of the reg
- *
- * @return -1 if error occurred, 0 otherwise
- */
-static int insertLoadXfer(int offset, int regNum, LowOpndRegType pType) {
-    //check whether it is already in currentBB->xferPoints
-    unsigned int k, max;
-    max = currentBB->xferPoints.size ();
-    for(k = 0; k < max; k++) {
-        if(currentBB->xferPoints[k].xtype == XFER_MEM_TO_XMM &&
-           currentBB->xferPoints[k].offsetPC == offset &&
-           currentBB->xferPoints[k].regNum == regNum &&
-           currentBB->xferPoints[k].physicalType == pType)
-            return 0;
-    }
-
-    //We are going to create a new one
-    XferPoint point;
-    point.xtype = XFER_MEM_TO_XMM;
-    point.regNum = regNum;
-    point.offsetPC = offset;
-    point.physicalType = pType;
-#ifdef DEBUG_XFER_POINTS
-    ALOGI("Insert to xferPoints %d: XFER_MEM_TO_XMM of VR %d %d at %d", max, regNum, pType, offset);
-#endif
-
-    //Insert new point
-    currentBB->xferPoints.push_back (point);
-
-    //Paranoid
-    if(max + 1 >= MAX_XFER_PER_BB) {
-        ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", max + 1, MAX_XFER_PER_BB);
-        SET_JIT_ERROR(kJitErrorMaxXferPoints);
-        return -1;
-    }
-    return 0;
-}
-
-/** update defUseTable by assuming a fake usage at END of a basic block for variable @ currentInfo
-    create a fake usage at end of a basic block for variable B (currentInfo.physicalType, currentInfo.regNum)
-    get reaching def info for variable B and store the info in currentInfo.reachingDefs
-        for each virtual register (variable A) accessed in the basic block
-            update reaching defs of B by checking reaching defs of variable A
-    update defUseTable
-*/
-int fakeUsageAtEndOfBB (BasicBlock_O1* bb, int vR, int physicalAndLogicalType)
-{
-    VirtualRegInfo currentInfo;
-
-    //Get the register number
-    currentInfo.regNum = vR;
-
-    //TODO The cast here is invalid because it creates an invalid LowOpndRegType.
-    //However, this invalidity seems to be required for properly creating usedef chains.
-    currentInfo.physicalType = static_cast<LowOpndRegType> (physicalAndLogicalType);
-
-    currentInfo.accessType = REGACCESS_U;
-    LowOpndRegType typeB = currentInfo.physicalType;
-    int regB = currentInfo.regNum;
-    unsigned int jj;
-    int k;
-    currentInfo.num_reaching_defs = 0;
-    unsigned int max = bb->infoBasicBlock.size ();
-    for(jj = 0; jj < max; jj++) {
-        int regA = bb->infoBasicBlock[jj].regNum;
-        LowOpndRegType typeA = bb->infoBasicBlock[jj].physicalType;
-        OverlapCase isBPartiallyOverlapA = getBPartiallyOverlapA(regB, typeB, regA, typeA);
-        if(regA == regB && typeA == typeB) {
-            /* copy reachingDefs from variable A */
-            currentInfo.num_reaching_defs = bb->infoBasicBlock[jj].num_reaching_defs;
-            for(k = 0; k < currentInfo.num_reaching_defs; k++)
-                currentInfo.reachingDefs[k] = bb->infoBasicBlock[jj].reachingDefs[k];
-            break;
-        }
-        else if(isBPartiallyOverlapA != OVERLAP_NO) {
-            VirtualRegInfo tmpInfo;
-
-            /* B overlaps with A */
-            /* update reaching defs of variable B by checking reaching defs of bb->infoBasicBlock[jj] */
-            int retCode = updateReachingDefB1 (currentInfo, tmpInfo, jj);
-            if (retCode < 0)
-                return retCode;
-            retCode = updateReachingDefB2 (currentInfo, tmpInfo); //merge currentInfo with tmpInfo
-            if (retCode < 0)
-                return retCode;
-        }
-    }
-    /* update defUseTable by checking currentInfo */
-    updateDefUseTable (currentInfo);
-    return 0;
-}
-
-/**
- * @brief Update xferPoints of currentBB
- * @return -1 on error, 0 otherwise
-*/
-int updateXferPoints (BasicBlock_O1 *bb)
-{
-    //First clear the XferPoints
-    bb->xferPoints.clear ();
-
-    //Get a local version of defUseTable
-    DefUsePair* ptr = bb->defUseTable;
-
-    /* Traverse the def-use chain of the basic block */
-    while(ptr != NULL) {
-        LowOpndRegType defType = ptr->def.physicalType;
-        //if definition is for a variable of 32 bits
-        if(getRegSize(defType) == OpndSize_32) {
-            /* check usages of the definition, whether it reaches a GPR, a XMM, a FS, or a SS */
-            bool hasGpUsage = false;
-            bool hasGpUsage2 = false; //not a fake usage
-            bool hasXmmUsage = false;
-            bool hasFSUsage = false;
-            bool hasSSUsage = false;
-
-            //Get the uses
-            DefOrUseLink* ptrUse = ptr->uses;
-            while(ptrUse != NULL) {
-                if(ptrUse->physicalType == LowOpndRegType_gp) {
-                    hasGpUsage = true;
-                    if(ptrUse->offsetPC != PC_FOR_END_OF_BB)
-                        hasGpUsage2 = true;
-                }
-                if(ptrUse->physicalType == LowOpndRegType_ss) hasSSUsage = true;
-                if(ptrUse->physicalType == LowOpndRegType_fs ||
-                   ptrUse->physicalType == LowOpndRegType_fs_s)
-                    hasFSUsage = true;
-                if(ptrUse->physicalType == LowOpndRegType_xmm) {
-                    hasXmmUsage = true;
-                }
-                if(ptrUse->physicalType == LowOpndRegType_xmm ||
-                   ptrUse->physicalType == LowOpndRegType_ss) {
-                    /* if a 32-bit definition reaches a xmm usage or a SS usage,
-                       insert a XFER_MEM_TO_XMM */
-                    int retCode = insertLoadXfer(ptrUse->offsetPC,
-                                   ptrUse->regNum, LowOpndRegType_xmm);
-                    if (retCode < 0)
-                        return retCode;
-                }
-                ptrUse = ptrUse->next;
-            }
-            if(((hasXmmUsage || hasFSUsage || hasSSUsage) && defType == LowOpndRegType_gp) ||
-               (hasGpUsage && defType == LowOpndRegType_fs) ||
-               (defType == LowOpndRegType_ss && (hasGpUsage || hasXmmUsage || hasFSUsage))) {
-                /* insert a transfer if def is on a GPR, usage is on a XMM, FS or SS
-                                     if def is on a FS, usage is on a GPR
-                                     if def is on a SS, usage is on a GPR, XMM or FS
-                   transfer type is XFER_DEF_TO_GP_MEM if a real GPR usage exisits
-                   transfer type is XFER_DEF_TO_GP otherwise*/
-                XferPoint point;
-                point.offsetPC = ptr->def.offsetPC;
-                point.regNum = ptr->def.regNum;
-                point.physicalType = ptr->def.physicalType;
-                if(hasGpUsage2) { //create an entry XFER_DEF_TO_GP_MEM
-                    point.xtype = XFER_DEF_TO_GP_MEM;
-                }
-                else { //create an entry XFER_DEF_TO_MEM
-                    point.xtype = XFER_DEF_TO_MEM;
-                }
-                point.tableIndex = 0;
-#ifdef DEBUG_XFER_POINTS
-                ALOGI("Insert XFER %d at def %d: V%d %d", bb->xferPoints.size (), ptr->def.offsetPC, ptr->def.regNum, defType);
-#endif
-
-                //Push new point
-                bb->xferPoints.push_back (point);
-
-                if(bb->xferPoints.size () >= MAX_XFER_PER_BB) {
-                    ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", bb->xferPoints.size (), MAX_XFER_PER_BB);
-                    SET_JIT_ERROR(kJitErrorMaxXferPoints);
-                    return -1;
-                }
-            }
-        }
-        else { /* def is on 64 bits */
-            bool hasGpUsageOfL = false; //exist a GPR usage of the low half
-            bool hasGpUsageOfH = false; //exist a GPR usage of the high half
-            bool hasGpUsageOfL2 = false;
-            bool hasGpUsageOfH2 = false;
-            bool hasMisaligned = false;
-            bool hasAligned = false;
-            bool hasFSUsage = false;
-            bool hasSSUsage = false;
-
-            //Get the uses
-            DefOrUseLink* ptrUse = ptr->uses;
-            while(ptrUse != NULL) {
-                if(ptrUse->physicalType == LowOpndRegType_gp &&
-                   ptrUse->regNum == ptr->def.regNum) {
-                    hasGpUsageOfL = true;
-                    if(ptrUse->offsetPC != PC_FOR_END_OF_BB)
-                        hasGpUsageOfL2 = true;
-                }
-                if(ptrUse->physicalType == LowOpndRegType_gp &&
-                   ptrUse->regNum == ptr->def.regNum + 1) {
-                    hasGpUsageOfH = true;
-                    if(ptrUse->offsetPC != PC_FOR_END_OF_BB)
-                        hasGpUsageOfH2 = true;
-                }
-                if(ptrUse->physicalType == LowOpndRegType_xmm &&
-                   ptrUse->regNum == ptr->def.regNum) {
-                    hasAligned = true;
-                    /* if def is on FS and use is on XMM, insert a XFER_MEM_TO_XMM */
-                    if(defType == LowOpndRegType_fs) {
-                        int retCode = insertLoadXfer(ptrUse->offsetPC,
-                                       ptrUse->regNum, LowOpndRegType_xmm);
-                        if (retCode < 0)
-                            return retCode;
-                    }
-                }
-                if(ptrUse->physicalType == LowOpndRegType_fs ||
-                   ptrUse->physicalType == LowOpndRegType_fs_s)
-                    hasFSUsage = true;
-                if(ptrUse->physicalType == LowOpndRegType_xmm &&
-                   ptrUse->regNum != ptr->def.regNum) {
-                    hasMisaligned = true;
-                    /* if use is on XMM and use and def are misaligned, insert a XFER_MEM_TO_XMM */
-                    int retCode = insertLoadXfer(ptrUse->offsetPC,
-                                   ptrUse->regNum, LowOpndRegType_xmm);
-                    if (retCode < 0)
-                        return retCode;
-                }
-                if(ptrUse->physicalType == LowOpndRegType_ss) {
-                    hasSSUsage = true;
-                    /* if use is on SS, insert a XFER_MEM_TO_XMM */
-                    int retCode = insertLoadXfer(ptrUse->offsetPC,
-                                   ptrUse->regNum, LowOpndRegType_ss);
-                    if (retCode < 0)
-                        return retCode;
-                }
-                ptrUse = ptrUse->next;
-            }
-            if(defType == LowOpndRegType_fs && !hasGpUsageOfL && !hasGpUsageOfH) {
-                ptr = ptr->next;
-                continue;
-            }
-            if(defType == LowOpndRegType_xmm && !hasFSUsage &&
-               !hasGpUsageOfL && !hasGpUsageOfH && !hasMisaligned && !hasSSUsage) {
-                ptr = ptr->next;
-                continue;
-            }
-            /* insert a XFER_DEF_IS_XMM */
-            XferPoint point;
-
-            point.regNum = ptr->def.regNum;
-            point.offsetPC = ptr->def.offsetPC;
-            point.physicalType = ptr->def.physicalType;
-            point.xtype = XFER_DEF_IS_XMM;
-            point.vr_gpl = -1;
-            point.vr_gph = -1;
-            if(hasGpUsageOfL2) {
-                point.vr_gpl = ptr->def.regNum;
-            }
-            if(hasGpUsageOfH2) {
-                point.vr_gph = ptr->def.regNum+1;
-            }
-            point.dumpToMem = true;
-            point.dumpToXmm = false; //not used in updateVirtualReg
-            if(hasAligned) {
-                point.dumpToXmm = true;
-            }
-            point.tableIndex = 0;
-#ifdef DEBUG_XFER_POINTS
-            ALOGI("Insert XFER %d at def %d: V%d %d", bb->xferPoints.size (), ptr->def.offsetPC, ptr->def.regNum, defType);
-#endif
-            //Push new point
-            bb->xferPoints.push_back (point);
-
-            if(bb->xferPoints.size () >= MAX_XFER_PER_BB) {
-                ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", bb->xferPoints.size (), MAX_XFER_PER_BB);
-                SET_JIT_ERROR(kJitErrorMaxXferPoints);
-                return -1;
-            }
-        }
-
-        //Get next pointer
-        ptr = ptr->next;
-    } //while ptr
-
-#ifdef DEBUG_XFER_POINTS
-    ALOGI("XFER points for current basic block ------");
-    unsigned int max = bb->xferPoints.size ();
-    for(unsigned int k = 0; k < max; k++) {
-        ALOGI("  at offset %x, VR %d %d: type %d, vr_gpl %d, vr_gph %d, dumpToMem %d, dumpToXmm %d",
-              bb->xferPoints[k].offsetPC, bb->xferPoints[k].regNum,
-              bb->xferPoints[k].physicalType, bb->xferPoints[k].xtype,
-              bb->xferPoints[k].vr_gpl, bb->xferPoints[k].vr_gph,
-              bb->xferPoints[k].dumpToMem, bb->xferPoints[k].dumpToXmm);
-    }
-#endif
-
-    //Report success
-    return 0;
-}
-
-/**
- * @brief Used to handle type transfer points at the start of the BB
- * @param bb The current basic block
- */
-void handleStartOfBBXferPoints (BasicBlock_O1 *bb)
-{
-    //Walk through the BB's transfer points
-    for (std::vector<XferPoint>::const_iterator xferIter = bb->xferPoints.begin (); xferIter != bb->xferPoints.end ();
-            xferIter++)
-    {
-        const XferPoint &transfer = *xferIter;
-
-        //If we have a transfer of VR defined at start of BB and the transfer involves storing back to memory
-        //then we handle it right now.
-        if (transfer.offsetPC == PC_FOR_START_OF_BB
-                && (transfer.xtype == XFER_DEF_TO_MEM
-                        || transfer.xtype == XFER_DEF_TO_GP_MEM
-                        || transfer.xtype == XFER_DEF_IS_XMM))
-        {
-            int vR = transfer.regNum;
-
-            //Look through compile table to find the physical register for this VR
-            for (CompileTable::iterator compileIter = compileTable.begin (); compileIter != compileTable.end ();
-                    compileIter++)
-            {
-                CompileTableEntry &compileEntry = *compileIter;
-
-                //We found what we're looking for when we find the VR we care about in a physical register
-                if (compileEntry.isVirtualReg () == true
-                        && compileEntry.getRegisterNumber () == vR
-                        && compileEntry.inPhysicalRegister( ) == true)
-                {
-                    //Write back the VR to memory
-                    writeBackVR (vR, compileEntry.getPhysicalType (), compileEntry.getPhysicalReg ());
-                }
-            }
-        }
-    }
-}
-
-/* @brief update memVRTable[].ranges by browsing the defUseTable
- *
- * @details each virtual register has a list of live ranges, and
- * each live range has a list of PCs that access the VR
- *
- * @return -1 if error happened, 0 otherwise
- */
-int updateLiveTable (BasicBlock_O1 *bb)
-{
-    int retCode = 0;
-    DefUsePair* ptr = bb->defUseTable;
-    while(ptr != NULL) {
-        bool updateUse = false;
-        if(ptr->num_uses == 0) {
-            ptr->num_uses = 1;
-            ptr->uses = (DefOrUseLink*)malloc(sizeof(DefOrUseLink));
-            if(ptr->uses == NULL) {
-                ALOGI("JIT_INFO: Memory allocation failed in updateLiveTable");
-                SET_JIT_ERROR(kJitErrorMallocFailed);
-                return -1;
-            }
-            ptr->uses->accessType = REGACCESS_D;
-            ptr->uses->regNum = ptr->def.regNum;
-            ptr->uses->offsetPC = ptr->def.offsetPC;
-            ptr->uses->physicalType = ptr->def.physicalType;
-            ptr->uses->next = NULL;
-            ptr->useTail = ptr->uses;
-            updateUse = true;
-        }
-        DefOrUseLink* ptrUse = ptr->uses;
-        while(ptrUse != NULL) {
-            RegAccessType useType = ptrUse->accessType;
-            if(useType == REGACCESS_L || useType == REGACCESS_D) {
-                int indexL = searchMemTable(ptrUse->regNum);
-                if(indexL >= 0) {
-                    retCode = mergeLiveRange(indexL, ptr->def.offsetPC,
-                                   ptrUse->offsetPC); //tableIndex, start PC, end PC
-                    if (retCode < 0)
-                        return retCode;
-                }
-            }
-            if(getRegSize(ptrUse->physicalType) == OpndSize_64 &&
-               (useType == REGACCESS_H || useType == REGACCESS_D)) {
-                int indexH = searchMemTable(ptrUse->regNum+1);
-                if(indexH >= 0) {
-                    retCode = mergeLiveRange(indexH, ptr->def.offsetPC,
-                                   ptrUse->offsetPC);
-                    if (retCode < 0)
-                        return retCode;
-                }
-            }
-            ptrUse = ptrUse->next;
-        }//while ptrUse
-        if(updateUse) {
-            ptr->num_uses = 0;
-            free(ptr->uses);
-            ptr->uses = NULL;
-            ptr->useTail = NULL;
-        }
-        ptr = ptr->next;
-    }//while ptr
-#ifdef DEBUG_LIVE_RANGE
-    ALOGI("LIVE TABLE");
-    for(int k = 0; k < num_memory_vr; k++) {
-        ALOGI("VR %d live ", memVRTable[k].regNum);
-        LiveRange* ptr = memVRTable[k].ranges;
-        while(ptr != NULL) {
-            ALOGI("[%x %x] (", ptr->start, ptr->end);
-            for(int k3 = 0; k3 < ptr->num_access; k3++)
-                ALOGI("%x ", ptr->accessPC[k3]);
-            ALOGI(") ");
-            ptr = ptr->next;
-        }
-        ALOGI("");
-    }
-#endif
-    return 0;
-}
-
-/* @brief Add a live range [rangeStart, rangeEnd] to ranges of memVRTable,
- * merge to existing live ranges if necessary
- *
- * @details ranges are in increasing order of startPC
- *
- * @param tableIndex index into memVRTable
- * @param rangeStart start of live range
- * @param rangeEnd end of live range
- *
- * @return -1 if error, 0 otherwise
- */
-static int mergeLiveRange(int tableIndex, int rangeStart, int rangeEnd) {
-    if(rangeStart == PC_FOR_START_OF_BB) rangeStart = currentBB->pc_start;
-    if(rangeEnd == PC_FOR_END_OF_BB) rangeEnd = currentBB->pc_end;
-#ifdef DEBUG_LIVE_RANGE
-    ALOGI("LIVERANGE call mergeLiveRange on tableIndex %d with [%x %x]", tableIndex, rangeStart, rangeEnd);
-#endif
-    int startIndex = -1, endIndex = -1;
-    bool startBeforeRange = false, endBeforeRange = false; //before the index or in the range
-    bool startDone = false, endDone = false;
-    LiveRange* ptr = memVRTable[tableIndex].ranges;
-    LiveRange* ptrStart = NULL;
-    LiveRange* ptrStart_prev = NULL;
-    LiveRange* ptrEnd = NULL;
-    LiveRange* ptrEnd_prev = NULL;
-    int k = 0;
-    while(ptr != NULL) {
-        if(!startDone) {
-            if(ptr->start <= rangeStart &&
-               ptr->end >= rangeStart) {
-                startIndex = k;
-                ptrStart = ptr;
-                startBeforeRange = false;
-                startDone = true;
-            }
-            else if(ptr->start > rangeStart) {
-                startIndex = k;
-                ptrStart = ptr;
-                startBeforeRange = true;
-                startDone = true;
-            }
-        }
-        if(!startDone) ptrStart_prev = ptr;
-        if(!endDone) {
-            if(ptr->start <= rangeEnd &&
-               ptr->end >= rangeEnd) {
-                endIndex = k;
-                ptrEnd = ptr;
-                endBeforeRange = false;
-                endDone = true;
-            }
-            else if(ptr->start > rangeEnd) {
-                endIndex = k;
-                ptrEnd = ptr;
-                endBeforeRange = true;
-                endDone = true;
-            }
-        }
-        if(!endDone) ptrEnd_prev = ptr;
-        ptr = ptr->next;
-        k++;
-    } //while
-    if(!startDone) { //both can be NULL
-        startIndex = memVRTable[tableIndex].num_ranges;
-        ptrStart = NULL; //ptrStart_prev should be the last live range
-        startBeforeRange = true;
-    }
-    //if endDone, ptrEnd is not NULL, ptrEnd_prev can be NULL
-    if(!endDone) { //both can be NULL
-        endIndex = memVRTable[tableIndex].num_ranges;
-        ptrEnd = NULL;
-        endBeforeRange = true;
-    }
-    if(startIndex == endIndex && startBeforeRange && endBeforeRange) { //insert at startIndex
-        //3 cases depending on BeforeRange when startIndex == endIndex
-        //insert only if both true
-        //merge otherwise
-        /////////// insert before ptrStart
-        LiveRange* currRange = (LiveRange *)malloc(sizeof(LiveRange));
-        if(ptrStart_prev == NULL) {
-            currRange->next = memVRTable[tableIndex].ranges;
-            memVRTable[tableIndex].ranges = currRange;
-        } else {
-            currRange->next = ptrStart_prev->next;
-            ptrStart_prev->next = currRange;
-        }
-        currRange->start = rangeStart;
-        currRange->end = rangeEnd;
-        currRange->accessPC = (int *)malloc(sizeof(int) * NUM_ACCESS_IN_LIVERANGE);
-        currRange->num_alloc = NUM_ACCESS_IN_LIVERANGE;
-        if(rangeStart != rangeEnd) {
-            currRange->num_access = 2;
-            currRange->accessPC[0] = rangeStart;
-            currRange->accessPC[1] = rangeEnd;
-        } else {
-            currRange->num_access = 1;
-            currRange->accessPC[0] = rangeStart;
-        }
-        memVRTable[tableIndex].num_ranges++;
-#ifdef DEBUG_LIVE_RANGE
-        ALOGI("LIVERANGE insert one live range [%x %x] to tableIndex %d", rangeStart, rangeEnd, tableIndex);
-#endif
-        return 0;
-    }
-    if(!endBeforeRange) { //here ptrEnd is not NULL
-        endIndex++; //next
-        ptrEnd_prev = ptrEnd; //ptrEnd_prev is not NULL
-        ptrEnd = ptrEnd->next; //ptrEnd can be NULL
-    }
-
-    if(endIndex < startIndex+1) {
-        ALOGI("JIT_INFO: mergeLiveRange endIndex %d is less than startIndex %d\n", endIndex, startIndex);
-        SET_JIT_ERROR(kJitErrorMergeLiveRange);
-        return -1;
-    }
-    ///////// use ptrStart & ptrEnd_prev
-    if(ptrStart == NULL || ptrEnd_prev == NULL) {
-        ALOGI("JIT_INFO: mergeLiveRange ptr is NULL\n");
-        SET_JIT_ERROR(kJitErrorMergeLiveRange);
-        return -1;
-    }
-    //endIndex > startIndex (merge the ranges between startIndex and endIndex-1)
-    //update ptrStart
-    if(ptrStart->start > rangeStart)
-        ptrStart->start = rangeStart; //min of old start & rangeStart
-    ptrStart->end = ptrEnd_prev->end; //max of old end & rangeEnd
-    if(rangeEnd > ptrStart->end)
-        ptrStart->end = rangeEnd;
-#ifdef DEBUG_LIVE_RANGE
-    ALOGI("LIVERANGE merge entries for tableIndex %d from %d to %d", tableIndex, startIndex+1, endIndex-1);
-#endif
-    if(ptrStart->num_access <= 0) {
-        ALOGI("JIT_INFO: mergeLiveRange number of access");
-        SET_JIT_ERROR(kJitErrorMergeLiveRange);
-    }
-#ifdef DEBUG_LIVE_RANGE
-    ALOGI("LIVERANGE tableIndex %d startIndex %d num_access %d (", tableIndex, startIndex, ptrStart->num_access);
-    for(k = 0; k < ptrStart->num_access; k++)
-        ALOGI("%x ", ptrStart->accessPC[k]);
-    ALOGI(")");
-#endif
-    ///// go through pointers from ptrStart->next to ptrEnd
-    //from startIndex+1 to endIndex-1
-    ptr = ptrStart->next;
-    while(ptr != NULL && ptr != ptrEnd) {
-        int k2;
-        for(k2 = 0; k2 < ptr->num_access; k2++) { //merge to startIndex
-            insertAccess(tableIndex, ptrStart, ptr->accessPC[k2]);
-        }//k2
-        ptr = ptr->next;
-    }
-    insertAccess(tableIndex, ptrStart, rangeStart);
-    insertAccess(tableIndex, ptrStart, rangeEnd);
-    //remove startIndex+1 to endIndex-1
-    if(startIndex+1 < endIndex) {
-        ptr = ptrStart->next;
-        while(ptr != NULL && ptr != ptrEnd) {
-            LiveRange* tmpP = ptr->next;
-            free(ptr->accessPC);
-            free(ptr);
-            ptr = tmpP;
-        }
-        ptrStart->next = ptrEnd;
-    }
-    memVRTable[tableIndex].num_ranges -= (endIndex - startIndex - 1);
-#ifdef DEBUG_LIVE_RANGE
-    ALOGI("num_ranges for VR %d: %d", memVRTable[tableIndex].regNum, memVRTable[tableIndex].num_ranges);
-#endif
-    return 0;
-}
-
-//! insert an access to a given live range, in order
-
-//!
-void insertAccess(int tableIndex, LiveRange* startP, int rangeStart) {
-    int k3, k4;
-#ifdef DEBUG_LIVE_RANGE
-    ALOGI("LIVERANGE insertAccess %d %x", tableIndex, rangeStart);
-#endif
-    int insertIndex = -1;
-    for(k3 = 0; k3 < startP->num_access; k3++) {
-        if(startP->accessPC[k3] == rangeStart) {
-            return;
-        }
-        if(startP->accessPC[k3] > rangeStart) {
-            insertIndex = k3;
-            break;
-        }
-    }
-
-    //insert here
-    k3 = insertIndex;
-    if(insertIndex == -1) {
-        k3 = startP->num_access;
-    }
-    if(startP->num_access == startP->num_alloc) {
-        int currentAlloc = startP->num_alloc;
-        startP->num_alloc += NUM_ACCESS_IN_LIVERANGE;
-        int* tmpPtr = (int *)malloc(sizeof(int) * startP->num_alloc);
-        for(k4 = 0; k4 < currentAlloc; k4++)
-            tmpPtr[k4] = startP->accessPC[k4];
-        free(startP->accessPC);
-        startP->accessPC = tmpPtr;
-    }
-    //insert accessPC
-    for(k4 = startP->num_access-1; k4 >= k3; k4--)
-        startP->accessPC[k4+1] = startP->accessPC[k4];
-    startP->accessPC[k3] = rangeStart;
-#ifdef DEBUG_LIVE_RANGE
-    ALOGI("LIVERANGE insert %x to tableIndex %d", rangeStart, tableIndex);
-#endif
-    startP->num_access++;
-    return;
-}
-
-/////////////////////////////////////////////////////////////////////
-bool isVRLive(int vA);
-int getSpillIndex (OpndSize size);
-void clearVRToMemory(int regNum, OpndSize size);
-void clearVRNullCheck(int regNum, OpndSize size);
-
-inline int getSpillLocDisp(int offset) {
-#ifdef SPILL_IN_THREAD
-    return offset+offsetof(Thread, spillRegion);;
-#else
-    return offset+offEBP_spill;
-#endif
-}
-#if 0
-/* used if we keep self pointer in a physical register */
-inline int getSpillLocReg(int offset) {
-    return PhysicalReg_Glue;
-}
-#endif
-#ifdef SPILL_IN_THREAD
-inline void loadFromSpillRegion_with_self(OpndSize size, int reg_self, bool selfPhysical, int reg, int offset) {
-    /* only 1 instruction is generated by move_mem_to_reg_noalloc */
-    move_mem_to_reg_noalloc(size,
-                            getSpillLocDisp(offset), reg_self, selfPhysical,
-                            MemoryAccess_SPILL, offset,
-                            reg, true);
-}
-inline void loadFromSpillRegion(OpndSize size, int reg, int offset) {
-    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
-    int reg_self = registerAlloc(LowOpndRegType_scratch, C_SCRATCH_1, isScratchPhysical, false, true);
-    /* only 1 instruction is generated by move_mem_to_reg_noalloc */
-    move_mem_to_reg_noalloc(size,
-                            getSpillLocDisp(offset), reg_self, true,
-                            MemoryAccess_SPILL, offset,
-                            reg, true);
-}
-inline void saveToSpillRegion_with_self(OpndSize size, int selfReg, bool selfPhysical, int reg, int offset) {
-    move_reg_to_mem_noalloc(size,
-                            reg, true,
-                            getSpillLocDisp(offset), selfReg, selfPhysical,
-                            MemoryAccess_SPILL, offset);
-}
-inline void saveToSpillRegion(OpndSize size, int reg, int offset) {
-    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
-    int reg_self = registerAlloc(LowOpndRegType_scratch, C_SCRATCH_1, isScratchPhysical, false);
-    move_reg_to_mem_noalloc(size,
-                            reg, true,
-                            getSpillLocDisp(offset), reg_self, true,
-                            MemoryAccess_SPILL, offset);
-}
-#else
-inline void loadFromSpillRegion(OpndSize size, int reg, int offset) {
-    /* only 1 instruction is generated by move_mem_to_reg_noalloc */
-    move_mem_to_reg_noalloc(size,
-                            getSpillLocDisp(offset), PhysicalReg_EBP, true,
-                            MemoryAccess_SPILL, offset,
-                            reg, true);
-}
-inline void saveToSpillRegion(OpndSize size, int reg, int offset) {
-    move_reg_to_mem_noalloc(size,
-                            reg, true,
-                            getSpillLocDisp(offset), PhysicalReg_EBP, true,
-                            MemoryAccess_SPILL, offset);
-}
-#endif
-
-/**
- * @brief If VR is dirty, it writes the constant value to the VR on stack
- * @details If VR is wide, this function should be called separately twice,
- * once with the low bits and once with the high bits.
- * @param vR virtual register
- * @param value constant value of the VR
- */
-void writeBackConstVR(int vR, int value) {
-    // If VR is already in memory, we do not need to write it back
-    if(isInMemory(vR, OpndSize_32)) {
-        DEBUG_SPILL(ALOGD("Skip dumpImmToMem v%d size %d", vR, size));
-        return;
-    }
-
-    // Do the actual move to set constant to VR in stack
-    set_VR_to_imm_noalloc(vR, OpndSize_32, value);
-
-    // Mark the VR as in memory now
-    setVRMemoryState(vR, OpndSize_32, true);
-}
-
-/**
- * @brief Writes back VR to memory if dirty.
- * @param vR virtual register
- * @param type physical type as noted by compile table
- * @param physicalReg physical register
- */
-void writeBackVR(int vR, LowOpndRegType type, int physicalReg) {
-    int physicalType = type & MASK_FOR_TYPE;
-
-    // Paranoid check because we only handle writing back if in either
-    // GP or XMM registers
-    assert((physicalReg >= PhysicalReg_StartOfGPMarker &&
-            physicalReg <= PhysicalReg_EndOfGPMarker) ||
-            (physicalReg >= PhysicalReg_StartOfXmmMarker &&
-                    physicalReg <= PhysicalReg_EndOfXmmMarker));
-
-    // If VR is already in memory, we can skip writing it back
-    if (isInMemory(vR, getRegSize(physicalType))) {
-        DEBUG_SPILL(ALOGD("Skip writeBackVR v%d type %d", vR, physicalType));
-        return;
-    }
-
-    // Handle writing back different types of VRs
-    if (physicalType == LowOpndRegType_gp || physicalType == LowOpndRegType_xmm)
-        set_virtual_reg_noalloc(vR, getRegSize(physicalType), physicalReg,
-                true);
-    if (physicalType == LowOpndRegType_ss)
-        move_ss_reg_to_mem_noalloc(physicalReg, true, 4 * vR, PhysicalReg_FP,
-                true, MemoryAccess_VR, vR);
-
-    // Mark it in memory because we have written it back
-    setVRMemoryState (vR, getRegSize (physicalType), true);
-}
-//! dump part of a 64-bit VR to memory and update inMemory
-
-//! isLow tells whether low half or high half is dumped
-void dumpPartToMem(int reg /*xmm physical reg*/, int vA, bool isLow) {
-    if(isLow) {
-        if(isInMemory(vA, OpndSize_32)) {
-            DEBUG_SPILL(ALOGD("Skip dumpPartToMem isLow %d v%d", isLow, vA));
-            return;
-        }
-    }
-    else {
-        if(isInMemory(vA+1, OpndSize_32)) {
-            DEBUG_SPILL(ALOGD("Skip dumpPartToMem isLow %d v%d", isLow, vA));
-            return;
-        }
-    }
-    if(isLow) {
-        if(!isVRLive(vA)) return;
-    }
-    else {
-        if(!isVRLive(vA+1)) return;
-    }
-    //move part to vA or vA+1
-    if(isLow) {
-        move_ss_reg_to_mem_noalloc(reg, true,
-                                   4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
-    } else {
-        int k = getSpillIndex (OpndSize_64);
-        //H, L in 4*k+4 & 4*k
-#ifdef SPILL_IN_THREAD
-        get_self_pointer(PhysicalReg_SCRATCH_1, isScratchPhysical);
-        saveToSpillRegion_with_self(OpndSize_64, PhysicalReg_SCRATCH_1, isScratchPhysical, reg, 4*k);
-        //update low 32 bits of xmm reg from 4*k+4
-        move_ss_mem_to_reg(NULL,
-                                   getSpillLocDisp(4*k+4), PhysicalReg_SCRATCH_1, isScratchPhysical,
-                                   reg, true);
-#else
-        // right shift high half of xmm to low half xmm
-        dump_imm_reg_noalloc(Mnemonic_PSRLQ, OpndSize_64, 32, reg, true, LowOpndRegType_xmm);
-#endif
-        //move low 32 bits of xmm reg to vA+1
-        move_ss_reg_to_mem_noalloc(reg, true, 4*(vA+1), PhysicalReg_FP, true, MemoryAccess_VR, vA+1);
-    }
-
-    if (isLow)
-    {
-        setVRMemoryState (vA, OpndSize_32, true);
-    }
-    else
-    {
-        setVRMemoryState (vA + 1, OpndSize_32, true);
-    }
-}
-void clearVRBoundCheck(int regNum, OpndSize size);
-//! the content of a VR is no longer in memory or in physical register if the latest content of a VR is constant
-
-//! clear nullCheckDone; if another VR is overlapped with the given VR, the content of that VR is no longer in physical register
-void invalidateVRDueToConst(int reg, OpndSize size) {
-    clearVRToMemory(reg, size); //memory content is out-dated
-    clearVRNullCheck(reg, size);
-    clearVRBoundCheck(reg, size);
-    //check reg,gp reg,ss reg,xmm reg-1,xmm
-    //if size is 64: check reg+1,gp|ss reg+1,xmm
-    int index;
-    //if VR is xmm, check whether we need to dump part of VR to memory
-    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg);
-    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-        ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_xmm);
-#endif
-        if(size == OpndSize_32)
-            dumpPartToMem(compileTable[index].physicalReg, reg, false); //dump high of xmm to memory
-
-        compileTable[index].setPhysicalReg (PhysicalReg_Null);
-    }
-    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg-1);
-    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-        ALOGI("INVALIDATE virtual reg %d type %d", reg-1, LowOpndRegType_xmm);
-#endif
-        dumpPartToMem(compileTable[index].physicalReg, reg-1, true); //dump low of xmm to memory
-
-        compileTable[index].setPhysicalReg (PhysicalReg_Null);
-    }
-    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg);
-    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-        ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_gp);
-#endif
-
-        compileTable[index].setPhysicalReg (PhysicalReg_Null);
-    }
-    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg);
-    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-        ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_ss);
-#endif
-
-        compileTable[index].setPhysicalReg (PhysicalReg_Null);
-    }
-    if(size == OpndSize_64) {
-        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg+1);
-        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_xmm);
-#endif
-            dumpPartToMem(compileTable[index].physicalReg, reg+1, false); //dump high of xmm to memory
-
-            compileTable[index].setPhysicalReg (PhysicalReg_Null);
-        }
-        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg+1);
-        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_gp);
-#endif
-
-            compileTable[index].setPhysicalReg (PhysicalReg_Null);
-        }
-        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg+1);
-        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_ss);
-#endif
-
-            compileTable[index].setPhysicalReg (PhysicalReg_Null);
-        }
-    }
-}
-//! check which physical registers hold out-dated content if there is a def
-
-//! if another VR is overlapped with the given VR, the content of that VR is no longer in physical register
-//! should we update inMemory?
-void invalidateVR(int reg, LowOpndRegType pType) {
-    //def at fs: content of xmm & gp & ss are out-dated (reg-1,xmm reg,xmm reg+1,xmm) (reg,gp|ss reg+1,gp|ss)
-    //def at xmm: content of misaligned xmm & gp are out-dated (reg-1,xmm reg+1,xmm) (reg,gp|ss reg+1,gp|ss)
-    //def at fs_s: content of xmm & gp are out-dated (reg-1,xmm reg,xmm) (reg,gp|ss)
-    //def at gp:   content of xmm is out-dated (reg-1,xmm reg,xmm) (reg,ss)
-    //def at ss:   content of xmm & gp are out-dated (reg-1,xmm reg,xmm) (reg,gp)
-    int index;
-    if(pType != LowOpndRegType_xmm) { //check xmm @reg
-        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg);
-        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-            ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_xmm);
-#endif
-            if(getRegSize(pType) == OpndSize_32)
-                dumpPartToMem(compileTable[index].physicalReg, reg, false); //dump high of xmm to memory
-
-            compileTable[index].setPhysicalReg (PhysicalReg_Null);
-        }
-    }
-    //check misaligned xmm @ reg-1
-    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg-1);
-    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-        ALOGI("INVALIDATE virtual reg %d type %d", reg-1, LowOpndRegType_xmm);
-#endif
-        dumpPartToMem(compileTable[index].physicalReg, reg-1, true); //dump low of xmm to memory
-
-        compileTable[index].setPhysicalReg (PhysicalReg_Null);
-    }
-    //check misaligned xmm @ reg+1
-    if(pType == LowOpndRegType_xmm || pType == LowOpndRegType_fs) {
-        //check reg+1,xmm
-        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg+1);
-        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_xmm);
-#endif
-            dumpPartToMem(compileTable[index].physicalReg, reg+1, false); //dump high of xmm to memory
-
-            compileTable[index].setPhysicalReg (PhysicalReg_Null);
-        }
-    }
-    if(pType != LowOpndRegType_gp) {
-        //check reg,gp
-        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg);
-        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-            ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_gp);
-#endif
-
-            compileTable[index].setPhysicalReg (PhysicalReg_Null);
-        }
-    }
-    if(pType == LowOpndRegType_xmm || pType == LowOpndRegType_fs) {
-        //check reg+1,gp
-        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg+1);
-        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_gp);
-#endif
-
-            compileTable[index].setPhysicalReg (PhysicalReg_Null);
-        }
-    }
-    if(pType != LowOpndRegType_ss) {
-        //check reg,ss
-        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg);
-        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-            ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_ss);
-#endif
-
-            compileTable[index].setPhysicalReg (PhysicalReg_Null);
-        }
-    }
-    if(pType == LowOpndRegType_xmm || pType == LowOpndRegType_fs) {
-        //check reg+1,ss
-        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg+1);
-        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_INVALIDATE
-            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_ss);
-#endif
-
-            compileTable[index].setPhysicalReg (PhysicalReg_Null);
-        }
-    }
-}
-//! bookkeeping when a VR is updated
-
-//! invalidate contents of some physical registers, clear nullCheckDone, and update inMemory;
-//! check whether there exist tranfer points for this bytecode, if yes, perform the transfer
-int updateVirtualReg(int reg, LowOpndRegType pType) {
-    OpndSize size = getRegSize(pType);
-    //WAS only invalidate xmm VRs for the following cases:
-    //if def reaches a use of vA,xmm and (the def is not xmm or is misaligned xmm)
-    //  invalidate "vA,xmm"
-    invalidateVR(reg, pType);
-    clearVRNullCheck(reg, size);
-    clearVRBoundCheck(reg, size);
-    if (pType == LowOpndRegType_fs || pType == LowOpndRegType_fs_s)
-    {
-        setVRMemoryState (reg, size, true);
-    }
-    else
-    {
-        clearVRToMemory (reg, size);
-    }
-
-    unsigned int max = currentBB->xferPoints.size ();
-    for(unsigned int k = 0; k < max; k++) {
-        if(currentBB->xferPoints[k].offsetPC == offsetPC &&
-           currentBB->xferPoints[k].regNum == reg &&
-           currentBB->xferPoints[k].physicalType == pType &&
-           currentBB->xferPoints[k].xtype != XFER_MEM_TO_XMM) {
-            //perform the corresponding action for the def
-            PhysicalReg regAll;
-            if(currentBB->xferPoints[k].xtype == XFER_DEF_IS_XMM) {
-                //def at fs: content of xmm is out-dated
-                //def at xmm: content of misaligned xmm is out-dated
-                //invalidateXmmVR(currentBB->xferPoints[k].tableIndex);
-#ifdef DEBUG_XFER_POINTS
-                if(currentBB->xferPoints[k].dumpToXmm)
-                    ALOGI("XFER set_virtual_reg to xmm: xmm VR %d", reg);
-#endif
-                if(pType == LowOpndRegType_xmm)  {
-#ifdef DEBUG_XFER_POINTS
-                    ALOGI("XFER set_virtual_reg to memory: xmm VR %d", reg);
-#endif
-                    PhysicalReg regAll = (PhysicalReg)checkVirtualReg(reg, LowOpndRegType_xmm, 0 /* do not update*/);
-                    writeBackVR(reg, LowOpndRegType_xmm, regAll);
-                }
-                if(currentBB->xferPoints[k].vr_gpl >= 0) { //
-                }
-                if(currentBB->xferPoints[k].vr_gph >= 0) {
-                }
-            }
-            if((pType == LowOpndRegType_gp || pType == LowOpndRegType_ss) &&
-               (currentBB->xferPoints[k].xtype == XFER_DEF_TO_MEM ||
-                currentBB->xferPoints[k].xtype == XFER_DEF_TO_GP_MEM)) {
-                //the defined gp VR already in register
-                //invalidateXmmVR(currentBB->xferPoints[k].tableIndex);
-                regAll = (PhysicalReg)checkVirtualReg(reg, pType, 0 /* do not update*/);
-                writeBackVR(reg, pType, regAll);
-#ifdef DEBUG_XFER_POINTS
-                ALOGI("XFER set_virtual_reg to memory: gp VR %d", reg);
-#endif
-            }
-            if((pType == LowOpndRegType_fs_s || pType == LowOpndRegType_ss) &&
-               currentBB->xferPoints[k].xtype == XFER_DEF_TO_GP_MEM) {
-            }
-        }
-    }
-    return 0;
-}
-////////////////////////////////////////////////////////////////
-//REGISTER ALLOCATION
-int spillForHardReg(int regNum, int type);
-void decreaseRefCount(int index);
-int getFreeReg(int type, int reg, int indexToCompileTable);
-PhysicalReg spillForLogicalReg(int type, int reg, int indexToCompileTable);
-int unspillLogicalReg(int spill_index, int physicalReg);
-int searchVirtualInfoOfBB(LowOpndRegType type, int regNum, BasicBlock_O1* bb);
-bool isTemp8Bit(int type, int reg);
-bool matchType(int typeA, int typeB);
-int getNextAccess(int compileIndex);
-void dumpCompileTable();
-
-//! allocate a register for a variable
-
-//!if no physical register is free, call spillForLogicalReg to free up a physical register;
-//!if the variable is a temporary and it was spilled, call unspillLogicalReg to load from spill location to the allocated physical register;
-//!if updateRefCount is true, reduce reference count of the variable by 1
-//!if isDest is true, we inform the compileTable about it
-int registerAlloc(int type, int reg, bool isPhysical, bool updateRefCount, bool isDest) {
-#ifdef DEBUG_REGALLOC
-    ALOGI("%p: try to allocate register %d type %d isPhysical %d", currentBB, reg, type, isPhysical);
-#endif
-    if(currentBB == NULL) {
-        if(type & LowOpndRegType_virtual) {
-            return PhysicalReg_Null;
-        }
-        if(isPhysical) return reg; //for helper functions
-        return PhysicalReg_Null;
-    }
-    //ignore EDI, ESP, EBP (glue)
-    if(isPhysical && (reg == PhysicalReg_EDI || reg == PhysicalReg_ESP ||
-                      reg == PhysicalReg_EBP || reg == PhysicalReg_Null))
-        return reg;
-
-    int newType = convertType(type, reg, isPhysical);
-    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
-    int tIndex = searchCompileTable(newType, reg);
-    if(tIndex < 0) {
-      ALOGI("JIT_INFO: reg %d type %d not found in registerAlloc\n", reg, newType);
-      SET_JIT_ERROR(kJitErrorRegAllocFailed);
-      return PhysicalReg_Null;
-    }
-
-    //physical register
-    if(isPhysical) {
-        if(allRegs[reg].isUsed) { //if used by a non hard-coded register
-            spillForHardReg(reg, newType);
-        }
-        allRegs[reg].isUsed = true;
-#ifdef DEBUG_REG_USED
-        ALOGI("REGALLOC: allocate a reg %d", reg);
-#endif
-
-        //Update the physical register
-        compileTable[tIndex].setPhysicalReg (reg);
-        //Update the isWritten field, if isDest is true, set it to true
-        if (isDest == true) {
-            compileTable[tIndex].isWritten = true;
-        }
-
-        if(updateRefCount)
-            decreaseRefCount(tIndex);
-#ifdef DEBUG_REGALLOC
-        ALOGI("REGALLOC: allocate register %d for logical register %d %d",
-               compileTable[tIndex].physicalReg, reg, newType);
-#endif
-        return reg;
-    }
-    //already allocated
-    if(compileTable[tIndex].physicalReg != PhysicalReg_Null) {
-#ifdef DEBUG_REGALLOC
-        ALOGI("already allocated to physical register %d", compileTable[tIndex].physicalReg);
-#endif
-        //Update the isWritten field, if isDest is true, set it to true
-        if (isDest == true) {
-            compileTable[tIndex].isWritten = true;
-        }
-        if(updateRefCount)
-            decreaseRefCount(tIndex);
-        return compileTable[tIndex].physicalReg;
-    }
-
-    //at this point, the logical register is not hard-coded and is mapped to Reg_Null
-    //first check whether there is a free reg
-    //if not, call spillForLogicalReg
-    int index = getFreeReg(newType, reg, tIndex);
-    if(index >= 0 && index < PhysicalReg_Null) {
-        //update compileTable & allRegs
-        compileTable[tIndex].setPhysicalReg (allRegs[index].physicalReg);
-        allRegs[index].isUsed = true;
-#ifdef DEBUG_REG_USED
-        ALOGI("REGALLOC: register %d is free", allRegs[index].physicalReg);
-#endif
-    } else {
-        PhysicalReg allocR = spillForLogicalReg(newType, reg, tIndex);
-        compileTable[tIndex].setPhysicalReg (allocR);
-    }
-    if(compileTable[tIndex].spill_loc_index >= 0) {
-        unspillLogicalReg(tIndex, compileTable[tIndex].physicalReg);
-    }
-
-    //In this case, it's a new register, set isWritten to isDest
-    compileTable[tIndex].isWritten = isDest;
-    if(updateRefCount)
-        decreaseRefCount(tIndex);
-#ifdef DEBUG_REGALLOC
-    ALOGI("REGALLOC: allocate register %d for logical register %d %d",
-           compileTable[tIndex].physicalReg, reg, newType);
-#endif
-    return compileTable[tIndex].physicalReg;
-}
-//!a variable will use a physical register allocated for another variable
-
-//!This is used when MOVE_OPT is on, it tries to alias a virtual register with a temporary to remove a move
-int registerAllocMove(int reg, int type, bool isPhysical, int srcReg, bool isDest) {
-    if(srcReg == PhysicalReg_EDI || srcReg == PhysicalReg_ESP || srcReg == PhysicalReg_EBP) {
-        ALOGI("JIT_INFO: Cannot move from srcReg EDI or ESP or EBP");
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-#ifdef DEBUG_REGALLOC
-    ALOGI("in registerAllocMove: reg %d type %d srcReg %d", reg, type, srcReg);
-#endif
-    int newType = convertType(type, reg, isPhysical);
-    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
-    int index = searchCompileTable(newType, reg);
-    if(index < 0) {
-        ALOGI("JIT_INFO: reg %d type %d not found in registerAllocMove", reg, newType);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-
-    //Update the isWritten field, if isDest is true, set it to true
-    if (isDest == true) {
-        compileTable[index].isWritten = true;
-    }
-    decreaseRefCount(index);
-    compileTable[index].setPhysicalReg (srcReg);
-
-#ifdef DEBUG_REGALLOC
-    ALOGI("REGALLOC: registerAllocMove %d for logical register %d %d",
-           compileTable[index].physicalReg, reg, newType);
-#endif
-    return srcReg;
-}
-
-//! check whether a physical register is available to be used by a variable
-
-//! data structures accessed:
-//! 1> currentBB->infoBasicBlock[index].allocConstraintsSorted
-//!    sorted from high count to low count
-//! 2> currentBB->allocConstraintsSorted
-//!    sorted from low count to high count
-//! 3> allRegs: whether a physical register is available, indexed by PhysicalReg
-//! NOTE: if a temporary variable is 8-bit, only %eax, %ebx, %ecx, %edx can be used
-int getFreeReg(int type, int reg, int indexToCompileTable) {
-    syncAllRegs();
-    /* handles requests for xmm or ss registers */
-    int k;
-    if(((type & MASK_FOR_TYPE) == LowOpndRegType_xmm) ||
-       ((type & MASK_FOR_TYPE) == LowOpndRegType_ss)) {
-        for(k = PhysicalReg_XMM0; k <= PhysicalReg_XMM7; k++) {
-            if(!allRegs[k].isUsed) return k;
-        }
-        return -1;
-    }
-#ifdef DEBUG_REGALLOC
-    ALOGI("USED registers: ");
-    for(k = 0; k < 8; k++)
-        ALOGI("%d used: %d time freed: %d callee-saveld: %d", k, allRegs[k].isUsed,
-             allRegs[k].freeTimeStamp, allRegs[k].isCalleeSaved);
-    ALOGI("");
-#endif
-
-    /* a VR is requesting a physical register */
-    if(isVirtualReg(type)) { //find a callee-saved register
-        int index = searchVirtualInfoOfBB((LowOpndRegType)(type & MASK_FOR_TYPE), reg, currentBB);
-        if(index < 0) {
-            ALOGI("JIT_INFO: VR %d %d not found in infoBasicBlock of currentBB %d (num of VRs %d)",
-                  reg, type, currentBB->id, currentBB->infoBasicBlock.size ());
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
-            return -1;
-        }
-
-        /* check allocConstraints for this VR,
-           return an available physical register with the highest constraint > 0 */
-        for(k = 0; k < 8; k++) {
-            if(currentBB->infoBasicBlock[index].allocConstraintsSorted[k].count == 0) break;
-            int regCandidateT = currentBB->infoBasicBlock[index].allocConstraintsSorted[k].physicalReg;
-            assert(regCandidateT < PhysicalReg_Null);
-            if(!allRegs[regCandidateT].isUsed) return regCandidateT;
-        }
-
-        /* WAS: return an available physical register with the lowest constraint
-           NOW: consider a new factor (freeTime) when there is a tie
-                if 2 available physical registers have the same number of constraints
-                choose the one with smaller free time stamp */
-        int currentCount = -1;
-        int index1 = -1;
-        int smallestTime = -1;
-        for(k = 0; k < 8; k++) {
-            int regCandidateT = currentBB->allocConstraintsSorted[k].physicalReg;
-            assert(regCandidateT < PhysicalReg_Null);
-            if(index1 >= 0 && currentBB->allocConstraintsSorted[k].count > currentCount)
-                break; //candidate has higher count than index1
-            if(!allRegs[regCandidateT].isUsed) {
-                if(index1 < 0) {
-                    index1 = k;
-                    currentCount = currentBB->allocConstraintsSorted[k].count;
-                    smallestTime = allRegs[regCandidateT].freeTimeStamp;
-                } else if(allRegs[regCandidateT].freeTimeStamp < smallestTime) {
-                    index1 = k;
-                    smallestTime = allRegs[regCandidateT].freeTimeStamp;
-                }
-            }
-        }
-        if(index1 >= 0) return currentBB->allocConstraintsSorted[index1].physicalReg;
-        return -1;
-    }
-    /* handle request from a temporary variable */
-    else {
-        bool is8Bit = isTemp8Bit(type, reg);
-
-        /* if the temporary variable is linked to a VR and
-              the VR is not yet allocated to any physical register */
-        int vr_num = compileTable[indexToCompileTable].getLinkedVR ();
-        if(vr_num >= 0) {
-            int index3 = searchCompileTable(LowOpndRegType_gp | LowOpndRegType_virtual, vr_num);
-            if(index3 < 0) {
-                ALOGI("JIT_INFO: Inavlid linkage VR for temporary register %d", vr_num);
-                SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
-                return -1;
-            }
-
-            if(compileTable[index3].physicalReg == PhysicalReg_Null) {
-                int index2 = searchVirtualInfoOfBB(LowOpndRegType_gp, vr_num, currentBB);
-                if(index2 < 0) {
-                    ALOGI("JIT_INFO: In tracing linkage to VR %d", vr_num);
-                    SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                    //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
-                    return -1;
-                }
-#ifdef DEBUG_REGALLOC
-                ALOGI("In getFreeReg for temporary reg %d, trace the linkage to VR %d",
-                     reg, vr_num);
-#endif
-
-                /* check allocConstraints on the VR
-                   return an available physical register with the highest constraint > 0
-                */
-                for(k = 0; k < 8; k++) {
-                    if(currentBB->infoBasicBlock[index2].allocConstraintsSorted[k].count == 0) break;
-                    int regCandidateT = currentBB->infoBasicBlock[index2].allocConstraintsSorted[k].physicalReg;
-#ifdef DEBUG_REGALLOC
-                    ALOGI("check register %d with count %d", regCandidateT,
-                          currentBB->infoBasicBlock[index2].allocConstraintsSorted[k].count);
-#endif
-                    /* if the requesting variable is 8 bit */
-                    if(is8Bit && regCandidateT > PhysicalReg_EDX) continue;
-                    assert(regCandidateT < PhysicalReg_Null);
-                    if(!allRegs[regCandidateT].isUsed) return regCandidateT;
-                }
-            }
-        }
-        /* check allocConstraints of the basic block
-           if 2 available physical registers have the same constraint count,
-              return the non callee-saved physical reg */
-        /* enhancement: record the time when a register is freed (freeTimeStamp)
-                        the purpose is to reduce false dependency
-           priority: constraint count, non callee-saved, time freed
-               let x be the lowest constraint count
-               set A be available callee-saved physical registers with count == x
-               set B be available non callee-saved physical registers with count == x
-               if set B is not null, return the one with smallest free time
-               otherwise, return the one in A with smallest free time
-           To ignore whether it is callee-saved, add all candidates to set A
-        */
-        int setAIndex[8];
-        int num_A = 0;
-        int setBIndex[8];
-        int num_B = 0;
-        int index1 = -1; //points to an available physical reg with lowest count
-        int currentCount = -1;
-        for(k = 0; k < 8; k++) {
-            int regCandidateT = currentBB->allocConstraintsSorted[k].physicalReg;
-            if(is8Bit && regCandidateT > PhysicalReg_EDX) continue;
-
-            if(index1 >= 0 && currentBB->allocConstraintsSorted[k].count > currentCount)
-                break; //candidate has higher count than index1
-            assert(regCandidateT < PhysicalReg_Null);
-            if(!allRegs[regCandidateT].isUsed) {
-                /*To ignore whether it is callee-saved, add all candidates to set A */
-                if(false) {//!allRegs[regCandidateT].isCalleeSaved) { //add to set B
-                    setBIndex[num_B++] = k;
-                } else { //add to set A
-                    setAIndex[num_A++] = k;
-                }
-                if(index1 < 0) {
-                    /* index1 points to a physical reg with lowest count */
-                    index1 = k;
-                    currentCount = currentBB->allocConstraintsSorted[k].count;
-                }
-            }
-        }
-
-        int kk;
-        int smallestTime = -1;
-        index1 = -1;
-        for(kk = 0; kk < num_B; kk++) {
-            k = setBIndex[kk];
-            int regCandidateT = currentBB->allocConstraintsSorted[k].physicalReg;
-            assert(regCandidateT < PhysicalReg_Null);
-            if(kk == 0 || allRegs[regCandidateT].freeTimeStamp < smallestTime) {
-                index1 = k;
-                smallestTime = allRegs[regCandidateT].freeTimeStamp;
-            }
-        }
-        if(index1 >= 0)
-            return currentBB->allocConstraintsSorted[index1].physicalReg;
-        index1 = -1;
-        for(kk = 0; kk < num_A; kk++) {
-            k = setAIndex[kk];
-            int regCandidateT = currentBB->allocConstraintsSorted[k].physicalReg;
-            if(kk == 0 || allRegs[regCandidateT].freeTimeStamp < smallestTime) {
-                index1 = k;
-                smallestTime = allRegs[regCandidateT].freeTimeStamp;
-            }
-        }
-        if(index1 >= 0) return currentBB->allocConstraintsSorted[index1].physicalReg;
-        return -1;
-    }
-    return -1;
-}
-
-//! find a candidate physical register for a variable and spill all variables that are mapped to the candidate
-
-//!
-PhysicalReg spillForLogicalReg(int type, int reg, int indexToCompileTable) {
-    //choose a used register to spill
-    //when a GG virtual register is spilled, write it to interpretd stack, set physicalReg to Null
-    //  at end of the basic block, load spilled GG VR to physical reg
-    //when other types of VR is spilled, write it to interpreted stack, set physicalReg to Null
-    //when a temporary (non-virtual) register is spilled, write it to stack, set physicalReg to Null
-    //can we spill a hard-coded temporary register? YES
-    int k, k2;
-    PhysicalReg allocR;
-
-    //do not try to free a physical reg that is used by more than one logical registers
-    //fix on sep 28, 2009
-    //do not try to spill a hard-coded logical register
-    //do not try to free a physical reg that is outside of the range for 8-bit logical reg
-    /* for each physical register,
-       collect number of non-hardcode entries that are mapped to the physical register */
-    int numOfUses[PhysicalReg_Null];
-    for(k = PhysicalReg_EAX; k < PhysicalReg_Null; k++)
-        numOfUses[k] = 0;
-    for(k = 0; k < compileTable.size (); k++) {
-        if((compileTable[k].physicalReg != PhysicalReg_Null) &&
-           matchType(type, compileTable[k].physicalType) &&
-           (compileTable[k].physicalType & LowOpndRegType_hard) == 0) {
-            numOfUses[compileTable[k].physicalReg]++;
-        }
-    }
-
-    /* candidates: all non-hardcode entries that are mapped to
-           a physical register that is used by only one entry*/
-    bool is8Bit = isTemp8Bit(type, reg);
-    int candidates[compileTable.size ()];
-    int num_cand = 0;
-    for(k = 0; k < compileTable.size (); k++) {
-        if(matchType(type, compileTable[k].physicalType) && compileTable[k].physicalReg != PhysicalReg_Null) {
-            //If we care about 8 bits, we can't have a register over EDX
-            if(is8Bit == true && compileTable[k].physicalReg > PhysicalReg_EDX)
-            {
-                continue;
-            }
-
-            //If we can spill it, ignore it
-            if(gCompilationUnit->getCanSpillRegister (compileTable[k].physicalReg) == false)
-            {
-                continue;
-            }
-
-            //If it isn't a hard register or it only a few uses left, it can be a candidate
-            if((compileTable[k].physicalType & LowOpndRegType_hard) == 0 && numOfUses[compileTable[k].physicalReg] <= 1) {
-                candidates[num_cand++] = k;
-            }
-        }
-    }
-
-    int spill_index = -1;
-
-    /* out of the candates, find a VR that has the furthest next use */
-    int furthestUse = offsetPC;
-    for(k2 = 0; k2 < num_cand; k2++) {
-        k = candidates[k2];
-        if((compileTable[k].physicalReg != PhysicalReg_Null) &&
-           matchType(type, compileTable[k].physicalType) &&
-           isVirtualReg(compileTable[k].physicalType)) {
-            int nextUse = getNextAccess(k);
-            if(spill_index < 0 || nextUse > furthestUse) {
-                spill_index = k;
-                furthestUse = nextUse;
-            }
-        }
-    }
-
-    /* spill the VR with the furthest next use */
-    if(spill_index >= 0) {
-        allocR = (PhysicalReg)spillLogicalReg(spill_index, true);
-        return allocR; //the register is still being used
-    }
-
-    /* spill an entry with the smallest refCount */
-    int baseLeftOver = 0;
-    int index = -1;
-    for(k2 = 0; k2 < num_cand; k2++) {
-        k = candidates[k2];
-        if((compileTable[k].physicalReg != PhysicalReg_Null) &&
-           (compileTable[k].physicalType & LowOpndRegType_hard) == 0 && //not hard-coded
-           matchType(type, compileTable[k].physicalType)) {
-            if((index < 0) || (compileTable[k].refCount < baseLeftOver)) {
-                baseLeftOver = compileTable[k].refCount;
-                index = k;
-            }
-        }
-    }
-    if(index < 0) {
-        dumpCompileTable();
-        ALOGI("JIT_INFO: no register to spill for logical %d %d\n", reg, type);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
-        return PhysicalReg_Null;
-    }
-    allocR = (PhysicalReg)spillLogicalReg(index, true);
-#ifdef DEBUG_REGALLOC
-    ALOGI("SPILL register used by num %d type %d it is a temporary register with refCount %d",
-           compileTable[index].regNum, compileTable[index].physicalType, compileTable[index].refCount);
-#endif
-    return allocR;
-}
-
-/**
- * @brief Spill a variable to memory, the variable is specified by an index to compileTable
- * @details If the variable is a temporary, get a spill location that is not in use and spill the content to the spill location;
- *       If updateTable is true, set physicalReg to Null;
- * @param spill_index the index into the compile table
- * @param updateTable do we update the table?
- * @return Return the physical register that was allocated to the variable
- */
-int spillLogicalReg(int spill_index, bool updateTable) {
-    if((compileTable[spill_index].physicalType & LowOpndRegType_hard) != 0) {
-        ALOGI("JIT_INFO: can't spill a hard-coded register");
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
-        return -1;
-    }
-
-#ifdef PRINT_WARNING
-    int physicalReg = compileTable[spill_index].physicalReg;
-
-    //If we can't spill it, print out a warning
-    if(gCompilationUnit->getCanSpillReg (physicalReg) == false) {
-        // This scenario can occur whenever a VR is allocated to the
-        // same physical register as a hardcoded temporary
-        ALOGW("Shouldn't spill register %s but going to do it anyway.",
-                physicalRegToString(static_cast<PhysicalReg>(physicalReg)));
-    }
-#endif
-
-    if (compileTable[spill_index].isVirtualReg ())
-    {
-        //Write VR back to memory
-        writeBackVR (compileTable[spill_index].getRegisterNumber (), compileTable[spill_index].getPhysicalType (),
-                compileTable[spill_index].getPhysicalReg ());
-    }
-    else {
-        //If the gCompilationUnit has maximumRegisterization set
-        if (gCompilationUnit->maximumRegisterization > 0)
-        {
-            //Signal the error framework we spilled: this is a warning that can help recompile if possible
-            SET_JIT_ERROR (kJitErrorSpill);
-        }
-        //update spill_loc_index
-        int k = getSpillIndex (compileTable[spill_index].getSize ());
-        compileTable[spill_index].spill_loc_index = 4*k;
-        if(k >= 0)
-            spillIndexUsed[k] = 1;
-        saveToSpillRegion(getRegSize(compileTable[spill_index].physicalType),
-                          compileTable[spill_index].physicalReg, 4*k);
-    }
-    //compileTable[spill_index].physicalReg_prev = compileTable[spill_index].physicalReg;
-#ifdef DEBUG_REGALLOC
-    ALOGI("REGALLOC: SPILL logical reg %d %d with refCount %d allocated to %d",
-           compileTable[spill_index].regNum,
-           compileTable[spill_index].physicalType, compileTable[spill_index].refCount,
-           compileTable[spill_index].physicalReg);
-#endif
-    if(!updateTable) return PhysicalReg_Null;
-
-    int allocR = compileTable[spill_index].physicalReg;
-    compileTable[spill_index].setPhysicalReg (PhysicalReg_Null);
-
-    return allocR;
-}
-//! load a varible from memory to physical register, the variable is specified with an index to compileTable
-
-//!If the variable is a temporary, load from spill location and set the flag for the spill location to not used
-int unspillLogicalReg(int spill_index, int physicalReg) {
-    //can't un-spill to %eax in afterCall!!!
-    //what if GG VR is allocated to %eax!!!
-    if(isVirtualReg(compileTable[spill_index].physicalType)) {
-        get_virtual_reg_noalloc(compileTable[spill_index].regNum,
-                                getRegSize(compileTable[spill_index].physicalType),
-                                physicalReg, true);
-    }
-    else {
-        loadFromSpillRegion(getRegSize(compileTable[spill_index].physicalType),
-                            physicalReg, compileTable[spill_index].spill_loc_index);
-        spillIndexUsed[compileTable[spill_index].spill_loc_index >> 2] = 0;
-        compileTable[spill_index].spill_loc_index = -1;
-    }
-#ifdef DEBUG_REGALLOC
-    ALOGI("REGALLOC: UNSPILL logical reg %d %d with refCount %d", compileTable[spill_index].regNum,
-           compileTable[spill_index].physicalType, compileTable[spill_index].refCount);
-#endif
-    return PhysicalReg_Null;
-}
-
-//!spill a virtual register to memory
-
-//!if the current value of a VR is constant, write immediate to memory;
-//!if the current value of a VR is in a physical register, call spillLogicalReg to dump content of the physical register to memory;
-//!ifupdateTable is true, set the physical register for VR to Null and decrease reference count of the virtual register
-int spillVirtualReg(int vrNum, LowOpndRegType type, bool updateTable) {
-    int index = searchCompileTable(type | LowOpndRegType_virtual, vrNum);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Cannot find VR %d %d in spillVirtualReg", vrNum, type);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-    //check whether it is const
-    int value[2];
-    int isConst = isVirtualRegConstant(vrNum, type, value, false); //do not update refCount
-    if(isConst == 1 || isConst == 3) {
-        writeBackConstVR(vrNum, value[0]);
-    }
-    if(getRegSize(type) == OpndSize_64 && (isConst == 2 || isConst == 3)) {
-        writeBackConstVR(vrNum+1, value[1]);
-    }
-    if(isConst != 3 && compileTable[index].physicalReg != PhysicalReg_Null)
-        spillLogicalReg(index, updateTable);
-    if(updateTable) decreaseRefCount(index);
-    return -1;
-}
-
-/**
- * @brief Writes virtual register back to memory if it holds a constant value
- * @param vR virtual register number
- * @param type the physical type register type that can be associated with
- * this VR
- * @return true if the entire VR was written back to memory
- */
-bool writeBackVRIfConstant(int vR, LowOpndRegType type) {
-    int constantValue[2];
-    bool writtenBack = false;
-
-    // Check if the VR is a constant. This function returns 3 if VR
-    // is 32-bit and constant, or if VR is 64-bit and both high order
-    // bits and low order bits are constant
-    int isConst = isVirtualRegConstant(vR, type, constantValue, false);
-
-    // If the VR is a constant, then write it back to memory
-    if (isConst == 3) {
-        writeBackConstVR(vR, constantValue[0]);
-        writtenBack |= true;
-    }
-
-    // If VR is wide and high order bits are constant, then write them to memory
-    if (getRegSize(type) == OpndSize_64 && isConst == 3) {
-        writeBackConstVR(vR + 1, constantValue[1]);
-        writtenBack |= true;
-    }
-
-    return writtenBack;
-}
-
-//! spill variables that are mapped to physical register (regNum)
-
-//!
-int spillForHardReg(int regNum, int type) {
-    //find an entry that uses the physical register
-    int spill_index = -1;
-    int k;
-    for(k = 0; k < compileTable.size (); k++) {
-        if(compileTable[k].physicalReg == regNum &&
-           matchType(type, compileTable[k].physicalType)) {
-            spill_index = k;
-            if(compileTable[k].regNum == regNum && compileTable[k].physicalType == type)
-                continue;
-            if(inGetVR_num >= 0 && compileTable[k].regNum == inGetVR_num && compileTable[k].physicalType == (type | LowOpndRegType_virtual))
-                continue;
-#ifdef DEBUG_REGALLOC
-            ALOGI("SPILL logical reg %d %d to free hard-coded reg %d %d",
-                   compileTable[spill_index].regNum, compileTable[spill_index].physicalType,
-                   regNum, type);
-            if(compileTable[spill_index].physicalType & LowOpndRegType_hard) dumpCompileTable();
-#endif
-            assert(spill_index < compileTable.size ());
-            spillLogicalReg(spill_index, true);
-        }
-    }
-    return regNum;
-}
-////////////////////////////////////////////////////////////////
-//! update allocConstraints of the current basic block
-
-//! allocConstraints specify how many times a hardcoded register is used in this basic block
-void updateCurrentBBWithConstraints(PhysicalReg reg) {
-    if (currentBB != 0)
-    {
-        if(reg > PhysicalReg_EBP) {
-            ALOGI("JIT_INFO: Register %d out of range in updateCurrentBBWithConstraints\n", reg);
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return;
-        }
-        currentBB->allocConstraints[reg].count++;
-    }
-}
-//! sort allocConstraints and save the result in allocConstraintsSorted
-
-//! allocConstraints specify how many times a virtual register is linked to a hardcode register
-//! it is updated in getVirtualRegInfo and merged by mergeEntry2
-int sortAllocConstraint(RegAllocConstraint* allocConstraints,
-                        RegAllocConstraint* allocConstraintsSorted, bool fromHighToLow) {
-    int ii, jj;
-    int num_sorted = 0;
-    for(jj = 0; jj < 8; jj++) {
-        //figure out where to insert allocConstraints[jj]
-        int count = allocConstraints[jj].count;
-        int regT = allocConstraints[jj].physicalReg;
-        assert(regT < PhysicalReg_Null);
-        int insertIndex = -1;
-        for(ii = 0; ii < num_sorted; ii++) {
-            int regT2 = allocConstraintsSorted[ii].physicalReg;
-            assert(regT2 < PhysicalReg_Null);
-            if(allRegs[regT].isCalleeSaved &&
-               count == allocConstraintsSorted[ii].count) {
-                insertIndex = ii;
-                break;
-            }
-            if((!allRegs[regT].isCalleeSaved) &&
-               count == allocConstraintsSorted[ii].count &&
-               (!allRegs[regT2].isCalleeSaved)) { //skip until found one that is not callee-saved
-                insertIndex = ii;
-                break;
-            }
-            if((fromHighToLow && count > allocConstraintsSorted[ii].count) ||
-               ((!fromHighToLow) && count < allocConstraintsSorted[ii].count)) {
-                insertIndex = ii;
-                break;
-            }
-        }
-        if(insertIndex < 0) {
-            allocConstraintsSorted[num_sorted].physicalReg = (PhysicalReg)regT;
-            allocConstraintsSorted[num_sorted].count = count;
-            num_sorted++;
-        } else {
-            for(ii = num_sorted-1; ii >= insertIndex; ii--) {
-                allocConstraintsSorted[ii+1] = allocConstraintsSorted[ii];
-            }
-            allocConstraintsSorted[insertIndex] = allocConstraints[jj];
-            num_sorted++;
-        }
-    } //for jj
-#ifdef DEBUG_ALLOC_CONSTRAINT
-    for(jj = 0; jj < 8; jj++) {
-        if(allocConstraintsSorted[jj].count > 0)
-            ALOGI("%d: register %d has count %d", jj, allocConstraintsSorted[jj].physicalReg, allocConstraintsSorted[jj].count);
-    }
-#endif
-    return 0;
-}
-
-//! \brief find the entry for a given virtual register in compileTable
-//! \param vA The VR to search for
-//! \param type Register type
-//! \return the virtual reg if found, else -1 as error.
-int findVirtualRegInTable(int vA, LowOpndRegType type) {
-    int k = searchCompileTable(type | LowOpndRegType_virtual, vA);
-    if(k < 0) {
-        ALOGI("JIT_INFO: Couldn't find virtual register %d type %d in compiler table\n", vA, type);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
-        return -1;
-    }
-    return k;
-}
-
-/**
- * @brief Checks whether the virtual register is constant.
- * @param regNum The virtual register to check.
- * @param opndRegType The physical type of this virtual register.
- * @param valuePtr If non-null, this is updated by function to contain the constant values.
- * @param updateRefCount When set, lowers reference count in compile table for this VR.
- * @return Returns information about the constantness of this VR.
- */
-VirtualRegConstantness isVirtualRegConstant(int regNum, int opndRegType, int *valuePtr, bool updateRefCount)
-{
-    //Determine the size of the VR by looking at its physical type
-    OpndSize size = getRegSize (opndRegType);
-
-    //Use these to keep track of index in the constant table of the VR we are looking for
-    int indexL = -1;
-    int indexH = -1;
-
-    //Iterate through constant table to find the desired virtual register
-    for(int k = 0; k < num_const_vr; k++)
-    {
-#ifdef DEBUG_CONST
-        ALOGI("constVRTable VR %d isConst %d value %x", constVRTable[k].regNum, constVRTable[k].isConst, constVRTable[k].value);
-#endif
-        if(constVRTable[k].regNum == regNum)
-        {
-            indexL = k;
-            continue;
-        }
-
-        if(constVRTable[k].regNum == regNum + 1 && size == OpndSize_64)
-        {
-            indexH = k;
-            continue;
-        }
-    }
-
-    //Eagerly assume that we won't find constantness of this VR
-    bool isConstL = false;
-    bool isConstH = false;
-
-    //If we found an entry in constant table for this VR, check if it is constant.
-    if(indexL >= 0)
-    {
-        isConstL = constVRTable[indexL].isConst;
-    }
-
-    //If we found an entry in constant table for the high part of VR, check if it is constant.
-    if(size == OpndSize_64 && indexH >= 0)
-    {
-        isConstH = constVRTable[indexH].isConst;
-    }
-
-    //Only tell the caller the values of constant if space has been provided for this purpose
-    if (valuePtr != 0)
-    {
-        //Are either the low bits or high bits constant?
-        if (isConstL == true || isConstH == true)
-        {
-            //If the high bits are constant and we care about them, then set value.
-            if (size == OpndSize_64 && isConstH == true)
-            {
-                valuePtr[1] = constVRTable[indexH].value;
-            }
-
-            //Now see if we can set value for the low bits
-            if (isConstL == true)
-            {
-                valuePtr[0] = constVRTable[indexL].value;
-            }
-        }
-    }
-
-    //If we are looking at non-wide VR that is constant or wide VR whose low and high parts are both constant,
-    //then we say that this VR is constant.
-    if((isConstL == true && size == OpndSize_32) || (isConstL == true && isConstH == true))
-    {
-        if(updateRefCount)
-        {
-            //We want to find entry in the compile table that matches the physical type we want.
-            //Since compile table keeps track of physical type along with logical type in same field,
-            //we do a binary bitwise inclusive or including the virtual register type.
-            int indexOrig = searchCompileTable(opndRegType | LowOpndRegType_virtual, regNum);
-
-            if(indexOrig < 0)
-            {
-                //We were not able to find the virtual register in compile table so just set an error
-                //and say that it is not constant.
-                ALOGI("JIT_INFO: Cannot find VR in isVirtualRegConstant num %d type %d\n", regNum, opndRegType);
-                SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                return VR_IS_NOT_CONSTANT;
-            }
-
-            //Decrement the reference count for this VR
-            decreaseRefCount(indexOrig);
-        }
-#ifdef DEBUG_CONST
-        ALOGI("VR %d %d is const case", regNum, type);
-#endif
-        return VR_IS_CONSTANT;
-    }
-    else if (isConstL == true && size != OpndSize_32)
-    {
-        //If the VR is wide and only low part is constant, we return saying that
-        return VR_LOW_IS_CONSTANT;
-    }
-    else if (isConstH == true && size != OpndSize_32)
-    {
-        //If the VR is wide and only high part is constant, we return saying that
-        return VR_HIGH_IS_CONSTANT;
-    }
-    else
-    {
-        //If we make it here, this VR is not constant
-        return VR_IS_NOT_CONSTANT;
-    }
-}
-
-//!update RegAccessType of virtual register vB given RegAccessType of vA
-
-//!RegAccessType can be D, L, H
-//!D means full definition, L means only lower-half is defined, H means only higher half is defined
-//!we say a VR has no exposed usage in a basic block if the accessType is D or DU
-//!we say a VR has exposed usage in a basic block if the accessType is not D nor DU
-//!we say a VR has exposed usage in other basic blocks (hasOtherExposedUsage) if
-//!  there exists another basic block where VR has exposed usage in that basic block
-//!A can be U, D, L, H, UD, UL, UH, DU, LU, HU (merged result)
-//!B can be U, D, UD, DU (an entry for the current bytecode)
-//!input isAPartiallyOverlapB can be any value between -1 to 6
-//!if A is xmm: gp B lower half of A, (isAPartiallyOverlapB is 1)
-//!             gp B higher half of A, (isAPartiallyOverlapB is 2)
-//!             lower half of A covers the higher half of xmm B  (isAPartiallyOverlapB is 4)
-//!             higher half of A covers the lower half of xmm B   (isAPartiallyOverlapB is 3)
-//!if A is gp:  A covers the lower half of xmm B, (isAPartiallyOverlapB is 5)
-//!             A covers the higher half of xmm B (isAPartiallyOverlapB is 6)
-RegAccessType updateAccess1(RegAccessType A, OverlapCase isAPartiallyOverlapB) {
-    if(A == REGACCESS_D || A == REGACCESS_DU || A == REGACCESS_UD) {
-        if(isAPartiallyOverlapB == OVERLAP_ALIGN) return REGACCESS_D;
-        if(isAPartiallyOverlapB == OVERLAP_B_IS_LOW_OF_A || isAPartiallyOverlapB == OVERLAP_B_IS_HIGH_OF_A)
-            return REGACCESS_D;
-        if(isAPartiallyOverlapB == OVERLAP_LOW_OF_A_IS_HIGH_OF_B || isAPartiallyOverlapB == OVERLAP_A_IS_LOW_OF_B)
-            return REGACCESS_L;
-        return REGACCESS_H;
-    }
-    if(A == REGACCESS_L || A == REGACCESS_LU || A == REGACCESS_UL) {
-        if(isAPartiallyOverlapB == OVERLAP_ALIGN || isAPartiallyOverlapB == OVERLAP_A_IS_LOW_OF_B)
-            return REGACCESS_L;
-        if(isAPartiallyOverlapB == OVERLAP_B_IS_LOW_OF_A) return REGACCESS_D;
-        if(isAPartiallyOverlapB == OVERLAP_B_IS_HIGH_OF_A || isAPartiallyOverlapB == OVERLAP_LOW_OF_A_IS_HIGH_OF_B)
-            return REGACCESS_N;
-        if(isAPartiallyOverlapB == OVERLAP_HIGH_OF_A_IS_LOW_OF_B || isAPartiallyOverlapB == OVERLAP_A_IS_HIGH_OF_B)
-            return REGACCESS_H;
-    }
-    if(A == REGACCESS_H || A == REGACCESS_HU || A == REGACCESS_UH) {
-        if(isAPartiallyOverlapB == OVERLAP_ALIGN || isAPartiallyOverlapB == OVERLAP_A_IS_HIGH_OF_B)
-            return REGACCESS_H;
-        if(isAPartiallyOverlapB == OVERLAP_B_IS_LOW_OF_A || isAPartiallyOverlapB == OVERLAP_HIGH_OF_A_IS_LOW_OF_B)
-            return REGACCESS_N;
-        if(isAPartiallyOverlapB == OVERLAP_B_IS_HIGH_OF_A) return REGACCESS_D;
-        if(isAPartiallyOverlapB == OVERLAP_LOW_OF_A_IS_HIGH_OF_B || isAPartiallyOverlapB == OVERLAP_A_IS_LOW_OF_B)
-            return REGACCESS_L;
-    }
-    return REGACCESS_N;
-}
-//! merge RegAccessType C1 with RegAccessType C2
-
-//!C can be N,L,H,D
-RegAccessType updateAccess2(RegAccessType C1, RegAccessType C2) {
-    if(C1 == REGACCESS_D || C2 == REGACCESS_D) return REGACCESS_D;
-    if(C1 == REGACCESS_N) return C2;
-    if(C2 == REGACCESS_N) return C1;
-    if(C1 == REGACCESS_L && C2 == REGACCESS_H) return REGACCESS_D;
-    if(C1 == REGACCESS_H && C2 == REGACCESS_L) return REGACCESS_D;
-    return C1;
-}
-//! merge RegAccessType C with RegAccessType B
-
-//!C can be N,L,H,D
-//!B can be U, D, UD, DU
-RegAccessType updateAccess3(RegAccessType C, RegAccessType B) {
-    if(B == REGACCESS_D || B == REGACCESS_DU) return B; //no exposed usage
-    if(B == REGACCESS_U || B == REGACCESS_UD) {
-        if(C == REGACCESS_N) return B;
-        if(C == REGACCESS_L) return REGACCESS_LU;
-        if(C == REGACCESS_H) return REGACCESS_HU;
-        if(C == REGACCESS_D) return REGACCESS_DU;
-    }
-    return B;
-}
-//! merge RegAccessType A with RegAccessType B
-
-//!argument isBPartiallyOverlapA can be any value between -1 and 2
-//!0 means fully overlapping, 1 means B is the lower half, 2 means B is the higher half
-RegAccessType mergeAccess2(RegAccessType A, RegAccessType B, OverlapCase isBPartiallyOverlapA) {
-    if(A == REGACCESS_UD || A == REGACCESS_UL || A == REGACCESS_UH ||
-       A == REGACCESS_DU || A == REGACCESS_LU || A == REGACCESS_HU) return A;
-    if(A == REGACCESS_D) {
-        if(B == REGACCESS_D) return REGACCESS_D;
-        if(B == REGACCESS_U) return REGACCESS_DU;
-        if(B == REGACCESS_UD) return REGACCESS_DU;
-        if(B == REGACCESS_DU) return B;
-    }
-    if(A == REGACCESS_U) {
-        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_UL;
-        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_UH;
-        if(B == REGACCESS_D && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_UD;
-        if(B == REGACCESS_U) return A;
-        if(B == REGACCESS_UD && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_UL;
-        if(B == REGACCESS_UD && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_UH;
-        if(B == REGACCESS_UD && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_UD;
-        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_UL;
-        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_UH;
-        if(B == REGACCESS_DU && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_UD;
-    }
-    if(A == REGACCESS_L) {
-        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_L;
-        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_D;
-        if(B == REGACCESS_D && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_D;
-        if(B == REGACCESS_U) return REGACCESS_LU;
-        if(B == REGACCESS_UD) return REGACCESS_LU;
-        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_LU;
-        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_DU;
-        if(B == REGACCESS_DU && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_DU;
-    }
-    if(A == REGACCESS_H) {
-        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_D;
-        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_H;
-        if(B == REGACCESS_D && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_D;
-        if(B == REGACCESS_U) return REGACCESS_HU;
-        if(B == REGACCESS_UD) return REGACCESS_HU;
-        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_DU;
-        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_HU;
-        if(B == REGACCESS_DU && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_DU;
-    }
-    return REGACCESS_N;
-}
-
-//!determines which part of a use is from a given definition
-
-//!reachingDefLive tells us which part of the def is live at this point
-//!isDefPartiallyOverlapUse can be any value between -1 and 2
-RegAccessType setAccessTypeOfUse(OverlapCase isDefPartiallyOverlapUse, RegAccessType reachingDefLive) {
-    if(isDefPartiallyOverlapUse == OVERLAP_B_COVER_A)
-        return reachingDefLive;
-    if(isDefPartiallyOverlapUse == OVERLAP_B_COVER_LOW_OF_A) { //def covers the low half of use
-        return REGACCESS_L;
-    }
-    if(isDefPartiallyOverlapUse == OVERLAP_B_COVER_HIGH_OF_A) {
-        return REGACCESS_H;
-    }
-    return REGACCESS_N;
-}
-
-//! search currentBB->defUseTable to find a def for regNum at offsetPC
-
-//!
-DefUsePair* searchDefUseTable(int offsetPC, int regNum, LowOpndRegType pType) {
-    DefUsePair* ptr = currentBB->defUseTable;
-    while(ptr != NULL) {
-        if(ptr->def.offsetPC == offsetPC &&
-           ptr->def.regNum == regNum &&
-           ptr->def.physicalType == pType) {
-            return ptr;
-        }
-        ptr = ptr->next;
-    }
-    return NULL;
-}
-void printDefUseTable() {
-    ALOGI("PRINT defUseTable --------");
-    DefUsePair* ptr = currentBB->defUseTable;
-    while(ptr != NULL) {
-        ALOGI("  def @ %x of VR %d %d has %d uses", ptr->def.offsetPC,
-              ptr->def.regNum, ptr->def.physicalType,
-              ptr->num_uses);
-        DefOrUseLink* ptr2 = ptr->uses;
-        while(ptr2 != NULL) {
-            ALOGI("    use @ %x of VR %d %d accessType %d", ptr2->offsetPC,
-                  ptr2->regNum,
-                  ptr2->physicalType,
-                  ptr2->accessType);
-            ptr2 = ptr2->next;
-        }
-        ptr = ptr->next;
-    }
-}
-
-/**
- * @brief Update a VR use
- * @param reg the register
- * @param pType the type we want for the register
- * @param regAll
- */
-void updateVRAtUse(int reg, LowOpndRegType pType, int regAll) {
-
-    //Get a local size of xferPoints' size
-    unsigned int max = currentBB->xferPoints.size ();
-
-    //Go through each element
-    for(unsigned int k = 0; k < max; k++) {
-
-        //If the xferPoint matches and says we want memory to xmm
-        if(currentBB->xferPoints[k].offsetPC == offsetPC &&
-           currentBB->xferPoints[k].xtype == XFER_MEM_TO_XMM &&
-           currentBB->xferPoints[k].regNum == reg &&
-           currentBB->xferPoints[k].physicalType == pType) {
-#ifdef DEBUG_XFER_POINTS
-            ALOGI("XFER from memory to xmm %d", reg);
-#endif
-            // If we get to this point, it is possible that we believe we need
-            // to load the wide VR from memory, but in reality this VR might
-            // already be in a physical register.
-
-            // TODO Figure out why this transfer point is inserted even when we already
-            // have VR in xmm.
-
-            int xmmVRType = static_cast<int>(LowOpndRegType_virtual)
-                    | static_cast<int>(LowOpndRegType_xmm);
-            int ssVRType = static_cast<int>(LowOpndRegType_virtual)
-                    | static_cast<int>(LowOpndRegType_ss);
-            bool loadFromMemory = true;
-
-            // Look in compile table for this VR
-            int entry = searchCompileTable(xmmVRType, reg);
-
-            if (entry == -1) {
-                // Single FP VRs can also use xmm, so try looking for this
-                // as well if we haven't already found an entry
-                entry = searchCompileTable(ssVRType, reg);
-            }
-
-            if (entry != -1) {
-                // If we found an entry, check whether its physical register
-                // is not null. If we have a physical register, we shouldn't be
-                // loading from memory
-                if (compileTable[entry].physicalReg != PhysicalReg_Null)
-                    loadFromMemory = false;
-            }
-
-            // Load from memory into the physical register
-            if (loadFromMemory) {
-                move_mem_to_reg_noalloc(OpndSize_64,
-                        4 * currentBB->xferPoints[k].regNum, PhysicalReg_FP,
-                        true, MemoryAccess_VR, currentBB->xferPoints[k].regNum,
-                        regAll, true);
-            }
-        }
-    }
-}
-
-/////////////////////////////////////////////////////////////
-//!search memVRTable for a given virtual register
-
-/**
- * @brief Search in the memory table for a register
- * @param regNum the register we are looking for
- * @return the index for the register, -1 if not found
- */
-int searchMemTable(int regNum) {
-    int k;
-    for(k = 0; k < num_memory_vr; k++) {
-        if(memVRTable[k].regNum == regNum) {
-            return k;
-        }
-    }
-    ALOGI("JIT_INFO: Can't find VR %d num_memory_vr %d at searchMemTable", regNum, num_memory_vr);
-    return -1;
-}
-/////////////////////////////////////////////////////////////////////////
-// A VR is already in memory && NULL CHECK
-//!check whether the latest content of a VR is in memory
-
-//!
-bool isInMemory(int regNum, OpndSize size) {
-    int indexL = searchMemTable(regNum);
-    int indexH = -1;
-    if(size == OpndSize_64) indexH = searchMemTable(regNum+1);
-    if(indexL < 0) return false;
-    if(size == OpndSize_64 && indexH < 0) return false;
-    if(!memVRTable[indexL].inMemory) return false;
-    if(size == OpndSize_64 && (!memVRTable[indexH].inMemory)) return false;
-    return true;
-}
-
-/**
- * @brief Used to update the in memory state of a virtual register
- * @param vR The virtual register
- * @param size The size of the virtual register
- * @param inMemory The new in memory state of the virtual register
- */
-void setVRMemoryState (int vR, OpndSize size, bool inMemory)
-{
-    //Look for the index in the mem table for the virtual register
-    int indexL = searchMemTable (vR);
-
-    //If virtual register is wide, we want to find the index for the high part as well
-    int indexH = -1;
-    if (size == OpndSize_64)
-    {
-        indexH = searchMemTable (vR + 1);
-    }
-
-    if (indexL < 0)
-    {
-        ALOGI ("JIT_INFO: VR %d not in memVRTable at setVRToMemory", vR);
-        SET_JIT_ERROR (kJitErrorRegAllocFailed);
-        return;
-    }
-
-    //Update the in memory state of the VR
-    memVRTable[indexL].setInMemoryState (inMemory);
-
-    DEBUG_MEMORYVR (ALOGD ("Setting state of v%d %sin memory", memVRTable[indexL].vR,
-            (memVRTable[indexL].inMemory ? "" : "NOT ")));
-
-    if (size == OpndSize_64)
-    {
-        if (indexH < 0)
-        {
-            ALOGI ("JIT_INFO: VR %d not in memVRTable at setVRToMemory for upper 64-bits", vR+1);
-            SET_JIT_ERROR (kJitErrorRegAllocFailed);
-            return;
-        }
-
-        //Update the in memory state of the upper 64-bits of the VR
-        memVRTable[indexH].setInMemoryState (inMemory);
-
-        DEBUG_MEMORYVR (ALOGD ("Setting state of v%d %sin memory", memVRTable[indexH].vR,
-                (memVRTable[indexH].inMemory ? "" : "NOT ")));
-    }
-}
-
-//! check whether null check for a VR is performed previously
-
-//!
-bool isVRNullCheck(int regNum, OpndSize size) {
-    if(size != OpndSize_32) {
-        ALOGI("JIT_INFO: isVRNullCheck size is not 32 for register %d", regNum);
-        SET_JIT_ERROR(kJitErrorNullBoundCheckFailed);
-        return false;
-    }
-    int indexL = searchMemTable(regNum);
-    if(indexL < 0) {
-        ALOGI("JIT_INFO: VR %d not in memVRTable at isVRNullCheck", regNum);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    return memVRTable[indexL].nullCheckDone;
-}
-bool isVRBoundCheck(int vr_array, int vr_index) {
-    int indexL = searchMemTable(vr_array);
-    if(indexL < 0) {
-        ALOGI("JIT_INFO: VR %d not in memVRTable at isVRBoundCheck", vr_array);
-        SET_JIT_ERROR(kJitErrorNullBoundCheckFailed);
-        return false;
-    }
-    if(memVRTable[indexL].boundCheck.indexVR == vr_index)
-        return memVRTable[indexL].boundCheck.checkDone;
-    return false;
-}
-//! \brief set nullCheckDone in memVRTable to true
-//!
-//! \param regNum the register number
-//! \param size the register size
-//!
-//! \return -1 if error happened, 0 otherwise
-int setVRNullCheck(int regNum, OpndSize size) {
-    if(size != OpndSize_32) {
-        ALOGI("JIT_INFO: setVRNullCheck size should be 32\n");
-        SET_JIT_ERROR(kJitErrorNullBoundCheckFailed);
-        return -1;
-    }
-    int indexL = searchMemTable(regNum);
-    if(indexL < 0) {
-        ALOGI("JIT_INFO: VR %d not in memVRTable at setVRNullCheck", regNum);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-    memVRTable[indexL].nullCheckDone = true;
-    return 0;
-}
-void setVRBoundCheck(int vr_array, int vr_index) {
-    int indexL = searchMemTable(vr_array);
-    if(indexL < 0) {
-        ALOGI("JIT_INFO: VR %d not in memVRTable at setVRBoundCheck", vr_array);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return;
-    }
-    memVRTable[indexL].boundCheck.indexVR = vr_index;
-    memVRTable[indexL].boundCheck.checkDone = true;
-}
-void clearVRBoundCheck(int regNum, OpndSize size) {
-    int k;
-    for(k = 0; k < num_memory_vr; k++) {
-        if(memVRTable[k].regNum == regNum ||
-           (size == OpndSize_64 && memVRTable[k].regNum == regNum+1)) {
-            memVRTable[k].boundCheck.checkDone = false;
-        }
-        if(memVRTable[k].boundCheck.indexVR == regNum ||
-           (size == OpndSize_64 && memVRTable[k].boundCheck.indexVR == regNum+1)) {
-            memVRTable[k].boundCheck.checkDone = false;
-        }
-    }
-}
-//! set inMemory of memVRTable to false
-
-//!
-void clearVRToMemory(int regNum, OpndSize size) {
-    int indexL = searchMemTable(regNum);
-    int indexH = -1;
-    if(size == OpndSize_64) indexH = searchMemTable(regNum+1);
-    if(indexL >= 0) {
-        memVRTable[indexL].inMemory = false;
-
-        DEBUG_MEMORYVR(ALOGD("Setting state of v%d %sin memory",
-                memVRTable[indexL].regNum,
-                (memVRTable[indexL].inMemory ? "" : "NOT ")));
-    }
-    if(size == OpndSize_64 && indexH >= 0) {
-        memVRTable[indexH].inMemory = false;
-
-        DEBUG_MEMORYVR(ALOGD("Setting state of v%d %sin memory",
-                memVRTable[indexH].regNum,
-                (memVRTable[indexH].inMemory ? "" : "NOT ")));
-    }
-}
-//! set nullCheckDone of memVRTable to false
-
-//!
-void clearVRNullCheck(int regNum, OpndSize size) {
-    int indexL = searchMemTable(regNum);
-    int indexH = -1;
-    if(size == OpndSize_64) indexH = searchMemTable(regNum+1);
-    if(indexL >= 0) {
-        memVRTable[indexL].nullCheckDone = false;
-    }
-    if(size == OpndSize_64 && indexH >= 0) {
-        memVRTable[indexH].nullCheckDone = false;
-    }
-}
-
-//! Extend life for all VRs
-
-//! Affects only VRs, stored in physical reg on last bytecode of their live range
-//! @see VRFreeDelayCounters
-//! @param reason explains what freeing delay request should be canceled.
-//! A single VRFreeDelayCounters index should be used.
-//! @return true if at least one VR changed it's state
-bool requestVRFreeDelayAll(u4 reason) {
-    bool state_changed = false;
-    // Delay only VRs, which could be freed by freeReg
-    for(int k = 0; k < compileTable.size (); k++) {
-
-        if(compileTable[k].physicalReg != PhysicalReg_Null) {
-
-            if(isVirtualReg(compileTable[k].physicalType) == true) {
-                bool freeCrit = isLastByteCodeOfLiveRange(k);
-
-                if(freeCrit == true) {
-                    int res = requestVRFreeDelay(compileTable[k].regNum, reason);
-                    if(res >= 0) {
-                        state_changed = true;
-                    }
-                }
-            }
-        }
-    }
-#ifdef DEBUG_REGALLOC
-    if(state_changed) {
-        ALOGI("requestVRFreeDelayAll: state_changed=%i", state_changed);
-    }
-#endif
-    return state_changed;
-}
-
-//! Cancel request for all VR life extension
-
-//! Affects only VRs, stored in physical reg on last bytecode of theirs live range
-//! @see VRFreeDelayCounters
-//! @param reason explains what freeing delay request should be canceled.
-//! A single VRFreeDelayCounters index should be used.
-//! @return true if at least one VR changed it's state
-bool cancelVRFreeDelayRequestAll(u4 reason) {
-    bool state_changed = false;
-    // Cancel delay for VRs only
-    for(int k = 0; k < compileTable.size (); k++) {
-        if(isVirtualReg(compileTable[k].physicalType) == true) {
-            bool freeCrit = isLastByteCodeOfLiveRange(k);
-
-            if(freeCrit == true) {
-                int res = cancelVRFreeDelayRequest(compileTable[k].regNum, reason);
-                if(res >= 0) {
-                    state_changed = true;
-                }
-            }
-        }
-    }
-#ifdef DEBUG_REGALLOC
-    if(state_changed) {
-        ALOGI("cancelVRFreeDelayRequestAll: state_changed=%i", state_changed);
-    }
-#endif
-    return state_changed;
-}
-
-//! Extend Virtual Register life
-
-//! Requests that the life of a specific virtual register be extended. This ensures
-//! that its mapping to a physical register won't be canceled while the extension
-//! request is valid. NOTE: This does not support 64-bit values (when two adjacent
-//! VRs are used)
-//! @see cancelVRFreeDelayRequest
-//! @see getVRFreeDelayRequested
-//! @see VRFreeDelayCounters
-//! @param regNum is the VR number
-//! @param reason explains why freeing must be delayed.
-//! A single VRFreeDelayCounters index should be used.
-//! @return negative value if request failed
-int requestVRFreeDelay(int regNum, u4 reason) {
-    // TODO Add 64-bit operand support when needed
-    int indexL = searchMemTable(regNum);
-    if(indexL >= 0) {
-        if(reason < VRDELAY_COUNT) {
-#ifdef DEBUG_REGALLOC
-            ALOGI("requestFreeDelay: reason=%i VR=%d count=%i", reason, regNum, memVRTable[indexL].delayFreeCounters[reason]);
-#endif
-            memVRTable[indexL].delayFreeCounters[reason]++;
-        } else {
-            ALOGI("JIT_INFO: At requestVRFreeDelay: reason %i is unknown (VR=%d)", reason, regNum);
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return -1;
-        }
-    } else {
-        ALOGI("JIT_INFO: At requestVRFreeDelay: VR %d not in memVRTable", regNum);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-
-    return indexL;
-}
-
-//! Cancel request for virtual register life extension
-
-//! Cancels any outstanding requests to extended liveness of VR. Additionally,
-//! this ensures that if the VR is no longer life after this point, it will
-//! no longer be associated with a physical register which can then be reused.
-//! NOTE: This does not support 64-bit values (when two adjacent VRs are used)
-//! @see requestVRFreeDelay
-//! @see getVRFreeDelayRequested
-//! @see VRFreeDelayCounters
-//! @param regNum is the VR number
-//! @param reason explains what freeing delay request should be canceled.
-//! A single VRFreeDelayCounters index should be used.
-//! @return negative value if request failed
-int cancelVRFreeDelayRequest(int regNum, u4 reason) {
-    //TODO Add 64-bit operand support when needed
-    bool needCallToFreeReg = false;
-    int indexL = searchMemTable(regNum);
-    if(indexL >= 0) {
-        if(reason < VRDELAY_COUNT) { // don't cancel delay if it wasn't requested
-            if(memVRTable[indexL].delayFreeCounters[reason] > 0) {
-#ifdef DEBUG_REGALLOC
-                ALOGI("cancelVRFreeDelay: reason=%i VR=%d count=%i", reason, regNum, memVRTable[indexL].delayFreeCounters[reason]);
-#endif
-                memVRTable[indexL].delayFreeCounters[reason]--; // only cancel this particular reason, not all others
-
-                // freeReg might want to free this VR now if there is no longer a valid delay
-                needCallToFreeReg = !getVRFreeDelayRequested(regNum);
-            } else {
-                return -1;
-            }
-        } else {
-            ALOGI("JIT_INFO: At cancelVRFreeDelay: reason %i is unknown (VR: %d)", reason, regNum);
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return -1;
-        }
-    }
-    if(needCallToFreeReg)
-        freeReg(false);
-
-    return indexL;
-}
-
-//! Gets status of virtual register free delay request
-
-//! Finds out if there was a delay request for freeing this VR.
-//! NOTE: This does not support 64-bit values (when two adjacent VRs are used)
-//! @see requestVRFreeDelay
-//! @see cancelVRFreeDelayRequest
-//! @param regNum is the VR number
-//! @return true if VR has an active delay request
-bool getVRFreeDelayRequested(int regNum) {
-    //TODO Add 64-bit operand support when needed
-    int indexL = searchMemTable(regNum);
-    if(indexL >= 0) {
-        for(int c=0; c<VRDELAY_COUNT; c++) {
-            if(memVRTable[indexL].delayFreeCounters[c] != 0) {
-                return true;
-            }
-        }
-        return false;
-    }
-    return false;
-}
-
-int current_bc_size = -1;
-
-/**
- * @brief Search the basic block information for a register number and type
- * @param type the register type we are looking for (masked with MASK_FOR_TYPE)
- * @param regNum the register number we are looking for
- * @param bb the BasicBlock
- * @return true if found
- */
-bool isUsedInBB(int regNum, int type, BasicBlock_O1* bb) {
-    unsigned int k, max;
-
-    //Get a local version of the infoBasicBlock's size
-    max = bb->infoBasicBlock.size ();
-
-    //Go through each element, if it matches, return true
-    for(k = 0; k < max; k++) {
-        if(bb->infoBasicBlock[k].physicalType == (type & MASK_FOR_TYPE) && bb->infoBasicBlock[k].regNum == regNum) {
-            return true;
-        }
-    }
-
-    //Report failure
-    return false;
-}
-
-/**
- * @brief Search the basic block information for a register number and type
- * @param type the register type we are looking for
- * @param regNum the register number we are looking for
- * @param bb the BasicBlock
- * @return the index in the infoBasicBlock table, -1 if not found
- */
-int searchVirtualInfoOfBB(LowOpndRegType type, int regNum, BasicBlock_O1* bb) {
-    unsigned int k, max;
-
-    //Get a local version of the infoBasicBlock's size
-    max = bb->infoBasicBlock.size ();
-
-    //Go through each element, if it matches, return the index
-    for(k = 0; k < max; k++) {
-        if(bb->infoBasicBlock[k].physicalType == type && bb->infoBasicBlock[k].regNum == regNum) {
-            return k;
-        }
-    }
-
-    //Not found is not always an error, so don't set any
-    return -1;
-}
-//! return the index to compileTable for a given virtual register
-
-//! return -1 if not found
-int searchCompileTable(int type, int regNum) { //returns the index
-    int k;
-    for(k = 0; k < compileTable.size (); k++) {
-        if(compileTable[k].physicalType == type && compileTable[k].regNum == regNum)
-            return k;
-    }
-
-    //Returning -1 might not always be an error, so we don't set any
-    return -1;
-}
-
-/**
- * @brief Update the compileTable entry with the new register
- * @param vR what virtual register are we interested in
- * @param oldReg the old register that used to be used
- * @param newReg the new register that is now used
- * @return whether the update was successful
- */
-bool updatePhysicalRegForVR(int vR, PhysicalReg oldReg, PhysicalReg newReg) {
-    //Go through the entries in the compile table
-    for (int entry = 0; entry < compileTable.size (); entry++) {
-
-        //If it is a virtual register, the vR we are looking for, and is associated to the old register
-        if (isVirtualReg(compileTable[entry].physicalType)
-                && compileTable[entry].regNum == vR
-                && compileTable[entry].physicalReg == oldReg) {
-
-            //Update it and report success
-            compileTable[entry].setPhysicalReg (newReg);
-
-            return true;
-        }
-    }
-
-    //We did not find it
-    return false;
-}
-
-//!check whether a physical register for a variable with typeA will work for another variable with typeB
-
-//!Type LowOpndRegType_ss is compatible with type LowOpndRegType_xmm
-bool matchType(int typeA, int typeB) {
-    if((typeA & MASK_FOR_TYPE) == (typeB & MASK_FOR_TYPE)) return true;
-    if((typeA & MASK_FOR_TYPE) == LowOpndRegType_ss &&
-       (typeB & MASK_FOR_TYPE) == LowOpndRegType_xmm) return true;
-    if((typeA & MASK_FOR_TYPE) == LowOpndRegType_xmm &&
-       (typeB & MASK_FOR_TYPE) == LowOpndRegType_ss) return true;
-    return false;
-}
-
-//! obsolete
-bool defineFirst(int atype) {
-    if(atype == REGACCESS_D || atype == REGACCESS_L || atype == REGACCESS_H || atype == REGACCESS_DU)
-        return true;
-    return false;
-}
-//!check whether a virtual register is updated in a basic block
-
-//!
-bool notUpdated(RegAccessType atype) {
-    if(atype == REGACCESS_U) return true;
-    return false;
-}
-//!check whether a virtual register has exposed usage within a given basic block
-
-//!
-bool hasExposedUsage2(BasicBlock_O1* bb, int index) {
-    RegAccessType atype = bb->infoBasicBlock[index].accessType;
-    if(atype == REGACCESS_D || atype == REGACCESS_L || atype == REGACCESS_H || atype == REGACCESS_DU)
-        return false;
-    return true;
-}
-//! return the spill location that is not used
-
-//!
-int getSpillIndex (OpndSize size) {
-    int k;
-    for(k = 1; k <= MAX_SPILL_JIT_IA - 1; k++) {
-        if(size == OpndSize_64) {
-            if(k < MAX_SPILL_JIT_IA - 1 && spillIndexUsed[k] == 0 && spillIndexUsed[k+1] == 0)
-                return k;
-        }
-        else if(spillIndexUsed[k] == 0) {
-            return k;
-        }
-    }
-    ALOGI("JIT_INFO: Cannot find spill position in spillLogicalReg\n");
-    SET_JIT_ERROR(kJitErrorRegAllocFailed);
-    return -1;
-}
-
-//!this is called before generating a native code, it resets the spill information
-//!startNativeCode must be paired with endNativeCode
-void startNativeCode(int vr_num, int vr_type) {
-    //Reset the spilling information
-    gCompilationUnit->resetCanSpillRegisters ();
-
-    //Set the inGetVR_num and type now
-    inGetVR_num = vr_num;
-    inGetVR_type = vr_type;
-}
-
-//! called right after generating a native code
-//!It resets the spill information and resets inGetVR_num to -1
-void endNativeCode(void) {
-    //Reset the spilling information
-    gCompilationUnit->resetCanSpillRegisters ();
-
-    //Reset the inGetVR_num now
-    inGetVR_num = -1;
-}
-
-//! touch hardcoded register %ecx and reduce its reference count
-
-//!
-int touchEcx() {
-    //registerAlloc will spill logical reg that is mapped to ecx
-    //registerAlloc will reduce refCount
-    registerAlloc(LowOpndRegType_gp, PhysicalReg_ECX, true, true);
-    return 0;
-}
-//! touch hardcoded register %eax and reduce its reference count
-
-//!
-int touchEax() {
-    registerAlloc(LowOpndRegType_gp, PhysicalReg_EAX, true, true);
-    return 0;
-}
-int touchEsi() {
-    registerAlloc(LowOpndRegType_gp, PhysicalReg_ESI, true, true);
-    return 0;
-}
-int touchXmm1() {
-    registerAlloc(LowOpndRegType_xmm, XMM_1, true, true);
-    return 0;
-}
-int touchEbx() {
-    registerAlloc(LowOpndRegType_gp, PhysicalReg_EBX, true, true);
-    return 0;
-}
-
-//! touch hardcoded register %edx and reduce its reference count
-
-//!
-int touchEdx() {
-    registerAlloc(LowOpndRegType_gp, PhysicalReg_EDX, true, true);
-    return 0;
-}
-
-#ifdef HACK_FOR_DEBUG
-//for debugging purpose, instructions are added at a certain place
-bool hacked = false;
-void hackBug() {
-  if(!hacked && iget_obj_inst == 13) {
-#if 0
-    move_reg_to_reg_noalloc(OpndSize_32, PhysicalReg_EBX, true, PhysicalReg_ECX, true);
-    //move from ebx to ecx & update compileTable for v3
-    int tIndex = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, 3);
-    if(tIndex < 0) ALOGE("hack can't find VR3");
-    compileTable[tIndex].physicalReg = PhysicalReg_ECX;
-#else
-    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EBX, true, 12, PhysicalReg_FP, true);
-#endif
-  }
-}
-void hackBug2() {
-  if(!hacked && iget_obj_inst == 13) {
-    dump_imm_mem_noalloc(Mnemonic_MOV, OpndSize_32, 0, 12, PhysicalReg_FP, true);
-    hacked = true;
-  }
-}
-#endif
-
-//! this function is called before calling a helper function or a vm function
-int beforeCall(const char* target) { //spill all live registers
-    if(currentBB == NULL) return -1;
-
-    /* special case for ncgGetEIP: this function only updates %edx */
-    if(!strcmp(target, "ncgGetEIP")) {
-        touchEdx();
-        return -1;
-    }
-
-    /* these functions use %eax for the return value */
-    if((!strcmp(target, "dvmInstanceofNonTrivial")) ||
-       (!strcmp(target, "dvmUnlockObject")) ||
-       (!strcmp(target, "dvmAllocObject")) ||
-       (!strcmp(target, "dvmAllocArrayByClass")) ||
-       (!strcmp(target, "dvmAllocPrimitiveArray")) ||
-       (!strcmp(target, "dvmInterpHandleFillArrayData")) ||
-       (!strcmp(target, "dvmFindInterfaceMethodInCache")) ||
-       (!strcmp(target, "dvmNcgHandlePackedSwitch")) ||
-       (!strcmp(target, "dvmNcgHandleSparseSwitch")) ||
-       (!strcmp(target, "dvmCanPutArrayElement")) ||
-       (!strcmp(target, "moddi3")) || (!strcmp(target, "divdi3")) ||
-       (!strcmp(target, "execute_inline"))
-       || (!strcmp(target, "dvmJitToPatchPredictedChain"))
-       || (!strcmp(target, "dvmJitHandlePackedSwitch"))
-       || (!strcmp(target, "dvmJitHandleSparseSwitch"))
-#if defined(WITH_SELF_VERIFICATION)
-       || (!strcmp(target, "selfVerificationLoad"))
-#endif
-       ) {
-        touchEax();
-    }
-
-    //these two functions also use %edx for the return value
-    if((!strcmp(target, "moddi3")) || (!strcmp(target, "divdi3"))) {
-        touchEdx();
-    }
-    if((!strcmp(target, ".new_instance_helper"))) {
-        touchEsi(); touchEax();
-    }
-#if defined(ENABLE_TRACING)
-    if((!strcmp(target, "common_periodicChecks4"))) {
-        touchEdx();
-    }
-#endif
-    if((!strcmp(target, ".const_string_helper"))) {
-        touchEcx(); touchEax();
-    }
-    if((!strcmp(target, ".check_cast_helper"))) {
-        touchEbx(); touchEsi();
-    }
-    if((!strcmp(target, ".instance_of_helper"))) {
-        touchEbx(); touchEsi(); touchEcx();
-    }
-    if((!strcmp(target, ".monitor_enter_helper"))) {
-        touchEbx();
-    }
-    if((!strcmp(target, ".monitor_exit_helper"))) {
-        touchEbx();
-    }
-    if((!strcmp(target, ".aget_wide_helper"))) {
-        touchEbx(); touchEcx(); touchXmm1();
-    }
-    if((!strcmp(target, ".aget_helper")) || (!strcmp(target, ".aget_char_helper")) ||
-       (!strcmp(target, ".aget_short_helper")) || (!strcmp(target, ".aget_bool_helper")) ||
-       (!strcmp(target, ".aget_byte_helper"))) {
-        touchEbx(); touchEcx(); touchEdx();
-    }
-    if((!strcmp(target, ".aput_helper")) || (!strcmp(target, ".aput_char_helper")) ||
-       (!strcmp(target, ".aput_short_helper")) || (!strcmp(target, ".aput_bool_helper")) ||
-       (!strcmp(target, ".aput_byte_helper")) || (!strcmp(target, ".aput_wide_helper"))) {
-        touchEbx(); touchEcx(); touchEdx();
-    }
-    if((!strcmp(target, ".sput_helper")) || (!strcmp(target, ".sput_wide_helper"))) {
-        touchEdx(); touchEax();
-    }
-    if((!strcmp(target, ".sget_helper"))) {
-        touchEdx(); touchEcx();
-    }
-    if((!strcmp(target, ".sget_wide_helper"))) {
-        touchEdx(); touchXmm1();
-    }
-    if((!strcmp(target, ".aput_obj_helper"))) {
-        touchEdx(); touchEcx(); touchEax();
-    }
-    if((!strcmp(target, ".iput_helper")) || (!strcmp(target, ".iput_wide_helper"))) {
-        touchEbx(); touchEcx(); touchEsi();
-    }
-    if((!strcmp(target, ".iget_helper"))) {
-        touchEbx(); touchEcx(); touchEdx();
-    }
-    if((!strcmp(target, ".iget_wide_helper"))) {
-        touchEbx(); touchEcx(); touchXmm1();
-    }
-    if((!strcmp(target, ".new_array_helper"))) {
-        touchEbx(); touchEdx(); touchEax();
-    }
-    if((!strcmp(target, ".invoke_virtual_helper"))) {
-        touchEbx(); touchEcx();
-    }
-    if((!strcmp(target, ".invoke_direct_helper"))) {
-        touchEsi(); touchEcx();
-    }
-    if((!strcmp(target, ".invoke_super_helper"))) {
-        touchEbx(); touchEcx();
-    }
-    if((!strcmp(target, ".invoke_interface_helper"))) {
-        touchEbx(); touchEcx();
-    }
-    if((!strcmp(target, ".invokeMethodNoRange_5_helper")) ||
-       (!strcmp(target, ".invokeMethodNoRange_4_helper"))) {
-        touchEbx(); touchEsi(); touchEax(); touchEdx();
-    }
-    if((!strcmp(target, ".invokeMethodNoRange_3_helper"))) {
-        touchEbx(); touchEsi(); touchEax();
-    }
-    if((!strcmp(target, ".invokeMethodNoRange_2_helper"))) {
-        touchEbx(); touchEsi();
-    }
-    if((!strcmp(target, ".invokeMethodNoRange_1_helper"))) {
-        touchEbx();
-    }
-    if((!strcmp(target, ".invokeMethodRange_helper"))) {
-        touchEdx(); touchEsi();
-    }
-#ifdef DEBUG_REGALLOC
-    ALOGI("enter beforeCall");
-#endif
-
-    freeReg(true); //to avoid spilling dead logical registers
-    int k;
-    for(k = 0; k < compileTable.size (); k++) {
-        if(compileTable[k].physicalReg != PhysicalReg_Null &&
-           (compileTable[k].physicalType & LowOpndRegType_hard) == 0) {
-            /* handles non hardcoded variables that are in physical registers */
-            if(!strcmp(target, "exception")) {
-                /* before throwing an exception
-                   update contents of all VRs in Java stack */
-                if(!isVirtualReg(compileTable[k].physicalType)) continue;
-                /* to have correct GC, we should update contents for L VRs as well */
-                //if(compileTable[k].gType == GLOBALTYPE_L) continue;
-            }
-            if((!strcmp(target, ".const_string_resolve")) ||
-               (!strcmp(target, ".static_field_resolve")) ||
-               (!strcmp(target, ".inst_field_resolve")) ||
-               (!strcmp(target, ".class_resolve")) ||
-               (!strcmp(target, ".direct_method_resolve")) ||
-               (!strcmp(target, ".virtual_method_resolve")) ||
-               (!strcmp(target, ".static_method_resolve"))) {
-               /* physical register %ebx will keep its content
-                  but to have correct GC, we should dump content of a VR
-                     that is mapped to %ebx */
-                if(compileTable[k].physicalReg == PhysicalReg_EBX &&
-                   (!isVirtualReg(compileTable[k].physicalType)))
-                    continue;
-            }
-            if((!strncmp(target, "dvm", 3)) || (!strcmp(target, "moddi3")) ||
-               (!strcmp(target, "divdi3")) ||
-               (!strcmp(target, "fmod")) || (!strcmp(target, "fmodf"))) {
-                /* callee-saved registers (%ebx, %esi, %ebp, %edi) will keep the content
-                   but to have correct GC, we should dump content of a VR
-                      that is mapped to a callee-saved register */
-                if((compileTable[k].physicalReg == PhysicalReg_EBX ||
-                    compileTable[k].physicalReg == PhysicalReg_ESI) &&
-                   (!isVirtualReg(compileTable[k].physicalType)))
-                    continue;
-            }
-
-            if(strncmp(target, "dvmUnlockObject", 15) == 0) {
-               continue;
-            }
-
-#ifdef DEBUG_REGALLOC
-            ALOGI("SPILL logical register %d %d in beforeCall",
-                  compileTable[k].regNum, compileTable[k].physicalType);
-#endif
-            spillLogicalReg(k, true);
-        }
-    }
-
-    cancelVRFreeDelayRequestAll(VRDELAY_CAN_THROW);
-
-#ifdef DEBUG_REGALLOC
-    ALOGI("exit beforeCall");
-#endif
-    return 0;
-}
-int getFreeReg(int type, int reg, int indexToCompileTable);
-//! after calling a helper function or a VM function
-
-//!
-int afterCall(const char* target) { //un-spill
-    if(currentBB == NULL) return -1;
-    if(!strcmp(target, "ncgGetEIP")) return -1;
-
-    return 0;
-}
-//! check whether a temporary is 8-bit
-
-//!
-bool isTemp8Bit(int type, int reg) {
-    if(currentBB == NULL) return false;
-    if(!isTemporary(type, reg)) return false;
-    int k;
-    for(k = 0; k < num_temp_regs_per_bytecode; k++) {
-        if(infoByteCodeTemp[k].physicalType == type &&
-           infoByteCodeTemp[k].regNum == reg) {
-            return infoByteCodeTemp[k].is8Bit;
-        }
-    }
-    ALOGI("JIT_INFO: Could not find reg %d type %d at isTemp8Bit", reg, type);
-    SET_JIT_ERROR(kJitErrorRegAllocFailed);
-    return false;
-}
-
-/* functions to access live ranges of a VR
-   Live range info is stored in memVRTable[].ranges, which is a linked list
-*/
-//! check whether a VR is live at the current bytecode
-
-//!
-bool isVRLive(int vA) {
-    int index = searchMemTable(vA);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find VR %d in memTable at isVRLive", vA);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    LiveRange* ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        if(offsetPC >= ptr->start && offsetPC <= ptr->end) return true;
-        ptr = ptr->next;
-    }
-    return false;
-}
-
-//! check whether the current bytecode is the last access to a VR within a live range
-
-//!for 64-bit VR, return true only when true for both low half and high half
-bool isLastByteCodeOfLiveRange(int compileIndex) {
-    int k = compileIndex;
-    OpndSize tSize = getRegSize(compileTable[k].physicalType);
-    int index;
-    LiveRange* ptr = NULL;
-    if(tSize == OpndSize_32) {
-        /* check live ranges for the VR */
-        index = searchMemTable(compileTable[k].regNum);
-        if(index < 0) {
-            ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at isLastByteCodeOfLiveRange", compileTable[k].regNum);
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return false;
-        }
-        ptr = memVRTable[index].ranges;
-        while(ptr != NULL) {
-            if(offsetPC == ptr->end) return true;
-            ptr = ptr->next;
-        }
-        return false;
-    }
-    /* size of the VR is 64 */
-    /* check live ranges of the low half */
-    index = searchMemTable(compileTable[k].regNum);
-    bool tmpB = false;
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 64-bit VR %d (lower 32) in memTable at isLastByteCodeOfLiveRange", compileTable[k].regNum);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        if(offsetPC == ptr->end) {
-            tmpB = true;
-            break;
-        }
-        ptr = ptr->next;
-    }
-    if(!tmpB) return false;
-    /* check live ranges of the high half */
-    index = searchMemTable(compileTable[k].regNum+1);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 64-bit VR %d (upper 32) in memTable at isLastByteCodeOfLiveRange", compileTable[k].regNum+1);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        if(offsetPC == ptr->end) {
-            return true;
-        }
-        ptr = ptr->next;
-    }
-    return false;
-}
-
-// check if virtual register has loop independent dependence
-bool loopIndepUse(int compileIndex) {
-    int k = compileIndex;
-    OpndSize tSize = getRegSize(compileTable[k].physicalType);
-    int index;
-    bool retCode = false;
-
-    /* check live ranges of the low half */
-    index = searchMemTable(compileTable[k].regNum);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at loopIndepUse", compileTable[k].regNum);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    LiveRange* ptr = memVRTable[index].ranges;
-    if (ptr != NULL && ptr->start > 0)
-        retCode = true;
-    if(!retCode) return false;
-    if(tSize == OpndSize_32) return true;
-
-    /* check for the high half */
-    index = searchMemTable(compileTable[k].regNum+1);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at loopIndepUse", compileTable[k].regNum+1);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    ptr = memVRTable[index].ranges;
-    if (ptr != NULL && ptr->start > 0)
-       return true;
-    return false;
-}
-
-
-//! check whether the current bytecode is in a live range that extends to end of a basic block
-
-//!for 64 bit, return true if true for both low half and high half
-bool reachEndOfBB(int compileIndex) {
-    int k = compileIndex;
-    OpndSize tSize = getRegSize(compileTable[k].physicalType);
-    int index;
-    bool retCode = false;
-    /* check live ranges of the low half */
-    index = searchMemTable(compileTable[k].regNum);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at reachEndOfBB", compileTable[k].regNum);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    LiveRange* ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        if(offsetPC >= ptr->start &&
-           offsetPC <= ptr->end) {
-            if(ptr->end == currentBB->pc_end) {
-                retCode = true;
-            }
-            break;
-        }
-        ptr = ptr->next;
-    }
-    if(!retCode) return false;
-    if(tSize == OpndSize_32) return true;
-    /* check live ranges of the high half */
-    index = searchMemTable(compileTable[k].regNum+1);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at reachEndOfBB", compileTable[k].regNum+1);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        if(offsetPC >= ptr->start &&
-           offsetPC <= ptr->end) {
-            if(ptr->end == currentBB->pc_end) return true;
-            return false;
-        }
-        ptr = ptr->next;
-    }
-#ifdef PRINT_WARNING
-    ALOGW("offsetPC %d not in live range of VR %d", offsetPC, compileTable[k].regNum+1);
-#endif
-    return false;
-}
-
-//!check whether the current bytecode is the next to last access to a VR within a live range
-
-//!for 64 bit, return true if true for both low half and high half
-bool isNextToLastAccess(int compileIndex) {
-    int k = compileIndex;
-    OpndSize tSize = getRegSize(compileTable[k].physicalType);
-    int index;
-    /* check live ranges for the low half */
-    bool retCode = false;
-    index = searchMemTable(compileTable[k].regNum);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at isNextToLastAccess", compileTable[k].regNum);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    LiveRange* ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        int num_access = ptr->num_access;
-
-        if(num_access < 2) {
-           ptr = ptr->next;
-           continue;
-        }
-
-        if(offsetPC == ptr->accessPC[num_access-2]) {
-           retCode = true;
-           break;
-        }
-        ptr = ptr->next;
-    }
-    if(!retCode) return false;
-    if(tSize == OpndSize_32) return true;
-    /* check live ranges for the high half */
-    index = searchMemTable(compileTable[k].regNum+1);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at isNextToLastAccess", compileTable[k].regNum+1);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return false;
-    }
-    ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        int num_access = ptr->num_access;
-
-        if(num_access < 2) {
-           ptr = ptr->next;
-           continue;
-        }
-
-        if(offsetPC == ptr->accessPC[num_access-2]) return true;
-        ptr = ptr->next;
-    }
-    return false;
-}
-
-/** return bytecode offset corresponding to offsetPC
-*/
-int convertOffsetPCtoBytecodeOffset(int offPC) {
-    if(offPC == PC_FOR_START_OF_BB)
-        return currentBB->pc_start;
-    if(offPC == PC_FOR_END_OF_BB)
-        return currentBB->pc_end;
-    for(MIR * mir = currentBB->firstMIRInsn; mir; mir = mir->next) {
-       if(mir->seqNum == offPC)
-         return mir->offset;
-    }
-    return currentBB->pc_end;
-}
-
-/** return the start of the next live range
-    if there does not exist a next live range, return pc_end of the basic block
-    for 64 bits, return the larger one for low half and high half
-    Assume live ranges are sorted in order
-*/
-int getNextLiveRange(int compileIndex) {
-    int k = compileIndex;
-    OpndSize tSize = getRegSize(compileTable[k].physicalType);
-    /* check live ranges of the low half */
-    int index;
-    index = searchMemTable(compileTable[k].regNum);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at getNextLiveRange", compileTable[k].regNum);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return offsetPC;
-    }
-    bool found = false;
-    int nextUse = offsetPC;
-    LiveRange* ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        if(ptr->start > offsetPC) {
-            nextUse = ptr->start;
-            found = true;
-            break;
-        }
-        ptr = ptr->next;
-    }
-    if(!found) return PC_FOR_END_OF_BB;
-    if(tSize == OpndSize_32) return nextUse;
-
-    /* check live ranges of the high half */
-    found = false;
-    index = searchMemTable(compileTable[k].regNum+1);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at getNextLiveRange", compileTable[k].regNum+1);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return offsetPC;
-    }
-    int nextUse2 = offsetPC;
-    ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        if(ptr->start > offsetPC) {
-            nextUse2 = ptr->start;
-            found = true;
-            break;
-        }
-        ptr = ptr->next;
-    }
-    if(!found) return PC_FOR_END_OF_BB;
-    /* return the larger one */
-    return (nextUse2 > nextUse ? nextUse2 : nextUse);
-}
-
-/** return the next access to a variable
-    If variable is 64-bit, get the next access to the lower half and the high half
-        return the eariler one
-*/
-int getNextAccess(int compileIndex) {
-    int k = compileIndex;
-    OpndSize tSize = getRegSize(compileTable[k].physicalType);
-    int index, k3;
-    /* check live ranges of the low half */
-    index = searchMemTable(compileTable[k].regNum);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at getNextAccess", compileTable[k].regNum);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return offsetPC;
-    }
-    bool found = false;
-    int nextUse = offsetPC;
-    LiveRange* ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        if(offsetPC >= ptr->start &&
-           offsetPC <= ptr->end) {
-            /* offsetPC belongs to this live range */
-            for(k3 = 0; k3 < ptr->num_access; k3++) {
-                if(ptr->accessPC[k3] > offsetPC) {
-                    nextUse = ptr->accessPC[k3];
-                    break;
-                }
-            }
-            found = true;
-            break;
-        }
-        ptr = ptr->next;
-    }
-    if(!found) {
-#ifdef PRINT_WARNING
-        ALOGW("offsetPC %d not in live range of VR %d", offsetPC, compileTable[k].regNum);
-#endif
-    }
-    if(tSize == OpndSize_32) return nextUse;
-
-    /* check live ranges of the high half */
-    found = false;
-    index = searchMemTable(compileTable[k].regNum+1);
-    if(index < 0) {
-        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at getNextAccess", compileTable[k].regNum+1);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return offsetPC;
-    }
-    int nextUse2 = offsetPC;
-    ptr = memVRTable[index].ranges;
-    while(ptr != NULL) {
-        if(offsetPC >= ptr->start &&
-           offsetPC <= ptr->end) {
-            for(k3 = 0; k3 < ptr->num_access; k3++) {
-                if(ptr->accessPC[k3] > offsetPC) {
-                    nextUse2 = ptr->accessPC[k3];
-                    break;
-                }
-            }
-            found = true;
-            break;
-        }
-        ptr = ptr->next;
-    }
-    if(!found) {
-#ifdef PRINT_WARNING
-        ALOGW("offsetPC %d not in live range of VR %d", offsetPC, compileTable[k].regNum+1);
-#endif
-    }
-    /* return the earlier one */
-    if(nextUse2 < nextUse) return nextUse2;
-    return nextUse;
-}
-
-/**
- * @brief Free variables that are no longer in use.
- * @param writeBackAllVRs When true, writes back all dirty VRs including
- * constants
- * @return Returns value >= 0 on success.
-*/
-int freeReg(bool writeBackAllVRs) {
-    //If the current BasicBlock is 0, we have nothing to do
-    if(currentBB == NULL) {
-        return 0;
-    }
-
-    //If writeBackAllVRs is true, we also spill the constants
-    if (writeBackAllVRs == true) {
-        for (int k = 0; k < num_const_vr; k++) {
-            if (constVRTable[k].isConst) {
-                writeBackConstVR(constVRTable[k].regNum, constVRTable[k].value);
-            }
-        }
-    }
-
-    for(int k = 0; k < compileTable.size (); k++) {
-        if (writeBackAllVRs && isVirtualReg(compileTable[k].physicalType)
-                && compileTable[k].inPhysicalRegister () == true) {
-#ifdef DEBUG_REGALLOC
-            ALOGI("FREE v%d with type %d allocated to %s",
-                    compileTable[k].regNum, compileTable[k].physicalType,
-                    physicalRegToString(static_cast<PhysicalReg>(
-                            compileTable[k].physicalReg)));
-#endif
-
-            spillLogicalReg(k, true);
-        }
-
-        if(compileTable[k].refCount == 0 && compileTable[k].inPhysicalRegister () == true) {
-            bool isTemp = (isVirtualReg(compileTable[k].physicalType) == false);
-
-            if (isTemp) {
-#ifdef DEBUG_REGALLOC
-                ALOGI("FREE temporary %d with type %d allocated to %s",
-                       compileTable[k].regNum, compileTable[k].physicalType,
-                       physicalRegToString(static_cast<PhysicalReg>(
-                               compileTable[k].physicalReg)));
-#endif
-
-                compileTable[k].setPhysicalReg (PhysicalReg_Null);
-
-                if(compileTable[k].spill_loc_index >= 0) {
-                    /* update spill info for temporaries */
-                    spillIndexUsed[compileTable[k].spill_loc_index >> 2] = 0;
-                    compileTable[k].spill_loc_index = -1;
-                    ALOGI("JIT_INFO: free a temporary register with TRSTATE_SPILLED\n");
-                    SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                    return -1;
-                }
-            }
-        }
-    }
-    syncAllRegs(); //sync up allRegs (isUsed & freeTimeStamp) with compileTable
-    return 0;
-}
-
-//! reduce the reference count by 1
-
-//! input: index to compileTable
-void decreaseRefCount(int index) {
-#ifdef DEBUG_REFCOUNT
-    ALOGI("REFCOUNT: %d in decreaseRefCount %d %d", compileTable[index].refCount,
-            compileTable[index].regNum, compileTable[index].physicalType);
-#endif
-    compileTable[index].refCount--;
-    if(compileTable[index].refCount < 0) {
-        ALOGI("JIT_INFO: refCount is negative for REG %d %d at decreaseRefCount",
-                compileTable[index].regNum, compileTable[index].physicalType);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return;
-    }
-}
-//! reduce the reference count of a VR by 1
-
-//! input: reg & type
-int updateRefCount(int reg, LowOpndRegType type) {
-    if(currentBB == NULL) return 0;
-    int index = searchCompileTable(LowOpndRegType_virtual | type, reg);
-    if(index < 0) {
-        ALOGI("JIT_INFO: virtual reg %d type %d not found in updateRefCount\n", reg, type);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-    decreaseRefCount(index);
-    return 0;
-}
-//! reduce the reference count of a variable by 1
-
-//! The variable is named with lowering module's naming mechanism
-int updateRefCount2(int reg, int type, bool isPhysical) {
-    if(currentBB == NULL) return 0;
-    int newType = convertType(type, reg, isPhysical);
-    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
-    int index = searchCompileTable(newType, reg);
-    if(index < 0) {
-        ALOGI("JIT_INFO: reg %d type %d not found in updateRefCount\n", reg, newType);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-    decreaseRefCount(index);
-    return 0;
-}
-
-//! check whether a virtual register is in a physical register
-
-//! If updateRefCount is 0, do not update reference count;
-//!If updateRefCount is 1, update reference count only when VR is in a physical register
-//!If updateRefCount is 2, update reference count
-int checkVirtualReg(int reg, LowOpndRegType type, int updateRefCount) {
-    if(currentBB == NULL) return PhysicalReg_Null;
-    int index = searchCompileTable(LowOpndRegType_virtual | type, reg);
-    if(index < 0) {
-        ALOGI("JIT_INFO: virtual reg %d type %d not found in checkVirtualReg\n", reg, type);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return PhysicalReg_Null;
-    }
-    //reduce reference count
-    if(compileTable[index].physicalReg != PhysicalReg_Null) {
-        if(updateRefCount != 0) decreaseRefCount(index);
-        return compileTable[index].physicalReg;
-    }
-    if(updateRefCount == 2) decreaseRefCount(index);
-    return PhysicalReg_Null;
-}
-//!check whether a temporary can share the same physical register with a VR
-
-//!This is called in get_virtual_reg
-//!If this function returns false, new register will be allocated for this temporary
-bool checkTempReg2(int reg, int type, bool isPhysical, int physicalRegForVR, int vB) {
-    if (isPhysical) {
-        // If temporary is already physical, we cannot share with VR
-        return false;
-    }
-
-    int newType = convertType(type, reg, isPhysical);
-    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
-
-    // Look through all of the temporaries used by this bytecode implementation
-    for(int k = 0; k < num_temp_regs_per_bytecode; k++) {
-
-        if(infoByteCodeTemp[k].physicalType == newType &&
-           infoByteCodeTemp[k].regNum == reg) {
-            // We found a matching temporary
-
-            if (!infoByteCodeTemp[k].is8Bit
-                    || (physicalRegForVR >= PhysicalReg_EAX
-                            && physicalRegForVR <= PhysicalReg_EDX)) {
-                DEBUG_MOVE_OPT(ALOGD("Temp%d can%s share %s with v%d",
-                        reg, infoByteCodeTemp[k].shareWithVR ? "" : " NOT",
-                        physicalRegToString(static_cast<PhysicalReg>(
-                                physicalRegForVR)), vB));
-
-                return infoByteCodeTemp[k].shareWithVR;
-            } else {
-                DEBUG_MOVE_OPT(ALOGD("Temp%d can NOT share %s with v%d",
-                        reg, physicalRegToString(static_cast<PhysicalReg>(
-                                physicalRegForVR)), vB));
-
-                // We cannot share same physical register as VR
-                return false;
-            }
-        }
-    }
-    ALOGI("JIT_INFO: in checkTempReg2 %d %d\n", reg, newType);
-    SET_JIT_ERROR(kJitErrorRegAllocFailed);
-    return false;
-}
-//!check whether a temporary can share the same physical register with a VR
-
-//!This is called in set_virtual_reg
-int checkTempReg(int reg, int type, bool isPhysical, int vrNum) {
-    if(currentBB == NULL) return PhysicalReg_Null;
-
-    int newType = convertType(type, reg, isPhysical);
-    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
-    int index = searchCompileTable(newType, reg);
-    if(index < 0) {
-        ALOGI("JIT_INFO: temp reg %d type %d not found in checkTempReg\n", reg, newType);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return PhysicalReg_Null;
-    }
-
-    //a temporary register can share the same physical reg with a VR if registerAllocMove is called
-    //this will cause problem with move bytecode
-    //get_VR(v1, t1) t1 and v1 point to the same physical reg
-    //set_VR(t1, v2) t1 and v2 point to the same physical reg
-    //this will cause v1 and v2 point to the same physical reg
-    //FIX: if this temp reg shares a physical reg with another reg
-    if(compileTable[index].physicalReg != PhysicalReg_Null) {
-        int k;
-        for(k = 0; k < compileTable.size (); k++) {
-            if(k == index) continue;
-            if(compileTable[k].physicalReg == compileTable[index].physicalReg) {
-                return PhysicalReg_Null; //will allocate a register for VR
-            }
-        }
-        decreaseRefCount(index);
-        return compileTable[index].physicalReg;
-    }
-    if(compileTable[index].spill_loc_index >= 0) {
-        //registerAlloc will call unspillLogicalReg (load from memory)
-#ifdef DEBUG_REGALLOC
-        ALOGW("in checkTempReg, the temporary register %d %d was spilled", reg, type);
-#endif
-        //No need for written, we don't write in it yet, just aliasing at worse
-        int regAll = registerAlloc(type, reg, isPhysical, true/* updateRefCount */);
-        return regAll;
-    }
-    return PhysicalReg_Null;
-}
-//!check whether a variable has exposed usage in a basic block
-
-//!It calls hasExposedUsage2
-bool hasExposedUsage(LowOpndRegType type, int regNum, BasicBlock_O1* bb) {
-    int index = searchVirtualInfoOfBB(type, regNum, bb);
-    if(index >= 0 && hasExposedUsage2(bb, index)) {
-        return true;
-    }
-    return false;
-}
-
-/**
- * @brief Handle the spilling of registers at the end of a BasicBlock.
- * @param syncChildren If sync children is set to true, then we create or sync association table for child.
- * @return -1 if error, 0 otherwise
- */
-int handleRegistersEndOfBB (bool syncChildren)
-{
-    //First we call freeReg to get rid of any temporaries that might be using physical registers.
-    //Since we are at the end of the BB, we don't need to spill the temporaries because we
-    //are done using them.
-    freeReg (false);
-
-    //If it's a jump, then we don't update the association tables. The reason is
-    //that the implementation of jumping bytecode (for example "if" bytecode) will
-    //create association tables for children
-    if (syncChildren == true)
-    {
-        //Update association tables of child. There should technically be just one child
-        //but we generically try to pass our information to all children.
-
-        if (AssociationTable::createOrSyncTable(currentBB, true) == false)
-        {
-            //If syncing failed, the error code will be already set so we just pass along error information
-            return -1;
-        }
-
-        if (AssociationTable::createOrSyncTable(currentBB, false) == false)
-        {
-            //If syncing failed, the error code will be already set so we just pass along error information
-            return -1;
-        }
-    }
-
-    syncAllRegs();
-
-    return 0;
-}
-
-//! get ready for the next version of a hard-coded register
-
-//!set its physicalReg to Null and update its reference count
-int nextVersionOfHardReg(PhysicalReg pReg, int refCount) {
-    int indexT = searchCompileTable(LowOpndRegType_gp | LowOpndRegType_hard, pReg);
-    if(indexT < 0) {
-        ALOGI("JIT_INFO: Physical reg not found at nextVersionOfHardReg");
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-    compileTable[indexT].setPhysicalReg (PhysicalReg_Null);
-#ifdef DEBUG_REFCOUNT
-    ALOGI("REFCOUNT: to %d in nextVersionOfHardReg %d", refCount, pReg);
-#endif
-    compileTable[indexT].refCount = refCount;
-    return 0;
-}
-
-/**
- * @brief Updates compile table with virtual register information.
- * @details If compile table already contains information about virtual register, only the
- * reference counts are updated. Otherwise a new entry is created in the compile table.
- * @param regInfo Information about the virtual register.
- */
-void insertFromVirtualInfo (const VirtualRegInfo &regInfo)
-{
-    int vR = regInfo.regNum;
-
-    //We want to find entry in the compile table that matches the physical type we want.
-    //Since compile table keeps track of physical type along with logical type in same field,
-    //we do a binary bitwise inclusive or including the virtual register type.
-    int index = searchCompileTable (LowOpndRegType_virtual | regInfo.physicalType, vR);
-
-    if (index < 0)
-    {
-        //If we get here it means that the VR is not in the compile table
-
-        //Create the new entry
-        CompileTableEntry newEntry (regInfo);
-
-        //Create the new entry and then copy it to the table
-        compileTable.insert (newEntry);
-    }
-    else
-    {
-        //Just update the ref count when we already have an entry
-        compileTable[index].updateRefCount (regInfo.refCount);
-    }
-}
-
-/**
- * @brief Updates compile table with temporary information.
- * @param tempRegInfo Information about the temporary.
- */
-static void insertFromTempInfo (const TempRegInfo &tempRegInfo)
-{
-    int index = searchCompileTable(tempRegInfo.physicalType, tempRegInfo.regNum);
-
-    //If we did not find it in compile table simply insert it.
-    if (index < 0)
-    {
-        CompileTableEntry newEntry (tempRegInfo);
-
-        compileTable.insert (newEntry);
-    }
-    else
-    {
-        //Set the physical register
-        compileTable[index].setPhysicalReg (PhysicalReg_Null);
-
-        //Update the reference count for this temporary
-        compileTable[index].updateRefCount (tempRegInfo.refCount);
-
-        //Create link with the corresponding VR if needed
-        compileTable[index].linkToVR (tempRegInfo.linkageToVR);
-
-        //Reset spill location because this is a new temp which has not been spilled
-        compileTable[index].resetSpillLocation ();
-    }
-}
-
-/** print infoBasicBlock of the given basic block
-*/
-void dumpVirtualInfoOfBasicBlock(BasicBlock_O1* bb) {
-    unsigned int jj, max;
-    ALOGI("Virtual Info for BB%d --------", bb->id);
-    max = bb->infoBasicBlock.size ();
-    for(jj = 0; jj < max; jj++) {
-        ALOGI("regNum %d physicalType %d accessType %d refCount %d def ",
-               bb->infoBasicBlock[jj].regNum, bb->infoBasicBlock[jj].physicalType,
-               bb->infoBasicBlock[jj].accessType, bb->infoBasicBlock[jj].refCount);
-        int k;
-        for(k = 0; k < bb->infoBasicBlock[jj].num_reaching_defs; k++)
-            ALOGI("[%x %d %d %d] ", bb->infoBasicBlock[jj].reachingDefs[k].offsetPC,
-                   bb->infoBasicBlock[jj].reachingDefs[k].regNum,
-                   bb->infoBasicBlock[jj].reachingDefs[k].physicalType,
-                   bb->infoBasicBlock[jj].reachingDefs[k].accessType);
-    }
-}
-
-/** print compileTable
-*/
-void dumpCompileTable() {
-    ALOGD("+++++++++++++++++++++ Compile Table +++++++++++++++++++++");
-    ALOGD("%d entries\t%d memory_vr\t%d const_vr", compileTable.size (),
-            num_memory_vr, num_const_vr);
-    for(int entry = 0; entry < compileTable.size (); entry++) {
-        ALOGD("regNum %d physicalType %d refCount %d physicalReg %s",
-               compileTable[entry].regNum, compileTable[entry].physicalType,
-               compileTable[entry].refCount, physicalRegToString(
-                       static_cast<PhysicalReg>(compileTable[entry].physicalReg)));
-    }
-    for(int entry = 0; entry < num_memory_vr; entry++) {
-        ALOGD("v%d inMemory:%s", memVRTable[entry].regNum,
-                memVRTable[entry].inMemory ? "yes" : "no");
-    }
-
-    for(int entry = 0; entry < num_const_vr; entry++) {
-        ALOGD("v%d isConst:%s value:%d", constVRTable[entry].regNum,
-                constVRTable[entry].isConst ? "yes" : "no",
-                constVRTable[entry].value);
-    }
-    ALOGD("---------------------------------------------------------");
-}
-
-/* BEGIN code to handle state transfers */
-//! save the current state of register allocator to a state table
-
-//!
-void rememberState(int stateNum) {
-#ifdef DEBUG_STATE
-    ALOGI("STATE: remember state %d", stateNum);
-#endif
-    int k;
-    for(k = 0; k < compileTable.size (); k++) {
-        compileTable[k].rememberState (stateNum);
-#ifdef DEBUG_STATE
-        ALOGI("logical reg %d %d mapped to physical reg %d with spill index %d refCount %d",
-               compileTable[k].regNum, compileTable[k].physicalType, compileTable[k].physicalReg,
-               compileTable[k].spill_loc_index, compileTable[k].refCount);
-#endif
-    }
-    for(k = 0; k < num_memory_vr; k++) {
-        if(stateNum == 1) {
-            stateTable2_1[k].regNum = memVRTable[k].regNum;
-            stateTable2_1[k].inMemory = memVRTable[k].inMemory;
-        }
-        else if(stateNum == 2) {
-            stateTable2_2[k].regNum = memVRTable[k].regNum;
-            stateTable2_2[k].inMemory = memVRTable[k].inMemory;
-        }
-        else if(stateNum == 3) {
-            stateTable2_3[k].regNum = memVRTable[k].regNum;
-            stateTable2_3[k].inMemory = memVRTable[k].inMemory;
-        }
-        else if(stateNum == 4) {
-            stateTable2_4[k].regNum = memVRTable[k].regNum;
-            stateTable2_4[k].inMemory = memVRTable[k].inMemory;
-        }
-        else {
-            ALOGI("JIT_INFO: state table overflow at goToState for compileTable\n");
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return;
-        }
-#ifdef DEBUG_STATE
-        ALOGI("virtual reg %d in memory %d", memVRTable[k].regNum, memVRTable[k].inMemory);
-#endif
-    }
-}
-
-//!update current state of register allocator with a state table
-
-//!
-void goToState(int stateNum) {
-    int k;
-#ifdef DEBUG_STATE
-    ALOGI("STATE: go to state %d", stateNum);
-#endif
-    for(k = 0; k < compileTable.size (); k++) {
-        compileTable[k].goToState (stateNum);
-    }
-    int retCode = updateSpillIndexUsed();
-    if (retCode < 0)
-        return;
-    syncAllRegs(); //to sync up allRegs CAN'T call freeReg here
-    //since it will change the state!!!
-    for(k = 0; k < num_memory_vr; k++) {
-        if(stateNum == 1) {
-            memVRTable[k].regNum = stateTable2_1[k].regNum;
-            memVRTable[k].inMemory = stateTable2_1[k].inMemory;
-        }
-        else if(stateNum == 2) {
-            memVRTable[k].regNum = stateTable2_2[k].regNum;
-            memVRTable[k].inMemory = stateTable2_2[k].inMemory;
-        }
-        else if(stateNum == 3) {
-            memVRTable[k].regNum = stateTable2_3[k].regNum;
-            memVRTable[k].inMemory = stateTable2_3[k].inMemory;
-        }
-        else if(stateNum == 4) {
-            memVRTable[k].regNum = stateTable2_4[k].regNum;
-            memVRTable[k].inMemory = stateTable2_4[k].inMemory;
-        }
-        else {
-            ALOGI("JIT_INFO: state table overflow at goToState for memVRTable\n");
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return;
-        }
-        DEBUG_MEMORYVR(ALOGD("Updating state of v%d %sin memory",
-                memVRTable[k].regNum, (memVRTable[k].inMemory ? "" : "NOT ")));
-    }
-}
-typedef struct TransferOrder {
-    int targetReg;
-    int targetSpill;
-    int compileIndex;
-} TransferOrder;
-#define MAX_NUM_DEST 20
-//! a source register is used as a source in transfer
-//! it can have a maximum of MAX_NUM_DEST destinations
-typedef struct SourceReg {
-    int physicalReg;
-    int num_dests; //check bound
-    TransferOrder dsts[MAX_NUM_DEST];
-} SourceReg;
-int num_src_regs = 0; //check bound
-//! physical registers that are used as a source in transfer
-//! we allow a maximum of MAX_NUM_DEST sources in a transfer
-SourceReg srcRegs[MAX_NUM_DEST];
-//! tell us whether a source register is handled already
-bool handledSrc[MAX_NUM_DEST];
-//! in what order should the source registers be handled
-int handledOrder[MAX_NUM_DEST];
-
-//! \brief insert a source register with a single destination
-//!
-//! \param srcPhysical
-//! \param targetReg
-//! \param targetSpill
-//! \param index
-//!
-//! \return -1 on error, 0 otherwise
-int insertSrcReg(int srcPhysical, int targetReg, int targetSpill, int index) {
-    int k = 0;
-    for(k = 0; k < num_src_regs; k++) {
-        if(srcRegs[k].physicalReg == srcPhysical) { //increase num_dests
-            if(srcRegs[k].num_dests >= MAX_NUM_DEST) {
-                ALOGI("JIT_INFO: Exceed number dst regs for a source reg\n");
-                SET_JIT_ERROR(kJitErrorMaxDestRegPerSource);
-                return -1;
-            }
-            srcRegs[k].dsts[srcRegs[k].num_dests].targetReg = targetReg;
-            srcRegs[k].dsts[srcRegs[k].num_dests].targetSpill = targetSpill;
-            srcRegs[k].dsts[srcRegs[k].num_dests].compileIndex = index;
-            srcRegs[k].num_dests++;
-            return 0;
-        }
-    }
-    if(num_src_regs >= MAX_NUM_DEST) {
-        ALOGI("JIT_INFO: Exceed number of source regs\n");
-        SET_JIT_ERROR(kJitErrorMaxDestRegPerSource);
-        return -1;
-    }
-    srcRegs[num_src_regs].physicalReg = srcPhysical;
-    srcRegs[num_src_regs].num_dests = 1;
-    srcRegs[num_src_regs].dsts[0].targetReg = targetReg;
-    srcRegs[num_src_regs].dsts[0].targetSpill = targetSpill;
-    srcRegs[num_src_regs].dsts[0].compileIndex = index;
-    num_src_regs++;
-    return 0;
-}
-
-//! check whether a register is a source and the source is not yet handled
-
-//!
-bool dstStillInUse(int dstReg) {
-    if(dstReg == PhysicalReg_Null) return false;
-    int k;
-    int index = -1;
-    for(k = 0; k < num_src_regs; k++) {
-        if(dstReg == srcRegs[k].physicalReg) {
-            index = k;
-            break;
-        }
-    }
-    if(index < 0) return false; //not in use
-    if(handledSrc[index]) return false; //not in use
-    return true;
-}
-
-//! construct a legal order of the source registers in this transfer
-
-//!
-void constructSrcRegs(int stateNum) {
-    int k;
-    num_src_regs = 0;
-#ifdef DEBUG_STATE
-    ALOGI("IN constructSrcRegs");
-#endif
-
-    for(k = 0; k < compileTable.size (); k++) {
-#ifdef DEBUG_STATE
-        ALOGI("logical reg %d %d mapped to physical reg %d with spill index %d refCount %d",
-               compileTable[k].regNum, compileTable[k].physicalType, compileTable[k].physicalReg,
-               compileTable[k].spill_loc_index, compileTable[k].refCount);
-#endif
-
-        int pType = compileTable[k].physicalType;
-        //ignore hardcoded logical registers
-        if((pType & LowOpndRegType_hard) != 0) continue;
-        //ignore type _fs
-        if((pType & MASK_FOR_TYPE) == LowOpndRegType_fs) continue;
-        if((pType & MASK_FOR_TYPE) == LowOpndRegType_fs_s) continue;
-
-        //GL VR refCount is zero, can't ignore
-        //L VR refCount is zero, ignore
-        //GG VR refCount is zero, can't ignore
-        //temporary refCount is zero, ignore
-
-        /* get the target state */
-        int targetReg = compileTable[k].getStatePhysicalRegister (stateNum);
-        int targetSpill = compileTable[k].getStateSpillLocation (stateNum);
-
-        /* there exists an ordering problem
-           for example:
-             for a VR, move from memory to a physical reg esi
-             for a temporary regsiter, from esi to ecx
-             if we handle VR first, content of the temporary reg. will be overwritten
-           there are 4 cases:
-             I: a variable is currently in memory and its target is in physical reg
-             II: a variable is currently in a register and its target is in memory
-             III: a variable is currently in a different register
-             IV: a variable is currently in a different memory location (for non-VRs)
-           For now, case IV is not handled since it didn't show
-        */
-        if(compileTable[k].physicalReg != targetReg &&
-           isVirtualReg(compileTable[k].physicalType)) {
-            /* handles VR for case I to III */
-
-            if(compileTable[k].physicalReg == PhysicalReg_Null && targetReg != PhysicalReg_Null) {
-                /* handles VR for case I:
-                   insert a xfer order from PhysicalReg_Null to targetReg */
-                 if (insertSrcReg(PhysicalReg_Null, targetReg, targetSpill, k) == -1)
-                     return;
-#ifdef DEBUG_STATE
-                ALOGI("insert for VR Null %d %d %d", targetReg, targetSpill, k);
-#endif
-            }
-
-            if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg != PhysicalReg_Null) {
-                /* handles VR for case III
-                   insert a xfer order from srcReg to targetReg */
-                if (insertSrcReg(compileTable[k].physicalReg, targetReg, targetSpill, k) == -1)
-                    return;
-            }
-
-            if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg == PhysicalReg_Null) {
-                /* handles VR for case II
-                   insert a xfer order from srcReg to memory */
-                if (insertSrcReg(compileTable[k].physicalReg, targetReg, targetSpill, k) == -1)
-                    return;
-            }
-        }
-
-        if(compileTable[k].physicalReg != targetReg &&
-           !isVirtualReg(compileTable[k].physicalType)) {
-            /* handles non-VR for case I to III */
-
-            if(compileTable[k].physicalReg == PhysicalReg_Null && targetReg != PhysicalReg_Null) {
-                /* handles non-VR for case I */
-                if(compileTable[k].spill_loc_index < 0) {
-                    /* this variable is freed, no need to transfer */
-#ifdef DEBUG_STATE
-                    ALOGW("in transferToState spill_loc_index is negative for temporary %d", compileTable[k].regNum);
-#endif
-                } else {
-                    /* insert a xfer order from memory to targetReg */
-#ifdef DEBUG_STATE
-                    ALOGI("insert Null %d %d %d", targetReg, targetSpill, k);
-#endif
-                    if (insertSrcReg(PhysicalReg_Null, targetReg, targetSpill, k) == -1)
-                        return;
-                }
-            }
-
-            if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg != PhysicalReg_Null) {
-                /* handles non-VR for case III
-                   insert a xfer order from srcReg to targetReg */
-                if (insertSrcReg(compileTable[k].physicalReg, targetReg, targetSpill, k) == -1)
-                    return;
-            }
-
-            if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg == PhysicalReg_Null) {
-                /* handles non-VR for case II */
-                if(targetSpill < 0) {
-                    /* this variable is freed, no need to transfer */
-#ifdef DEBUG_STATE
-                    ALOGW("in transferToState spill_loc_index is negative for temporary %d", compileTable[k].regNum);
-#endif
-                } else {
-                    /* insert a xfer order from srcReg to memory */
-                    if (insertSrcReg(compileTable[k].physicalReg, targetReg, targetSpill, k) == -1)
-                        return;
-                }
-            }
-
-        }
-    }//for compile entries
-
-    int k2;
-#ifdef DEBUG_STATE
-    for(k = 0; k < num_src_regs; k++) {
-        ALOGI("SRCREG %d: ", srcRegs[k].physicalReg);
-        for(k2 = 0; k2 < srcRegs[k].num_dests; k2++) {
-            int index = srcRegs[k].dsts[k2].compileIndex;
-            ALOGI("[%d %d %d: %d %d %d] ", srcRegs[k].dsts[k2].targetReg,
-                   srcRegs[k].dsts[k2].targetSpill, srcRegs[k].dsts[k2].compileIndex,
-                   compileTable[index].regNum, compileTable[index].physicalType,
-                   compileTable[index].spill_loc_index);
-        }
-        ALOGI("");
-    }
-#endif
-
-    /* construct an order: xfers from srcReg first, then xfers from memory */
-    int num_handled = 0;
-    int num_in_order = 0;
-    for(k = 0; k < num_src_regs; k++) {
-        if(srcRegs[k].physicalReg == PhysicalReg_Null) {
-            handledSrc[k] = true;
-            num_handled++;
-        } else {
-            handledSrc[k] = false;
-        }
-    }
-    while(num_handled < num_src_regs) {
-        int prev_handled = num_handled;
-        for(k = 0; k < num_src_regs; k++) {
-            if(handledSrc[k]) continue;
-            bool canHandleNow = true;
-            for(k2 = 0; k2 < srcRegs[k].num_dests; k2++) {
-                if(dstStillInUse(srcRegs[k].dsts[k2].targetReg)) {
-                    canHandleNow = false;
-                    break;
-                }
-            }
-            if(canHandleNow) {
-                handledSrc[k] = true;
-                num_handled++;
-                handledOrder[num_in_order] = k;
-                num_in_order++;
-            }
-        } //for k
-        if(num_handled == prev_handled) {
-            ALOGI("JIT_INFO: No progress in selecting order while in constructSrcReg");
-            SET_JIT_ERROR(kJitErrorStateTransfer);
-            return;
-        }
-    } //while
-    for(k = 0; k < num_src_regs; k++) {
-        if(srcRegs[k].physicalReg == PhysicalReg_Null) {
-            handledOrder[num_in_order] = k;
-            num_in_order++;
-        }
-    }
-    if(num_in_order != num_src_regs) {
-        ALOGI("JIT_INFO: num_in_order != num_src_regs while in constructSrcReg");
-        SET_JIT_ERROR(kJitErrorStateTransfer);
-        return;
-    }
-#ifdef DEBUG_STATE
-    ALOGI("ORDER: ");
-    for(k = 0; k < num_src_regs; k++) {
-        ALOGI("%d ", handledOrder[k]);
-    }
-#endif
-}
-//! transfer the state of register allocator to a state specified in a state table
-
-//!
-void transferToState(int stateNum) {
-    freeReg(false); //do not spill GL
-    int k;
-#ifdef DEBUG_STATE
-    ALOGI("STATE: transfer to state %d", stateNum);
-#endif
-    if(stateNum > 4 || stateNum < 1) {
-        ALOGI("JIT_INFO: State table overflow at transferToState");
-        SET_JIT_ERROR(kJitErrorStateTransfer);
-        return;
-    }
-    constructSrcRegs(stateNum);
-    int k4, k3;
-    for(k4 = 0; k4 < num_src_regs; k4++) {
-        int k2 = handledOrder[k4]; //index to srcRegs
-        for(k3 = 0; k3 < srcRegs[k2].num_dests; k3++) {
-            k = srcRegs[k2].dsts[k3].compileIndex;
-            int targetReg = srcRegs[k2].dsts[k3].targetReg;
-            int targetSpill = srcRegs[k2].dsts[k3].targetSpill;
-            if(compileTable[k].physicalReg != targetReg && isVirtualReg(compileTable[k].physicalType)) {
-                OpndSize oSize = getRegSize(compileTable[k].physicalType);
-                bool isSS = ((compileTable[k].physicalType & MASK_FOR_TYPE) == LowOpndRegType_ss);
-                if(compileTable[k].physicalReg == PhysicalReg_Null && targetReg != PhysicalReg_Null) {
-                    if(isSS)
-                        move_ss_mem_to_reg_noalloc(4*compileTable[k].regNum,
-                                                   PhysicalReg_FP, true,
-                                                   MemoryAccess_VR, compileTable[k].regNum,
-                                                   targetReg, true);
-                    else
-                        move_mem_to_reg_noalloc(oSize, 4*compileTable[k].regNum,
-                                                PhysicalReg_FP, true,
-                                                MemoryAccess_VR, compileTable[k].regNum,
-                                                targetReg, true);
-                }
-                if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg != PhysicalReg_Null) {
-                    move_reg_to_reg_noalloc((isSS ? OpndSize_64 : oSize),
-                                            compileTable[k].physicalReg, true,
-                                            targetReg, true);
-                }
-                if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg == PhysicalReg_Null) {
-                    writeBackVR(compileTable[k].regNum, (LowOpndRegType)(compileTable[k].physicalType & MASK_FOR_TYPE),
-                              compileTable[k].physicalReg);
-                }
-            } //VR
-            if(compileTable[k].physicalReg != targetReg && !isVirtualReg(compileTable[k].physicalType)) {
-                OpndSize oSize = getRegSize(compileTable[k].physicalType);
-                if(compileTable[k].physicalReg == PhysicalReg_Null && targetReg != PhysicalReg_Null) {
-                    loadFromSpillRegion(oSize, targetReg,
-                                        compileTable[k].spill_loc_index);
-                }
-                //both are not null, move from one to the other
-                if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg != PhysicalReg_Null) {
-                    move_reg_to_reg_noalloc(oSize, compileTable[k].physicalReg, true,
-                                            targetReg, true);
-                }
-                //current is not null, target is null (move from reg to memory)
-                if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg == PhysicalReg_Null) {
-                    saveToSpillRegion(oSize, compileTable[k].physicalReg, targetSpill);
-                }
-            } //temporary
-        }//for
-    }//for
-    for(k = 0; k < num_memory_vr; k++) {
-        bool targetBool = false;
-        int targetReg = -1;
-        if(stateNum == 1) {
-            targetReg = stateTable2_1[k].regNum;
-            targetBool = stateTable2_1[k].inMemory;
-        }
-        else if(stateNum == 2) {
-            targetReg = stateTable2_2[k].regNum;
-            targetBool = stateTable2_2[k].inMemory;
-        }
-        else if(stateNum == 3) {
-            targetReg = stateTable2_3[k].regNum;
-            targetBool = stateTable2_3[k].inMemory;
-        }
-        else if(stateNum == 4) {
-            targetReg = stateTable2_4[k].regNum;
-            targetBool = stateTable2_4[k].inMemory;
-        }
-        if(targetReg != memVRTable[k].regNum) {
-            ALOGI("JIT_INFO: regNum mismatch in transferToState");
-            SET_JIT_ERROR(kJitErrorStateTransfer);
-            return;
-        }
-        if(targetBool && (!memVRTable[k].inMemory)) {
-            //dump to memory, check entries in compileTable: vA gp vA xmm vA ss
-#ifdef DEBUG_STATE
-            ALOGW("inMemory mismatch for VR %d in transferToState", targetReg);
-#endif
-            bool doneXfer = false;
-
-            int index = searchCompileTable(LowOpndRegType_xmm | LowOpndRegType_virtual, targetReg);
-            if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-                writeBackVR(targetReg, LowOpndRegType_xmm, compileTable[index].physicalReg);
-                doneXfer = true;
-            } else if (index >= 0 && writeBackVRIfConstant(targetReg, LowOpndRegType_xmm) == true) {
-                doneXfer = true;
-            }
-
-            if(!doneXfer) { //vA-1, xmm
-                index = searchCompileTable(LowOpndRegType_xmm | LowOpndRegType_virtual, targetReg-1);
-                if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-                    writeBackVR(targetReg-1, LowOpndRegType_xmm, compileTable[index].physicalReg);
-                    doneXfer = true;
-                }
-                else if (index >= 0 && writeBackVRIfConstant(targetReg - 1, LowOpndRegType_xmm) == true) {
-                    doneXfer = true;
-                }
-            }
-            if(!doneXfer) { //vA gp
-                index = searchCompileTable(LowOpndRegType_gp | LowOpndRegType_virtual, targetReg);
-                if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-                    writeBackVR(targetReg, LowOpndRegType_gp, compileTable[index].physicalReg);
-                    doneXfer = true;
-                }
-                else if (index >= 0 && writeBackVRIfConstant(targetReg, LowOpndRegType_gp) == true) {
-                    doneXfer = true;
-                }
-            }
-            if(!doneXfer) { //vA, ss
-                index = searchCompileTable(LowOpndRegType_ss | LowOpndRegType_virtual, targetReg);
-                if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
-                    writeBackVR(targetReg, LowOpndRegType_ss, compileTable[index].physicalReg);
-                    doneXfer = true;
-                }
-                else if (index >= 0 && writeBackVRIfConstant(targetReg, LowOpndRegType_ss) == true) {
-                    doneXfer = true;
-                }
-            }
-            if(!doneXfer) {
-                ALOGI("JIT_INFO: Can't match inMemory state of v%d in "
-                        "transferToState.", targetReg);
-                SET_JIT_ERROR(kJitErrorStateTransfer);
-                return;
-            }
-        }
-        if((!targetBool) && memVRTable[k].inMemory) {
-            //do nothing
-        }
-    }
-#ifdef DEBUG_STATE
-    ALOGI("END transferToState %d", stateNum);
-#endif
-    goToState(stateNum);
-}
-/* END code to handle state transfers */
diff --git a/vm/compiler/codegen/x86/AnalysisO1.h b/vm/compiler/codegen/x86/AnalysisO1.h
deleted file mode 100644
index 93e20ee..0000000
--- a/vm/compiler/codegen/x86/AnalysisO1.h
+++ /dev/null
@@ -1,445 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file AnalysisO1.h
-    \brief A header file to define data structures used by register allocator & const folding
-*/
-#ifndef _DALVIK_NCG_ANALYSISO1_H
-#define _DALVIK_NCG_ANALYSISO1_H
-
-#include <set>
-#include "Dalvik.h"
-#include "enc_wrapper.h"
-#include "Lower.h"
-#ifdef WITH_JIT
-#include "compiler/CompilerIR.h"
-#endif
-#include "RegisterizationBE.h"
-
-//! maximal number of edges per basic block
-#define MAX_NUM_EDGE_PER_BB 300
-//! maximal number of virtual registers per basic block
-#define MAX_REG_PER_BASICBLOCK 140
-//! maximal number of virtual registers per bytecode
-#define MAX_REG_PER_BYTECODE 40
-//! maximal number of virtual registers per method
-#define MAX_REG_PER_METHOD 200
-//! maximal number of temporaries per bytecode
-#define MAX_TEMP_REG_PER_BYTECODE 30
-
-#define MAX_CONST_REG 150
-#define NUM_MEM_VR_ENTRY 140
-
-#define MASK_FOR_TYPE 7 //last 3 bits 111
-
-#define LOOP_COUNT 10
-
-//! maximal number of transfer points per basic block
-#define MAX_XFER_PER_BB 1000  //on Jan 4
-#define PC_FOR_END_OF_BB -999
-#define PC_FOR_START_OF_BB -998
-
-//! various cases of overlapping between 2 variables
-typedef enum OverlapCase {
-  OVERLAP_ALIGN = 0,
-  OVERLAP_B_IS_LOW_OF_A,
-  OVERLAP_B_IS_HIGH_OF_A,
-  OVERLAP_LOW_OF_A_IS_HIGH_OF_B,
-  OVERLAP_HIGH_OF_A_IS_LOW_OF_B,
-  OVERLAP_A_IS_LOW_OF_B,
-  OVERLAP_A_IS_HIGH_OF_B,
-  OVERLAP_B_COVER_A,
-  OVERLAP_B_COVER_LOW_OF_A,
-  OVERLAP_B_COVER_HIGH_OF_A,
-  OVERLAP_NO
-} OverlapCase;
-
-//!access type of a variable
-typedef enum RegAccessType {
-  REGACCESS_D = 0,
-  REGACCESS_U,
-  REGACCESS_DU,
-  REGACCESS_UD,
-  REGACCESS_L,
-  REGACCESS_H,
-  REGACCESS_UL,
-  REGACCESS_UH,
-  REGACCESS_LU,
-  REGACCESS_HU,
-  REGACCESS_N, //no access
-  REGACCESS_UNKNOWN
-} RegAccessType;
-
-//! helper state indexes to determine if freeing VRs needs to be delayed
-enum VRDelayFreeCounters {
-  VRDELAY_NULLCHECK = 0, // used when VR is used for null check and freeing must be delayed
-  VRDELAY_BOUNDCHECK = 1, // used when VR is used for bound check and freeing must be delayed
-  VRDELAY_CAN_THROW = 2, // used when bytecode can throw exception, in fact delays freeing any VR
-  VRDELAY_COUNT = 3, // Count of delay reasons
-};
-
-//!information about a physical register
-typedef struct RegisterInfo {
-  PhysicalReg physicalReg;
-  bool isUsed;
-  bool isCalleeSaved;
-  int freeTimeStamp;
-} RegisterInfo;
-
-//!specifies the weight of a VR allocated to a specific physical register
-//!it is used for GPR VR only
-typedef struct RegAllocConstraint {
-  PhysicalReg physicalReg;
-  int count;
-} RegAllocConstraint;
-
-typedef enum XferType {
-  XFER_MEM_TO_XMM, //for usage
-  XFER_DEF_TO_MEM, //def is gp
-  XFER_DEF_TO_GP_MEM,
-  XFER_DEF_TO_GP,
-  XFER_DEF_IS_XMM //def is xmm
-} XferType;
-typedef struct XferPoint {
-  int tableIndex; //generated from a def-use pair
-  XferType xtype;
-  int offsetPC;
-  int regNum; //get or set VR at offsetPC
-  LowOpndRegType physicalType;
-
-  //if XFER_DEF_IS_XMM
-  int vr_gpl; //a gp VR that uses the lower half of the def
-  int vr_gph;
-  bool dumpToXmm;
-  bool dumpToMem;
-} XferPoint;
-
-//!for def: accessType means which part of the VR defined at offestPC is live now
-//!for use: accessType means which part of the usage comes from the reachingDef
-typedef struct DefOrUse {
-  int offsetPC; //!the program point
-  int regNum; //!access the virtual reg
-  LowOpndRegType physicalType; //!xmm or gp or ss
-  RegAccessType accessType; //!D, L, H, N
-} DefOrUse;
-//!a link list of DefOrUse
-typedef struct DefOrUseLink {
-  int offsetPC;
-  int regNum; //access the virtual reg
-  LowOpndRegType physicalType; //xmm or gp
-  RegAccessType accessType; //D, L, H, N
-  struct DefOrUseLink* next;
-} DefOrUseLink;
-//!pair of def and uses
-typedef struct DefUsePair {
-  DefOrUseLink* uses;
-  DefOrUseLink* useTail;
-  int num_uses;
-  DefOrUse def;
-  struct DefUsePair* next;
-} DefUsePair;
-
-//!information associated with a virtual register
-//!the pair <regNum, physicalType> uniquely determines a variable
-typedef struct VirtualRegInfo {
-   VirtualRegInfo () : regNum (-1), physicalType (LowOpndRegType_invalid), refCount (0),
-           accessType (REGACCESS_UNKNOWN), num_reaching_defs (0)
-  {
-        //Set up allocation constraints for hardcoded registers
-        for (int reg = PhysicalReg_StartOfGPMarker; reg <= PhysicalReg_EndOfGPMarker; reg++)
-        {
-            PhysicalReg pReg = static_cast<PhysicalReg> (reg);
-
-            //For now we know of no constraints for each reg so we set count to zero
-            allocConstraints[reg].physicalReg = pReg;
-            allocConstraints[reg].count = 0;
-            allocConstraintsSorted[reg].physicalReg = pReg;
-            allocConstraintsSorted[reg].count = 0;
-        }
-  }
-
-  int regNum;
-  LowOpndRegType physicalType;
-  int refCount;
-  RegAccessType accessType;
-  RegAllocConstraint allocConstraints[PhysicalReg_EndOfGPMarker + 1];
-  RegAllocConstraint allocConstraintsSorted[PhysicalReg_EndOfGPMarker + 1];
-
-  DefOrUse reachingDefs[3]; //!reaching defs to the virtual register
-  int num_reaching_defs;
-} VirtualRegInfo;
-
-//!information of whether a VR is constant and its value
-typedef struct ConstVRInfo {
-  int regNum;
-  int value;
-  bool isConst;
-} ConstVRInfo;
-
-/**
- * @class constInfo
- * @brief information on 64 bit constants and their locations in a trace
-*/
-typedef struct ConstInfo {
-    int valueL;             /**< @brief The lower 32 bits of the constant */
-    int valueH;             /**< @brief The higher 32 bits of the constant */
-    int regNum;             /**< @brief The register number of the constant */
-    int offsetAddr;         /**< @brief The offset from start of instruction */
-    char* streamAddr;       /**< @brief The address of instruction in stream */
-    char* constAddr;        /**< @brief The address of the constant at the end of trace */
-    bool constAlign;        /**< @brief Decide whether to Align constAddr to 16 bytes */
-    struct ConstInfo *next; /**< @brief The pointer to the next 64 bit constant */
-} ConstInfo;
-
-#define NUM_ACCESS_IN_LIVERANGE 10
-//!specifies one live range
-typedef struct LiveRange {
-  int start;
-  int end; //inclusive
-  //all accesses in the live range
-  int num_access;
-  int num_alloc;
-  int* accessPC;
-  struct LiveRange* next;
-} LiveRange;
-typedef struct BoundCheckIndex {
-  int indexVR;
-  bool checkDone;
-} BoundCheckIndex;
-
-/**
- * @brief Used to keep track of virtual register's in-memory state.
- */
-typedef struct regAllocStateEntry2 {
-  int regNum;    //!< The virtual register
-  bool inMemory; //!< Whether 4-byte virtual register is in memory
-} regAllocStateEntry2;
-
-/**
- * @class MemoryVRInfo
- * @brief information for a virtual register such as live ranges, in memory
- */
-typedef struct MemoryVRInfo {
-    int regNum;                   /**< @brief The register number */
-    bool inMemory;                /**< @brief Is it in memory or not */
-    bool nullCheckDone;           /**< @brief Has a null check been done for it? */
-    BoundCheckIndex boundCheck;   /**< @brief Bound check information for the VR */
-    int num_ranges;               /**< @brief Number of ranges, used as a size for ranges */
-    LiveRange* ranges;            /**< @brief Live range information for the entry */
-    int delayFreeCounters[VRDELAY_COUNT]; /**< @brief Used with indexes defined by VRDelayFreeCounters enum to delay freeing */
-
-    /**
-     * @brief Default constructor which initializes all fields but sets an invalid virtual register.
-     */
-    MemoryVRInfo (void)
-    {
-        reset ();
-    }
-
-    /**
-     * @brief Initializes all fields and sets a virtual register associated with this information.
-     */
-    MemoryVRInfo (int vR)
-    {
-        reset ();
-        regNum = vR;
-    }
-
-    /**
-     * @brief Used to reset information about the VR to default values.
-     * @details Creates a logically invalid entry.
-     */
-    void reset (void);
-
-    /**
-     * @brief Returns the virtual register represented by this entry.
-     */
-    int getVirtualRegister (void)
-    {
-        return regNum;
-    }
-
-    /**
-     * @brief Sets the virtual register represented by this entry.
-     * @param regNum The virtual register number.
-     */
-    void setVirtualRegister (int regNum)
-    {
-        this->regNum = regNum;
-    }
-
-    /**
-     * @brief Sets the in memory state of this entry which represent specific virtual register.
-     * @param inMemory The in memory state to set to this entry.
-     */
-    void setInMemoryState (bool inMemory)
-    {
-        this->inMemory = inMemory;
-    }
-} MemoryVRInfo;
-
-//!information of a temporary
-//!the pair <regNum, physicalType> uniquely determines a variable
-typedef struct TempRegInfo {
-  int regNum;
-  int physicalType;
-  int refCount;
-  int linkageToVR;
-  int versionNum;
-  bool shareWithVR; //for temp. regs updated by get_virtual_reg
-  bool is8Bit;
-} TempRegInfo;
-struct BasicBlock_O1;
-
-//Forward declaration
-struct LowOpBlockLabel;
-
-//!information associated with a basic block
-struct BasicBlock_O1 : BasicBlock {
-  int pc_start;       //!inclusive
-  int pc_end;
-  char *streamStart;        //Where the code generation started for the BasicBlock
-
-  std::vector<VirtualRegInfo> infoBasicBlock;
-
-  RegAllocConstraint allocConstraints[PhysicalReg_EndOfGPMarker+1]; //# of times a hardcoded register is used in this basic block
-  //a physical register that is used many times has a lower priority to get picked in getFreeReg
-  RegAllocConstraint allocConstraintsSorted[PhysicalReg_EndOfGPMarker+1]; //count from low to high
-
-  DefUsePair* defUseTable;
-  DefUsePair* defUseTail;
-
-  std::vector <XferPoint> xferPoints; //program points where the transfer is required
-
-  AssociationTable associationTable;    //Association table to keep track of physical registers beyond a BasicBlock
-
-  LowOpBlockLabel *label;               //Label for the BasicBlock
-
-  //Constructor
-  BasicBlock_O1 (void);
-  //Clear function: do we allocate the label (default: false)
-  void clear (bool allocateLabel = false);
-
-  //Clear and free everything
-  void freeIt (void);
-};
-
-extern MemoryVRInfo memVRTable[NUM_MEM_VR_ENTRY];
-extern int num_memory_vr;
-extern TempRegInfo infoByteCodeTemp[MAX_TEMP_REG_PER_BYTECODE];
-extern int num_temp_regs_per_bytecode;
-extern VirtualRegInfo infoMethod[MAX_REG_PER_METHOD];
-extern int num_regs_per_method;
-extern BasicBlock_O1* currentBB;
-
-extern int num_const_vr;
-extern ConstVRInfo constVRTable[MAX_CONST_REG];
-
-/**
- * @brief Provides a mapping between physical type and the size represented.
- * @param type The physical type represented by LowOpndRegType.
- * @return Returns size represented by the physical type.
- */
-OpndSize getRegSize (int type);
-
-void forwardAnalysis(int type);
-
-//functions in bc_visitor.c
-int getConstInfo(BasicBlock_O1* bb, const MIR * currentMIR);
-int getVirtualRegInfo(VirtualRegInfo* infoArray, const MIR * currentMIR, bool updateBBConstraints = false);
-int getTempRegInfo(TempRegInfo* infoArray, const MIR * currentMIR);
-int createCFGHandler(Method* method);
-
-int findVirtualRegInTable(int vA, LowOpndRegType type);
-int searchCompileTable(int type, int regNum);
-void handleJump(BasicBlock_O1* bb_prev, int relOff);
-void connectBasicBlock(BasicBlock_O1* src, BasicBlock_O1* dst);
-int insertWorklist(BasicBlock_O1* bb_prev, int targetOff);
-
-//Collects virtual register usage for BB and sets up defuse tables
-int collectInfoOfBasicBlock (BasicBlock_O1* bb);
-
-void updateCurrentBBWithConstraints(PhysicalReg reg);
-void updateConstInfo(BasicBlock_O1*);
-void invalidateVRDueToConst(int reg, OpndSize size);
-
-//Set a VR to a constant value
-bool setVRToConst(int regNum, OpndSize size, int* tmpValue);
-
-//Set that VR is not a constant
-void setVRToNonConst(int regNum, OpndSize size);
-
-/**
- * @brief Used the represent the constantness of a virtual register.
- */
-enum VirtualRegConstantness
-{
-    VR_IS_NOT_CONSTANT = 0,  //!< virtual register is not constant
-    VR_LOW_IS_CONSTANT = 1,  //!< only the low 32-bit of virtual register is constant
-    VR_HIGH_IS_CONSTANT = 2, //!< only the high 32-bit of virtual register is constant
-    VR_IS_CONSTANT = 3       //!< virtual register is entirely constant
-};
-
-//Checks if VR is constant
-VirtualRegConstantness isVirtualRegConstant (int regNum, int opndRegType, int *valuePtr, bool updateRef = false);
-
-//If VR is dirty, it writes the constant value to the VR on stack
-void writeBackConstVR (int vR, int value);
-
-//Writes virtual register back to memory if it holds a constant value
-bool writeBackVRIfConstant (int vR, LowOpndRegType type);
-
-//Write back virtual register to memory when it is in given physical register
-void writeBackVR (int vR, LowOpndRegType type, int physicalReg);
-
-//Check if VR is in memory
-bool isInMemory(int regNum, OpndSize size);
-
-//Update the in memory state of the virtual register
-void setVRMemoryState (int vR, OpndSize size, bool inMemory);
-
-//Find free registers and update the set
-void findFreeRegisters (std::set<PhysicalReg> &outFreeRegisters, bool includeGPs = true, bool includeXMMs = true);
-
-//Get a scratch register of a given type
-PhysicalReg getScratch(const std::set<PhysicalReg> &scratchCandidates, LowOpndRegType type);
-
-//Synchronize all registers in the compileTable
-void syncAllRegs(void);
-
-//Get a type for a given register
-LowOpndRegType getTypeOfRegister(PhysicalReg reg);
-
-//Update the physical register in the compile table from oldReg to newReg
-bool updatePhysicalRegForVR(int vR, PhysicalReg oldReg, PhysicalReg newReg);
-
-//Check whether a type is a virtual register type
-bool isVirtualReg(int type);
-
-//Spill a logical register using a index in the compileTable
-int spillLogicalReg(int spill_index, bool updateTable);
-
-//Search in the memory table for a register
-int searchMemTable(int regNum);
-
-//Adds a virtual register to the memory table
-bool addToMemVRTable (int vR, bool inMemory);
-
-//! check whether the current bytecode is the last access to a VR within a live
-bool isLastByteCodeOfLiveRange(int compileIndex);
-#endif
-
diff --git a/vm/compiler/codegen/x86/BytecodeVisitor.cpp b/vm/compiler/codegen/x86/BytecodeVisitor.cpp
deleted file mode 100644
index 2342613..0000000
--- a/vm/compiler/codegen/x86/BytecodeVisitor.cpp
+++ /dev/null
@@ -1,6421 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file BytecodeVisitor.cpp
-    \brief This file implements visitors of the bytecode
-*/
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "AnalysisO1.h"
-
-#if 0 /* This is dead code and has been disabled. If reenabling,
-         the MIR or opcode must be passed in as a parameter */
-//! Returns size of the current bytecode in u2 unit
-
-//!
-int getByteCodeSize() { //uses inst, unit in u2
-    switch (INST_INST(inst)) {
-    case OP_NOP:
-        return 1;
-    case OP_MOVE:
-    case OP_MOVE_OBJECT:
-        return 1;
-    case OP_MOVE_FROM16:
-    case OP_MOVE_OBJECT_FROM16:
-        return 2;
-    case OP_MOVE_16:
-    case OP_MOVE_OBJECT_16:
-        return 3;
-    case OP_MOVE_WIDE:
-        return 1;
-    case OP_MOVE_WIDE_FROM16:
-        return 2;
-    case OP_MOVE_WIDE_16:
-        return 3;
-    case OP_MOVE_RESULT:
-    case OP_MOVE_RESULT_OBJECT:
-        return 1;
-    case OP_MOVE_RESULT_WIDE:
-        return 1;
-    case OP_MOVE_EXCEPTION:
-        return 1;
-    case OP_RETURN_VOID:
-    case OP_RETURN_VOID_BARRIER:
-        return 1;
-    case OP_RETURN:
-    case OP_RETURN_OBJECT:
-        return 1;
-    case OP_RETURN_WIDE:
-        return 1;
-    case OP_CONST_4:
-        return 1;
-    case OP_CONST_16:
-        return 2;
-    case OP_CONST:
-        return 3;
-    case OP_CONST_HIGH16:
-        return 2;
-    case OP_CONST_WIDE_16:
-        return 2;
-    case OP_CONST_WIDE_32:
-        return 3;
-    case OP_CONST_WIDE:
-        return 5;
-    case OP_CONST_WIDE_HIGH16:
-        return 2;
-    case OP_CONST_STRING:
-        return 2;
-    case OP_CONST_STRING_JUMBO:
-        return 3;
-    case OP_CONST_CLASS:
-        return 2;
-    case OP_MONITOR_ENTER:
-        return 1;
-    case OP_MONITOR_EXIT:
-        return 1;
-    case OP_CHECK_CAST:
-        return 2;
-    case OP_INSTANCE_OF:
-        return 2;
-    case OP_ARRAY_LENGTH:
-        return 1;
-    case OP_NEW_INSTANCE:
-        return 2;
-    case OP_NEW_ARRAY:
-        return 2;
-    case OP_FILLED_NEW_ARRAY:
-        return 3;
-    case OP_FILLED_NEW_ARRAY_RANGE:
-        return 3;
-    case OP_FILL_ARRAY_DATA:
-        return 3;
-    case OP_THROW:
-        return 1;
-    case OP_THROW_VERIFICATION_ERROR:
-        return 2;
-    case OP_GOTO:
-        return 1;
-    case OP_GOTO_16:
-        return 2;
-    case OP_GOTO_32:
-        return 3;
-    case OP_PACKED_SWITCH:
-        return 3;
-    case OP_SPARSE_SWITCH:
-        return 3;
-    case OP_CMPL_FLOAT:
-        return 2;
-    case OP_CMPG_FLOAT:
-        return 2;
-    case OP_CMPL_DOUBLE:
-        return 2;
-    case OP_CMPG_DOUBLE:
-        return 2;
-    case OP_CMP_LONG:
-        return 2;
-    case OP_IF_EQ:
-        return 2;
-    case OP_IF_NE:
-        return 2;
-    case OP_IF_LT:
-        return 2;
-    case OP_IF_GE:
-        return 2;
-    case OP_IF_GT:
-        return 2;
-    case OP_IF_LE:
-        return 2;
-    case OP_IF_EQZ:
-        return 2;
-    case OP_IF_NEZ:
-        return 2;
-    case OP_IF_LTZ:
-        return 2;
-    case OP_IF_GEZ:
-        return 2;
-    case OP_IF_GTZ:
-        return 2;
-    case OP_IF_LEZ:
-        return 2;
-    case OP_AGET:
-        return 2;
-    case OP_AGET_WIDE:
-        return 2;
-    case OP_AGET_OBJECT:
-        return 2;
-    case OP_AGET_BOOLEAN:
-        return 2;
-    case OP_AGET_BYTE:
-        return 2;
-    case OP_AGET_CHAR:
-        return 2;
-    case OP_AGET_SHORT:
-        return 2;
-    case OP_APUT:
-        return 2;
-    case OP_APUT_WIDE:
-        return 2;
-    case OP_APUT_OBJECT:
-        return 2;
-    case OP_APUT_BOOLEAN:
-        return 2;
-    case OP_APUT_BYTE:
-        return 2;
-    case OP_APUT_CHAR:
-        return 2;
-    case OP_APUT_SHORT:
-        return 2;
-    case OP_IGET:
-    case OP_IGET_WIDE:
-    case OP_IGET_OBJECT:
-    case OP_IGET_VOLATILE:
-    case OP_IGET_WIDE_VOLATILE:
-    case OP_IGET_OBJECT_VOLATILE:
-    case OP_IGET_BOOLEAN:
-    case OP_IGET_BYTE:
-    case OP_IGET_CHAR:
-    case OP_IGET_SHORT:
-    case OP_IPUT:
-    case OP_IPUT_WIDE:
-    case OP_IPUT_OBJECT:
-    case OP_IPUT_VOLATILE:
-    case OP_IPUT_WIDE_VOLATILE:
-    case OP_IPUT_OBJECT_VOLATILE:
-    case OP_IPUT_BOOLEAN:
-    case OP_IPUT_BYTE:
-    case OP_IPUT_CHAR:
-    case OP_IPUT_SHORT:
-        return 2;
-    case OP_SGET:
-    case OP_SGET_WIDE:
-    case OP_SGET_OBJECT:
-    case OP_SGET_VOLATILE:
-    case OP_SGET_WIDE_VOLATILE:
-    case OP_SGET_OBJECT_VOLATILE:
-    case OP_SGET_BOOLEAN:
-    case OP_SGET_BYTE:
-    case OP_SGET_CHAR:
-    case OP_SGET_SHORT:
-    case OP_SPUT:
-    case OP_SPUT_WIDE:
-    case OP_SPUT_OBJECT:
-    case OP_SPUT_VOLATILE:
-    case OP_SPUT_WIDE_VOLATILE:
-    case OP_SPUT_OBJECT_VOLATILE:
-    case OP_SPUT_BOOLEAN:
-    case OP_SPUT_BYTE:
-    case OP_SPUT_CHAR:
-    case OP_SPUT_SHORT:
-        return 2;
-    case OP_INVOKE_VIRTUAL:
-    case OP_INVOKE_SUPER:
-    case OP_INVOKE_DIRECT:
-    case OP_INVOKE_STATIC:
-    case OP_INVOKE_INTERFACE:
-    case OP_INVOKE_VIRTUAL_RANGE:
-    case OP_INVOKE_SUPER_RANGE:
-    case OP_INVOKE_DIRECT_RANGE:
-    case OP_INVOKE_STATIC_RANGE:
-    case OP_INVOKE_INTERFACE_RANGE:
-        return 3;
-
-    case OP_NEG_INT:
-    case OP_NOT_INT:
-    case OP_NEG_LONG:
-    case OP_NOT_LONG:
-    case OP_NEG_FLOAT:
-    case OP_NEG_DOUBLE:
-    case OP_INT_TO_LONG:
-    case OP_INT_TO_FLOAT:
-    case OP_INT_TO_DOUBLE:
-    case OP_LONG_TO_INT:
-    case OP_LONG_TO_FLOAT:
-    case OP_LONG_TO_DOUBLE:
-    case OP_FLOAT_TO_INT:
-    case OP_FLOAT_TO_LONG:
-    case OP_FLOAT_TO_DOUBLE:
-    case OP_DOUBLE_TO_INT:
-    case OP_DOUBLE_TO_LONG:
-    case OP_DOUBLE_TO_FLOAT:
-    case OP_INT_TO_BYTE:
-    case OP_INT_TO_CHAR:
-    case OP_INT_TO_SHORT:
-        return 1;
-
-    case OP_ADD_INT:
-    case OP_SUB_INT:
-    case OP_MUL_INT:
-    case OP_DIV_INT:
-    case OP_REM_INT:
-    case OP_AND_INT:
-    case OP_OR_INT:
-    case OP_XOR_INT:
-    case OP_SHL_INT:
-    case OP_SHR_INT:
-    case OP_USHR_INT:
-    case OP_ADD_LONG:
-    case OP_SUB_LONG:
-    case OP_MUL_LONG:
-    case OP_DIV_LONG:
-    case OP_REM_LONG:
-    case OP_AND_LONG:
-    case OP_OR_LONG:
-    case OP_XOR_LONG:
-    case OP_SHL_LONG:
-    case OP_SHR_LONG:
-    case OP_USHR_LONG:
-    case OP_ADD_FLOAT:
-    case OP_SUB_FLOAT:
-    case OP_MUL_FLOAT:
-    case OP_DIV_FLOAT:
-    case OP_REM_FLOAT:
-    case OP_ADD_DOUBLE:
-    case OP_SUB_DOUBLE:
-    case OP_MUL_DOUBLE:
-    case OP_DIV_DOUBLE:
-    case OP_REM_DOUBLE:
-        return 2;
-
-    case OP_ADD_INT_2ADDR:
-    case OP_SUB_INT_2ADDR:
-    case OP_MUL_INT_2ADDR:
-    case OP_DIV_INT_2ADDR:
-    case OP_REM_INT_2ADDR:
-    case OP_AND_INT_2ADDR:
-    case OP_OR_INT_2ADDR:
-    case OP_XOR_INT_2ADDR:
-    case OP_SHL_INT_2ADDR:
-    case OP_SHR_INT_2ADDR:
-    case OP_USHR_INT_2ADDR:
-    case OP_ADD_LONG_2ADDR:
-    case OP_SUB_LONG_2ADDR:
-    case OP_MUL_LONG_2ADDR:
-    case OP_DIV_LONG_2ADDR:
-    case OP_REM_LONG_2ADDR:
-    case OP_AND_LONG_2ADDR:
-    case OP_OR_LONG_2ADDR:
-    case OP_XOR_LONG_2ADDR:
-    case OP_SHL_LONG_2ADDR:
-    case OP_SHR_LONG_2ADDR:
-    case OP_USHR_LONG_2ADDR:
-    case OP_ADD_FLOAT_2ADDR:
-    case OP_SUB_FLOAT_2ADDR:
-    case OP_MUL_FLOAT_2ADDR:
-    case OP_DIV_FLOAT_2ADDR:
-    case OP_REM_FLOAT_2ADDR:
-    case OP_ADD_DOUBLE_2ADDR:
-    case OP_SUB_DOUBLE_2ADDR:
-    case OP_MUL_DOUBLE_2ADDR:
-    case OP_DIV_DOUBLE_2ADDR:
-    case OP_REM_DOUBLE_2ADDR:
-        return 1;
-
-    case OP_ADD_INT_LIT16:
-    case OP_RSUB_INT:
-    case OP_MUL_INT_LIT16:
-    case OP_DIV_INT_LIT16:
-    case OP_REM_INT_LIT16:
-    case OP_AND_INT_LIT16:
-    case OP_OR_INT_LIT16:
-    case OP_XOR_INT_LIT16:
-        return 2;
-
-    case OP_ADD_INT_LIT8:
-    case OP_RSUB_INT_LIT8:
-    case OP_MUL_INT_LIT8:
-    case OP_DIV_INT_LIT8:
-    case OP_REM_INT_LIT8:
-    case OP_AND_INT_LIT8:
-    case OP_OR_INT_LIT8:
-    case OP_XOR_INT_LIT8:
-    case OP_SHL_INT_LIT8:
-    case OP_SHR_INT_LIT8:
-    case OP_USHR_INT_LIT8:
-        return 2;
-
-    case OP_EXECUTE_INLINE:
-    case OP_EXECUTE_INLINE_RANGE:
-        return 3;
-#if FIXME
-    case OP_INVOKE_OBJECT_INIT_RANGE:
-        return 3;
-#endif
-
-    case OP_IGET_QUICK:
-    case OP_IGET_WIDE_QUICK:
-    case OP_IGET_OBJECT_QUICK:
-    case OP_IPUT_QUICK:
-    case OP_IPUT_WIDE_QUICK:
-    case OP_IPUT_OBJECT_QUICK:
-        return 2;
-
-    case OP_INVOKE_VIRTUAL_QUICK:
-    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
-    case OP_INVOKE_SUPER_QUICK:
-    case OP_INVOKE_SUPER_QUICK_RANGE:
-        return 3;
-#ifdef SUPPORT_HLO
-    case kExtInstruction:
-        switch(inst) {
-        case OP_X_AGET_QUICK:
-        case OP_X_AGET_WIDE_QUICK:
-        case OP_X_AGET_OBJECT_QUICK:
-    case OP_X_AGET_BOOLEAN_QUICK:
-    case OP_X_AGET_BYTE_QUICK:
-    case OP_X_AGET_CHAR_QUICK:
-    case OP_X_AGET_SHORT_QUICK:
-    case OP_X_APUT_QUICK:
-    case OP_X_APUT_WIDE_QUICK:
-    case OP_X_APUT_OBJECT_QUICK:
-    case OP_X_APUT_BOOLEAN_QUICK:
-    case OP_X_APUT_BYTE_QUICK:
-    case OP_X_APUT_CHAR_QUICK:
-    case OP_X_APUT_SHORT_QUICK:
-        return 3;
-    case OP_X_DEREF_GET:
-    case OP_X_DEREF_GET_OBJECT:
-    case OP_X_DEREF_GET_WIDE:
-    case OP_X_DEREF_GET_BOOLEAN:
-    case OP_X_DEREF_GET_BYTE:
-    case OP_X_DEREF_GET_CHAR:
-    case OP_X_DEREF_GET_SHORT:
-    case OP_X_DEREF_PUT:
-    case OP_X_DEREF_PUT_WIDE:
-    case OP_X_DEREF_PUT_OBJECT:
-    case OP_X_DEREF_PUT_BOOLEAN:
-    case OP_X_DEREF_PUT_BYTE:
-    case OP_X_DEREF_PUT_CHAR:
-    case OP_X_DEREF_PUT_SHORT:
-        return 2;
-    case OP_X_ARRAY_CHECKS:
-    case OP_X_ARRAY_OBJECT_CHECKS:
-        return 3;
-    case OP_X_CHECK_BOUNDS:
-    case OP_X_CHECK_NULL:
-    case OP_X_CHECK_TYPE:
-        return 2;
-    }
-#endif
-    default:
-        ALOGI("JIT_INFO: JIT does not support getting size of bytecode 0x%hx\n",
-                currentMIR->dalvikInsn.opcode);
-        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
-        assert(false && "All opcodes should be supported.");
-        break;
-    }
-    return -1;
-}
-#endif
-
-//! \brief reduces refCount of a virtual register
-//!
-//! \param vA
-//! \param type
-//!
-//! \return -1 on error, 0 otherwise
-static int touchOneVR(int vA, LowOpndRegType type) {
-    int index = searchCompileTable(LowOpndRegType_virtual | type, vA);
-    if(index < 0) {
-        ALOGI("JIT_INFO: virtual reg %d type %d not found in touchOneVR\n", vA, type);
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return -1;
-    }
-    compileTable[index].refCount--;
-    return 0;
-}
-
-/** @brief count of const in worklist  */
-int num_const_worklist;
-
-/** @brief  worklist to update constVRTable later */
-int constWorklist[10];
-
-/**
-  * @brief Clears the list registers with killed constants
-  */
-static void clearConstKills(void) {
-    num_const_worklist = 0;
-}
-
-/**
-  * @brief  Adds a register for which any previous constant
-  *  held is killed by an operation.
-  * @param v a virtual register
-  */
-static void addConstKill(u2 v) {
-    constWorklist[num_const_worklist++] = v;
-}
-
-int num_const_vr; //in a basic block
-//! table to store the constant information for virtual registers
-ConstVRInfo constVRTable[MAX_CONST_REG];
-//! update constVRTable for a given virtual register
-
-//! set "isConst" to false
-void setVRToNonConst(int regNum, OpndSize size) {
-    int k;
-    int indexL = -1;
-    int indexH = -1;
-    for(k = 0; k < num_const_vr; k++) {
-        if(constVRTable[k].regNum == regNum) {
-            indexL = k;
-            continue;
-        }
-        if(constVRTable[k].regNum == regNum + 1 && size == OpndSize_64) {
-            indexH = k;
-            continue;
-        }
-    }
-    if(indexL >= 0) {
-        //remove this entry??
-        constVRTable[indexL].isConst = false;
-    }
-    if(size == OpndSize_64 && indexH >= 0) {
-        constVRTable[indexH].isConst = false;
-    }
-}
-
-/**
- * @brief Marks a virtual register as being constant.
- * @param regNum The virtual register number.
- * @param size The size of the virtual register.
- * @param tmpValue Array representing the constant values for this VR. It must be the correct size to match
- * the size argument.
- * @return Returns true if setting VR to constant succeeded. On failure it returns false.
- */
-bool setVRToConst (int regNum, OpndSize size, int *tmpValue)
-{
-    assert (tmpValue != 0);
-
-    int k;
-    int indexL = -1;
-    int indexH = -1;
-    for(k = 0; k < num_const_vr; k++) {
-        if(constVRTable[k].regNum == regNum) {
-            indexL = k;
-            continue;
-        }
-        if(constVRTable[k].regNum == regNum + 1 && size == OpndSize_64) {
-            indexH = k;
-            continue;
-        }
-    }
-
-    //Add the entry for the VR to the table if we don't have it
-    if(indexL < 0)
-    {
-        //Now check for possible table overflow. If we don't have an entry for this,
-        //then we must add it. Check now for possible overflow.
-        if (num_const_vr >= MAX_CONST_REG)
-        {
-            ALOGI("JIT_INFO: constVRTable overflows at setVRToConst.");
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return false;
-        }
-
-        indexL = num_const_vr;
-        constVRTable[indexL].regNum = regNum;
-        num_const_vr++;
-    }
-
-    //Now initialize the entry with the constant value
-    constVRTable[indexL].isConst = true;
-    constVRTable[indexL].value = tmpValue[0];
-
-    //If we have a 64-bit VR, we must also initialize the high bits
-    if (size == OpndSize_64)
-    {
-        //Add entry to the table if we don't have it
-        if (indexH < 0)
-        {
-            //Now check for possible table overflow. If we don't have an entry for this,
-            //then we must add it. Check now for possible overflow.
-            if (num_const_vr >= MAX_CONST_REG)
-            {
-                ALOGI("JIT_INFO: constVRTable overflows at setVRToConst.");
-                SET_JIT_ERROR(kJitErrorRegAllocFailed);
-                return false;
-            }
-
-            indexH = num_const_vr;
-            constVRTable[indexH].regNum = regNum + 1;
-            num_const_vr++;
-        }
-
-        //Now initialize the entry with the constant value
-        constVRTable[indexH].isConst = true;
-        constVRTable[indexH].value = tmpValue[1];
-    }
-
-    //This VR just became a constant so invalidate other information we have about it
-    invalidateVRDueToConst (regNum, size);
-
-    //If we make it here we were successful
-    return true;
-}
-
-//! perform work on constWorklist
-
-//!
-void updateConstInfo(BasicBlock_O1* bb) {
-    if(bb == NULL) return;
-    int k;
-    for(k = 0; k < num_const_worklist; k++) {
-        //int indexOrig = constWorklist[k];
-        //compileTable[indexOrig].isConst = false;
-        //int A = compileTable[indexOrig].regNum;
-        //LowOpndRegType type = compileTable[indexOrig].physicalType & MASK_FOR_TYPE;
-        setVRToNonConst(constWorklist[k], OpndSize_32);
-    }
-}
-
-//! \brief check whether the current bytecode generates a const
-//!
-//! \details if yes, update constVRTable; otherwise, update constWorklist
-//! if a bytecode uses vA (const), and updates vA to non const, getConstInfo
-//! will return 0 and update constWorklist to make sure when lowering the
-//! bytecode, vA is treated as constant
-//!
-//! \param bb the BasicBlock_O1 to analyze
-//! \param currentMIR
-//!
-//! \return 1 if the bytecode generates a const, 0 otherwise, and -1 if an
-//! error occured.
-int getConstInfo(BasicBlock_O1* bb, const MIR * currentMIR) {
-    //retCode and success are used to keep track of success of function calls from this function
-    int retCode = 0;
-    bool success = false;
-
-    Opcode inst_op = currentMIR->dalvikInsn.opcode;
-    int vA = 0, vB = 0, v1, v2;
-    u2 BBBB;
-    u2 tmp_u2;
-    s4 tmp_s4;
-    u4 tmp_u4;
-    int entry, tmpValue[2], tmpValue2[2];
-
-    clearConstKills ();
-
-    /* A bytecode with the MIR_INLINED op will be treated as
-     * no-op during codegen */
-    if (currentMIR->OptimizationFlags & MIR_INLINED)
-        return 0; // does NOT generate a constant
-
-    // Check if we need to handle an extended MIR
-    if (currentMIR->dalvikInsn.opcode >= static_cast<Opcode> (kMirOpFirst)) {
-        //Currently no extended MIR generates constants
-        switch (static_cast<ExtendedMIROpcode>(currentMIR->dalvikInsn.opcode)) {
-            default:
-                // No constant is generated
-                return 0;
-        }
-    }
-
-    switch(inst_op) {
-        //for other opcode, if update the register, set isConst to false
-    case OP_MOVE:
-    case OP_MOVE_OBJECT:
-    case OP_MOVE_FROM16:
-    case OP_MOVE_OBJECT_FROM16:
-    case OP_MOVE_16:
-    case OP_MOVE_OBJECT_16:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
-            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-            if (entry < 0)
-                return -1;
-
-            success = setVRToConst(vA, OpndSize_32, tmpValue);
-            if (success == false)
-            {
-                //setVRToConst set an error message when it failed so we just pass along the failure information
-                return -1;
-            }
-
-            compileTable[entry].refCount--;
-            retCode = touchOneVR(vB, LowOpndRegType_gp);
-            if (retCode < 0)
-                return retCode;
-            return 1;
-        } else {
-            addConstKill(vA);
-        }
-        return 0;
-    case OP_MOVE_WIDE:
-    case OP_MOVE_WIDE_FROM16:
-    case OP_MOVE_WIDE_16:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        if(isVirtualRegConstant(vB, LowOpndRegType_xmm, tmpValue, false) == 3) {
-            entry = findVirtualRegInTable(vA, LowOpndRegType_xmm);
-            if (entry < 0)
-                return -1;
-
-            success = setVRToConst(vA, OpndSize_64, tmpValue);
-            if (success == false)
-            {
-                //setVRToConst set an error message when it failed so we just pass along the failure information
-                return -1;
-            }
-
-            compileTable[entry].refCount--;
-            retCode = touchOneVR(vB, LowOpndRegType_xmm);
-            if (retCode < 0)
-                return retCode;
-            return 1;
-        } else {
-            addConstKill(vA);
-            addConstKill(vA+1);
-        }
-        return 0;
-    case OP_MOVE_RESULT:
-    case OP_MOVE_RESULT_OBJECT:
-    case OP_MOVE_EXCEPTION:
-    case OP_CONST_STRING:
-    case OP_CONST_STRING_JUMBO:
-    case OP_CONST_CLASS:
-    case OP_NEW_INSTANCE:
-    case OP_CMPL_FLOAT:
-    case OP_CMPG_FLOAT:
-    case OP_CMPL_DOUBLE:
-    case OP_CMPG_DOUBLE:
-    case OP_AGET:
-    case OP_AGET_OBJECT:
-    case OP_AGET_BOOLEAN:
-    case OP_AGET_BYTE:
-    case OP_AGET_CHAR:
-    case OP_AGET_SHORT:
-    case OP_SGET:
-    case OP_SGET_OBJECT:
-    case OP_SGET_VOLATILE:
-    case OP_SGET_OBJECT_VOLATILE:
-    case OP_SGET_BOOLEAN:
-    case OP_SGET_BYTE:
-    case OP_SGET_CHAR:
-    case OP_SGET_SHORT:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        return 0;
-    case OP_MOVE_RESULT_WIDE:
-    case OP_AGET_WIDE:
-    case OP_SGET_WIDE:
-    case OP_SGET_WIDE_VOLATILE:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-    case OP_INSTANCE_OF:
-    case OP_ARRAY_LENGTH:
-    case OP_NEW_ARRAY:
-    case OP_IGET:
-    case OP_IGET_OBJECT:
-    case OP_IGET_VOLATILE:
-    case OP_IGET_OBJECT_VOLATILE:
-    case OP_IGET_BOOLEAN:
-    case OP_IGET_BYTE:
-    case OP_IGET_CHAR:
-    case OP_IGET_SHORT:
-    case OP_IGET_QUICK:
-    case OP_IGET_OBJECT_QUICK:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        return 0;
-    case OP_IGET_WIDE:
-    case OP_IGET_WIDE_VOLATILE:
-    case OP_IGET_WIDE_QUICK:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-        //TODO: constant folding for float/double/long ALU
-    case OP_ADD_FLOAT:
-    case OP_SUB_FLOAT:
-    case OP_MUL_FLOAT:
-    case OP_DIV_FLOAT:
-    case OP_REM_FLOAT:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        return 0;
-    case OP_ADD_DOUBLE:
-    case OP_SUB_DOUBLE:
-    case OP_MUL_DOUBLE:
-    case OP_DIV_DOUBLE:
-    case OP_REM_DOUBLE:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-    case OP_NEG_FLOAT:
-    case OP_INT_TO_FLOAT:
-    case OP_LONG_TO_FLOAT:
-    case OP_FLOAT_TO_INT:
-    case OP_DOUBLE_TO_INT:
-    case OP_ADD_FLOAT_2ADDR:
-    case OP_SUB_FLOAT_2ADDR:
-    case OP_MUL_FLOAT_2ADDR:
-    case OP_DIV_FLOAT_2ADDR:
-    case OP_REM_FLOAT_2ADDR:
-    case OP_DOUBLE_TO_FLOAT:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        return 0;
-    case OP_FLOAT_TO_LONG:
-    case OP_DOUBLE_TO_LONG:
-    case OP_FLOAT_TO_DOUBLE:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-    case OP_NEG_DOUBLE:
-    case OP_INT_TO_DOUBLE: //fp stack
-    case OP_LONG_TO_DOUBLE:
-    case OP_ADD_DOUBLE_2ADDR:
-    case OP_SUB_DOUBLE_2ADDR:
-    case OP_MUL_DOUBLE_2ADDR:
-    case OP_DIV_DOUBLE_2ADDR:
-    case OP_REM_DOUBLE_2ADDR:
-        //ops on float, double
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-    case OP_NEG_INT:
-    case OP_NOT_INT:
-    case OP_LONG_TO_INT:
-    case OP_INT_TO_BYTE:
-    case OP_INT_TO_CHAR:
-    case OP_INT_TO_SHORT:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
-            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-            if (entry < 0)
-            {
-                return -1;
-            }
-
-            switch (inst_op)
-            {
-                case OP_NEG_INT:
-                    tmpValue[0] = -tmpValue[0];
-                    break;
-                case OP_NOT_INT:
-                    tmpValue[0] = ~tmpValue[0]; //CHECK
-                    break;
-                case OP_LONG_TO_INT:
-                    //Nothing to do for long to int to convert value
-                    break;
-                case OP_INT_TO_BYTE: // sar
-                    tmpValue[0] = (tmpValue[0] << 24) >> 24;
-                    break;
-                case OP_INT_TO_CHAR: //shr
-                    tmpValue[0] = ((unsigned int) (tmpValue[0] << 16)) >> 16;
-                    break;
-                case OP_INT_TO_SHORT: //sar
-                    tmpValue[0] = (tmpValue[0] << 16) >> 16;
-                    break;
-                default:
-                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
-                    SET_JIT_ERROR (kJitErrorConstantFolding);
-                    return -1;
-            }
-
-            success = setVRToConst(vA, OpndSize_32, tmpValue);
-            if (success == false)
-            {
-                //setVRToConst set an error message when it failed so we just pass along the failure information
-                return -1;
-            }
-
-            compileTable[entry].refCount--;
-            retCode = touchOneVR(vB, LowOpndRegType_gp);
-            if (retCode < 0)
-                return retCode;
-#ifdef DEBUG_CONST
-            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
-#endif
-            return 1;
-        }
-        else {
-            addConstKill(vA);
-            return 0;
-        }
-    case OP_NEG_LONG:
-    case OP_NOT_LONG:
-    case OP_INT_TO_LONG:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-    case OP_DIV_INT_2ADDR:
-    case OP_REM_INT_2ADDR:
-    case OP_REM_INT_LIT16:
-    case OP_DIV_INT_LIT16:
-    case OP_REM_INT_LIT8:
-    case OP_DIV_INT_LIT8:
-    case OP_DIV_INT:
-    case OP_REM_INT:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        return 0;
-    case OP_ADD_INT_2ADDR:
-    case OP_SUB_INT_2ADDR:
-    case OP_MUL_INT_2ADDR:
-    case OP_AND_INT_2ADDR:
-    case OP_OR_INT_2ADDR:
-    case OP_XOR_INT_2ADDR:
-    case OP_SHL_INT_2ADDR:
-    case OP_SHR_INT_2ADDR:
-    case OP_USHR_INT_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        if(isVirtualRegConstant(vA, LowOpndRegType_gp, tmpValue, false) == 3 &&
-           isVirtualRegConstant(v2, LowOpndRegType_gp, tmpValue2, false) == 3) {
-            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-            if (entry < 0)
-            {
-                return -1;
-            }
-
-            switch (inst_op)
-            {
-                case OP_ADD_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] + tmpValue2[0];
-                    break;
-                case OP_SUB_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] - tmpValue2[0];
-                    break;
-                case OP_MUL_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] * tmpValue2[0];
-                    break;
-                case OP_DIV_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] / tmpValue2[0];
-                    break;
-                case OP_REM_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] % tmpValue2[0];
-                    break;
-                case OP_AND_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] & tmpValue2[0];
-                    break;
-                case OP_OR_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] | tmpValue2[0];
-                    break;
-                case OP_XOR_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] ^ tmpValue2[0];
-                    break;
-                case OP_SHL_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] << tmpValue2[0];
-                    break;
-                case OP_SHR_INT_2ADDR:
-                    tmpValue[0] = tmpValue[0] >> tmpValue2[0];
-                    break;
-                case OP_USHR_INT_2ADDR:
-                    tmpValue[0] = (unsigned int) tmpValue[0] >> tmpValue2[0];
-                    break;
-                default:
-                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
-                    SET_JIT_ERROR (kJitErrorConstantFolding);
-                    return -1;
-            }
-
-            success = setVRToConst(vA, OpndSize_32, tmpValue);
-            if (success == false)
-            {
-                //setVRToConst set an error message when it failed so we just pass along the failure information
-                return -1;
-            }
-
-            compileTable[entry].refCount--;
-            retCode = touchOneVR(v2, LowOpndRegType_gp);
-            if (retCode < 0)
-                return retCode;
-#ifdef DEBUG_CONST
-            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
-#endif
-            return 1;
-        }
-        else {
-            addConstKill(vA);
-            return 0;
-        }
-    case OP_ADD_INT_LIT16:
-    case OP_RSUB_INT:
-    case OP_MUL_INT_LIT16:
-    case OP_AND_INT_LIT16:
-    case OP_OR_INT_LIT16:
-    case OP_XOR_INT_LIT16:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        tmp_s4 = currentMIR->dalvikInsn.vC;
-        if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
-            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-            if (entry < 0)
-            {
-                return -1;
-            }
-
-            switch (inst_op)
-            {
-                case OP_ADD_INT_LIT16:
-                    tmpValue[0] = tmpValue[0] + tmp_s4;
-                    break;
-                case OP_RSUB_INT:
-                    tmpValue[0] = tmp_s4 - tmpValue[0];
-                    break;
-                case OP_MUL_INT_LIT16:
-                    tmpValue[0] = tmpValue[0] * tmp_s4;
-                    break;
-                case OP_DIV_INT_LIT16:
-                    tmpValue[0] = tmpValue[0] / tmp_s4;
-                    break;
-                case OP_REM_INT_LIT16:
-                    tmpValue[0] = tmpValue[0] % tmp_s4;
-                    break;
-                case OP_AND_INT_LIT16:
-                    tmpValue[0] = tmpValue[0] & tmp_s4;
-                    break;
-                case OP_OR_INT_LIT16:
-                    tmpValue[0] = tmpValue[0] | tmp_s4;
-                    break;
-                case OP_XOR_INT_LIT16:
-                    tmpValue[0] = tmpValue[0] ^ tmp_s4;
-                    break;
-                default:
-                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
-                    SET_JIT_ERROR (kJitErrorConstantFolding);
-                    return -1;
-            }
-
-            success = setVRToConst(vA, OpndSize_32, tmpValue);
-            if (success == false)
-            {
-                //setVRToConst set an error message when it failed so we just pass along the failure information
-                return -1;
-            }
-
-            compileTable[entry].refCount--;
-            retCode = touchOneVR(vB, LowOpndRegType_gp);
-            if (retCode < 0)
-                return retCode;
-#ifdef DEBUG_CONST
-            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
-#endif
-            return 1;
-        }
-        else {
-            addConstKill(vA);
-            return 0;
-        }
-    case OP_ADD_INT:
-    case OP_SUB_INT:
-    case OP_MUL_INT:
-    case OP_AND_INT:
-    case OP_OR_INT:
-    case OP_XOR_INT:
-    case OP_SHL_INT:
-    case OP_SHR_INT:
-    case OP_USHR_INT:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        if(isVirtualRegConstant(v1, LowOpndRegType_gp, tmpValue, false) == 3 &&
-           isVirtualRegConstant(v2, LowOpndRegType_gp, tmpValue2, false) == 3) {
-            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-            if (entry < 0)
-            {
-                return -1;
-            }
-
-            switch (inst_op)
-            {
-                case OP_ADD_INT:
-                    tmpValue[0] = tmpValue[0] + tmpValue2[0];
-                    break;
-                case OP_SUB_INT:
-                    tmpValue[0] = tmpValue[0] - tmpValue2[0];
-                    break;
-                case OP_MUL_INT:
-                    tmpValue[0] = tmpValue[0] * tmpValue2[0];
-                    break;
-                case OP_DIV_INT:
-                    tmpValue[0] = tmpValue[0] / tmpValue2[0];
-                    break;
-                case OP_REM_INT:
-                    tmpValue[0] = tmpValue[0] % tmpValue2[0];
-                    break;
-                case OP_AND_INT:
-                    tmpValue[0] = tmpValue[0] & tmpValue2[0];
-                    break;
-                case OP_OR_INT:
-                    tmpValue[0] = tmpValue[0] | tmpValue2[0];
-                    break;
-                case OP_XOR_INT:
-                    tmpValue[0] = tmpValue[0] ^ tmpValue2[0];
-                    break;
-                case OP_SHL_INT:
-                    tmpValue[0] = tmpValue[0] << tmpValue2[0];
-                    break;
-                case OP_SHR_INT:
-                    tmpValue[0] = tmpValue[0] >> tmpValue2[0];
-                    break;
-                case OP_USHR_INT:
-                    tmpValue[0] = (unsigned int) tmpValue[0] >> tmpValue2[0];
-                    break;
-                default:
-                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
-                    SET_JIT_ERROR (kJitErrorConstantFolding);
-                    return -1;
-            }
-
-            success = setVRToConst(vA, OpndSize_32, tmpValue);
-            if (success == false)
-            {
-                //setVRToConst set an error message when it failed so we just pass along the failure information
-                return -1;
-            }
-
-            compileTable[entry].refCount--;
-            retCode = touchOneVR(v1, LowOpndRegType_gp);
-            if (retCode < 0)
-                return retCode;
-            retCode = touchOneVR(v2, LowOpndRegType_gp);
-            if (retCode < 0)
-                return retCode;
-#ifdef DEBUG_CONST
-            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
-#endif
-            return 1;
-        }
-        else {
-            addConstKill(vA);
-            return 0;
-        }
-    case OP_ADD_INT_LIT8: //INST_AA
-    case OP_RSUB_INT_LIT8:
-    case OP_MUL_INT_LIT8:
-    case OP_AND_INT_LIT8:
-    case OP_OR_INT_LIT8:
-    case OP_XOR_INT_LIT8:
-    case OP_SHL_INT_LIT8:
-    case OP_SHR_INT_LIT8:
-    case OP_USHR_INT_LIT8:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        tmp_s4 = currentMIR->dalvikInsn.vC;
-        if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
-            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-            if (entry < 0)
-            {
-                return -1;
-            }
-
-            switch (inst_op)
-            {
-                case OP_ADD_INT_LIT8:
-                    tmpValue[0] = tmpValue[0] + tmp_s4;
-                    break;
-                case OP_RSUB_INT_LIT8:
-                    tmpValue[0] = tmp_s4 - tmpValue[0];
-                    break;
-                case OP_MUL_INT_LIT8:
-                    tmpValue[0] = tmpValue[0] * tmp_s4;
-                    break;
-                case OP_DIV_INT_LIT8:
-                    tmpValue[0] = tmpValue[0] / tmp_s4;
-                    break;
-                case OP_REM_INT_LIT8:
-                    tmpValue[0] = tmpValue[0] % tmp_s4;
-                    break;
-                case OP_AND_INT_LIT8:
-                    tmpValue[0] = tmpValue[0] & tmp_s4;
-                    break;
-                case OP_OR_INT_LIT8:
-                    tmpValue[0] = tmpValue[0] | tmp_s4;
-                    break;
-                case OP_XOR_INT_LIT8:
-                    tmpValue[0] = tmpValue[0] ^ tmp_s4;
-                    break;
-                case OP_SHL_INT_LIT8:
-                    tmpValue[0] = tmpValue[0] << tmp_s4;
-                    break;
-                case OP_SHR_INT_LIT8:
-                    tmpValue[0] = tmpValue[0] >> tmp_s4;
-                    break;
-                case OP_USHR_INT_LIT8:
-                    tmpValue[0] = (unsigned int) tmpValue[0] >> tmp_s4;
-                    break;
-                default:
-                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
-                    SET_JIT_ERROR (kJitErrorConstantFolding);
-                    return -1;
-            }
-
-            success = setVRToConst(vA, OpndSize_32, tmpValue);
-            if (success == false)
-            {
-                //setVRToConst set an error message when it failed so we just pass along the failure information
-                return -1;
-            }
-
-            compileTable[entry].refCount--;
-            retCode = touchOneVR(vB, LowOpndRegType_gp);
-            if (retCode < 0)
-                return retCode;
-#ifdef DEBUG_CONST
-            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
-#endif
-            return 1;
-        }
-        else {
-            addConstKill(vA);
-            return 0;
-        }
-    case OP_ADD_LONG:
-    case OP_SUB_LONG:
-    case OP_AND_LONG:
-    case OP_OR_LONG:
-    case OP_XOR_LONG:
-    case OP_MUL_LONG:
-    case OP_DIV_LONG:
-    case OP_REM_LONG:
-    case OP_SHL_LONG:
-    case OP_SHR_LONG:
-    case OP_USHR_LONG:
-        //TODO bytecode is not going to update state registers
-        //constant folding
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-    case OP_CMP_LONG:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        return 0;
-    case OP_ADD_LONG_2ADDR:
-    case OP_SUB_LONG_2ADDR:
-    case OP_AND_LONG_2ADDR:
-    case OP_OR_LONG_2ADDR:
-    case OP_XOR_LONG_2ADDR:
-    case OP_MUL_LONG_2ADDR:
-    case OP_DIV_LONG_2ADDR:
-    case OP_REM_LONG_2ADDR:
-    case OP_SHL_LONG_2ADDR:
-    case OP_SHR_LONG_2ADDR:
-    case OP_USHR_LONG_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-    case OP_CONST_4:
-        vA = currentMIR->dalvikInsn.vA;
-        tmp_s4 = currentMIR->dalvikInsn.vB;
-        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[0] = tmp_s4;
-
-        success = setVRToConst(vA, OpndSize_32, tmpValue);
-        if (success == false)
-        {
-            //setVRToConst set an error message when it failed so we just pass along the failure information
-            return -1;
-        }
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %d", vA, tmp_s4);
-#endif
-        return 1;
-    case OP_CONST_16:
-        BBBB = currentMIR->dalvikInsn.vB;
-        vA = currentMIR->dalvikInsn.vA;
-        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[0] = (s2)BBBB;
-
-        success = setVRToConst(vA, OpndSize_32, tmpValue);
-        if (success == false)
-        {
-            //setVRToConst set an error message when it failed so we just pass along the failure information
-            return -1;
-        }
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
-#endif
-        return 1;
-    case OP_CONST:
-        vA = currentMIR->dalvikInsn.vA;
-        tmp_u4 = currentMIR->dalvikInsn.vB;
-        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[0] = (s4)tmp_u4;
-
-        success = setVRToConst(vA, OpndSize_32, tmpValue);
-        if (success == false)
-        {
-            //setVRToConst set an error message when it failed so we just pass along the failure information
-            return -1;
-        }
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
-#endif
-        return 1;
-    case OP_CONST_HIGH16:
-        vA = currentMIR->dalvikInsn.vA;
-        tmp_u2 = currentMIR->dalvikInsn.vB;
-        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[0] = ((s4)tmp_u2)<<16;
-
-        success = setVRToConst(vA, OpndSize_32, tmpValue);
-        if (success == false)
-        {
-            //setVRToConst set an error message when it failed so we just pass along the failure information
-            return -1;
-        }
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
-#endif
-        return 1;
-    case OP_CONST_WIDE_16:
-        vA = currentMIR->dalvikInsn.vA;
-        tmp_u2 = currentMIR->dalvikInsn.vB;
-        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[0] = (s2)tmp_u2;
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %x", vA, infoArray[entry].value[0]);
-#endif
-
-        entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[1] = ((s2)tmp_u2)>>31;
-
-        success = setVRToConst(vA, OpndSize_64, tmpValue);
-        if (success == false)
-        {
-            //setVRToConst set an error message when it failed so we just pass along the failure information
-            return -1;
-        }
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %x", vA+1, infoArray[entry].value[0]);
-#endif
-        return 1;
-    case OP_CONST_WIDE_32:
-        vA = currentMIR->dalvikInsn.vA;
-        tmp_u4 = currentMIR->dalvikInsn.vB;
-        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[0] = (s4)tmp_u4;
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %x", vA, infoArray[entry].value[0]);
-#endif
-
-        entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[1] = ((s4)tmp_u4)>>31;
-
-        success = setVRToConst(vA, OpndSize_64, tmpValue);
-        if (success == false)
-        {
-            //setVRToConst set an error message when it failed so we just pass along the failure information
-            return -1;
-        }
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %x", vA+1, infoArray[entry].value[0]);
-#endif
-        return 1;
-    case OP_CONST_WIDE:
-        vA = currentMIR->dalvikInsn.vA;
-        tmp_u4 = (s4)currentMIR->dalvikInsn.vB_wide;
-        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[0] = (s4)tmp_u4;
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %x", vA, infoArray[entry].value[0]);
-#endif
-
-        tmp_u4 = (s4)(currentMIR->dalvikInsn.vB_wide >> 32);
-        entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[1] = (s4)tmp_u4;
-
-        success = setVRToConst(vA, OpndSize_64, tmpValue);
-        if (success == false)
-        {
-            //setVRToConst set an error message when it failed so we just pass along the failure information
-            return -1;
-        }
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %x", vA+1, infoArray[entry].value[0]);
-#endif
-        return 1;
-    case OP_CONST_WIDE_HIGH16:
-        vA = currentMIR->dalvikInsn.vA;
-        tmp_u2 = currentMIR->dalvikInsn.vB;
-        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[0] = 0;
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %x", vA, infoArray[entry].value[0]);
-#endif
-
-        entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp);
-        if (entry < 0)
-            return -1;
-        tmpValue[1] = ((s4)tmp_u2)<<16;
-
-        success = setVRToConst(vA, OpndSize_64, tmpValue);
-        if (success == false)
-        {
-            //setVRToConst set an error message when it failed so we just pass along the failure information
-            return -1;
-        }
-
-        compileTable[entry].refCount--;
-#ifdef DEBUG_CONST
-        ALOGD("getConstInfo: set VR %d to %x", vA+1, infoArray[entry].value[0]);
-#endif
-        return 1;
-#ifdef SUPPORT_HLO
-    case OP_X_AGET_QUICK:
-    case OP_X_AGET_OBJECT_QUICK:
-    case OP_X_AGET_BOOLEAN_QUICK:
-    case OP_X_AGET_BYTE_QUICK:
-    case OP_X_AGET_CHAR_QUICK:
-    case OP_X_AGET_SHORT_QUICK:
-        vA = FETCH(1) & 0xff;
-        addConstKill(vA);
-        return 0;
-    case OP_X_AGET_WIDE_QUICK:
-        vA = FETCH(1) & 0xff;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-    case OP_X_DEREF_GET:
-    case OP_X_DEREF_GET_OBJECT:
-    case OP_X_DEREF_GET_BOOLEAN:
-    case OP_X_DEREF_GET_BYTE:
-    case OP_X_DEREF_GET_CHAR:
-    case OP_X_DEREF_GET_SHORT:
-        vA = FETCH(1) & 0xff;
-        addConstKill(vA);
-        return 0;
-    case OP_X_DEREF_GET_WIDE:
-        vA = FETCH(1) & 0xff;
-        addConstKill(vA);
-        addConstKill(vA+1);
-        return 0;
-#endif
-    default:
-        // Bytecode does not generate a const
-        break;
-    }
-    return 0;
-}
-
-/**
- * @brief Updates infoArray with virtual registers accessed when lowering the bytecode.
- * @param infoArray Must be an array of size MAX_REG_PER_BYTECODE. This is updated by function.
- * @param currentMIR The MIR to examine.
- * @param updateBBConstraints should we update the BB constraints?
- * @return Returns the number of registers for the bytecode. Returns -1 in case of error.
- */
-int getVirtualRegInfo (VirtualRegInfo* infoArray, const MIR * currentMIR, bool updateBBConstraints)
-{
-    u2 inst_op = currentMIR->dalvikInsn.opcode;
-    int vA = 0, vB = 0, vref, vindex;
-    int v1, v2, vD, vE, vF;
-    u2 length, count;
-    int kk, num, num_entry;
-    s2 tmp_s2;
-    int num_regs_per_bytecode = 0;
-    //update infoArray[xx].allocConstraints
-    for(num = 0; num < MAX_REG_PER_BYTECODE; num++) {
-        for(kk = 0; kk < 8; kk++) {
-            infoArray[num].allocConstraints[kk].physicalReg = (PhysicalReg)kk;
-            infoArray[num].allocConstraints[kk].count = 0;
-        }
-    }
-
-    //A bytecode with the inlined flag is treated as a nop so therefore simply return
-    //that we have 0 regs for this bytecode
-    if (currentMIR->OptimizationFlags & MIR_INLINED)
-    {
-        return 0;
-    }
-
-    bool isExtended = false;
-
-    // Check if we need to handle an extended MIR
-    if (currentMIR->dalvikInsn.opcode >= static_cast<Opcode> (kMirOpFirst)) {
-        //We have an extended MIR
-        isExtended = true;
-
-        switch (static_cast<ExtendedMIROpcode>(currentMIR->dalvikInsn.opcode)) {
-            case kMirOpRegisterize:
-                infoArray[0].regNum = currentMIR->dalvikInsn.vA;
-                infoArray[0].refCount = 2;
-
-                //The access type is use and then def because we use the VR when loading it into temporary
-                //and then we alias virtual register to that temporary thus "defining" it.
-                infoArray[0].accessType = REGACCESS_UD;
-
-                //Decide the type depending on the register class
-                switch (static_cast<RegisterClass> (currentMIR->dalvikInsn.vB))
-                {
-                    case kCoreReg:
-                        infoArray[0].physicalType = LowOpndRegType_gp;
-                        break;
-                    case kSFPReg:
-                        infoArray[0].physicalType = LowOpndRegType_ss;
-                        break;
-                    case kDFPReg:
-                        infoArray[0].physicalType = LowOpndRegType_xmm;
-                        break;
-                    default:
-                        ALOGI("JIT_INFO: kMirOpRegisterize does not support regClass %d", currentMIR->dalvikInsn.vB);
-                        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
-                        break;
-                }
-                num_regs_per_bytecode = 1;
-                break;
-            default:
-                ALOGI("JIT_INFO: Extended MIR not supported in getVirtualRegInfo");
-                SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
-                return -1;
-        }
-    }
-
-    //If we have an extended bytecode, we have nothing else to do
-    if (isExtended == true)
-    {
-        return num_regs_per_bytecode;
-    }
-
-    switch (inst_op) {
-    case OP_NOP:
-        break;
-    case OP_MOVE:
-    case OP_MOVE_OBJECT:
-    case OP_MOVE_FROM16:
-    case OP_MOVE_OBJECT_FROM16:
-    case OP_MOVE_16:
-    case OP_MOVE_OBJECT_16:
-        if(inst_op == OP_MOVE || inst_op == OP_MOVE_OBJECT) {
-            vA = currentMIR->dalvikInsn.vA;
-            vB = currentMIR->dalvikInsn.vB;
-        }
-        else if(inst_op == OP_MOVE_FROM16 || inst_op == OP_MOVE_OBJECT_FROM16) {
-            vA = currentMIR->dalvikInsn.vA;
-            vB = currentMIR->dalvikInsn.vB;
-        }
-        else if(inst_op == OP_MOVE_16 || inst_op == OP_MOVE_OBJECT_16) {
-            vA = currentMIR->dalvikInsn.vA;
-            vB = currentMIR->dalvikInsn.vB;
-        }
-        infoArray[1].regNum = vA; //dst
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = vB; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_MOVE_WIDE:
-    case OP_MOVE_WIDE_FROM16:
-    case OP_MOVE_WIDE_16:
-        if(inst_op == OP_MOVE_WIDE) {
-            vA = currentMIR->dalvikInsn.vA;
-            vB = currentMIR->dalvikInsn.vB;
-        }
-        else if(inst_op == OP_MOVE_WIDE_FROM16) {
-            vA = currentMIR->dalvikInsn.vA;
-            vB = currentMIR->dalvikInsn.vB;
-        }
-        else if(inst_op == OP_MOVE_WIDE_16) {
-            vA = currentMIR->dalvikInsn.vA;
-            vB = currentMIR->dalvikInsn.vB;
-        }
-        infoArray[1].regNum = vA; //dst
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        infoArray[0].regNum = vB; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_MOVE_RESULT: //access memory
-    case OP_MOVE_RESULT_OBJECT:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_MOVE_RESULT_WIDE: //note: 2 destinations
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_MOVE_EXCEPTION: //access memory
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_RETURN_VOID:
-    case OP_RETURN_VOID_BARRIER:
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-        }
-        num_regs_per_bytecode = 0;
-        break;
-    case OP_RETURN:
-    case OP_RETURN_OBJECT:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-        }
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_RETURN_WIDE:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_CONST_4:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_CONST_16:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_CONST:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_CONST_HIGH16:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_CONST_WIDE_16:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vA+1;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_CONST_WIDE_32:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vA+1;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_CONST_WIDE:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vA+1;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_CONST_WIDE_HIGH16:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vA+1;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_CONST_STRING:
-    case OP_CONST_STRING_JUMBO:
-    case OP_CONST_CLASS:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-        }
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_MONITOR_ENTER:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_MONITOR_EXIT:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX); //eax is used as return value from c function
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_CHECK_CAST:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_ECX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_INSTANCE_OF:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = vB; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vA; //dst
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-        }
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_ARRAY_LENGTH:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = vB; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vA; //dst
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        //%edx is used in this bytecode, update currentBB->allocConstraints
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_NEW_INSTANCE:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //dst
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_D;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_ECX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_NEW_ARRAY:
-        vA = currentMIR->dalvikInsn.vA; //destination
-        vB = currentMIR->dalvikInsn.vB; //length
-        infoArray[0].regNum = vB; //src
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vA; //dst
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_FILLED_NEW_ARRAY: {//update return value
-        //can use up to 5 registers to fill the content of array
-        length = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.arg[0];
-        v2 = currentMIR->dalvikInsn.arg[1];
-        int v3 = currentMIR->dalvikInsn.arg[2];
-        int v4 = currentMIR->dalvikInsn.arg[3];
-        int v5 = currentMIR->dalvikInsn.arg[4];
-        if(length >= 1) {
-            infoArray[0].regNum = v1; //src
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            infoArray[0].physicalType = LowOpndRegType_gp;
-        }
-        if(length >= 2) {
-            infoArray[1].regNum = v2; //src
-            infoArray[1].refCount = 1;
-            infoArray[1].accessType = REGACCESS_U;
-            infoArray[1].physicalType = LowOpndRegType_gp;
-        }
-        if(length >= 3) {
-            infoArray[2].regNum = v3; //src
-            infoArray[2].refCount = 1;
-            infoArray[2].accessType = REGACCESS_U;
-            infoArray[2].physicalType = LowOpndRegType_gp;
-        }
-        if(length >= 4) {
-            infoArray[3].regNum = v4; //src
-            infoArray[3].refCount = 1;
-            infoArray[3].accessType = REGACCESS_U;
-            infoArray[3].physicalType = LowOpndRegType_gp;
-        }
-        if(length >= 5) {
-            infoArray[4].regNum = v5; //src
-            infoArray[4].refCount = 1;
-            infoArray[4].accessType = REGACCESS_U;
-            infoArray[4].physicalType = LowOpndRegType_gp;
-        }
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = length;
-        break;
-    }
-    case OP_FILLED_NEW_ARRAY_RANGE: {//use "length" virtual registers
-        length = currentMIR->dalvikInsn.vA;
-        u4 vC = currentMIR->dalvikInsn.vC;
-        for(kk = 0; kk < length; kk++) {
-            infoArray[kk].regNum = vC+kk; //src
-            infoArray[kk].refCount = 1;
-            infoArray[kk].accessType = REGACCESS_U;
-            infoArray[kk].physicalType = LowOpndRegType_gp;
-        }
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = length;
-        break;
-    }
-    case OP_FILL_ARRAY_DATA: //update content of array, read memory
-        vA = currentMIR->dalvikInsn.vA; //use virtual register, but has side-effect, update memory
-        infoArray[0].regNum = vA; //use
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_THROW: //update glue->exception
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //use
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_THROW_VERIFICATION_ERROR:
-    case OP_GOTO:
-    case OP_GOTO_16:
-    case OP_GOTO_32:
-        num_regs_per_bytecode = 0;
-        break;
-    case OP_PACKED_SWITCH:
-    case OP_SPARSE_SWITCH:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //use
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 1;
-        break;
-
-    case OP_CMPL_FLOAT: //move 32 bits from memory to lower part of XMM register
-    case OP_CMPG_FLOAT:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        num_regs_per_bytecode = 1;
-        infoArray[0].regNum = v1; //use ss or sd CHECK
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_ss;
-        infoArray[1].regNum = v2; //use
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_ss;
-        num_regs_per_bytecode = 3;
-        num_entry = 2;
-        infoArray[num_entry].regNum = vA; //define
-        infoArray[num_entry].refCount = 3;
-        infoArray[num_entry].accessType = REGACCESS_D;
-        infoArray[num_entry].physicalType = LowOpndRegType_gp;
-        break;
-    case OP_CMPL_DOUBLE: //move 64 bits from memory to lower part of XMM register
-    case OP_CMPG_DOUBLE:
-    case OP_CMP_LONG: //load v1, v1+1, v2, v2+1 to gpr
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        num_regs_per_bytecode = 1;
-        if(inst_op == OP_CMP_LONG) {
-            infoArray[0].regNum = v1; //use
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            infoArray[0].physicalType = LowOpndRegType_gp;
-            infoArray[1].regNum = v1 + 1; //use
-            infoArray[1].refCount = 1;
-            infoArray[1].accessType = REGACCESS_U;
-            infoArray[1].physicalType = LowOpndRegType_gp;
-            infoArray[2].regNum = v2; //use
-            infoArray[2].refCount = 1;
-            infoArray[2].accessType = REGACCESS_U;
-            infoArray[2].physicalType = LowOpndRegType_gp;
-            infoArray[3].regNum = v2 + 1; //use
-            infoArray[3].refCount = 1;
-            infoArray[3].accessType = REGACCESS_U;
-            infoArray[3].physicalType = LowOpndRegType_gp;
-            num_regs_per_bytecode = 5;
-            num_entry = 4;
-            infoArray[num_entry].regNum = vA; //define
-            infoArray[num_entry].refCount = 5;
-            infoArray[num_entry].accessType = REGACCESS_D;
-            infoArray[num_entry].physicalType = LowOpndRegType_gp;
-        }
-        else {
-            infoArray[0].regNum = v1; //use ss or sd CHECK
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            infoArray[0].physicalType = LowOpndRegType_xmm;
-            infoArray[1].regNum = v2; //use
-            infoArray[1].refCount = 1;
-            infoArray[1].accessType = REGACCESS_U;
-            infoArray[1].physicalType = LowOpndRegType_xmm;
-            num_regs_per_bytecode = 3;
-            num_entry = 2;
-            infoArray[num_entry].regNum = vA; //define
-            infoArray[num_entry].refCount = 3;
-            infoArray[num_entry].accessType = REGACCESS_D;
-            infoArray[num_entry].physicalType = LowOpndRegType_gp;
-        }
-        break;
-    case OP_IF_EQ:
-    case OP_IF_NE:
-    case OP_IF_LT:
-    case OP_IF_GE:
-    case OP_IF_GT:
-    case OP_IF_LE:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = vA; //use
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vB;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_IF_EQZ:
-    case OP_IF_NEZ:
-    case OP_IF_LTZ:
-    case OP_IF_GEZ:
-    case OP_IF_GTZ:
-    case OP_IF_LEZ:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = vA; //use
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_AGET:
-    case OP_AGET_WIDE:
-    case OP_AGET_OBJECT:
-    case OP_AGET_BOOLEAN: //movez 8
-    case OP_AGET_BYTE: //moves 8
-    case OP_AGET_CHAR: //movez 16
-    case OP_AGET_SHORT: //moves 16
-        vA = currentMIR->dalvikInsn.vA;
-        vref = currentMIR->dalvikInsn.vB;
-        vindex = currentMIR->dalvikInsn.vC;
-        if(inst_op == OP_AGET_WIDE) {
-            infoArray[2].regNum = vA;
-            infoArray[2].refCount = 1;
-            infoArray[2].accessType = REGACCESS_D;
-            infoArray[2].physicalType = LowOpndRegType_xmm; //64, 128 not used in lowering
-        } else {
-            infoArray[2].regNum = vA;
-            infoArray[2].refCount = 1;
-            infoArray[2].accessType = REGACCESS_D;
-            infoArray[2].physicalType = LowOpndRegType_gp;
-        }
-        infoArray[0].regNum = vref; //use
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vindex; //use
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_APUT:
-    case OP_APUT_WIDE:
-    case OP_APUT_OBJECT:
-    case OP_APUT_BOOLEAN:
-    case OP_APUT_BYTE:
-    case OP_APUT_CHAR:
-    case OP_APUT_SHORT:
-        vA = currentMIR->dalvikInsn.vA;
-        vref = currentMIR->dalvikInsn.vB;
-        vindex = currentMIR->dalvikInsn.vC;
-        if(inst_op == OP_APUT_WIDE) {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            infoArray[0].physicalType = LowOpndRegType_xmm; //64, 128 not used in lowering
-        } else {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            infoArray[0].physicalType = LowOpndRegType_gp;
-        }
-        infoArray[1].regNum = vref; //use
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = vindex; //use
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_U;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        if (inst_op == OP_APUT_OBJECT && updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 3;
-        break;
-
-    case OP_IGET:
-    case OP_IGET_WIDE:
-    case OP_IGET_OBJECT:
-    case OP_IGET_VOLATILE:
-    case OP_IGET_WIDE_VOLATILE:
-    case OP_IGET_OBJECT_VOLATILE:
-    case OP_IGET_BOOLEAN:
-    case OP_IGET_BYTE:
-    case OP_IGET_CHAR:
-    case OP_IGET_SHORT:
-    case OP_IGET_QUICK:
-    case OP_IGET_WIDE_QUICK:
-    case OP_IGET_OBJECT_QUICK:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = vB; //object instance
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-#ifndef WITH_SELF_VERIFICATION
-        if(inst_op == OP_IGET_WIDE || inst_op == OP_IGET_WIDE_QUICK) {
-            infoArray[1].regNum = vA;
-            infoArray[1].refCount = 1;
-            infoArray[1].accessType = REGACCESS_D;
-            infoArray[1].physicalType = LowOpndRegType_xmm; //64
-        } else if(inst_op == OP_IGET_WIDE_VOLATILE) {
-            infoArray[1].regNum = vA;
-            infoArray[1].refCount = 1;
-            infoArray[1].accessType = REGACCESS_D;
-            infoArray[1].physicalType = LowOpndRegType_gp;
-            infoArray[2].regNum = vA+1;
-            infoArray[2].refCount = 1;
-            infoArray[2].accessType = REGACCESS_D;
-            infoArray[2].physicalType = LowOpndRegType_gp;
-            ///Update num regs per bytecode in this case
-            num_regs_per_bytecode = 3;
-        } else {
-            infoArray[1].regNum = vA;
-            infoArray[1].refCount = 1;
-            infoArray[1].accessType = REGACCESS_D;
-            infoArray[1].physicalType = LowOpndRegType_gp;
-        }
-#else
-        if(inst_op == OP_IGET_WIDE || inst_op == OP_IGET_WIDE_QUICK ||
-           inst_op == OP_IGET_WIDE_VOLATILE) {
-            infoArray[1].regNum = vA;
-            infoArray[1].refCount = 1;
-            infoArray[1].accessType = REGACCESS_D;
-            infoArray[1].physicalType = LowOpndRegType_xmm; //64
-        } else {
-            infoArray[1].regNum = vA;
-            infoArray[1].refCount = 1;
-            infoArray[1].accessType = REGACCESS_D;
-            infoArray[1].physicalType = LowOpndRegType_gp;
-        }
-#endif
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        break;
-    case OP_IPUT:
-    case OP_IPUT_WIDE:
-    case OP_IPUT_OBJECT:
-    case OP_IPUT_VOLATILE:
-    case OP_IPUT_WIDE_VOLATILE:
-    case OP_IPUT_OBJECT_VOLATILE:
-    case OP_IPUT_BOOLEAN:
-    case OP_IPUT_BYTE:
-    case OP_IPUT_CHAR:
-    case OP_IPUT_SHORT:
-    case OP_IPUT_QUICK:
-    case OP_IPUT_WIDE_QUICK:
-    case OP_IPUT_OBJECT_QUICK:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        if(inst_op == OP_IPUT_WIDE || inst_op == OP_IPUT_WIDE_QUICK || inst_op == OP_IPUT_WIDE_VOLATILE) {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            infoArray[0].physicalType = LowOpndRegType_xmm; //64
-        } else {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            infoArray[0].physicalType = LowOpndRegType_gp;
-        }
-        infoArray[1].regNum = vB; //object instance
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_SGET:
-    case OP_SGET_WIDE:
-    case OP_SGET_OBJECT:
-    case OP_SGET_VOLATILE:
-    case OP_SGET_WIDE_VOLATILE:
-    case OP_SGET_OBJECT_VOLATILE:
-    case OP_SGET_BOOLEAN:
-    case OP_SGET_BYTE:
-    case OP_SGET_CHAR:
-    case OP_SGET_SHORT:
-        vA = currentMIR->dalvikInsn.vA;
-#ifndef WITH_SELF_VERIFICATION
-        if(inst_op == OP_SGET_WIDE) {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_D;
-            infoArray[0].physicalType = LowOpndRegType_xmm; //64
-        } else if(inst_op == OP_SGET_WIDE_VOLATILE) {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_D;
-            infoArray[0].physicalType = LowOpndRegType_gp;
-            infoArray[1].regNum = vA+1;
-            infoArray[1].refCount = 1;
-            infoArray[1].accessType = REGACCESS_D;
-            infoArray[1].physicalType = LowOpndRegType_gp;
-        } else {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_D;
-            infoArray[0].physicalType = LowOpndRegType_gp;
-        }
-        if(inst_op == OP_SGET_WIDE_VOLATILE)
-            num_regs_per_bytecode = 2;
-        else
-            num_regs_per_bytecode = 1;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-        }
-        break;
-#else
-        if(inst_op == OP_SGET_WIDE || inst_op == OP_SGET_WIDE_VOLATILE) {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_D;
-            infoArray[0].physicalType = LowOpndRegType_xmm; //64
-        }  else {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_D;
-            infoArray[0].physicalType = LowOpndRegType_gp;
-        }
-        num_regs_per_bytecode = 1;
-        updateCurrentBBWithConstraints(PhysicalReg_EAX);
-        break;
-#endif
-    case OP_SPUT:
-    case OP_SPUT_WIDE:
-    case OP_SPUT_OBJECT:
-    case OP_SPUT_VOLATILE:
-    case OP_SPUT_WIDE_VOLATILE:
-    case OP_SPUT_OBJECT_VOLATILE:
-    case OP_SPUT_BOOLEAN:
-    case OP_SPUT_BYTE:
-    case OP_SPUT_CHAR:
-    case OP_SPUT_SHORT:
-        vA = currentMIR->dalvikInsn.vA;
-        if(inst_op == OP_SPUT_WIDE || inst_op == OP_SPUT_WIDE_VOLATILE) {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            infoArray[0].physicalType = LowOpndRegType_xmm; //64
-        } else {
-            infoArray[0].regNum = vA;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            infoArray[0].physicalType = LowOpndRegType_gp;
-        }
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-        }
-        num_regs_per_bytecode = 1;
-        break;
-    case OP_INVOKE_VIRTUAL:
-    case OP_INVOKE_SUPER:
-    case OP_INVOKE_DIRECT:
-    case OP_INVOKE_STATIC:
-    case OP_INVOKE_INTERFACE:
-    case OP_INVOKE_VIRTUAL_QUICK:
-    case OP_INVOKE_SUPER_QUICK:
-        vD = currentMIR->dalvikInsn.arg[0]; //object for virtual,direct & interface
-        count = currentMIR->dalvikInsn.vA;
-        vE = currentMIR->dalvikInsn.arg[1];
-        vF = currentMIR->dalvikInsn.arg[2];
-        vA = currentMIR->dalvikInsn.arg[4]; //5th argument
-
-        for (int vrNum = 0; vrNum < count; vrNum++) {
-            if (vrNum == 0) {
-                infoArray[num_regs_per_bytecode].regNum = currentMIR->dalvikInsn.arg[vrNum];
-                if (inst_op == OP_INVOKE_VIRTUAL_QUICK || inst_op == OP_INVOKE_SUPER_QUICK
-                    || inst_op == OP_INVOKE_VIRTUAL || inst_op == OP_INVOKE_DIRECT
-                    || inst_op == OP_INVOKE_INTERFACE){
-                        infoArray[num_regs_per_bytecode].refCount = 2;
-                    } else {
-                        infoArray[num_regs_per_bytecode].refCount = 1;
-                    }
-                    infoArray[num_regs_per_bytecode].accessType = REGACCESS_U;
-                    infoArray[num_regs_per_bytecode].physicalType = LowOpndRegType_gp;
-                    num_regs_per_bytecode++;
-            } else if ((vrNum + 1 < count) && // Use XMM registers if adjacent VRs are accessed
-                       (currentMIR->dalvikInsn.arg[vrNum] + 1 == currentMIR->dalvikInsn.arg[vrNum + 1])) {
-                infoArray[num_regs_per_bytecode].regNum = currentMIR->dalvikInsn.arg[vrNum];
-                infoArray[num_regs_per_bytecode].refCount = 1;
-                infoArray[num_regs_per_bytecode].accessType = REGACCESS_U;
-                infoArray[num_regs_per_bytecode].physicalType = LowOpndRegType_xmm;
-                // We can now skip the vrNum+1 which represents the rest of the wide VR
-                vrNum++;
-                num_regs_per_bytecode++;
-            } else { // Use gp registers
-                infoArray[num_regs_per_bytecode].regNum = currentMIR->dalvikInsn.arg[vrNum];
-                infoArray[num_regs_per_bytecode].refCount = 1;
-                infoArray[num_regs_per_bytecode].accessType = REGACCESS_U;
-                infoArray[num_regs_per_bytecode].physicalType = LowOpndRegType_gp;
-                num_regs_per_bytecode++;
-            }
-        }
-
-        if (updateBBConstraints == true)
-        {
-            if (inst_op != OP_INVOKE_VIRTUAL_QUICK && inst_op != OP_INVOKE_SUPER_QUICK)
-            {
-                updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            }
-            updateCurrentBBWithConstraints (PhysicalReg_ECX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        break;
-    case OP_INVOKE_VIRTUAL_RANGE:
-    case OP_INVOKE_SUPER_RANGE:
-    case OP_INVOKE_DIRECT_RANGE:
-    case OP_INVOKE_STATIC_RANGE:
-    case OP_INVOKE_INTERFACE_RANGE:
-    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
-    case OP_INVOKE_SUPER_QUICK_RANGE:
-        vD = currentMIR->dalvikInsn.vC;
-        count = currentMIR->dalvikInsn.vA;
-        if(count == 0) {
-            if(inst_op == OP_INVOKE_VIRTUAL_RANGE || inst_op == OP_INVOKE_DIRECT_RANGE ||
-               inst_op == OP_INVOKE_INTERFACE_RANGE || inst_op == OP_INVOKE_VIRTUAL_QUICK_RANGE ||
-               inst_op == OP_INVOKE_SUPER_QUICK_RANGE) {
-                infoArray[0].regNum = vD;
-                infoArray[0].refCount = 1;
-                infoArray[0].accessType = REGACCESS_U;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-            }
-        }
-        if(count > 0) { //same for count > 10
-            for(kk = 0; kk < count; kk++) {
-                infoArray[kk].regNum = vD+kk; //src
-                if(kk == 0 && (inst_op == OP_INVOKE_VIRTUAL_QUICK_RANGE ||
-                               inst_op == OP_INVOKE_SUPER_QUICK_RANGE))
-                    infoArray[kk].refCount = 2;
-                else if(kk == 0 && (inst_op == OP_INVOKE_VIRTUAL_RANGE ||
-                                    inst_op == OP_INVOKE_DIRECT_RANGE ||
-                                    inst_op == OP_INVOKE_INTERFACE_RANGE))
-                    infoArray[kk].refCount = 2;
-                else
-                    infoArray[kk].refCount = 1;
-                infoArray[kk].accessType = REGACCESS_U;
-                infoArray[kk].physicalType = LowOpndRegType_gp;
-            }
-        }
-        if (updateBBConstraints == true)
-        {
-            if (inst_op != OP_INVOKE_VIRTUAL_QUICK_RANGE && inst_op != OP_INVOKE_SUPER_QUICK_RANGE)
-            {
-                updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            }
-            updateCurrentBBWithConstraints (PhysicalReg_ECX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = count;
-        break;
-    case OP_NEG_INT:
-    case OP_NOT_INT:
-    case OP_NEG_FLOAT:
-        vA = currentMIR->dalvikInsn.vA; //destination
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_NEG_LONG:
-    case OP_NOT_LONG:
-    case OP_NEG_DOUBLE:
-        vA = currentMIR->dalvikInsn.vA; //destination
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_INT_TO_LONG: //hard-coded registers
-        vA = currentMIR->dalvikInsn.vA; //destination
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp; //save from %eax
-        infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;
-        infoArray[2].regNum = vA+1;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[2].allocConstraints[PhysicalReg_EDX].count = 1;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_INT_TO_FLOAT: //32 to 32
-    case OP_INT_TO_DOUBLE: //32 to 64
-    case OP_LONG_TO_FLOAT: //64 to 32
-    case OP_LONG_TO_DOUBLE: //64 to 64
-    case OP_FLOAT_TO_DOUBLE: //32 to 64
-    case OP_DOUBLE_TO_FLOAT: //64 to 32
-        vA = currentMIR->dalvikInsn.vA; //destination
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        if(inst_op == OP_LONG_TO_DOUBLE || inst_op == OP_FLOAT_TO_DOUBLE)
-            infoArray[1].physicalType = LowOpndRegType_fs;
-        else if (inst_op == OP_INT_TO_DOUBLE)
-            infoArray[1].physicalType = LowOpndRegType_xmm;
-        else
-            infoArray[1].physicalType = LowOpndRegType_fs_s;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        if(inst_op == OP_INT_TO_FLOAT || inst_op == OP_FLOAT_TO_DOUBLE)
-            infoArray[0].physicalType = LowOpndRegType_fs_s; //float
-        else if (inst_op == OP_INT_TO_DOUBLE)
-            infoArray[0].physicalType = LowOpndRegType_gp;
-        else
-            infoArray[0].physicalType = LowOpndRegType_fs;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_LONG_TO_INT:
-        vA = currentMIR->dalvikInsn.vA; //destination
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_FLOAT_TO_INT:
-    case OP_DOUBLE_TO_INT: //for reaching-def analysis
-        vA = currentMIR->dalvikInsn.vA; //destination
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 3;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_fs_s; //store_int_fp_stack_VR
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        if(inst_op == OP_DOUBLE_TO_INT)
-            infoArray[0].physicalType = LowOpndRegType_fs;
-        else
-            infoArray[0].physicalType = LowOpndRegType_fs_s;
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_FLOAT_TO_LONG:
-    case OP_DOUBLE_TO_LONG:
-        vA = currentMIR->dalvikInsn.vA; //destination
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 3;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_fs;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        if(inst_op == OP_DOUBLE_TO_LONG)
-            infoArray[0].physicalType = LowOpndRegType_fs;
-        else
-            infoArray[0].physicalType = LowOpndRegType_fs_s;
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_INT_TO_BYTE:
-    case OP_INT_TO_CHAR:
-    case OP_INT_TO_SHORT:
-        vA = currentMIR->dalvikInsn.vA; //destination
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-
-    case OP_ADD_INT:
-    case OP_SUB_INT:
-    case OP_MUL_INT:
-    case OP_AND_INT:
-    case OP_OR_INT:
-    case OP_XOR_INT:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_DIV_INT:
-    case OP_REM_INT:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1; //for v1
-        if(inst_op == OP_REM_INT)
-            infoArray[2].allocConstraints[PhysicalReg_EDX].count = 1;//vA
-        else
-            infoArray[2].allocConstraints[PhysicalReg_EAX].count = 1;//vA
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_SHL_INT:
-    case OP_SHR_INT:
-    case OP_USHR_INT:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = v2; // in ecx
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[1].allocConstraints[PhysicalReg_ECX].count = 1;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_ECX);
-        }
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_ADD_LONG:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = v1+1;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = v2;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_U;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = v2+1;
-        infoArray[3].refCount = 1;
-        infoArray[3].accessType = REGACCESS_U;
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = vA;
-        infoArray[4].refCount = 1;
-        infoArray[4].accessType = REGACCESS_D;
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[5].regNum = vA+1;
-        infoArray[5].refCount = 1;
-        infoArray[5].accessType = REGACCESS_D;
-        infoArray[5].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 6;
-        break;
-    case OP_SUB_LONG:
-    case OP_AND_LONG:
-    case OP_OR_LONG:
-    case OP_XOR_LONG:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_xmm;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_MUL_LONG: //used int
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = v1+1;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = v2;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_U;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = v2+1;
-        infoArray[3].refCount = 1;
-        infoArray[3].accessType = REGACCESS_U;
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = vA;
-        infoArray[4].refCount = 1;
-        infoArray[4].accessType = REGACCESS_D;
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[5].regNum = vA+1;
-        infoArray[5].refCount = 1;
-        infoArray[5].accessType = REGACCESS_D;
-        infoArray[5].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_ECX);
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-        }
-        num_regs_per_bytecode = 6;
-        break;
-    case OP_DIV_LONG: //v1: xmm v2,vA:
-    case OP_REM_LONG:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = v2+1;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_U;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = vA;
-        infoArray[3].refCount = 1;
-        infoArray[3].accessType = REGACCESS_D;
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = vA+1;
-        infoArray[4].refCount = 1;
-        infoArray[4].accessType = REGACCESS_D;
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 5;
-        break;
-    case OP_SHL_LONG: //v2: 32, move_ss; v1,vA: xmm CHECK
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_ss;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_SHR_LONG: //v2: 32, move_ss; v1,vA: xmm CHECK
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_ss;
-        infoArray[2].regNum = v1+1;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_U;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = vA;
-        infoArray[3].refCount = 1;
-        infoArray[3].accessType = REGACCESS_D;
-        infoArray[3].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 4;
-        break;
-    case OP_USHR_LONG: //v2: move_ss; v1,vA: move_sd
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm; //sd
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_ss; //ss
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_xmm; //sd
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_ADD_FLOAT: //move_ss
-    case OP_SUB_FLOAT:
-    case OP_MUL_FLOAT:
-    case OP_DIV_FLOAT:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_ss;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_ss;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_ss;
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_REM_FLOAT: //32 bit GPR, fp_stack for output
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_fs_s;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_ADD_DOUBLE: //move_sd
-    case OP_SUB_DOUBLE:
-    case OP_MUL_DOUBLE:
-    case OP_DIV_DOUBLE:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_xmm;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 3;
-        break;
-    case OP_REM_DOUBLE: //64 bit XMM, fp_stack for output
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        v2 = currentMIR->dalvikInsn.vC;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_D;
-        infoArray[2].physicalType = LowOpndRegType_fs;
-        infoArray[0].regNum = v1;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 3;
-        break;
-
-    case OP_ADD_INT_2ADDR:
-    case OP_SUB_INT_2ADDR:
-    case OP_MUL_INT_2ADDR:
-    case OP_AND_INT_2ADDR:
-    case OP_OR_INT_2ADDR:
-    case OP_XOR_INT_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD; //use then define
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_DIV_INT_2ADDR:
-    case OP_REM_INT_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 3;
-        infoArray[1].accessType = REGACCESS_UD; //use then define
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1; //for v1 is vA
-        if(inst_op == OP_REM_INT_2ADDR)
-            infoArray[1].allocConstraints[PhysicalReg_EDX].count = 1;//vA
-        else
-            infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;//vA
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_SHL_INT_2ADDR:
-    case OP_SHR_INT_2ADDR:
-    case OP_USHR_INT_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD; //use then define
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[0].allocConstraints[PhysicalReg_ECX].count = 1; //v2
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_ECX);
-        }
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_ADD_LONG_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = vA+1;
-        infoArray[3].refCount = 2;
-        infoArray[3].accessType = REGACCESS_UD;
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = v2+1;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_U;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 4;
-        break;
-    case OP_SUB_LONG_2ADDR:
-    case OP_AND_LONG_2ADDR:
-    case OP_OR_LONG_2ADDR:
-    case OP_XOR_LONG_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_MUL_LONG_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        num_regs_per_bytecode = 4;
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = v2+1;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 2;
-        infoArray[2].accessType = REGACCESS_UD;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = vA+1;
-        infoArray[3].refCount = 2;
-        infoArray[3].accessType = REGACCESS_UD;
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_ECX);
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-        }
-        break;
-    case OP_DIV_LONG_2ADDR: //vA used as xmm, then updated as gps
-    case OP_REM_LONG_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        num_regs_per_bytecode = 5;
-        infoArray[0].regNum = vA;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = v2;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = v2+1;
-        infoArray[2].refCount = 1;
-        infoArray[2].accessType = REGACCESS_U;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = vA;
-        infoArray[3].refCount = 1;
-        infoArray[3].accessType = REGACCESS_D;
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = vA+1;
-        infoArray[4].refCount = 1;
-        infoArray[4].accessType = REGACCESS_D;
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        break;
-    case OP_SHL_LONG_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        num_regs_per_bytecode = 2;
-        infoArray[0].regNum = v2; //ss
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_ss;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        break;
-    case OP_SHR_LONG_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        num_regs_per_bytecode = 3;
-        infoArray[0].regNum = v2; //ss
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_ss;
-        infoArray[1].regNum = vA+1;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_U;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = vA;
-        infoArray[2].refCount = 2;
-        infoArray[2].accessType = REGACCESS_UD;
-        infoArray[2].physicalType = LowOpndRegType_xmm;
-        break;
-    case OP_USHR_LONG_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        num_regs_per_bytecode = 2;
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_ss; //ss CHECK
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD;
-        infoArray[1].physicalType = LowOpndRegType_xmm; //sd
-        break;
-    case OP_ADD_FLOAT_2ADDR:
-    case OP_SUB_FLOAT_2ADDR:
-    case OP_MUL_FLOAT_2ADDR:
-    case OP_DIV_FLOAT_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD;
-        infoArray[1].physicalType = LowOpndRegType_ss;
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_ss;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_REM_FLOAT_2ADDR: //load vA as GPR, store from fs
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD;
-        infoArray[1].physicalType = LowOpndRegType_gp; //CHECK
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_ADD_DOUBLE_2ADDR:
-    case OP_SUB_DOUBLE_2ADDR:
-    case OP_MUL_DOUBLE_2ADDR:
-    case OP_DIV_DOUBLE_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_REM_DOUBLE_2ADDR: //load to xmm, store from fs
-        vA = currentMIR->dalvikInsn.vA;
-        v2 = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 2;
-        infoArray[1].accessType = REGACCESS_UD;
-        infoArray[1].physicalType = LowOpndRegType_xmm; //CHECK
-        infoArray[0].regNum = v2;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        num_regs_per_bytecode = 2;
-        break;
-
-    case OP_ADD_INT_LIT16:
-    case OP_RSUB_INT:
-    case OP_MUL_INT_LIT16:
-    case OP_AND_INT_LIT16:
-    case OP_OR_INT_LIT16:
-    case OP_XOR_INT_LIT16:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_DIV_INT_LIT16:
-    case OP_REM_INT_LIT16:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        tmp_s2 = currentMIR->dalvikInsn.vC;
-        if(tmp_s2 == 0) {
-            num_regs_per_bytecode = 0;
-            break;
-        }
-        infoArray[1].regNum = vA; //in edx for rem, in eax
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = vB; //in eax
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        if(inst_op == OP_DIV_INT_LIT16) {
-            int power = isPowerOfTwo(tmp_s2);
-            if(power >= 1) { /* divide by a power of 2 constant */
-                infoArray[1].refCount = 1;
-                break;
-            }
-        }
-        if(tmp_s2 == -1)
-            infoArray[1].refCount = 2;
-        else
-            infoArray[1].refCount = 1;
-        if(inst_op == OP_REM_INT_LIT16)
-            infoArray[1].allocConstraints[PhysicalReg_EDX].count = 1;
-        else
-            infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;
-        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        break;
-    case OP_ADD_INT_LIT8:
-    case OP_RSUB_INT_LIT8:
-    case OP_MUL_INT_LIT8:
-    case OP_AND_INT_LIT8:
-    case OP_OR_INT_LIT8:
-    case OP_XOR_INT_LIT8:
-    case OP_SHL_INT_LIT8:
-    case OP_SHR_INT_LIT8:
-    case OP_USHR_INT_LIT8:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[1].regNum = vA;
-        infoArray[1].refCount = 1;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        break;
-    case OP_DIV_INT_LIT8:
-    case OP_REM_INT_LIT8:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        tmp_s2 = currentMIR->dalvikInsn.vC;
-        if(tmp_s2 == 0) {
-            num_regs_per_bytecode = 0;
-            break;
-        }
-
-        infoArray[1].regNum = vA;
-        infoArray[1].accessType = REGACCESS_D;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[0].regNum = vB;
-        infoArray[0].refCount = 1;
-        infoArray[0].accessType = REGACCESS_U;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        num_regs_per_bytecode = 2;
-        if(inst_op == OP_DIV_INT_LIT8) {
-            int power = isPowerOfTwo(tmp_s2);
-            if(power >= 1) { /* divide by a power of 2 constant */
-                infoArray[1].refCount = 1;
-                break;
-            }
-        }
-
-        if(tmp_s2 == -1)
-            infoArray[1].refCount = 2;
-        else
-            infoArray[1].refCount = 1;
-        if(inst_op == OP_REM_INT_LIT8)
-            infoArray[1].allocConstraints[PhysicalReg_EDX].count = 1;
-        else
-            infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;
-        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        break;
-    case OP_EXECUTE_INLINE: //update glue->retval
-    case OP_EXECUTE_INLINE_RANGE:
-        u4 vC;
-        num = currentMIR->dalvikInsn.vA;
-
-        // get inline method id
-        u2 inlineMethodId;
-        inlineMethodId = currentMIR->dalvikInsn.vB;
-        if(inst_op == OP_EXECUTE_INLINE) {
-            // Note that vC, vD, vE, and vF might have bad values
-            // depending on count. The variable "num" should be
-            // checked before using any of these.
-            vC = currentMIR->dalvikInsn.arg[0];
-            vD = currentMIR->dalvikInsn.arg[1];
-            vE = currentMIR->dalvikInsn.arg[2];
-            vF = currentMIR->dalvikInsn.arg[3];
-        } else {
-            vC = currentMIR->dalvikInsn.vC;
-            vD = vC + 1;
-            vE = vC + 2;
-            vF = vC + 3;
-        }
-        if(num >= 1) {
-            infoArray[0].regNum = vC;
-            infoArray[0].refCount = 1;
-            infoArray[0].accessType = REGACCESS_U;
-            if (inlineMethodId == INLINE_MATH_ABS_DOUBLE) {
-                infoArray[0].physicalType = LowOpndRegType_xmm;
-            }
-            else {
-                infoArray[0].physicalType = LowOpndRegType_gp;
-            }
-        }
-        if(num >= 2) {
-            if (inlineMethodId != INLINE_MATH_ABS_DOUBLE) {
-                infoArray[1].regNum = vD;
-                infoArray[1].refCount = 1;
-                infoArray[1].accessType = REGACCESS_U;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-            }
-            else {
-                num_regs_per_bytecode = 1;
-                break;
-            }
-        }
-        if(num >= 3) {
-            infoArray[2].regNum = vE;
-            infoArray[2].refCount = 1;
-            infoArray[2].accessType = REGACCESS_U;
-            infoArray[2].physicalType = LowOpndRegType_gp;
-        }
-        if(num >= 4) {
-            infoArray[3].regNum = vF;
-            infoArray[3].refCount = 1;
-            infoArray[3].accessType = REGACCESS_U;
-            infoArray[3].physicalType = LowOpndRegType_gp;
-        }
-        if (updateBBConstraints == true)
-        {
-            updateCurrentBBWithConstraints (PhysicalReg_EAX);
-            updateCurrentBBWithConstraints (PhysicalReg_EDX);
-        }
-        num_regs_per_bytecode = num;
-        break;
-#if FIXME
-    case OP_INVOKE_OBJECT_INIT_RANGE:
-        codeSize = 3;
-        num_regs_per_bytecode = 0;
-        break;
-#endif
-    default:
-        ALOGI("JIT_INFO: JIT does not support bytecode 0x%hx when updating VR accesses", currentMIR->dalvikInsn.opcode);
-        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
-        assert(false && "All opcodes should be supported.");
-        break;
-    }
-    return num_regs_per_bytecode;
-}
-
-/**
- * @brief Updates infoArray(TempRegInfo) with temporaries accessed by INVOKE_NO_RANGE type
- * @details Allocates both XMM and gp registers for INVOKE_(VIRTUAL,DIRECT,STATIC,INTERFACE,SUPER)
- * @param infoArray the array to be filled
- * @param startIndex the start index of infoArray
- * @param currentMIR the instruction we are looking at
- * @return j the new index of infoArray
- */
-int updateInvokeNoRange(TempRegInfo* infoArray, int startIndex, const MIR * currentMIR) {
-    int j = startIndex;
-    int count = currentMIR->dalvikInsn.vA;// Max value is 5 (#ofArguments)
-
-    // Use XMM registers to read and store max of 5 arguments
-    infoArray[j].regNum = 22;
-    infoArray[j].refCount = 4; //DUDU Read & Store a pair of VRs. Max refCount is 4, since max #of VR pairs is 2
-    infoArray[j].physicalType = LowOpndRegType_xmm;
-    j++;
-
-    // Use gp registers when 64 bit move is not possible. Upto 5 gp reg may be needed.
-    if(count == 5) {
-        infoArray[j].regNum = 27;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 4) {
-        infoArray[j].regNum = 26;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 3) {
-        infoArray[j].regNum = 25;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 2) {
-        infoArray[j].regNum = 24;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 1) {
-        infoArray[j].regNum = 23;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    return j;
-}
-//! Updates infoArray(TempRegInfo) with temporaries accessed by INVOKE_RANGE
-
-//! LOOP_COUNT is used to indicate a variable is live through a loop
-int updateInvokeRange(TempRegInfo* infoArray, int startIndex, const MIR * currentMIR) {
-    int j = startIndex;
-    int count = currentMIR->dalvikInsn.vA;
-    infoArray[j].regNum = 21;
-    if(count <= 10) {
-        infoArray[j].refCount = 1+count; //DU
-    } else {
-        infoArray[j].refCount = 2+3*LOOP_COUNT;
-    }
-    infoArray[j].physicalType = LowOpndRegType_gp;
-    j++;
-    if(count >= 1 && count <= 10) {
-        infoArray[j].regNum = 22;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 2 && count <= 10) {
-        infoArray[j].regNum = 23;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 3 && count <= 10) {
-        infoArray[j].regNum = 24;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 4 && count <= 10) {
-        infoArray[j].regNum = 25;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 5 && count <= 10) {
-        infoArray[j].regNum = 26;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 6 && count <= 10) {
-        infoArray[j].regNum = 27;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 7 && count <= 10) {
-        infoArray[j].regNum = 28;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 8 && count <= 10) {
-        infoArray[j].regNum = 29;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count >= 9 && count <= 10) {
-        infoArray[j].regNum = 30;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count == 10) {
-        infoArray[j].regNum = 31;
-        infoArray[j].refCount = 2; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    if(count > 10) {
-        //NOTE: inside a loop, LOOP_COUNT can't be 1
-        //      if LOOP_COUNT is 1, it is likely that a logical register is freed inside the loop
-        //         and the next iteration will have incorrect result
-        infoArray[j].regNum = 12;
-        infoArray[j].refCount = 1+3*LOOP_COUNT; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-        infoArray[j].regNum = 13;
-        infoArray[j].refCount = 1+LOOP_COUNT; //DU
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-        infoArray[j].regNum = 14;
-        //MUST be 2, otherwise, transferToState will think its state was in memory
-        infoArray[j].refCount = 2; //DU local
-        infoArray[j].physicalType = LowOpndRegType_gp;
-        j++;
-    }
-    return j;
-}
-
-/* update temporaries used by predicted INVOKE_VIRTUAL & INVOKE_INTERFACE */
-int updateGenPrediction(TempRegInfo* infoArray, bool isInterface) {
-    infoArray[0].regNum = 40;
-    infoArray[0].physicalType = LowOpndRegType_gp;
-    infoArray[1].regNum = 41;
-    infoArray[1].physicalType = LowOpndRegType_gp;
-    infoArray[2].regNum = 32;
-    infoArray[2].refCount = 2;
-    infoArray[2].physicalType = LowOpndRegType_gp;
-
-    if(isInterface) {
-        infoArray[0].refCount = 2+2;
-        infoArray[1].refCount = 3+2-1; //for temp41, -1 for gingerbread
-        infoArray[3].regNum = 33;
-        infoArray[3].refCount = 4+1;
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = PhysicalReg_EAX;
-        infoArray[4].refCount = 5;
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[5].regNum = PhysicalReg_ECX;
-        infoArray[5].refCount = 1+1+2; //used in ArgsDone (twice)
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[6].regNum = 10;
-        infoArray[6].refCount = 2;
-        infoArray[6].physicalType = LowOpndRegType_scratch;
-        infoArray[7].regNum = 9;
-        infoArray[7].refCount = 2;
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        infoArray[8].regNum = 8;
-        infoArray[8].refCount = 2;
-        infoArray[8].physicalType = LowOpndRegType_scratch;
-        infoArray[9].regNum = PhysicalReg_EDX; //space holder
-        infoArray[9].refCount = 1;
-        infoArray[9].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[10].regNum = 43;
-        infoArray[10].refCount = 3;
-        infoArray[10].physicalType = LowOpndRegType_gp;
-        infoArray[11].regNum = 44;
-        infoArray[11].refCount = 3;
-        infoArray[11].physicalType = LowOpndRegType_gp;
-        infoArray[12].regNum = 45;
-        infoArray[12].refCount = 2;
-        infoArray[12].physicalType = LowOpndRegType_gp;
-        infoArray[13].regNum = 7;
-        infoArray[13].refCount = 4;
-        infoArray[13].physicalType = LowOpndRegType_scratch;
-        return 14;
-    } else { //virtual or virtual_quick
-        infoArray[0].refCount = 2+2;
-        infoArray[1].refCount = 3+2-2; //for temp41, -2 for gingerbread
-        infoArray[2].refCount++; //for temp32 gingerbread
-        infoArray[3].regNum = 33;
-        infoArray[3].refCount = 4+1;
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 34;
-        infoArray[4].refCount = 2;
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[5].regNum = PhysicalReg_EAX;
-        infoArray[5].refCount = 2;
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[6].regNum = PhysicalReg_ECX;
-        infoArray[6].refCount = 1+3+2;
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[7].regNum = 10;
-        infoArray[7].refCount = 2;
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        infoArray[8].regNum = PhysicalReg_EDX; //space holder
-        infoArray[8].refCount = 1;
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[9].regNum = 43;
-        infoArray[9].refCount = 3;
-        infoArray[9].physicalType = LowOpndRegType_gp;
-        infoArray[10].regNum = 44;
-        infoArray[10].refCount = 3;
-        infoArray[10].physicalType = LowOpndRegType_gp;
-        infoArray[11].regNum = 7;
-        infoArray[11].refCount = 4;
-        infoArray[11].physicalType = LowOpndRegType_scratch;
-        return 12;
-    }
-}
-
-int updateMarkCard(TempRegInfo* infoArray, int j1/*valReg*/,
-                    int j2/*tgtAddrReg*/, int j3/*scratchReg*/) {
-    infoArray[j3].regNum = 11;
-    infoArray[j3].physicalType = LowOpndRegType_gp;
-    infoArray[j3].refCount = 3;
-    infoArray[j3].is8Bit = true;
-    infoArray[j1].refCount++;
-    infoArray[j2].refCount += 2;
-    infoArray[j3+1].regNum = 6;
-    infoArray[j3+1].physicalType = LowOpndRegType_scratch;
-    infoArray[j3+1].refCount = 2;
-    return j3+2;
-}
-
-int updateMarkCard_notNull(TempRegInfo* infoArray,
-                           int j2/*tgtAddrReg*/, int j3/*scratchReg*/) {
-    infoArray[j3].regNum = 11;
-    infoArray[j3].physicalType = LowOpndRegType_gp;
-    infoArray[j3].refCount = 3;
-    infoArray[j3].is8Bit = true;
-    infoArray[j2].refCount += 2;
-    infoArray[j3+1].regNum = 2;
-    infoArray[j3+1].refCount = 2; //DU
-    infoArray[j3+1].physicalType = LowOpndRegType_scratch;
-    return j3+2;
-}
-
-int iget_obj_inst = -1;
-//! This function updates infoArray with temporaries accessed when lowering the bytecode
-
-//! returns the number of temporaries
-int getTempRegInfo(TempRegInfo* infoArray, const MIR * currentMIR) { //returns an array of TempRegInfo
-    int k;
-    int numTmps;
-    for(k = 0; k < MAX_TEMP_REG_PER_BYTECODE; k++) {
-        infoArray[k].linkageToVR = -1;
-        infoArray[k].versionNum = 0;
-        infoArray[k].shareWithVR = true;
-        infoArray[k].is8Bit = false;
-    }
-    u2 length, num, tmp;
-    int vA, vB, v1, v2;
-    Opcode inst_op = currentMIR->dalvikInsn.opcode;
-    s2 tmp_s2;
-    int tmpvalue, isConst;
-
-    /* A bytecode with the MIR_INLINED op will be treated as
-     * no-op during codegen */
-    if (currentMIR->OptimizationFlags & MIR_INLINED)
-        return 0; // No temporaries accessed
-
-    // Check if we need to handle an extended MIR
-    if (currentMIR->dalvikInsn.opcode >= static_cast<Opcode> (kMirOpFirst)) {
-        switch (static_cast<ExtendedMIROpcode>(currentMIR->dalvikInsn.opcode)) {
-            case kMirOpPhi:
-                return 0;
-            case kMirOpRegisterize:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 2; //UD
-
-                //Decide the type depending on the register class
-                switch (static_cast<RegisterClass> (currentMIR->dalvikInsn.vB))
-                {
-                    case kCoreReg:
-                        infoArray[0].physicalType = LowOpndRegType_gp;
-                        break;
-                    case kSFPReg:
-                    case kDFPReg:
-                        //Temps don't have concept of SS type so the physical type of both
-                        //Single FP and Double FP must be xmm.
-                        infoArray[0].physicalType = LowOpndRegType_xmm;
-                        break;
-                    default:
-                        ALOGI("JIT_INFO: kMirOpRegisterize does not support regClass %d", currentMIR->dalvikInsn.vB);
-                        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
-                        break;
-                }
-                return 1;
-            default:
-                ALOGI("JIT_INFO: Extended MIR not supported in getTempRegInfo");
-                SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
-                return -1;
-        }
-    }
-
-    switch (inst_op) {
-    case OP_NOP:
-        return 0;
-    case OP_MOVE:
-    case OP_MOVE_OBJECT:
-    case OP_MOVE_FROM16:
-    case OP_MOVE_OBJECT_FROM16:
-    case OP_MOVE_16:
-    case OP_MOVE_OBJECT_16:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        return 1;
-    case OP_MOVE_WIDE:
-    case OP_MOVE_WIDE_FROM16:
-    case OP_MOVE_WIDE_16:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        return 1;
-    case OP_MOVE_RESULT:
-    case OP_MOVE_RESULT_OBJECT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 1;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-        return 2;
-    case OP_MOVE_RESULT_WIDE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = 1;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-        return 2;
-    case OP_MOVE_EXCEPTION:
-        infoArray[0].regNum = 2;
-        infoArray[0].refCount = 3; //DUU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 3;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 1;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_scratch;
-        return 3;
-
-    case OP_CONST_4:
-    case OP_CONST_16:
-    case OP_CONST:
-    case OP_CONST_HIGH16:
-    case OP_CONST_WIDE_16:
-    case OP_CONST_WIDE_32:
-    case OP_CONST_WIDE:
-    case OP_CONST_WIDE_HIGH16:
-        return 0;
-    case OP_CONST_STRING: //hardcode %eax
-    case OP_CONST_STRING_JUMBO:
-        infoArray[0].regNum = 3;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 1;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-        infoArray[2].regNum = 2;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_scratch;
-        infoArray[3].regNum = PhysicalReg_EAX;
-        infoArray[3].refCount = 4;
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 4;
-    case OP_CONST_CLASS:
-        infoArray[0].regNum = 3;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 1;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-        infoArray[2].regNum = 2;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_scratch;
-        infoArray[3].regNum = PhysicalReg_EAX;
-        infoArray[3].refCount = 4;
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 4;
-
-    case OP_MONITOR_ENTER:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 5; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 3;
-        infoArray[1].refCount = 7; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 1;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_scratch;
-        infoArray[3].regNum = 2;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_scratch;
-        infoArray[4].regNum = PhysicalReg_EDX;
-        infoArray[4].refCount = 2;
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[5].regNum = 2;
-        infoArray[5].refCount = 4; //DU
-        infoArray[5].physicalType = LowOpndRegType_gp;
-        infoArray[6].regNum = 4;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp;
-        infoArray[7].regNum = 5;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_gp;
-        infoArray[8].regNum = PhysicalReg_EAX;
-        infoArray[8].refCount = 2; //DU
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-       return 9;
-
-    case OP_MONITOR_EXIT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = PhysicalReg_EAX;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[2].regNum = 1;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_scratch;
-        infoArray[3].regNum = PhysicalReg_EDX;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[4].regNum = 2;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        infoArray[5].regNum = 3;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-        infoArray[6].regNum = 3;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp;
-        infoArray[7].regNum = 4;
-        infoArray[7].refCount = 3; //DU
-        infoArray[7].physicalType = LowOpndRegType_gp;
-        infoArray[8].regNum = 5;
-        infoArray[8].refCount = 4; //DU
-        infoArray[8].physicalType = LowOpndRegType_gp;
-        infoArray[9].regNum = 6;
-        infoArray[9].refCount = 3; //DU
-        infoArray[9].physicalType = LowOpndRegType_gp;
-        infoArray[10].regNum = 7;
-        infoArray[10].refCount = 3; //DU
-        infoArray[10].physicalType = LowOpndRegType_gp;
-        return 11;
-
-    case OP_CHECK_CAST:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 4;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 6;
-        infoArray[2].refCount = 3; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-
-        infoArray[3].regNum = 1;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_scratch;
-        infoArray[4].regNum = 2;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-
-        infoArray[5].regNum = PhysicalReg_EAX;
-        /* %eax has 3 live ranges
-           1> 5 accesses: to resolve the class object
-           2> call dvmInstanceofNonTrivial to define %eax, then use it once
-           3> move exception object to %eax, then jump to throw_exception
-           if WITH_JIT is true, the first live range has 6 accesses
-        */
-        infoArray[5].refCount = 6;
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[6].regNum = PhysicalReg_EDX;
-        infoArray[6].refCount = 2; //export_pc
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[7].regNum = PhysicalReg_ECX;
-        infoArray[7].refCount = 1;
-        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[8].regNum = 3;
-        infoArray[8].refCount = 2; //DU
-        infoArray[8].physicalType = LowOpndRegType_scratch;
-        return 9;
-    case OP_INSTANCE_OF:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 3;
-        infoArray[1].refCount = 4; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 4;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 6;
-        infoArray[3].refCount = 3; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-
-        infoArray[4].regNum = 1;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        infoArray[5].regNum = 2;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-
-        infoArray[6].regNum = PhysicalReg_EAX;
-        infoArray[6].refCount = 6;
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[7].regNum = 3;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        infoArray[8].regNum = PhysicalReg_EDX;
-        infoArray[8].refCount = 2; //export_pc for class_resolve
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 9;
-
-    case OP_ARRAY_LENGTH:
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[1].linkageToVR = vA;
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 3;
-    case OP_NEW_INSTANCE:
-        infoArray[0].regNum = PhysicalReg_EAX;
-        //6: class object
-        //3: defined by C function, used twice
-        infoArray[0].refCount = 6; //next version has 3 references
-        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[1].regNum = PhysicalReg_ECX; //before common_throw_message
-        infoArray[1].refCount = 1;
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 5;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[3].is8Bit = true;
-        infoArray[4].regNum = 6;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-
-        infoArray[5].regNum = 1;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-        infoArray[6].regNum = 2;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_scratch;
-        infoArray[7].regNum = 3;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-
-        infoArray[8].regNum = PhysicalReg_EDX; //before common_throw_message
-        infoArray[8].refCount = 2;
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[9].regNum = 4;
-        infoArray[9].refCount = 2; //DU
-        infoArray[9].physicalType = LowOpndRegType_scratch;
-        return 10;
-
-    case OP_NEW_ARRAY:
-        infoArray[0].regNum = PhysicalReg_EAX;
-        //4: class object
-        //3: defined by C function, used twice
-        infoArray[0].refCount = 4; //next version has 3 references
-        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 5;
-        infoArray[3].refCount = 3; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-
-        infoArray[4].regNum = 1;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        infoArray[5].regNum = 2;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-        infoArray[6].regNum = 3;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_scratch;
-        infoArray[7].regNum = 4;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        return 8;
-
-    case OP_FILLED_NEW_ARRAY:
-        length = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = PhysicalReg_EAX;
-        //4: class object
-        //3: defined by C function, used twice (array object)
-        //length: access array object to update the content
-        infoArray[0].refCount = 4; //next version has 5+length references
-        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 5;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 6;
-        infoArray[4].refCount = 8; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[4].is8Bit = true;
-
-        if(length >= 1) {
-            infoArray[5].regNum = 7;
-            infoArray[5].refCount = 2; //DU
-            infoArray[5].physicalType = LowOpndRegType_gp;
-        }
-        if(length >= 2) {
-            infoArray[6].regNum = 8;
-            infoArray[6].refCount = 2; //DU
-            infoArray[6].physicalType = LowOpndRegType_gp;
-        }
-        if(length >= 3) {
-            infoArray[7].regNum = 9;
-            infoArray[7].refCount = 2; //DU
-            infoArray[7].physicalType = LowOpndRegType_gp;
-        }
-        if(length >= 4) {
-            infoArray[8].regNum = 10;
-            infoArray[8].refCount = 2; //DU
-            infoArray[8].physicalType = LowOpndRegType_gp;
-        }
-        if(length >= 5) {
-            infoArray[9].regNum = 11;
-            infoArray[9].refCount = 2; //DU
-            infoArray[9].physicalType = LowOpndRegType_gp;
-        }
-        infoArray[5+length].regNum = 1;
-        infoArray[5+length].refCount = 2; //DU
-        infoArray[5+length].physicalType = LowOpndRegType_scratch;
-        infoArray[6+length].regNum = 2;
-        infoArray[6+length].refCount = 4; //DU
-        infoArray[6+length].physicalType = LowOpndRegType_scratch;
-        infoArray[7+length].regNum = 3;
-        infoArray[7+length].refCount = 2; //DU
-        infoArray[7+length].physicalType = LowOpndRegType_scratch;
-        infoArray[8+length].regNum = 4;
-        infoArray[8+length].refCount = 5; //DU
-        infoArray[8+length].physicalType = LowOpndRegType_scratch;
-        return 9+length;
-
-    case OP_FILLED_NEW_ARRAY_RANGE:
-        length = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = PhysicalReg_EAX;
-        //4: class object
-        //3: defined by C function, used twice (array object)
-        //if length is 0, no access to array object
-        //else, used inside a loop
-        infoArray[0].refCount = 4; //next version: 5+(length >= 1 ? LOOP_COUNT : 0)
-        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 5;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 6;
-        infoArray[4].refCount = 8; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[4].is8Bit = true;
-
-        infoArray[5].regNum = 1;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-        infoArray[6].regNum = 2;
-        infoArray[6].refCount = 4; //DU
-        infoArray[6].physicalType = LowOpndRegType_scratch;
-        infoArray[7].regNum = 3;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-
-        infoArray[8].regNum = 7;
-        infoArray[8].refCount = 3*(length >= 1 ? LOOP_COUNT : 0);
-        infoArray[8].physicalType = LowOpndRegType_gp;
-        infoArray[9].regNum = 8;
-        infoArray[9].refCount = 3*(length >= 1 ? LOOP_COUNT : 0);
-        infoArray[9].physicalType = LowOpndRegType_gp;
-        infoArray[10].regNum = 9;
-        infoArray[10].refCount = 2*(length >= 1 ? LOOP_COUNT : 0);
-        infoArray[10].physicalType = LowOpndRegType_gp;
-        infoArray[11].regNum = 10;
-        infoArray[11].refCount = 2*(length >= 1 ? LOOP_COUNT : 0);
-        infoArray[11].physicalType = LowOpndRegType_gp;
-        infoArray[12].regNum = 4;
-        infoArray[12].refCount = 5; //DU
-        infoArray[12].physicalType = LowOpndRegType_scratch;
-        return 13;
-
-    case OP_FILL_ARRAY_DATA:
-        infoArray[0].regNum = PhysicalReg_EAX;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
-#if 0//def HARDREG_OPT
-        infoArray[1].refCount = 3; //next version has refCount of 2
-#else
-        infoArray[1].refCount = 5;
-#endif
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[2].regNum =1;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-
-        infoArray[3].regNum = 1;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_scratch;
-        infoArray[4].regNum = 2;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        return 5;
-
-    case OP_THROW:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[2].regNum = 1;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_scratch;
-        infoArray[3].regNum = 2;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_scratch;
-        return 4;
-    case OP_THROW_VERIFICATION_ERROR:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = PhysicalReg_EDX; //export_pc
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[2].regNum = 1;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_scratch;
-        infoArray[3].regNum = 2;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_scratch;
-        return 4;
-
-    case OP_GOTO: //called function common_periodicChecks4
-#if defined(ENABLE_TRACING)
-        tt = INST_AA(inst);
-        tmp_s2 = (s2)((s2)tt << 8) >> 8;
-        if(tmp_s2 < 0) {
-            infoArray[0].regNum = PhysicalReg_EDX;
-            infoArray[0].refCount = 2;
-            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            return 1;
-        }
-#endif
-        return 0;
-    case OP_GOTO_16:
-#if defined(ENABLE_TRACING)
-        tmp_s2 = (s2)FETCH(1);
-        if(tmp_s2 < 0) {
-            infoArray[0].regNum = PhysicalReg_EDX;
-            infoArray[0].refCount = 2;
-            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            return 1;
-        }
-#endif
-        return 0;
-    case OP_GOTO_32:
-#if defined(ENABLE_TRACING)
-        tmp_u4 = (u4)FETCH(1);
-        tmp_u4 |= (u4)FETCH(2) << 16;
-        if(((s4)tmp_u4) < 0) {
-            infoArray[0].regNum = PhysicalReg_EDX;
-            infoArray[0].refCount = 2;
-            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            return 1;
-        }
-#endif
-        return 0;
-    case OP_IF_EQ:
-    case OP_IF_NE:
-    case OP_IF_LT:
-    case OP_IF_GE:
-    case OP_IF_GT:
-    case OP_IF_LE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-#if defined(ENABLE_TRACING)
-        tmp_s2 = (s2)FETCH(1);
-        if(tmp_s2 < 0) {
-            infoArray[1].regNum = PhysicalReg_EDX;
-            infoArray[1].refCount = 2;
-            infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            return 2;
-        }
-#endif
-        return 1;
-    case OP_IF_EQZ: //called function common_periodicChecks4
-    case OP_IF_NEZ:
-    case OP_IF_LTZ:
-    case OP_IF_GEZ:
-    case OP_IF_GTZ:
-    case OP_IF_LEZ:
-#if defined(ENABLE_TRACING)
-        tmp_s2 = (s2)FETCH(1);
-        if(tmp_s2 < 0) {
-            infoArray[0].regNum = PhysicalReg_EDX;
-            infoArray[0].refCount = 2;
-            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            return 1;
-        }
-#endif
-        return 0;
-    case OP_PACKED_SWITCH: //jump common_backwardBranch, which calls common_periodicChecks_entry, then jump_reg %eax
-    case OP_SPARSE_SWITCH: //%edx, %eax
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = PhysicalReg_EDX;
-        infoArray[1].refCount = 6;
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[2].regNum = PhysicalReg_EAX; //return by dvm helper
-        infoArray[2].refCount = 2+1; //2 uses
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[3].regNum = 1;
-        infoArray[3].refCount = 2;
-        infoArray[3].physicalType = LowOpndRegType_scratch;
-        infoArray[4].regNum = 2;
-        infoArray[4].refCount = 2;
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        return 5;
-
-    case OP_AGET:
-    case OP_AGET_OBJECT:
-    case OP_AGET_BOOLEAN:
-    case OP_AGET_BYTE:
-    case OP_AGET_CHAR:
-    case OP_AGET_SHORT:
-#ifdef INC_NCG_O0
-        if(gDvm.helper_switch[7]) {
-            infoArray[0].regNum = PhysicalReg_EBX;
-            infoArray[0].refCount = 2;
-            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            infoArray[1].regNum = PhysicalReg_ECX;
-            infoArray[1].refCount = 2;
-            infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            infoArray[2].regNum = PhysicalReg_EDX;
-            infoArray[2].refCount = 2;
-            infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            return 3;
-        }
-#endif
-        vA = currentMIR->dalvikInsn.vA;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 3; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 4;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[3].linkageToVR = vA;
-
-        infoArray[4].regNum = PhysicalReg_EDX;
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        if(inst_op == OP_AGET_BYTE || inst_op == OP_AGET_BOOLEAN)
-            infoArray[3].is8Bit = true;
-        infoArray[4].refCount = 2;
-        return 5;
-#else
-        infoArray[4].refCount = 4;
-        // Use temp 5 to store address of heap access
-        infoArray[5].regNum = 5;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_gp;
-        // Return value from calling loadFromShadowHeap will be in EAX
-        infoArray[6].regNum = PhysicalReg_EAX;
-        infoArray[6].refCount = 4;
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        // Scratch for calling loadFromShadowHeap
-        infoArray[7].regNum = 1;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        infoArray[8].regNum = PhysicalReg_ECX;
-        infoArray[8].refCount = 2;
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 9;
-#endif
-    case OP_AGET_WIDE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 3; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 1;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_xmm;
-
-        infoArray[4].regNum = PhysicalReg_EDX;
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[4].refCount = 2;
-        return 5;
-#else
-        infoArray[4].refCount = 4;
-        infoArray[5].regNum = PhysicalReg_XMM7;
-        infoArray[5].refCount = 1; //U
-        infoArray[5].physicalType = LowOpndRegType_xmm | LowOpndRegType_hard;
-        // Use temp 5 to store address of heap access
-        infoArray[6].regNum = 5;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp;
-        // Scratch for calling loadFromShadowHeap
-        infoArray[7].regNum = 1;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        infoArray[8].regNum = PhysicalReg_EAX;
-        infoArray[8].refCount = 2;
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[9].regNum = PhysicalReg_ECX;
-        infoArray[9].refCount = 2;
-        infoArray[9].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 10;
-#endif
-    case OP_APUT_BYTE:
-        for(k = 0; k < MAX_TEMP_REG_PER_BYTECODE; k++)
-            infoArray[k].shareWithVR = true; //false;
-        // Intentional fall through
-    case OP_APUT:
-    case OP_APUT_BOOLEAN:
-    case OP_APUT_CHAR:
-    case OP_APUT_SHORT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 3; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 4;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-
-        infoArray[4].regNum = PhysicalReg_EDX;
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        if(inst_op == OP_APUT_BYTE || inst_op == OP_APUT_BOOLEAN)
-            infoArray[3].is8Bit = true;
-        infoArray[4].refCount = 2;
-        return 5;
-#else
-        infoArray[4].refCount = 4;
-        // Use temp 5 to store address of heap access
-        infoArray[5].regNum = 5;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_gp;
-        // Scratch for calling storeToShadowHeap
-        infoArray[6].regNum = 1;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_scratch;
-        infoArray[7].regNum = PhysicalReg_ECX;
-        infoArray[7].refCount = 2;
-        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[8].regNum = PhysicalReg_EAX;
-        infoArray[8].refCount = 2;
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 9;
-#endif
-    case OP_APUT_WIDE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 3; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 1;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_xmm;
-
-        infoArray[4].regNum = PhysicalReg_EDX;
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[4].refCount = 2;
-        return 5;
-#else
-        infoArray[4].refCount = 4;
-        // Use temp 4 to store address of heap access
-        infoArray[5].regNum = 4;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_gp;
-        // Scratch for calling storeToShadowHeap
-        infoArray[6].regNum = 1;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_scratch;
-        infoArray[7].regNum = PhysicalReg_EAX;
-        infoArray[7].refCount = 2;
-        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[8].regNum = PhysicalReg_ECX;
-        infoArray[8].refCount = 2;
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 9;
-#endif
-    case OP_APUT_OBJECT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 5+1; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2; //live through function call dvmCanPut
-        infoArray[1].refCount = 3+1; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 4;
-        infoArray[3].refCount = 4+1; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 5;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[5].regNum = 6;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_gp;
-
-        infoArray[6].regNum = PhysicalReg_EDX;
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[7].regNum = PhysicalReg_EAX;
-        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[6].refCount = 2; //DU
-        infoArray[7].refCount = 2; //DU
-#else
-        infoArray[6].refCount = 4+2; //DU
-        infoArray[7].refCount = 4+2;
-#endif
-        infoArray[8].regNum = 1;
-        infoArray[8].refCount = 2; //DU
-        infoArray[8].physicalType = LowOpndRegType_scratch;
-        infoArray[0].shareWithVR = false;
-
-#ifndef WITH_SELF_VERIFICATION
-        return updateMarkCard_notNull(infoArray,
-                                      0/*index for tgtAddrReg*/, 9);
-#else
-        // Use temp 7 to store address of heap access
-        infoArray[9].regNum = 7;
-        infoArray[9].refCount = 4; //DU
-        infoArray[9].physicalType = LowOpndRegType_gp;
-        // Scratch for calling storeToShadowHeap
-        infoArray[10].regNum = 1;
-        infoArray[10].refCount = 6; //DU
-        infoArray[10].physicalType = LowOpndRegType_scratch;
-        infoArray[11].regNum = PhysicalReg_ECX;
-        infoArray[11].refCount = 2+2;
-        infoArray[11].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return updateMarkCard_notNull(infoArray,
-                                      0/*index for tgtAddrReg*/, 12);
-#endif
-
-    case OP_IGET:
-    case OP_IGET_OBJECT:
-    case OP_IGET_VOLATILE:
-    case OP_IGET_OBJECT_VOLATILE:
-    case OP_IGET_BOOLEAN:
-    case OP_IGET_BYTE:
-    case OP_IGET_CHAR:
-    case OP_IGET_SHORT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_scratch;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[3].regNum = PhysicalReg_EAX;
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[2].refCount = 2; //DU
-        infoArray[3].refCount = 3; //DU
-#else
-        infoArray[2].refCount = 4; //DU
-        // Return value from calling loadFromShadowHeap will be in EAX
-        infoArray[3].refCount = 6; //DU
-#endif
-        infoArray[4].regNum = 3;
-        infoArray[4].refCount = 3; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[5].regNum = 7;
-#ifdef DEBUG_IGET_OBJ
-        //add hack for a specific instance (iget_obj_inst) of IGET_OBJECT within a method
-        if(inst_op == OP_IGET_OBJECT && !strncmp(currentMethod->clazz->descriptor, "Lspec/benchmarks/_228_jack/Parse", 32) &&
-           !strncmp(currentMethod->name, "buildPhase3", 11))
-        {
-#if 0
-          if(iget_obj_inst == 12) {
-            ALOGD("increase count for instance %d of %s %s", iget_obj_inst, currentMethod->clazz->descriptor, currentMethod->name);
-            infoArray[5].refCount = 4; //DU
-          }
-          else
-#endif
-            infoArray[5].refCount = 3;
-          iget_obj_inst++;
-        }
-        else
-          infoArray[5].refCount = 3;
-#else
-        infoArray[5].refCount = 3; //DU
-#endif
-        infoArray[5].physicalType = LowOpndRegType_gp;
-        infoArray[6].regNum = 8;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp;
-        infoArray[7].regNum = 9;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_gp;
-#ifndef WITH_SELF_VERIFICATION
-        return 8;
-#else
-        // Use temp 10 to store address of heap access
-        infoArray[8].regNum = 10;
-        infoArray[8].refCount = 2; //DU
-        infoArray[8].physicalType = LowOpndRegType_gp;
-        infoArray[9].regNum = 5;
-        infoArray[9].refCount = 2; //DU
-        infoArray[9].physicalType = LowOpndRegType_scratch;
-        infoArray[10].regNum = PhysicalReg_ECX;
-        infoArray[10].refCount = 2;
-        infoArray[10].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 11;
-#endif
-    case OP_IPUT:
-    case OP_IPUT_OBJECT:
-    case OP_IPUT_VOLATILE:
-    case OP_IPUT_OBJECT_VOLATILE:
-    case OP_IPUT_BOOLEAN:
-    case OP_IPUT_BYTE:
-    case OP_IPUT_CHAR:
-    case OP_IPUT_SHORT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_scratch;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[3].regNum = PhysicalReg_EAX;
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[2].refCount = 2; //DU
-        infoArray[3].refCount = 3; //DU
-#else
-        infoArray[2].refCount = 4; //DU
-        infoArray[3].refCount = 5; //DU
-#endif
-        infoArray[4].regNum = 3;
-        infoArray[4].refCount = 3; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[5].regNum = 7;
-        infoArray[5].refCount = 3; //DU
-        infoArray[5].physicalType = LowOpndRegType_gp;
-        infoArray[6].regNum = 8;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp;
-        infoArray[7].regNum = 9;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_gp;
-#ifndef WITH_SELF_VERIFICATION
-        if(inst_op == OP_IPUT_OBJECT || inst_op == OP_IPUT_OBJECT_VOLATILE) {
-            infoArray[5].shareWithVR = false;
-            return updateMarkCard(infoArray, 7/*index for valReg*/,
-                                  5/*index for tgtAddrReg*/, 8);
-        }
-        return 8;
-#else
-        // Use temp 10 to store address of heap access
-        infoArray[8].regNum = 10;
-        infoArray[8].refCount = 2; //DU
-        infoArray[8].physicalType = LowOpndRegType_gp;
-        infoArray[9].regNum = 5;
-        infoArray[9].refCount = 2; //DU
-        infoArray[9].physicalType = LowOpndRegType_scratch;
-        infoArray[10].regNum = PhysicalReg_ECX;
-        infoArray[10].refCount = 2;
-        infoArray[10].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        if(inst_op == OP_IPUT_OBJECT || inst_op == OP_IPUT_OBJECT_VOLATILE) {
-            infoArray[5].shareWithVR = false;
-            return updateMarkCard(infoArray, 7/*index for valReg*/,
-                                  5/*index for tgtAddrReg*/, 11);
-        }
-        return 11;
-#endif
-
-    case OP_IGET_WIDE:
-    case OP_IGET_WIDE_VOLATILE:
-    case OP_IPUT_WIDE:
-    case OP_IPUT_WIDE_VOLATILE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_scratch;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[3].regNum = PhysicalReg_EAX;
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[2].refCount = 2; //DU
-        infoArray[3].refCount = 3; //DU
-#else
-        infoArray[2].refCount = 4;
-        infoArray[3].refCount = 5;
-#endif
-        infoArray[4].regNum = 3;
-        infoArray[4].refCount = 3; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[5].regNum = 7;
-        infoArray[5].refCount = 3; //DU
-        infoArray[5].physicalType = LowOpndRegType_gp;
-        infoArray[6].regNum = 8;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp;
-        infoArray[7].regNum = 1;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_xmm;
-#ifndef WITH_SELF_VERIFICATION
-        if(inst_op == OP_IPUT_WIDE_VOLATILE || inst_op == OP_IGET_WIDE_VOLATILE) {
-            infoArray[8].regNum = 3;
-            infoArray[8].refCount = 2; //DU
-            infoArray[8].physicalType = LowOpndRegType_scratch;
-            infoArray[9].regNum = 9;
-            infoArray[9].refCount = 2; //DU
-            infoArray[9].physicalType = LowOpndRegType_gp;
-            return 10;
-        }
-        return 8;
-
-#else
-        infoArray[8].regNum = PhysicalReg_XMM7;
-        infoArray[8].refCount = 1; //U
-        infoArray[8].physicalType = LowOpndRegType_xmm | LowOpndRegType_hard;
-        // Use temp 10 to store address of heap access
-        infoArray[9].regNum = 10;
-        infoArray[9].refCount = 4; //DU
-        infoArray[9].physicalType = LowOpndRegType_gp;
-        infoArray[10].regNum = 5;
-        infoArray[10].refCount = 4; //DU
-        infoArray[10].physicalType = LowOpndRegType_scratch;
-        infoArray[11].regNum = PhysicalReg_ECX;
-        infoArray[11].refCount = 2;
-        infoArray[11].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        if(inst_op == OP_IPUT_WIDE_VOLATILE || inst_op == OP_IGET_WIDE_VOLATILE) {
-            infoArray[12].regNum = 3;
-            infoArray[12].refCount = 4; //DU
-            infoArray[12].physicalType = LowOpndRegType_scratch;
-            infoArray[13].regNum = 9;
-            infoArray[13].refCount = 2; //DU
-            infoArray[13].physicalType = LowOpndRegType_gp;
-            return 14;
-        }
-        return 12;
-#endif
-    case OP_SGET:
-    case OP_SGET_OBJECT:
-    case OP_SGET_VOLATILE:
-    case OP_SGET_OBJECT_VOLATILE:
-    case OP_SGET_BOOLEAN:
-    case OP_SGET_BYTE:
-    case OP_SGET_CHAR:
-    case OP_SGET_SHORT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_scratch;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-
-        infoArray[2].regNum = PhysicalReg_EAX;
-#if defined(WITH_SELF_VERIFICATION)
-        // Return value from calling loadFromShadowHeap will be in EAX
-        infoArray[2].refCount = 6;
-#else
-        infoArray[2].refCount = 2;
-#endif
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[3].regNum = 3;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 7;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-
-        infoArray[5].regNum = PhysicalReg_EDX;
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[5].refCount = 2; //DU
-        return 6;
-#else
-        infoArray[5].refCount = 4; //DU
-        // Use temp 8 to store address of heap access
-        infoArray[6].regNum = 8;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp;
-        // Scratch for calling loadFromShadowHeap
-        infoArray[7].regNum = 5;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        infoArray[8].regNum = PhysicalReg_ECX;
-        infoArray[8].refCount = 2;
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 9;
-#endif
-
-    case OP_SPUT:
-    case OP_SPUT_OBJECT:
-    case OP_SPUT_VOLATILE:
-    case OP_SPUT_OBJECT_VOLATILE:
-    case OP_SPUT_BOOLEAN:
-    case OP_SPUT_BYTE:
-    case OP_SPUT_CHAR:
-    case OP_SPUT_SHORT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_scratch;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-
-        infoArray[2].regNum = PhysicalReg_EAX;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[2].refCount = 2+1; //access clazz of the field
-#else
-        infoArray[2].refCount = 4+2;
-#endif
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[3].regNum = 3;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 7;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-
-        infoArray[5].regNum = PhysicalReg_EDX;
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[5].refCount = 2; //DU
-        if(inst_op == OP_SPUT_OBJECT || inst_op == OP_SPUT_OBJECT_VOLATILE) {
-            infoArray[2].shareWithVR = false;
-            infoArray[6].regNum = 12;
-            infoArray[6].refCount = 1; //1 def, 2 uses in updateMarkCard
-            infoArray[6].physicalType = LowOpndRegType_gp;
-            return updateMarkCard(infoArray, 4/*index for valReg*/,
-                                  6/*index for tgtAddrReg */, 7);
-        }
-        return 6;
-#else
-        infoArray[5].refCount = 4; //DU
-        // Use temp 8 to store address of heap access
-        infoArray[6].regNum = 8;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp;
-        // Scratch for calling storeToShadowHeap
-        infoArray[7].regNum = 5;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        infoArray[8].regNum = PhysicalReg_ECX;
-        infoArray[8].refCount = 2;
-        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        if(inst_op == OP_SPUT_OBJECT || inst_op == OP_SPUT_OBJECT_VOLATILE) {
-            infoArray[2].shareWithVR = false;
-            infoArray[9].regNum = 12;
-            infoArray[9].refCount = 3; //1 def, 2 uses in updateMarkCard
-            infoArray[9].physicalType = LowOpndRegType_gp;
-            return updateMarkCard(infoArray, 4/*index for valReg*/,
-                                  6/*index for tgtAddrReg */, 10);
-        }
-        return 9;
-#endif
-    case OP_SGET_WIDE:
-    case OP_SGET_WIDE_VOLATILE:
-    case OP_SPUT_WIDE:
-    case OP_SPUT_WIDE_VOLATILE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_scratch;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_scratch;
-
-        infoArray[2].regNum = PhysicalReg_EAX;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[2].refCount = 2;
-#else
-        infoArray[2].refCount = 4;
-#endif
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[3].regNum = 3;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 1;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_xmm;
-
-        infoArray[5].regNum = PhysicalReg_EDX;
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[5].refCount = 2; //DU
-        if(inst_op == OP_SPUT_WIDE_VOLATILE || inst_op == OP_SGET_WIDE_VOLATILE) {
-            infoArray[6].regNum = 3;
-            infoArray[6].refCount = 2; //DU
-            infoArray[6].physicalType = LowOpndRegType_scratch;
-            infoArray[7].regNum = 9;
-            infoArray[7].refCount = 2; //DU
-            infoArray[7].physicalType = LowOpndRegType_gp;
-            return 8;
-        }
-        return 6;
-#else
-        infoArray[5].refCount = 4; //DU
-        // use temp 4 to store address of shadow heap access
-        infoArray[6].regNum = 4;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp;
-        // Scratch for calling storeToShadowHeap
-        infoArray[7].regNum = 5;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        infoArray[8].regNum = PhysicalReg_XMM7;
-        infoArray[8].refCount = 1; //U
-        infoArray[8].physicalType = LowOpndRegType_xmm | LowOpndRegType_hard;
-        infoArray[9].regNum = PhysicalReg_ECX;
-        infoArray[9].refCount = 2;
-        infoArray[9].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        if(inst_op == OP_SPUT_WIDE_VOLATILE || inst_op == OP_SGET_WIDE_VOLATILE) {
-            infoArray[10].regNum = 3;
-            infoArray[10].refCount = 2; //DU
-            infoArray[10].physicalType = LowOpndRegType_scratch;
-            infoArray[11].regNum = 9;
-            infoArray[11].refCount = 2; //DU
-            infoArray[11].physicalType = LowOpndRegType_gp;
-            return 12;
-        }
-        return 10;
-#endif
-
-
-    case OP_IGET_QUICK:
-    case OP_IGET_OBJECT_QUICK:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[2].refCount = 2; //DU
-        return 3;
-#else
-        infoArray[2].refCount = 4; //DU
-        // Use temp 3 to store address of heap access
-        infoArray[3].regNum = 3;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        // Return value from calling loadFromShadowHeap will be in EAX
-        infoArray[4].regNum = PhysicalReg_EAX;
-        infoArray[4].refCount = 4;
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[5].regNum = PhysicalReg_ECX;
-        infoArray[5].refCount = 2;
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        // Scratch for calling loadFromShadowHeap
-        infoArray[6].regNum = 1;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_scratch;
-        return 7;
-#endif
-    case OP_IPUT_QUICK:
-    case OP_IPUT_OBJECT_QUICK:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[2].refCount = 2; //DU
-        if(inst_op == OP_IPUT_OBJECT_QUICK) {
-            infoArray[0].shareWithVR = false;
-            return updateMarkCard(infoArray, 1/*index for valReg*/,
-                                  0/*index for tgtAddrReg*/, 3);
-        }
-        return 3;
-#else
-        infoArray[2].refCount = 4; //DU
-        // Use temp 3 to store address of heap access
-        infoArray[3].regNum = 3;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        // Scratch for calling storeToShadowHeap
-        infoArray[4].regNum = 1;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        infoArray[5].regNum = PhysicalReg_EAX;
-        infoArray[5].refCount = 2;
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[6].regNum = PhysicalReg_ECX;
-        infoArray[6].refCount = 2;
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        if(inst_op == OP_IPUT_OBJECT_QUICK) {
-            infoArray[0].shareWithVR = false;
-            return updateMarkCard(infoArray, 1/*index for valReg*/,
-                                  0/*index for tgtAddrReg*/, 7/*ScratchReg*/);
-        }
-        return 7;
-#endif
-    case OP_IGET_WIDE_QUICK:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 1;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[2].refCount = 2; //DU
-        return 3;
-#else
-        infoArray[2].refCount = 4; //DU
-        // use temp 3 to store address of heap access
-        infoArray[3].regNum = 3;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        // Scratch for calling storeToShadowHeap
-        infoArray[4].regNum = 1;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        infoArray[5].regNum = PhysicalReg_XMM7;
-        infoArray[5].refCount = 1; //U
-        infoArray[5].physicalType = LowOpndRegType_xmm | LowOpndRegType_hard;
-        infoArray[6].regNum = PhysicalReg_EAX;
-        infoArray[6].refCount = 2;
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[7].regNum = PhysicalReg_ECX;
-        infoArray[7].refCount = 2;
-        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 8;
-#endif
-    case OP_IPUT_WIDE_QUICK:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 1;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-#ifndef WITH_SELF_VERIFICATION
-        infoArray[2].refCount = 2; //DU
-        return 3;
-#else
-        infoArray[2].refCount = 4; //DU
-        // use temp 3 to store address of heap access
-        infoArray[3].regNum = 3;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        // Scratch for calling storeToShadowHeap
-        infoArray[4].regNum = 1;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        infoArray[5].regNum = PhysicalReg_EAX;
-        infoArray[5].refCount = 2;
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[6].regNum = PhysicalReg_ECX;
-        infoArray[6].refCount = 2;
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 7;
-#endif
-
-    case OP_RETURN_VOID:
-    case OP_RETURN_VOID_BARRIER:
-        infoArray[0].regNum = PhysicalReg_ECX;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[1].regNum = PhysicalReg_EDX;
-        infoArray[1].refCount = 1; //D
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 2;
-    case OP_RETURN:
-    case OP_RETURN_OBJECT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = PhysicalReg_ECX;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].refCount = 1; //D
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 3;
-    case OP_RETURN_WIDE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = PhysicalReg_ECX;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].refCount = 1; //D
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 3;
-    case OP_INVOKE_VIRTUAL:
-    case OP_INVOKE_VIRTUAL_RANGE:
-#ifdef PREDICTED_CHAINING
-        numTmps = updateGenPrediction(infoArray, false /*not interface*/);
-        infoArray[numTmps].regNum = 5;
-        infoArray[numTmps].refCount = 3; //DU
-        infoArray[numTmps].physicalType = LowOpndRegType_gp;
-        numTmps++;
-        if(inst_op == OP_INVOKE_VIRTUAL)
-            k = updateInvokeNoRange(infoArray, numTmps, currentMIR);
-        else
-            k = updateInvokeRange(infoArray, numTmps, currentMIR);
-        return k;
-#else
-        infoArray[0].regNum = 3;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 7;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 8;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 6;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 5;
-        infoArray[4].refCount = 3; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[5].regNum = PhysicalReg_EDX;
-        infoArray[5].refCount = 2; //2 versions, first version DU is for exception, 2nd version: eip right before jumping to invokeArgsDone
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[6].regNum = PhysicalReg_ECX; //ecx is ued in invokeArgsDone
-        infoArray[6].refCount = 1+1; //used in .invokeArgsDone
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        //when WITH_JIT is true and PREDICTED_CHAINING is false
-        //  temp 8 and EAX are not used; but it is okay to keep it here
-        infoArray[7].regNum = PhysicalReg_EAX;
-        infoArray[7].refCount = 4; //DU
-        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[8].regNum = 1;
-        infoArray[8].refCount = 2; //DU
-        infoArray[8].physicalType = LowOpndRegType_scratch;
-        infoArray[9].regNum = 2;
-        infoArray[9].refCount = 2; //DU
-        infoArray[9].physicalType = LowOpndRegType_scratch;
-        if(inst_op == OP_INVOKE_VIRTUAL)
-            k = updateInvokeNoRange(infoArray, 10);
-        else
-            k = updateInvokeRange(infoArray, 10);
-        return k;
-#endif
-    case OP_INVOKE_SUPER:
-    case OP_INVOKE_SUPER_RANGE:
-        infoArray[0].regNum = 3;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 7;
-        infoArray[1].refCount = 3; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 8;
-        infoArray[2].refCount = 3; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 6;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 9;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-
-        infoArray[5].regNum = PhysicalReg_EDX;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[6].regNum = PhysicalReg_ECX;
-        infoArray[6].refCount = 1+1; //DU
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[7].regNum = PhysicalReg_EAX;
-        infoArray[7].refCount = 4; //DU
-        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[8].regNum = 1;
-        infoArray[8].refCount = 2; //DU
-        infoArray[8].physicalType = LowOpndRegType_scratch;
-        infoArray[9].regNum = 2;
-        infoArray[9].refCount = 2; //DU
-        infoArray[9].physicalType = LowOpndRegType_scratch;
-        infoArray[10].regNum = 3;
-        infoArray[10].refCount = 2; //DU
-        infoArray[10].physicalType = LowOpndRegType_scratch;
-        infoArray[11].regNum = 4;
-        infoArray[11].refCount = 2; //DU
-        infoArray[11].physicalType = LowOpndRegType_scratch;
-        if(inst_op == OP_INVOKE_SUPER)
-            k = updateInvokeNoRange(infoArray, 12, currentMIR);
-        else
-            k = updateInvokeRange(infoArray, 12, currentMIR);
-        return k;
-    case OP_INVOKE_DIRECT:
-    case OP_INVOKE_DIRECT_RANGE:
-        infoArray[0].regNum = 3;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 5;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-
-        infoArray[2].regNum = PhysicalReg_EDX;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[3].regNum = PhysicalReg_ECX;
-        infoArray[3].refCount = 2;
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[4].regNum = PhysicalReg_EAX;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[5].regNum = 1;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-        infoArray[6].regNum = 2;
-        infoArray[6].refCount = 2; //DU
-        infoArray[6].physicalType = LowOpndRegType_scratch;
-        if(inst_op == OP_INVOKE_DIRECT)
-            k = updateInvokeNoRange(infoArray, 7, currentMIR);
-        else
-            k = updateInvokeRange(infoArray, 7, currentMIR);
-        return k;
-    case OP_INVOKE_STATIC:
-    case OP_INVOKE_STATIC_RANGE:
-        infoArray[0].regNum = 3;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-
-        infoArray[1].regNum = PhysicalReg_EDX;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[2].regNum = PhysicalReg_ECX;
-        infoArray[2].refCount = 2;
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[3].regNum = PhysicalReg_EAX;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[4].regNum = 1;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        infoArray[5].regNum = 2;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-        if(inst_op == OP_INVOKE_STATIC)
-            k = updateInvokeNoRange(infoArray, 6, currentMIR);
-        else
-            k = updateInvokeRange(infoArray, 6, currentMIR);
-        return k;
-    case OP_INVOKE_INTERFACE:
-    case OP_INVOKE_INTERFACE_RANGE:
-#ifdef PREDICTED_CHAINING
-        numTmps = updateGenPrediction(infoArray, true /*interface*/);
-        infoArray[numTmps].regNum = 1;
-        infoArray[numTmps].refCount = 3; //DU
-        infoArray[numTmps].physicalType = LowOpndRegType_gp;
-        numTmps++;
-        if(inst_op == OP_INVOKE_INTERFACE)
-            k = updateInvokeNoRange(infoArray, numTmps, currentMIR);
-        else
-            k = updateInvokeRange(infoArray, numTmps, currentMIR);
-        return k;
-#else
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 3;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 4;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 5;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-
-        infoArray[4].regNum = PhysicalReg_EDX;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[5].regNum = PhysicalReg_ECX;
-        infoArray[5].refCount = 1+1; //DU
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[6].regNum = PhysicalReg_EAX;
-        infoArray[6].refCount = 2+1; //2 uses
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[7].regNum = 1;
-        infoArray[7].refCount = 2; //DU
-        infoArray[7].physicalType = LowOpndRegType_scratch;
-        infoArray[8].regNum = 2;
-        infoArray[8].refCount = 2; //DU
-        infoArray[8].physicalType = LowOpndRegType_scratch;
-        infoArray[9].regNum = 3;
-        infoArray[9].refCount = 2; //DU
-        infoArray[9].physicalType = LowOpndRegType_scratch;
-        if(inst_op == OP_INVOKE_INTERFACE)
-            k = updateInvokeNoRange(infoArray, 10);
-        else
-            k = updateInvokeRange(infoArray, 10);
-        return k;
-#endif
-        ////////////////////////////////////////////// ALU
-    case OP_NEG_INT:
-    case OP_NOT_INT:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //define, update, use
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (vA != vB)
-            infoArray[0].shareWithVR = false;
-        return 1;
-    case OP_NEG_LONG:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //define, use
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 4; //define, update, use
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        return 2;
-    case OP_NOT_LONG:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //define, update, use
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        if (vA != vB)
-            infoArray[0].shareWithVR = false;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        return 2;
-    case OP_NEG_FLOAT:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //define, update, use
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (vA != vB)
-            infoArray[0].shareWithVR = false;
-        return 1;
-    case OP_NEG_DOUBLE:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //define, use
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //define, update, use
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        if (vA != vB)
-            infoArray[0].shareWithVR = false;
-        return 2;
-
-    case OP_INT_TO_LONG: //hard-code eax & edx
-        infoArray[0].regNum = PhysicalReg_EAX;
-        infoArray[0].refCount = 2+1;
-        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[0].shareWithVR = false;
-        infoArray[1].regNum = PhysicalReg_EDX;
-        infoArray[1].refCount = 1+1; //cdq accesses edx & eax
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 2;
-    case OP_INT_TO_DOUBLE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        return 2;
-    case OP_INT_TO_FLOAT:
-    case OP_LONG_TO_FLOAT:
-    case OP_LONG_TO_DOUBLE:
-    case OP_FLOAT_TO_DOUBLE:
-    case OP_DOUBLE_TO_FLOAT:
-        return 0; //fp stack
-    case OP_LONG_TO_INT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        return 1;
-    case OP_FLOAT_TO_INT:
-    case OP_DOUBLE_TO_INT: //fp stack
-        return 0;
-    case OP_FLOAT_TO_LONG:
-    case OP_DOUBLE_TO_LONG:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //define, use
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //define, use
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //define, use
-        infoArray[2].physicalType = LowOpndRegType_xmm;
-        return 3;
-    case OP_INT_TO_BYTE:
-    case OP_INT_TO_CHAR:
-    case OP_INT_TO_SHORT:
-        vA = currentMIR->dalvikInsn.vA;
-        vB = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4; //define, update, update, use
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (vA != vB)
-            infoArray[0].shareWithVR = false;
-        if (inst_op == OP_INT_TO_BYTE)
-            infoArray[0].is8Bit = true;
-        return 1;
-
-    case OP_ADD_INT:
-    case OP_SUB_INT:
-    case OP_MUL_INT:
-    case OP_AND_INT:
-    case OP_OR_INT:
-    case OP_XOR_INT:
-    case OP_ADD_INT_2ADDR:
-    case OP_SUB_INT_2ADDR:
-    case OP_MUL_INT_2ADDR:
-    case OP_AND_INT_2ADDR:
-    case OP_OR_INT_2ADDR:
-    case OP_XOR_INT_2ADDR:
-        if(inst_op == OP_ADD_INT || inst_op == OP_SUB_INT || inst_op == OP_MUL_INT ||
-           inst_op == OP_AND_INT || inst_op == OP_OR_INT || inst_op == OP_XOR_INT) {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = currentMIR->dalvikInsn.vB;
-        } else {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = vA;
-        }
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //define, update, use
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if(vA != v1)
-            infoArray[0].shareWithVR = false;
-        return 1; //common_alu_int
-
-    case OP_SHL_INT:
-    case OP_SHR_INT:
-    case OP_USHR_INT:
-    case OP_SHL_INT_2ADDR:
-    case OP_SHR_INT_2ADDR:
-    case OP_USHR_INT_2ADDR: //use %cl or %ecx?
-        if(inst_op == OP_SHL_INT || inst_op == OP_SHR_INT || inst_op == OP_USHR_INT) {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = currentMIR->dalvikInsn.vB;
-        } else {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = vA;
-        }
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //define, update, use
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if(vA != v1)
-            infoArray[0].shareWithVR = false;
-        infoArray[1].regNum = PhysicalReg_ECX;
-        infoArray[1].refCount = 2; //define, use
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 2;//common_shift_int
-
-    case OP_DIV_INT:
-    case OP_REM_INT:
-    case OP_DIV_INT_2ADDR:
-    case OP_REM_INT_2ADDR: //hard-code %eax, %edx (dividend in edx:eax; quotient in eax; remainder in edx)
-        if (inst_op == OP_DIV_INT || inst_op == OP_REM_INT) {
-            v2 = currentMIR->dalvikInsn.vC;
-        }else {
-            v2 = currentMIR->dalvikInsn.vB;
-        }
-
-        //Check if the virtual register is a constant because if it is we can figure out result without division
-        isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, &tmpvalue, false);
-
-        //If we have a constant, we can use a multiplication approach instead.
-        //However, we currently do not handle case of -1 constant so we take the divide path.
-        //It also does not make sense to optimize division by zero.
-        if (isConst == VR_IS_CONSTANT && tmpvalue != -1)
-        {
-            if (tmpvalue == 0 || tmpvalue == 1) {
-                infoArray[0].regNum = 2;
-                infoArray[0].refCount = 2;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[1].regNum = PhysicalReg_EAX;
-                infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-                infoArray[1].shareWithVR = false;
-                infoArray[1].refCount = 2;
-                infoArray[2].regNum = PhysicalReg_EDX;
-                infoArray[2].refCount = 1;
-                infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-                if (tmpvalue == 1 && (inst_op == OP_REM_INT || inst_op == OP_REM_INT_2ADDR)) {
-                    infoArray[2].refCount++;
-                } else if (tmpvalue == 1) {
-                    infoArray[1].refCount++;
-                }
-                return 3;
-            } else {
-                int magic, shift;
-                calculateMagicAndShift(tmpvalue, &magic, &shift);
-
-                infoArray[0].regNum = 2;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[1].regNum = PhysicalReg_EAX;
-                infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-                infoArray[1].shareWithVR = false;
-                infoArray[2].regNum = PhysicalReg_EDX;
-                infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-                infoArray[3].regNum = 1;
-                infoArray[3].physicalType = LowOpndRegType_gp;
-                if (inst_op == OP_REM_INT || inst_op == OP_REM_INT_2ADDR) {
-                    infoArray[0].refCount = 8;
-                    infoArray[1].refCount = 7;
-                    infoArray[2].refCount = 9;
-                    infoArray[3].refCount = 3;
-                    if ((tmpvalue > 0 && magic < 0) || (tmpvalue < 0 && magic > 0)) {
-                        infoArray[3].refCount++;
-                        infoArray[2].refCount++;
-                    }
-                    if (shift != 0) {
-                        infoArray[2].refCount++;
-                    }
-                }else {
-                    infoArray[0].refCount = 6;
-                    infoArray[1].refCount = 4;
-                    infoArray[2].refCount = 6;
-                    infoArray[3].refCount = 1;
-                    if ((tmpvalue > 0 && magic < 0) || (tmpvalue < 0 && magic > 0)) {
-                        infoArray[3].refCount++;
-                        infoArray[2].refCount++;
-                    }
-                    if (shift != 0) {
-                        infoArray[2].refCount++;
-                    }
-                }
-               return 4;
-            }
-        }else {
-            infoArray[0].regNum = 2;
-            infoArray[0].refCount = 7; //define, update, use
-            infoArray[0].physicalType = LowOpndRegType_gp;
-            infoArray[1].regNum = PhysicalReg_EAX; //dividend, quotient
-            infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            infoArray[1].shareWithVR = false;
-            infoArray[2].regNum = PhysicalReg_EDX; //export_pc, output for REM
-            infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            infoArray[3].regNum = 1;
-            infoArray[3].refCount = 2; //define, use
-            infoArray[3].physicalType = LowOpndRegType_scratch;
-            infoArray[4].regNum = 3;
-            infoArray[4].refCount = 4; //define, use
-            infoArray[4].physicalType = LowOpndRegType_gp;
-            infoArray[5].regNum = 4;
-            infoArray[5].refCount = 2; //define, use
-            infoArray[5].physicalType = LowOpndRegType_gp;
-            infoArray[5].is8Bit = true;
-            if(inst_op == OP_DIV_INT || inst_op == OP_DIV_INT_2ADDR) {
-                infoArray[1].refCount = 11;
-                infoArray[2].refCount = 9;
-            } else {
-                infoArray[1].refCount = 10;
-                infoArray[2].refCount = 12;
-            }
-            return 6;
-         }
-    case OP_ADD_INT_LIT16:
-    case OP_MUL_INT_LIT16:
-    case OP_AND_INT_LIT16:
-    case OP_OR_INT_LIT16:
-    case OP_XOR_INT_LIT16:
-    case OP_ADD_INT_LIT8:
-    case OP_MUL_INT_LIT8:
-    case OP_AND_INT_LIT8:
-    case OP_OR_INT_LIT8:
-    case OP_XOR_INT_LIT8:
-    case OP_SHL_INT_LIT8:
-    case OP_SHR_INT_LIT8:
-    case OP_USHR_INT_LIT8:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //define, update, use
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if(vA != v1)
-            infoArray[0].shareWithVR = false;
-        return 1;
-
-    case OP_RSUB_INT_LIT8:
-    case OP_RSUB_INT:
-        vA = currentMIR->dalvikInsn.vA;
-        v1 = currentMIR->dalvikInsn.vB;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if(vA != v1)
-            infoArray[0].shareWithVR = false;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 3;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        if(vA != v1)
-            infoArray[1].shareWithVR = false;
-        return 2;
-
-    case OP_DIV_INT_LIT16:
-    case OP_REM_INT_LIT16:
-    case OP_DIV_INT_LIT8:
-    case OP_REM_INT_LIT8:
-        tmp_s2 = currentMIR->dalvikInsn.vC;
-        if((inst_op == OP_DIV_INT_LIT8 || inst_op == OP_DIV_INT_LIT16)) {
-            int power = isPowerOfTwo(tmp_s2);
-            if(power >= 1) { /* divide by a power of 2 constant */
-                infoArray[0].regNum = 2;
-                infoArray[0].refCount = 3; //define, use, use
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[1].regNum = 1;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                if(power == 1) infoArray[1].refCount = 5;
-                else infoArray[1].refCount = 6;
-                return 2;
-            }
-        }
-        if(tmp_s2 == 0) {
-            //export_pc
-            infoArray[0].regNum = PhysicalReg_EDX; //export_pc, output for REM
-            infoArray[0].refCount = 2;
-            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-            return 1;
-        }
-        if(inst_op == OP_DIV_INT_LIT16 || inst_op == OP_DIV_INT_LIT8) {
-            if(tmp_s2 == -1)
-                infoArray[1].refCount = 4+1;
-            else
-                infoArray[1].refCount = 4;
-            infoArray[2].refCount = 2; //edx
-        } else {
-            if(tmp_s2 == -1)
-                infoArray[1].refCount = 3+1;
-            else
-                infoArray[1].refCount = 3;
-            infoArray[2].refCount = 3; //edx
-        }
-        infoArray[0].regNum = 2;
-        infoArray[0].refCount = 2; //define, use
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = PhysicalReg_EAX; //dividend, quotient
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[1].shareWithVR = false;
-        infoArray[2].regNum = PhysicalReg_EDX; //export_pc, output for REM
-        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 3;
-
-    case OP_ADD_LONG:
-    case OP_ADD_LONG_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-
-        //In the case of OP_ADD_LONG, we use vB otherwise we use vA
-        if (inst_op == OP_ADD_LONG)
-        {
-            v1 = currentMIR->dalvikInsn.vB;
-        }
-        else
-        {
-            v1 = vA;
-        }
-
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //define, update, use
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if (vA != v1)
-        {
-            infoArray[0].shareWithVR = false;
-        }
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 3; //define, use
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        if (vA != v1)
-        {
-            infoArray[1].shareWithVR = false;
-        }
-        return 2;
-    case OP_SUB_LONG:
-    case OP_AND_LONG:
-    case OP_OR_LONG:
-    case OP_XOR_LONG:
-    case OP_SUB_LONG_2ADDR:
-    case OP_AND_LONG_2ADDR:
-    case OP_OR_LONG_2ADDR:
-    case OP_XOR_LONG_2ADDR:
-        //In the case of non 2ADDR, we use vB otherwise we use vA
-        if(inst_op == OP_ADD_LONG || inst_op == OP_SUB_LONG || inst_op == OP_AND_LONG ||
-           inst_op == OP_OR_LONG || inst_op == OP_XOR_LONG) {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = currentMIR->dalvikInsn.vB;
-        } else {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = vA;
-        }
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //define, update, use
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        if(vA != v1)
-            infoArray[0].shareWithVR = false;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //define, use
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        return 2;
-
-    case OP_SHL_LONG:
-    case OP_SHL_LONG_2ADDR:
-        //In the case of non 2ADDR, we use vB and vC otherwise we use vA
-        if(inst_op == OP_SHL_LONG) {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = currentMIR->dalvikInsn.vB;
-            v2 = currentMIR->dalvikInsn.vC;
-            isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, &tmpvalue, false); //do not update refCount
-        } else {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = vA;
-            isConst = isVirtualRegConstant(v1, LowOpndRegType_gp, &tmpvalue, false); //do not update refCount
-        }
-        if(isConst == 3) {  // case where VR contents is a constant, shift amount is available
-           infoArray[0].regNum = 1;
-           infoArray[0].refCount = 3; //define, update, use
-           infoArray[0].physicalType = LowOpndRegType_xmm;
-           if(vA != v1)
-               infoArray[0].shareWithVR = false;
-           infoArray[1].regNum = 2;
-           infoArray[1].refCount = 1; //define, update, use
-           infoArray[1].physicalType = LowOpndRegType_xmm;
-           infoArray[1].shareWithVR = false;
-           return 2;
-        } else {      // case where VR content is not a constant, shift amount has to be read from VR.
-           infoArray[0].regNum = 1;
-           infoArray[0].refCount = 3; //define, update, use
-           infoArray[0].physicalType = LowOpndRegType_xmm;
-           if(vA != v1)
-               infoArray[0].shareWithVR = false;
-           infoArray[1].regNum = 2;
-           infoArray[1].refCount = 3; //define, update, use
-           infoArray[1].physicalType = LowOpndRegType_xmm;
-           infoArray[1].shareWithVR = false;
-           infoArray[2].regNum = 3;
-           infoArray[2].refCount = 2; //define, use
-           infoArray[2].physicalType = LowOpndRegType_xmm;
-           return 3;
-        }
-    case OP_SHR_LONG:
-    case OP_SHR_LONG_2ADDR:
-        if(inst_op == OP_SHR_LONG) {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = currentMIR->dalvikInsn.vB;
-        } else {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = vA;
-        }
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4; //define, update, use
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        if(vA != v1)
-            infoArray[0].shareWithVR = false;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 4; //define, update, use
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        infoArray[1].shareWithVR = false;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //define, use
-        infoArray[2].physicalType = LowOpndRegType_xmm;
-        infoArray[3].regNum = 4;
-        infoArray[3].refCount = 3;
-        infoArray[3].physicalType = LowOpndRegType_xmm;
-        infoArray[4].regNum = 5;
-        infoArray[4].refCount = 3;
-        infoArray[4].physicalType = LowOpndRegType_xmm;
-        return 5;
-
-    case OP_USHR_LONG:
-    case OP_USHR_LONG_2ADDR:
-        if(inst_op == OP_USHR_LONG) {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = currentMIR->dalvikInsn.vB;
-            v2 = currentMIR->dalvikInsn.vC;
-            isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, &tmpvalue, false);
-        } else {
-            vA = currentMIR->dalvikInsn.vA;
-            v1 = vA;
-            isConst = isVirtualRegConstant(v1, LowOpndRegType_gp, &tmpvalue, false);
-        }
-        if (isConst == 3) { // case where VR contents is a constant, shift amount is available
-            infoArray[0].regNum = 1;
-            infoArray[0].refCount = 3; //define, update, use
-            infoArray[0].physicalType = LowOpndRegType_xmm;
-            if(vA != v1)
-                infoArray[0].shareWithVR = false;
-            infoArray[1].regNum = 2;
-            infoArray[1].refCount = 1; //define, update, use
-            infoArray[1].physicalType = LowOpndRegType_xmm;
-            infoArray[1].shareWithVR = false;
-            return 2;
-         } else { // case where VR content is not a constant, shift amount has to be read from VR.
-            infoArray[0].regNum = 1;
-            infoArray[0].refCount = 3; //define, update, use
-            infoArray[0].physicalType = LowOpndRegType_xmm;
-            if(vA != v1)
-                infoArray[0].shareWithVR = false;
-            infoArray[1].regNum = 2;
-            infoArray[1].refCount = 3; //define, update, use
-            infoArray[1].physicalType = LowOpndRegType_xmm;
-            infoArray[1].shareWithVR = false;
-            infoArray[2].regNum = 3;
-            infoArray[2].refCount = 2; //define, use
-            infoArray[2].physicalType = LowOpndRegType_xmm;
-            return 3;
-         }
-
-    case OP_MUL_LONG:
-    case OP_MUL_LONG_2ADDR:
-        if(inst_op == OP_MUL_LONG)
-        {
-            v1 = currentMIR->dalvikInsn.vB;
-        }
-        else
-        {
-            //For 2addr form, the destination is also first operand
-            v1 = currentMIR->dalvikInsn.vA;
-        }
-        v2 = currentMIR->dalvikInsn.vC;
-
-        if (v1 != v2) // when the multiplicands are not the same
-        {
-           infoArray[0].regNum = 1;
-           infoArray[0].refCount = 6;
-           infoArray[0].physicalType = LowOpndRegType_gp;
-           infoArray[0].shareWithVR = false;
-           infoArray[1].regNum = 2;
-           infoArray[1].refCount = 3;
-           infoArray[1].physicalType = LowOpndRegType_gp;
-           infoArray[2].regNum = 3;
-           infoArray[2].refCount = 3;
-           infoArray[2].physicalType = LowOpndRegType_gp;
-           infoArray[3].regNum = PhysicalReg_EAX;
-           infoArray[3].refCount = 2+1; //for mul_opc
-           infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-           infoArray[4].regNum = PhysicalReg_EDX;
-           infoArray[4].refCount = 2; //for mul_opc
-           infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-           return 5;
-        }
-        else // when square of a number is to be computed
-        {
-           infoArray[0].regNum = 1;
-           infoArray[0].refCount = 8;
-           infoArray[0].physicalType = LowOpndRegType_gp;
-           infoArray[0].shareWithVR = false;
-           infoArray[1].regNum = PhysicalReg_EAX;
-           infoArray[1].refCount = 2+1; //for mul_opc
-           infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-           infoArray[2].regNum = PhysicalReg_EDX;
-           infoArray[2].refCount = 3+1; //for mul_opc
-           infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-           return 3;
-         }
-    case OP_DIV_LONG:
-    case OP_REM_LONG:
-    case OP_DIV_LONG_2ADDR:
-    case OP_REM_LONG_2ADDR:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[0].shareWithVR = false;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 3;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 1;
-        infoArray[2].refCount = 2;
-        infoArray[2].physicalType = LowOpndRegType_xmm;
-        infoArray[3].regNum = PhysicalReg_EAX;
-        infoArray[3].refCount = 2; //defined by function call
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[4].regNum = PhysicalReg_EDX;
-        infoArray[4].refCount = 2; //next version has 2 references
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[5].regNum = 1;
-        infoArray[5].refCount = 2;
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-        return 6;
-
-    case OP_ADD_FLOAT:
-    case OP_SUB_FLOAT:
-    case OP_MUL_FLOAT:
-    case OP_ADD_FLOAT_2ADDR:
-    case OP_SUB_FLOAT_2ADDR:
-    case OP_MUL_FLOAT_2ADDR:
-    case OP_ADD_DOUBLE: //PhysicalReg_FP TODO
-    case OP_SUB_DOUBLE:
-    case OP_MUL_DOUBLE:
-    case OP_ADD_DOUBLE_2ADDR:
-    case OP_SUB_DOUBLE_2ADDR:
-    case OP_MUL_DOUBLE_2ADDR:
-    case OP_DIV_FLOAT:
-    case OP_DIV_FLOAT_2ADDR:
-    case OP_DIV_DOUBLE:
-    case OP_DIV_DOUBLE_2ADDR:
-        vA = currentMIR->dalvikInsn.vA;
-        //In the case of non 2ADDR, we use vB and vC otherwise we use vA
-        if (inst_op == OP_ADD_FLOAT || inst_op == OP_SUB_FLOAT
-                || inst_op == OP_MUL_FLOAT || inst_op == OP_ADD_DOUBLE
-                || inst_op == OP_SUB_DOUBLE || inst_op == OP_MUL_DOUBLE
-                || inst_op == OP_DIV_FLOAT || inst_op == OP_DIV_DOUBLE)
-        {
-            v1 = currentMIR->dalvikInsn.vB;
-        }
-        else
-        {
-            v1 = vA;
-        }
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        if(vA != v1)
-        {
-            infoArray[0].shareWithVR = false;
-        }
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        return 2;
-
-    case OP_REM_FLOAT:
-    case OP_REM_FLOAT_2ADDR:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 1;
-        infoArray[2].refCount = 2;
-        infoArray[2].physicalType = LowOpndRegType_scratch;
-        return 3;
-
-    case OP_REM_DOUBLE:
-    case OP_REM_DOUBLE_2ADDR:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        infoArray[2].regNum = 1;
-        infoArray[2].refCount = 2;
-        infoArray[2].physicalType = LowOpndRegType_scratch;
-        return 3;
-
-    case OP_CMPL_FLOAT:
-    case OP_CMPL_DOUBLE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        infoArray[1].regNum = 1;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 2;
-        infoArray[2].refCount = 2;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 3;
-        infoArray[3].refCount = 2;
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 4; //return
-        infoArray[4].refCount = 5;
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        return 5;
-
-    case OP_CMPG_FLOAT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        return 1;
-
-    case OP_CMPG_DOUBLE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_xmm;
-        return 1;
-
-    case OP_CMP_LONG:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        return 2;
-
-    case OP_EXECUTE_INLINE:
-    case OP_EXECUTE_INLINE_RANGE:
-        num = currentMIR->dalvikInsn.vA;
-#if defined(WITH_JIT)
-        tmp = currentMIR->dalvikInsn.vB;
-        switch (tmp) {
-            case INLINE_STRING_LENGTH:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 3;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 2;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[2].regNum = 3;
-                infoArray[2].refCount = 2;
-                infoArray[2].physicalType = LowOpndRegType_gp;
-                infoArray[3].regNum = 1;
-                infoArray[3].refCount = 2;
-                infoArray[3].physicalType = LowOpndRegType_scratch;
-                return 4;
-            case INLINE_STRING_IS_EMPTY:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 3;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 4;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[2].regNum = 1;
-                infoArray[2].refCount = 2;
-                infoArray[2].physicalType = LowOpndRegType_scratch;
-                return 3;
-            case INLINE_STRING_CHARAT:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 7;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[0].shareWithVR = false;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 7;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[1].shareWithVR = false;
-                infoArray[2].regNum = 3;
-                infoArray[2].refCount = 2;
-                infoArray[2].physicalType = LowOpndRegType_gp;
-                return 3;
-            case INLINE_STRING_FASTINDEXOF_II:
-#if defined(USE_GLOBAL_STRING_DEFS)
-                break;
-#else
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 14 * LOOP_COUNT;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[0].shareWithVR = false;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 3 * LOOP_COUNT;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[2].regNum = 3;
-                infoArray[2].refCount = 11 * LOOP_COUNT;
-                infoArray[2].physicalType = LowOpndRegType_gp;
-                infoArray[2].shareWithVR = false;
-                infoArray[3].regNum = 4;
-                infoArray[3].refCount = 3 * LOOP_COUNT;
-                infoArray[3].physicalType = LowOpndRegType_gp;
-                infoArray[4].regNum = 5;
-                infoArray[4].refCount = 9 * LOOP_COUNT;
-                infoArray[4].physicalType = LowOpndRegType_gp;
-                infoArray[5].regNum = 6;
-                infoArray[5].refCount = 4 * LOOP_COUNT;
-                infoArray[5].physicalType = LowOpndRegType_gp;
-                infoArray[6].regNum = 7;
-                infoArray[6].refCount = 2;
-                infoArray[6].physicalType = LowOpndRegType_gp;
-                infoArray[7].regNum = 1;
-                infoArray[7].refCount = 2;
-                infoArray[7].physicalType = LowOpndRegType_scratch;
-                return 8;
-#endif
-            case INLINE_MATH_ABS_LONG:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 7;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[0].shareWithVR = false;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 2;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[2].regNum = 3;
-                infoArray[2].refCount = 3;
-                infoArray[2].physicalType = LowOpndRegType_gp;
-                infoArray[3].regNum = 4;
-                infoArray[3].refCount = 3;
-                infoArray[3].physicalType = LowOpndRegType_gp;
-                infoArray[4].regNum = 5;
-                infoArray[4].refCount = 2;
-                infoArray[4].physicalType = LowOpndRegType_gp;
-                infoArray[5].regNum = 6;
-                infoArray[5].refCount = 5;
-                infoArray[5].physicalType = LowOpndRegType_gp;
-                return 6;
-            case INLINE_MATH_ABS_INT:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 5;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[0].shareWithVR = false;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 4;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[2].regNum = 3;
-                infoArray[2].refCount = 2;
-                infoArray[2].physicalType = LowOpndRegType_gp;
-                return 3;
-            case INLINE_MATH_MAX_INT:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 4;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[0].shareWithVR = false;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 3;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[2].regNum = 3;
-                infoArray[2].refCount = 2;
-                infoArray[2].physicalType = LowOpndRegType_gp;
-                return 3;
-            case INLINE_MATH_MIN_INT:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 4;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[0].shareWithVR = false;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 3;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[2].regNum = 3;
-                infoArray[2].refCount = 2;
-                infoArray[2].physicalType = LowOpndRegType_gp;
-                return 3;
-            case INLINE_MATH_ABS_FLOAT:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 3;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[0].shareWithVR = false;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 2;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                return 2;
-            case INLINE_MATH_ABS_DOUBLE:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 3;
-                infoArray[0].physicalType = LowOpndRegType_xmm;
-                infoArray[0].shareWithVR = false;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 2;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                return 2;
-            case INLINE_FLOAT_TO_RAW_INT_BITS:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 2;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 2;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                return 2;
-            case INLINE_INT_BITS_TO_FLOAT:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 2;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 2;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                return 2;
-            case INLINE_DOUBLE_TO_RAW_LONG_BITS:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 2;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 2;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[2].regNum = 3;
-                infoArray[2].refCount = 3;
-                infoArray[2].physicalType = LowOpndRegType_gp;
-                return 3;
-            case INLINE_LONG_BITS_TO_DOUBLE:
-                infoArray[0].regNum = 1;
-                infoArray[0].refCount = 2;
-                infoArray[0].physicalType = LowOpndRegType_gp;
-                infoArray[1].regNum = 2;
-                infoArray[1].refCount = 2;
-                infoArray[1].physicalType = LowOpndRegType_gp;
-                infoArray[2].regNum = 3;
-                infoArray[2].refCount = 3;
-                infoArray[2].physicalType = LowOpndRegType_gp;
-                return 3;
-            default:
-                break;
-        }
-#endif
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 4;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        if(num >= 1) {
-            infoArray[1].regNum = 2;
-            infoArray[1].refCount = 2;
-            infoArray[1].physicalType = LowOpndRegType_gp;
-        }
-        if(num >= 2) {
-            infoArray[2].regNum = 3;
-            infoArray[2].refCount = 2;
-            infoArray[2].physicalType = LowOpndRegType_gp;
-        }
-        if(num >= 3) {
-            infoArray[3].regNum = 4;
-            infoArray[3].refCount = 2;
-            infoArray[3].physicalType = LowOpndRegType_gp;
-        }
-        if(num >= 4) {
-            infoArray[4].regNum = 5;
-            infoArray[4].refCount = 2;
-            infoArray[4].physicalType = LowOpndRegType_gp;
-        }
-        infoArray[num+1].regNum = 6;
-        infoArray[num+1].refCount = 2;
-        infoArray[num+1].physicalType = LowOpndRegType_gp;
-        infoArray[num+2].regNum = PhysicalReg_EAX;
-        infoArray[num+2].refCount = 2;
-        infoArray[num+2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[num+3].regNum = PhysicalReg_EDX;
-        infoArray[num+3].refCount = 2;
-        infoArray[num+3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[num+4].regNum = 1;
-        infoArray[num+4].refCount = 4;
-        infoArray[num+4].physicalType = LowOpndRegType_scratch;
-        return num+5;
-#if FIXME
-    case OP_INVOKE_OBJECT_INIT_RANGE:
-        return 0;
-#endif
-    case OP_INVOKE_VIRTUAL_QUICK:
-    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
-#ifdef PREDICTED_CHAINING
-        numTmps = updateGenPrediction(infoArray, false /*not interface*/);
-        infoArray[numTmps].regNum = 1;
-        infoArray[numTmps].refCount = 3; //DU
-        infoArray[numTmps].physicalType = LowOpndRegType_gp;
-        numTmps++;
-        if(inst_op == OP_INVOKE_VIRTUAL_QUICK)
-            k = updateInvokeNoRange(infoArray, numTmps, currentMIR);
-        else
-            k = updateInvokeRange(infoArray, numTmps, currentMIR);
-        return k;
-#else
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-
-        infoArray[3].regNum = PhysicalReg_ECX;
-        infoArray[3].refCount = 1+1;
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[4].regNum = PhysicalReg_EDX;
-        infoArray[4].refCount = 2;
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        if(inst_op == OP_INVOKE_VIRTUAL_QUICK_RANGE)
-            k = updateInvokeRange(infoArray, 5);
-        else
-            k = updateInvokeNoRange(infoArray, 5);
-        return k;
-#endif
-    case OP_INVOKE_SUPER_QUICK:
-    case OP_INVOKE_SUPER_QUICK_RANGE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2;
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 4;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 5;
-        infoArray[2].refCount = 2;
-        infoArray[2].physicalType = LowOpndRegType_gp;
-
-        infoArray[3].regNum = PhysicalReg_ECX;
-        infoArray[3].refCount = 1+1;
-        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        infoArray[4].regNum = PhysicalReg_EDX;
-        infoArray[4].refCount = 2;
-        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-
-        infoArray[5].regNum = 1;
-        infoArray[5].refCount = 2;
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-        infoArray[6].regNum = 2;
-        infoArray[6].refCount = 2;
-        infoArray[6].physicalType = LowOpndRegType_scratch;
-        if(inst_op == OP_INVOKE_SUPER_QUICK_RANGE)
-            k = updateInvokeRange(infoArray, 7, currentMIR);
-        else
-            k = updateInvokeNoRange(infoArray, 7, currentMIR);
-        return k;
-#ifdef SUPPORT_HLO
-    case kExtInstruction:
-        switch(inst) {
-    case OP_X_AGET_QUICK:
-    case OP_X_AGET_OBJECT_QUICK:
-    case OP_X_AGET_BOOLEAN_QUICK:
-    case OP_X_AGET_BYTE_QUICK:
-    case OP_X_AGET_CHAR_QUICK:
-    case OP_X_AGET_SHORT_QUICK:
-        vA = FETCH(1) & 0xff;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 4;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[3].linkageToVR = vA;
-        if(inst == OP_X_AGET_BYTE_QUICK || inst == OP_X_AGET_BOOLEAN_QUICK)
-            infoArray[3].is8Bit = true;
-        return 4;
-    case OP_X_AGET_WIDE_QUICK:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 1;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_xmm;
-        return 4;
-    case OP_X_APUT_QUICK:
-    case OP_X_APUT_OBJECT_QUICK:
-    case OP_X_APUT_BOOLEAN_QUICK:
-    case OP_X_APUT_BYTE_QUICK:
-    case OP_X_APUT_CHAR_QUICK:
-    case OP_X_APUT_SHORT_QUICK:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 4;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        if(inst == OP_X_APUT_BYTE_QUICK || inst == OP_X_APUT_BOOLEAN_QUICK)
-            infoArray[3].is8Bit = true;
-        return 4;
-    case OP_X_APUT_WIDE_QUICK:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 1;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_xmm;
-        return 4;
-    case OP_X_DEREF_GET:
-    case OP_X_DEREF_GET_OBJECT:
-    case OP_X_DEREF_GET_BOOLEAN:
-    case OP_X_DEREF_GET_BYTE:
-    case OP_X_DEREF_GET_CHAR:
-    case OP_X_DEREF_GET_SHORT:
-        vA = FETCH(1) & 0xff;
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[1].linkageToVR = vA;
-        if(inst == OP_X_DEREF_GET_BYTE || inst == OP_X_DEREF_GET_BOOLEAN)
-            infoArray[1].is8Bit = true;
-        return 2;
-    case OP_X_DEREF_GET_WIDE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 1;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        return 2;
-    case OP_X_DEREF_PUT:
-    case OP_X_DEREF_PUT_OBJECT:
-    case OP_X_DEREF_PUT_BOOLEAN:
-    case OP_X_DEREF_PUT_BYTE:
-    case OP_X_DEREF_PUT_CHAR:
-    case OP_X_DEREF_PUT_SHORT:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        if(inst == OP_X_DEREF_PUT_BYTE || inst == OP_X_DEREF_PUT_BOOLEAN)
-            infoArray[1].is8Bit = true;
-        return 2;
-    case OP_X_DEREF_PUT_WIDE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 1;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_xmm;
-        return 2;
-    case OP_X_ARRAY_CHECKS:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        return 2;
-    case OP_X_CHECK_BOUNDS:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 2; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        return 2;
-    case OP_X_CHECK_NULL:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 2; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = PhysicalReg_EDX;
-        infoArray[1].refCount = 2;
-        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 2;
-    case OP_X_CHECK_TYPE:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 3; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 5;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 6;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 1;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_scratch;
-        infoArray[5].regNum = PhysicalReg_EAX;
-        infoArray[5].refCount = 2;
-        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 6;
-    case OP_X_ARRAY_OBJECT_CHECKS:
-        infoArray[0].regNum = 1;
-        infoArray[0].refCount = 3; //DU
-        infoArray[0].physicalType = LowOpndRegType_gp;
-        infoArray[1].regNum = 2;
-        infoArray[1].refCount = 4; //DU
-        infoArray[1].physicalType = LowOpndRegType_gp;
-        infoArray[2].regNum = 3;
-        infoArray[2].refCount = 2; //DU
-        infoArray[2].physicalType = LowOpndRegType_gp;
-        infoArray[3].regNum = 5;
-        infoArray[3].refCount = 2; //DU
-        infoArray[3].physicalType = LowOpndRegType_gp;
-        infoArray[4].regNum = 6;
-        infoArray[4].refCount = 2; //DU
-        infoArray[4].physicalType = LowOpndRegType_gp;
-        infoArray[5].regNum = 1;
-        infoArray[5].refCount = 2; //DU
-        infoArray[5].physicalType = LowOpndRegType_scratch;
-        infoArray[6].regNum = PhysicalReg_EAX;
-        infoArray[6].refCount = 2;
-        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
-        return 7;
-    }
-#endif
-    default:
-        ALOGI("JIT_INFO: JIT does not support bytecode 0x%hx when updating temp accesses",
-                currentMIR->dalvikInsn.opcode);
-        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
-        assert(false && "All opcodes should be supported.");
-        break;
-    }
-    return -1;
-}
diff --git a/vm/compiler/codegen/x86/CodegenErrors.cpp b/vm/compiler/codegen/x86/CodegenErrors.cpp
deleted file mode 100644
index 0f82fbc..0000000
--- a/vm/compiler/codegen/x86/CodegenErrors.cpp
+++ /dev/null
@@ -1,259 +0,0 @@
-/*
- * Copyright (C) 2012-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "CodegenErrors.h"
-
-/**
- * @class ErrorInformation Used to keep track of information associated with an error
- */
-
-struct ErrorInformation
-{
-    /** @brief Type of error */
-    JitCompilationErrors type;
-
-    /** @brief Error message */
-    const char * errorMessage;
-
-    /** @brief Whether we can possibly fix the error */
-    bool canResolve;
-
-    /** @brief Whether error is fatal */
-    bool isFatal;
-};
-
-/**
- * @brief Three macros to help error definitions
- */
-
-
-#define START_ERRORS \
-    static ErrorInformation gErrorInformation[] = {
-
-#define NEW_ERROR(TYPE, MESSAGE, CANRESOLVE, ISFATAL) \
-        { \
-            TYPE, MESSAGE, CANRESOLVE, ISFATAL \
-        }
-
-#define END_ERRORS \
-    };
-
-/**
- * @brief Table that stores information about errors defined in
- * JitCompilationErrors
- */
-START_ERRORS
-    NEW_ERROR (kJitErrorMaxVR,                "Exceeded maximum allowed VRs in a basic block.",               false, false),
-    NEW_ERROR (kJitErrorShortJumpOffset,      "Jump offset greater than 8-bits.",                              true, false),
-    NEW_ERROR (kJitErrorUnsupportedBytecode,  "Trace contains bytecode with no implementation.",              false, false),
-    NEW_ERROR (kJitErrorUnresolvedField,      "Trace contains SGET / SPUT bytecode with unresolved field.",   false, false),
-    NEW_ERROR (kJitErrorInvalidBBId,          "Cannot find BasicBlock_O1 corresponding to a BasicBlock.",     false, false),
-    NEW_ERROR (kJitErrorCodeCacheFull,        "Jit code cache is full.",                                       true, false),
-    NEW_ERROR (kJitErrorRegAllocFailed,       "Failure in register allocator or register tables.",            false, false),
-    NEW_ERROR (kJitErrorMallocFailed,         "Malloc failure during trace compilation.",                      false, true),
-    NEW_ERROR (kJitErrorMaxXferPoints,        "Exceeded maximum number of transfer points per BB.",           false, false),
-    NEW_ERROR (kJitErrorMaxDestRegPerSource,  "Exceeded number of destination regs for a source reg.",        false, false),
-    NEW_ERROR (kJitErrorStateTransfer,        "Problem with state transfer in JIT.",                          false, false),
-    NEW_ERROR (kJitErrorTraceFormation,       "Problem with trace formation.",                                false, false),
-    NEW_ERROR (kJitErrorNullBoundCheckFailed, "Problem while performing null or bound check.",                false, false),
-    NEW_ERROR (kJitErrorMergeLiveRange,       "Problem while merging live ranges (mergeLiveRange).",          false, false),
-    NEW_ERROR (kJitErrorGlobalData,           "Global data not defined.",                                     false, false),
-    NEW_ERROR (kJitErrorInsScheduling,        "Problem during instruction scheduling.",                       false, false),
-    NEW_ERROR (kJitErrorBERegisterization,    "Issue registerizing the trace in the backend.",                 true, false),
-    NEW_ERROR (kJitErrorSpill,                "The trace provoked a spill.",                                   true, false),
-    NEW_ERROR (kJitErrorBBCannotBeHandled,    "The backend decided it cannot safely handle the Basic Block.", false, false),
-    NEW_ERROR (kJitErrorConstInitFail,        "Patching of Double/Long constants failed.",                     true, false),
-    NEW_ERROR (kJitErrorChainingCell,         "An issue was encountered while generating chaining cell.",     false, false),
-    NEW_ERROR (kJitErrorInvalidOperandSize,   "Invalid Operand Size was encountered.",                        false, false),
-    NEW_ERROR (kJitErrorPlugin,               "Problem with the plugin system.",                              false, false),
-    NEW_ERROR (kJitErrorConstantFolding,      "Constant folding failed due to unhandled case.",               false, false),
-    NEW_ERROR (kJitErrorCodegen,              "Undefined issues in trace formation.",                         false, false),
-END_ERRORS
-
-
-/**
- * @brief Tries to resolve errors from which we can recover.
- * @param cUnit Compilation unit
- * @param isLastAttempt whether the next compilation attempt will be the last one
- */
-inline void resolveErrors(CompilationUnit * cUnit, bool isLastAttempt) {
-    /* Handle any errors which can be handled */
-
-    //Handle error due to large jump offset
-    if ( IS_JIT_ERROR_SET(kJitErrorShortJumpOffset)) {
-        gDvmJit.disableOpt |= (1 << kShortJumpOffset);
-        ALOGI("JIT_INFO: Successfully resolved short jump offset issue");
-        //Clear the error:
-        CLEAR_JIT_ERROR(kJitErrorShortJumpOffset);
-    }
-
-    //Handle error due to spilling
-    if (IS_JIT_ERROR_SET(kJitErrorSpill)) {
-        //Clear the error:
-        CLEAR_JIT_ERROR(kJitErrorSpill);
-
-        //Ok we are going to see if we are registerizing something
-        int max = cUnit->maximumRegisterization;
-
-        // We should only get this error if maximum registerization is > 0
-        assert (max > 0);
-
-        // Divide it by 2, fastest way to get to 0 if we have issues across the board
-        // If it is a last attempt then force setting to 0
-        // It is better to compile instead of give a last try for registerization
-        int newMax = (isLastAttempt == true) ? 0 : max / 2;
-        cUnit->maximumRegisterization = newMax;
-        ALOGI("Trying less registerization from %d to %d", max, newMax);
-    }
-
-    //Handle error due to backend
-    if (IS_JIT_ERROR_SET(kJitErrorBERegisterization)) {
-        //If registerization in the Backend is on
-        if (gDvmJit.backEndRegisterization == true) {
-
-            //Turn off registerization
-            gDvmJit.backEndRegisterization = false;
-
-            //Set maximum registerization for this cUnit to 0
-            //since we disabled registerization
-            cUnit->maximumRegisterization = 0;
-
-            //Clear the error.
-            //Registerization can cause other errors
-            //Let's clear all errors for now, and see if they
-            //re-occur without registerization.
-            CLEAR_ALL_JIT_ERRORS();
-
-            //Notify about this special action
-            ALOGI("Ignoring other issues and retrying without backend registerization");
-        }
-    }
-
-    //Handle constant initialization failure
-    if (IS_JIT_ERROR_SET(kJitErrorConstInitFail) == true) {
-        gDvmJit.disableOpt |= (1 << kElimConstInitOpt);
-        ALOGI("Resolved error due to constant initialization failure");
-        //Clear the error:
-        CLEAR_JIT_ERROR(kJitErrorConstInitFail);
-    }
-}
-
-/**
- * @brief Checks whether a compilation should be re-attempted
- * Fixes anything which can be fixed. At the end we
- * either don't have an error, have fixed the error, or
- * cannot recover from the error. For the second case, we
- * retry the compilation.
- * @param cUnit Compilation Unit context
- * @param isLastAttempt whether the next compilation attempt will be the last one
- * @return whether we can retry the trace.
- */
-bool dvmCanFixErrorsAndRetry(CompilationUnit *cUnit, bool isLastAttempt){
-    // Make sure that the error information table is synced up with the
-    // error enum JitCompilationErrors
-    assert (NELEM(gErrorInformation) == static_cast<int>(kJitErrorMaxDefined));
-
-    // Checks if any error has occurred. If not, we do not need to retry.
-    if (IS_ANY_JIT_ERROR_SET() == false) {
-        return false;
-    }
-
-    bool firstError = true;
-
-    //Get the maximum error number the system is aware of so far:
-    int maxError = static_cast<int>(kJitErrorCodegen);
-
-    // Check which errors have been raised
-    for (int errorIndex = 0; errorIndex <= maxError; errorIndex++) {
-
-        //Get the error from the table
-        JitCompilationErrors error = gErrorInformation[errorIndex].type;
-
-        if (IS_JIT_ERROR_SET(error) == true) {
-
-            // If this is the first error we are seeing, print some information
-            // to be noticeable in the logs
-            if (firstError == true) {
-                ALOGD("++++++++++++++++++++++++++++++++++++++++++++");
-                ALOGD("JIT_INFO: Issues while compiling trace  %s%s, offset %d",
-                        cUnit->method->clazz->descriptor, cUnit->method->name,
-                        cUnit->traceDesc->trace[0].info.frag.startOffset);
-
-                // If kJitErrorCodegen is the first error we encounter,
-                // somebody forgot to raise an error flag somewhere.
-                // Otherwise, we should clear the flag because another
-                // non-generic message will be printed out.
-                if (error != kJitErrorCodegen) {
-                    CLEAR_JIT_ERROR(kJitErrorCodegen);
-                }
-
-                firstError = false;
-            }
-
-            // Paranoid. Make sure we actually found an entry
-            assert (errorIndex < NELEM(gErrorInformation));
-
-            // Find out if error is fatal
-            bool fatalError = gErrorInformation[errorIndex].isFatal;
-
-            // If we are set to abort on error and error cannot be resolved, then
-            // the error is fatal.
-            fatalError = (fatalError == true) ||
-                            (gDvmJit.abortOnCompilerError == true
-                                && gErrorInformation[errorIndex].canResolve == false);
-
-            // Print error message
-            if (fatalError == true) {
-                ALOGE("\t%s",
-                        gErrorInformation[errorIndex].errorMessage);
-                ALOGE("FATAL_ERRORS in JIT. Aborting compilation.");
-                dvmCompilerAbort(cUnit);
-            }
-            else {
-                ALOGD("\t%s",
-                        gErrorInformation[errorIndex].errorMessage);
-            }
-        }
-    }
-
-    // Handle any errors which can be resolved
-    resolveErrors(cUnit, isLastAttempt);
-
-    // If we have no errors set at this point, we have successfully resolved
-    // them and thus we can retry the trace now.
-    if (IS_ANY_JIT_ERROR_SET() == false){
-        ALOGD("JIT_INFO: Retrying trace %s%s, offset %d", cUnit->method->clazz->descriptor,
-                cUnit->method->name, cUnit->traceDesc->trace[0].info.frag.startOffset);
-        ALOGD("++++++++++++++++++++++++++++++++++++++++++++");
-        return true;
-    }
-
-    // Otherwise, error cannot be handled or does not have a handler
-    ALOGD("JIT_INFO: Terminating trace due to unresolved issues");
-    ALOGD("++++++++++++++++++++++++++++++++++++++++++++");
-
-    return false;
-}
-
-void dvmSaveOptimizationState(SErrorCompilationState &info) {
-    info.disableOpt = gDvmJit.disableOpt;
-    info.backEndRegisterization = gDvmJit.backEndRegisterization;
-}
-
-void dvmRestoreCompilationState(SErrorCompilationState &info) {
-    gDvmJit.disableOpt = info.disableOpt;
-    gDvmJit.backEndRegisterization = info.backEndRegisterization;
-}
diff --git a/vm/compiler/codegen/x86/CodegenErrors.h b/vm/compiler/codegen/x86/CodegenErrors.h
deleted file mode 100644
index 4301da0..0000000
--- a/vm/compiler/codegen/x86/CodegenErrors.h
+++ /dev/null
@@ -1,161 +0,0 @@
-/*
- * Copyright (C) 2012-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @enum JitCompilationErrors
- * @brief Possible errors which can happen during compilation
- * Values indicate bit position in DvmJitGlobals.jitErrorFlags + 1
- * IMPORTANT: Update jitErrorMessages when making changes to this list
- */
-
-#ifndef CODEGEN_X86_CODEGEN_ERRORS_H_
-#define CODEGEN_X86_CODEGEN_ERRORS_H_
-
-#include "Dalvik.h"
-#include "../../CompilerIR.h"
-
-/**
- * @brief Keep any information that can be changed by the error framework
- */
-typedef struct sErrorCompilationState {
-    int disableOpt;                 /**< @brief Disable the optimizations */
-    bool backEndRegisterization;    /**< @brief Backend registerization */
-}SErrorCompilationState;
-
-/**
- * @class ErrorFlags
- */
-enum JitCompilationErrors {
-    /** @brief Exceeded maximum allowed VRs in a basic block */
-    kJitErrorMaxVR = 0,
-    /** @brief 8-bit jump offset not enough to reach label */
-    kJitErrorShortJumpOffset,
-    /** @brief Trace contains a bytecode with no JIT implementation */
-    kJitErrorUnsupportedBytecode,
-    /** @brief Field ptr unresolved for SGET/SPUT bytecodes */
-    kJitErrorUnresolvedField,
-    /** @brief Cannot find BasicBlock_O1 corresponding to a BasicBlock */
-    kJitErrorInvalidBBId,
-    /** @brief JIT code cache is full */
-    kJitErrorCodeCacheFull,
-    /** @brief Failures while allocating registers or error
-     *  in locating / putting registers in register tables
-     */
-    kJitErrorRegAllocFailed,
-    /** @brief Malloc failed. */
-    kJitErrorMallocFailed,
-    /** @brief Exceeded maximum number of transfer points per BB */
-    kJitErrorMaxXferPoints,
-    /** @brief Exceeded number of destination regs for a source reg */
-    kJitErrorMaxDestRegPerSource,
-    /** @brief Problem with state transfer in JIT */
-    kJitErrorStateTransfer,
-    /** @brief General trace formation issues */
-    kJitErrorTraceFormation,
-    /** @brief Errors while performing Null and Bound checks */
-    kJitErrorNullBoundCheckFailed,
-    /** @brief Errors while merging LiveRanges */
-    kJitErrorMergeLiveRange,
-    /** @brief Errors while accessing global data */
-    kJitErrorGlobalData,
-    /** @brief Errors while scheduling instructions */
-    kJitErrorInsScheduling,
-    /** @brief Errors due to backend registerization */
-    kJitErrorBERegisterization,
-    /** @brief Errors due to spilling logical registers */
-    kJitErrorSpill,
-    /** @brief Set when a basic block is reject by backend */
-    kJitErrorBBCannotBeHandled,
-    /** @brief Errors while performing double/long constant initialization */
-    kJitErrorConstInitFail,
-    /** @brief Error while generating chaining cell */
-    kJitErrorChainingCell,
-    /** @brief Invalid operand size */
-    kJitErrorInvalidOperandSize,
-    /** @brief Problem with the plugin system */
-    kJitErrorPlugin,
-    /** @brief Unhandled case during constant folding */
-    kJitErrorConstantFolding,
-
-    /* ----- Add more errors above ---------------------------
-     * ----- Don't add new errors beyond this point ----------
-     * When adding more errors, update error information table
-     * in CodegenErrors.cpp
-     */
-
-    /** @brief Indicates "some" error happened
-     * Specifically, the purpose is that if someone forgets
-     * to use SET_JIT_ERROR at the specific error location,
-     * but does throw a return, the function handling that
-     * return can set this generic error. Also useful if a
-     * function can set multiple errors, the calling function
-     * won't have to worry about which one to set. Hopefully
-     * all of the errors have been individually set too.
-     * THIS NEEDS TO BE THE SECOND LAST VALUE
-     */
-    kJitErrorCodegen,
-    /** @brief Guarding value
-     * THIS NEEDS TO BE THE LAST VALUE
-     */
-    kJitErrorMaxDefined
-};
-
-/*
- * Getter / Setter for  JitCompilationErrors
- */
-/* Sets a particular error */
-#define SET_JIT_ERROR(jit_error)  gDvmJit.jitErrorFlags |= (1 << (int) jit_error);
-
-/* Clears only a particular error */
-#define CLEAR_JIT_ERROR(jit_error) gDvmJit.jitErrorFlags &= ~(1 << (int) jit_error);                                                                                    \
-
-/* Checks if a particular error is set */
-#define IS_JIT_ERROR_SET(jit_error) ( (gDvmJit.jitErrorFlags & (1 << (int) jit_error)) != 0 )
-
-/* Clears all errors */
-#define CLEAR_ALL_JIT_ERRORS() gDvmJit.jitErrorFlags = 0;                                                                                  \
-
-/* Checks if ANY error is set */
-#define IS_ANY_JIT_ERROR_SET() (gDvmJit.jitErrorFlags != 0)
-
-/* Max retries limits the number of retries for a given trace, used in CodegenInterface.cpp */
-#define MAX_RETRIES 2
-
-/**
- * @brief Checks whether a compilation should be re-attempted
- * Fixes anything which can be fixed. At the end we
- * either don't have an error, have fixed the error, or
- * cannot recover from the error. For the second case, we
- * retry the compilation.
- * @param cUnit Compilation Unit context
- * @param isLastAttempt whether the next compilation attempt will be the last one
- * @return whether we can retry the trace.
- */
-bool dvmCanFixErrorsAndRetry(CompilationUnit *cUnit, bool isLastAttempt);
-
-/**
- * @brief Save the error flags that can be changed by dvmCanFixErrorsAndRetry
- * @param info the information relating to flags that can be changed by the framework (updated by the function)
- */
-void dvmSaveOptimizationState(SErrorCompilationState &info);
-
-/**
- * @brief Restore compilation state
- * @param info the state we want to reset to
- */
-void dvmRestoreCompilationState(SErrorCompilationState &info);
-
-#endif
diff --git a/vm/compiler/codegen/x86/CompilationUnit.cpp b/vm/compiler/codegen/x86/CompilationUnit.cpp
deleted file mode 100644
index 5faea03..0000000
--- a/vm/compiler/codegen/x86/CompilationUnit.cpp
+++ /dev/null
@@ -1,45 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "CompilationUnit.h"
-
-bool CompilationUnit_O1::getCanSpillRegister (int reg)
-{
-    //Check overflow first
-    if (reg < 0 || reg >= PhysicalReg_Null)
-    {
-        return false;
-    }
-
-    //Otherwise, use what is in the array
-    return canSpillRegister[reg];
-}
-
-bool CompilationUnit_O1::setCanSpillRegister (int reg, bool value)
-{
-    //Check overflow first
-    if (reg < 0 || reg >= PhysicalReg_Null)
-    {
-        //Cannot update it
-        return false;
-    }
-
-    //Otherwise, use what is in the array
-    canSpillRegister[reg] = value;
-
-    //Update succeeded
-    return true;
-}
diff --git a/vm/compiler/codegen/x86/CompilationUnit.h b/vm/compiler/codegen/x86/CompilationUnit.h
deleted file mode 100644
index 6e93dc0..0000000
--- a/vm/compiler/codegen/x86/CompilationUnit.h
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef H_COMPILATIONUNIT
-#define H_COMPILATIONUNIT
-
-#include "Lower.h"
-#include "compiler/CompilerIR.h"
-
-class CompilationUnit_O1: public CompilationUnit
-{
-    protected:
-        /** @brief Physical registers that should not be spilled */
-        bool canSpillRegister[PhysicalReg_Null];
-
-    public:
-        /**
-         * @brief Can we spill a register?
-         * @param reg the register we care about
-         * @return true if reg can be spilled, false if outside of the range of the array or should not spill
-         */
-        bool getCanSpillRegister (int reg);
-
-        /**
-         * @brief Set whether we can spill a register? Does nothing if reg would overflow the array
-         * @param reg the register we care about
-         * @param value if we should spill or not
-         * @return whether the update was successful
-         */
-        bool setCanSpillRegister (int reg, bool value);
-
-        void resetCanSpillRegisters (void)
-        {
-            for(int k = 0; k < PhysicalReg_Null; k++) {
-                canSpillRegister[k] = true;
-            }
-        }
-};
-
-#endif
diff --git a/vm/compiler/codegen/x86/CompileTable.cpp b/vm/compiler/codegen/x86/CompileTable.cpp
deleted file mode 100644
index 341f670..0000000
--- a/vm/compiler/codegen/x86/CompileTable.cpp
+++ /dev/null
@@ -1,99 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "CompileTable.h"
-#include <algorithm>
-
-void CompileTableEntry::reset (void)
-{
-    //We do not reset regNum and physicalType because those uniquely represent an entry.
-    //If we reset those we would be creating an invalid CompileTableEntry so we do not.
-
-    //Initialize size based on physical type
-    size = getRegSize (physicalType);
-
-    //Reset physical register to null
-    physicalReg = PhysicalReg_Null;
-
-    //Unknown number of references
-    refCount = 0;
-
-    //If temporary, we don't know the VR it represents
-    linkageToVR = -1;
-
-    //We have not spilled this entry so no spill index
-    spill_loc_index = -1;
-
-    //We have not written to this
-    isWritten = false;
-}
-
-bool CompileTableEntry::rememberState (int stateNum)
-{
-    RegisterState newState;
-
-    newState.physicalReg = physicalReg;
-    newState.spill_loc_index = spill_loc_index;
-
-    state[stateNum] = newState;
-    return true;
-}
-
-bool CompileTableEntry::goToState (int stateNum)
-{
-    //Look to see if we have the state requested
-    std::map<int, RegisterState>::const_iterator stateIter = state.find (stateNum);
-
-    if (stateIter == state.end ())
-    {
-        //We do not have the state and therefore we cannot go to it. Fail now.
-        return false;
-    }
-
-    //Now load data from state
-    physicalReg = state[stateNum].physicalReg;
-    spill_loc_index = state[stateNum].spill_loc_index;
-
-    return true;
-}
-
-CompileTable::iterator CompileTable::find (int regNum, int physicalType)
-{
-    CompileTableEntry lookupEntry (regNum, physicalType);
-
-    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
-}
-
-CompileTable::const_iterator CompileTable::find (int regNum, int physicalType) const
-{
-    CompileTableEntry lookupEntry (regNum, physicalType);
-
-    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
-}
-
-CompileTable::iterator CompileTable::findVirtualRegister (int regNum, LowOpndRegType physicalType)
-{
-    CompileTableEntry lookupEntry (regNum, LowOpndRegType_virtual | physicalType);
-
-    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
-}
-
-CompileTable::const_iterator CompileTable::findVirtualRegister (int regNum, LowOpndRegType physicalType) const
-{
-    CompileTableEntry lookupEntry (regNum, LowOpndRegType_virtual | physicalType);
-
-    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
-}
diff --git a/vm/compiler/codegen/x86/CompileTable.h b/vm/compiler/codegen/x86/CompileTable.h
deleted file mode 100644
index 2465b8e..0000000
--- a/vm/compiler/codegen/x86/CompileTable.h
+++ /dev/null
@@ -1,502 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef COMPILETABLE_H_
-#define COMPILETABLE_H_
-
-#include <vector>
-#include <map>
-#include "AnalysisO1.h"
-
-/**
- * @brief Represents an entry to the compilation table, helping the compiler follow what register is where.
- * @details The pair <regNum, physicalType> uniquely determines a variable
- */
-class CompileTableEntry {
-public:
-    /**
-     * @brief Constructor which initializes an entry with its register number and type.
-     * @param regNum The register number: vr number, temp number, or hardcoded register number.
-     * @param physicalType the LowOpndRegType for this register. Should reflect both its physical
-     * type and its logical type.
-     */
-    CompileTableEntry (int regNum, int physicalType) :
-            regNum (regNum), physicalType (physicalType), physicalReg (PhysicalReg_Null),
-            refCount(0), spill_loc_index (-1), isWritten (false), linkageToVR (0)
-    {
-        logicalType = static_cast<LogicalRegType> (physicalType & ~MASK_FOR_TYPE);
-        size = getRegSize (physicalType);
-    }
-
-    /**
-     * @brief Constructor which initializes an entry with its register number, its logical type, and
-     * its physical type.
-     * @param regNum The register number: vr number, temp number, or hardcoded register number.
-     * @param physicalType The physical type for this register.
-     * @param logicalType The logical type for this register.
-     */
-    CompileTableEntry (int regNum, LowOpndRegType physicalType, LogicalRegType logicalType) :
-            regNum (regNum), physicalReg (PhysicalReg_Null), refCount(0), spill_loc_index (-1),
-            isWritten (false), logicalType (logicalType), linkageToVR (0)
-    {
-        this->physicalType = logicalType | physicalType;
-        size = getRegSize (physicalType);
-    }
-
-    /**
-     * @brief Constructs a compile table entry which represents a virtual register.
-     * @param vrInfo The virtual register info to use for initialization.
-     */
-    CompileTableEntry (const VirtualRegInfo &vrInfo) :
-            regNum (vrInfo.regNum), physicalType (LowOpndRegType_virtual | vrInfo.physicalType),
-            physicalReg (PhysicalReg_Null), refCount(vrInfo.refCount), spill_loc_index (-1),
-            isWritten (false), logicalType (LowOpndRegType_virtual), linkageToVR (0)
-    {
-        size = getRegSize (vrInfo.physicalType);
-    }
-
-    /**
-     * @brief Constructs a compile table entry which represents a temporary.
-     * @param tempInfo The temporary info to use for initialization.
-     */
-    CompileTableEntry (const TempRegInfo &tempInfo) :
-            regNum (tempInfo.regNum), physicalType (tempInfo.physicalType),
-            physicalReg (PhysicalReg_Null), refCount(tempInfo.refCount),
-            spill_loc_index (-1), isWritten (false), linkageToVR (tempInfo.linkageToVR)
-    {
-        logicalType = static_cast<LogicalRegType> (physicalType & ~MASK_FOR_TYPE);
-        size = getRegSize (tempInfo.physicalType);
-    }
-
-    /**
-     * @brief Destructor.
-     */
-    ~CompileTableEntry (void)
-    {
-        reset ();
-    }
-
-    /**
-     * @brief Get the register number.
-     * @return Returns the register number for this entry.
-     */
-    int getRegisterNumber (void) const
-    {
-        return regNum;
-    }
-
-    /**
-     * @brief Used to get the physical type for this entry.
-     * @details This returns only type of physical register usable for this entry.
-     * @return Returns the physical type for this entry
-     */
-    LowOpndRegType getPhysicalType (void) const
-    {
-        return static_cast<LowOpndRegType> (physicalType & MASK_FOR_TYPE);
-    }
-
-    /**
-     * @brief Used to get the logical type.
-     * @return Returns the logical type for the entry.
-     */
-    LogicalRegType getLogicalType (void) const
-    {
-        return logicalType;
-    }
-
-    /**
-     * @brief Used to get an integer whose low 3 bits represent the physical type and the high bits
-     * represent the logical type.
-     * @return Returns the representation of logical and physical types.
-     */
-    int getLogicalAndPhysicalTypes (void) const
-    {
-        //For now the physical type field holds both the logical and physical types so we return that
-        return physicalType;
-    }
-
-    /**
-     * @brief Used to get the physical register.
-     * @return Returns the physical register used for this entry. If no register is used, it returns
-     * PhysicalReg_Null.
-     */
-    PhysicalReg getPhysicalReg (void) const
-    {
-        return static_cast<PhysicalReg> (physicalReg);
-    }
-
-    /**
-     * @brief Used to get the size of this entry which depends on the physical type.
-     * @return Returns the size of the physical type for this entry.
-     */
-    OpndSize getSize (void) const
-    {
-        return size;
-    }
-
-    /**
-     * @brief Sets a new physical register for this entry.
-     * @param newReg The new physical register.
-     */
-    void setPhysicalReg (PhysicalReg newReg)
-    {
-        setPhysicalReg (static_cast<int> (newReg));
-    }
-
-    /**
-     * @brief Sets a new physical register for this entry.
-     * @param newReg The new physical register.
-     */
-    void setPhysicalReg (int newReg)
-    {
-        // It doesn't make sense to set physical register to a non-existent register.
-        // Thus we have this check here for sanity.
-        assert (newReg <= PhysicalReg_Null);
-        physicalReg = newReg;
-    }
-
-    /**
-     * @brief Updates the reference count for this entry.
-     * @param newCount The reference count to set.
-     */
-    void updateRefCount (int newCount)
-    {
-        refCount = newCount;
-    }
-
-    /**
-     * @brief Used to reset the spilled location of temporary thus marking it as non-spilled.
-     */
-    void resetSpillLocation ()
-    {
-        spill_loc_index = -1;
-    }
-
-    /**
-     * @brief Checks if entry is in a physical register.
-     * @return Returns whether this entry is in a physical register.
-     */
-    bool inPhysicalRegister (void) const
-    {
-        return physicalReg != PhysicalReg_Null;
-    }
-
-    /**
-     * @brief Checks if entry is in a general purpose register.
-     * @return Returns whether this entry is in a general purpose register.
-     */
-    bool inGeneralPurposeRegister (void) const
-    {
-        return (physicalReg >= PhysicalReg_StartOfGPMarker && physicalReg <= PhysicalReg_EndOfGPMarker);
-    }
-
-    /**
-     * @brief Checks if entry is in an xmm register.
-     * @return Returns whether this entry is in an xmm register.
-     */
-    bool inXMMRegister (void) const
-    {
-        return (physicalReg >= PhysicalReg_StartOfXmmMarker && physicalReg <= PhysicalReg_EndOfXmmMarker);
-    }
-
-    /**
-     * @brief Checks if entry is in an X87 register.
-     * @return Returns whether this entry is in an X87 register.
-     */
-    bool inX87Register (void) const
-    {
-        return (physicalReg >= PhysicalReg_StartOfX87Marker && physicalReg <= PhysicalReg_EndOfX87Marker);
-    }
-
-    /**
-     * @brief Checks whether logical type represents a virtual register.
-     * @return Returns whether this entry represent a virtual register.
-     */
-    bool isVirtualReg (void) const
-    {
-        return ((physicalType & LowOpndRegType_virtual) != 0);
-    }
-
-    /**
-     * @brief Checks if this is a backend temporary used during bytecode generation.
-     * @return Returns whether this entry represent a backend temporary.
-     */
-    bool isTemporary (void) const
-    {
-        return (isVirtualReg () == false);
-    }
-
-    /**
-     * @brief Links a temporary to a corresponding virtual register.
-     * @param vR The virtual register number.
-     */
-    void linkToVR (int vR)
-    {
-        assert (isTemporary () == true);
-        linkageToVR = vR;
-    }
-
-    /**
-     * @brief Given that the entry is a temporary, it returns the virtual register it is linked to.
-     * @return Returns corresponding virtual register.
-     */
-    int getLinkedVR (void) const
-    {
-        assert (isTemporary () == true);
-        return linkageToVR;
-    }
-
-    /**
-     * @brief Resets properties of compile entry to default values. Does not reset the type and register represented
-     * by this compile entry.
-     */
-    void reset (void);
-
-    /**
-     * @brief Equality operator for checking equivalence.
-     * @details The pair <regNum, physicalType> uniquely determines a variable.
-     * @param other The compile table entry to compare to
-     * @return Returns true if the two entries are equivalent.
-     */
-    bool operator== (const CompileTableEntry& other) const
-    {
-        if (regNum == other.regNum && physicalType == other.physicalType)
-        {
-            return true;
-        }
-        else
-        {
-            return false;
-        }
-    }
-
-    /**
-     * @brief For a given state number it remembers some properties about the compile entry.
-     * @param stateNum The state number to associate current state with.
-     * @return Returns true if it successfully remembered state.
-     */
-    bool rememberState (int stateNum);
-
-    /**
-     * @brief Updates the current state of the compile entry to match the state we are interested in.
-     * @param stateNum The state number to look at for updating self state.
-     * @return Returns true if it successfully changed to the other state.
-     */
-    bool goToState (int stateNum);
-
-    /**
-     * @brief Provides physical register for an entry for a specific state.
-     * @param stateNum The state to look at.
-     * @return Returns the physical register for that state.
-     */
-    int getStatePhysicalRegister (int stateNum)
-    {
-        return state[stateNum].physicalReg;
-    }
-
-    /**
-     * @brief Provides spill location for an entry for a specific state.
-     * @param stateNum The state to look at.
-     * @return Returns the spill location for that state.
-     */
-    int getStateSpillLocation (int stateNum)
-    {
-        return state[stateNum].spill_loc_index;
-    }
-
-    int regNum;               /**< @brief The register number */
-
-    /**
-     * @brief This field holds BOTH physical type (like XMM register) and the logical type (like virtual register)
-     * @details The low 7 bits hold LowOpndRegType and the rest of bits hold LogicalRegType
-     */
-    int physicalType;
-
-    int physicalReg;          /**< @brief Which physical register was chosen */
-
-    int refCount;             /**< @brief Number of reference counts for the entry */
-
-    int spill_loc_index;      /**< @brief what is the spill location index (for temporary registers only) */
-    bool isWritten;           /**< @brief is the entry written */
-
-private:
-    /**
-     * @brief Used to save the sate of register allocator.
-     */
-    struct RegisterState
-    {
-        int spill_loc_index;
-        int physicalReg;
-    };
-
-    /**
-     * @brief Keeps track of the register state for state number.
-     */
-    std::map<int, RegisterState> state;
-
-    LogicalRegType logicalType;  /**< @brief The logical type for this entry */
-    OpndSize size;               /**< @brief Used to keep track of size of entry */
-    int linkageToVR;             /**< @brief Linked to which VR, for temporary registers only */
-};
-
-class CompileTable
-{
-public:
-    /**
-     * @brief Used to access an element of the compile table.
-     * @details If index matches the key of an element in the container, the function returns a reference to its mapped
-     * value. If index does not match the key of any element in the container, the function inserts a new element with
-     * that key and returns a reference to its mapped value.
-     * @param index The index of the entry we want to access.
-     * @return Returns a reference to the mapped value of the element with a key value equivalent to index.
-     */
-    CompileTableEntry& operator[] (size_t index)
-    {
-        return compileTable[index];
-    }
-
-    /**
-     * @brief Used to access an element of the compile table in a constant fashion.
-     * @details If index matches the key of an element in the container, the function returns a reference to its mapped
-     * value. If index does not match the key of any element in the container, the function inserts a new element with
-     * that key and returns a reference to its mapped value.
-     * @param index The index of the entry we want to access.
-     * @return Returns a reference to the mapped value of the element with a key value equivalent to index.
-     */
-    const CompileTableEntry& operator[] (size_t index) const
-    {
-        return compileTable[index];
-    }
-
-    /**
-     * @brief Random access const iterator. This does not modify structure it is iterating.
-     */
-    typedef std::vector<CompileTableEntry>::const_iterator const_iterator;
-
-    /**
-     * @brief Random access iterator. This may modify structure it is iterating.
-     */
-    typedef std::vector<CompileTableEntry>::iterator iterator;
-
-    /**
-     * @brief Returns an iterator pointing to the first compile entry.
-     * @return iterator to beginning
-     */
-    iterator begin (void)
-    {
-        return compileTable.begin ();
-    }
-
-    /**
-     * @brief Returns a const iterator pointing to the first compile entry.
-     * @return iterator to beginning
-     */
-    const_iterator begin (void) const
-    {
-        return compileTable.begin ();
-    }
-
-    /**
-     * @brief Returns an iterator referring to the past-the-end compile entry.
-     * @return iterator past end
-     */
-    iterator end (void)
-    {
-        return compileTable.end ();
-    }
-
-    /**
-     * @brief Returns a const iterator referring to the past-the-end compile entry.
-     * @return iterator past end
-     */
-    const_iterator end (void) const
-    {
-        return compileTable.end ();
-    }
-
-    /**
-     * @brief Used to get an iterator pointing to the entry matching number and type.
-     * @param regNum The register number (can be temp, virtual, or hardcoded)
-     * @param physicalType The physical type and logical type representing the entry.
-     * @return Returns iterator pointing to the desired entry. If one is not found, it
-     * returns the past-the-end iterator.
-     */
-    iterator find (int regNum, int physicalType);
-
-    /**
-     * @brief Used to get a const iterator pointing to the entry matching number and type.
-     * @param regNum The register number (can be temp, virtual, or hardcoded)
-     * @param physicalType The physical type and logical type representing the entry.
-     * @return Returns const iterator pointing to the desired entry. If one is not found, it
-     * returns the past-the-end const iterator.
-     */
-    const_iterator find (int regNum, int physicalType) const;
-
-    /**
-     * @brief Used to get an iterator pointing to the virtual register whose physical type matches.
-     * @param regNum The virtual register number.
-     * @param physicalType The physical type of the virtual register.
-     * @return Returns iterator pointing to the desired entry. If one is not found, it
-     * returns the past-the-end iterator.
-     */
-    iterator findVirtualRegister (int regNum, LowOpndRegType physicalType);
-
-    /**
-     * @brief Used to get a const iterator pointing to the virtual register whose physical type matches.
-     * @param regNum The virtual register number.
-     * @param physicalType The physical type of the virtual register.
-     * @return Returns const iterator pointing to the desired entry. If one is not found, it
-     * returns the past-the-end const iterator.
-     */
-    const_iterator findVirtualRegister (int regNum, LowOpndRegType physicalType) const;
-
-    /**
-     * @brief Used to get size of compile table.
-     * @return Returns the size of the compile table.
-     */
-    int size (void) const
-    {
-        return compileTable.size ();
-    }
-
-    /**
-     * @brief Used to insert a new entry into the compile table.
-     * @param newEntry The compile table entry to insert into the table.
-     */
-    void insert (const CompileTableEntry &newEntry)
-    {
-        compileTable.push_back (newEntry);
-    }
-
-    /**
-     * @brief Used to clear the compile table.
-     */
-    void clear (void)
-    {
-        compileTable.clear ();
-    }
-
-private:
-    /**
-     * @brief Used to keep track of the entries in the compile table.
-     * @todo Ideally this should be a set or a map so that lookup is fast.
-     */
-    std::vector<CompileTableEntry> compileTable;
-};
-
-extern CompileTable compileTable;
-
-#endif /* COMPILETABLE_H_ */
diff --git a/vm/compiler/codegen/x86/ExceptionHandling.cpp b/vm/compiler/codegen/x86/ExceptionHandling.cpp
deleted file mode 100644
index 4c7ca3c..0000000
--- a/vm/compiler/codegen/x86/ExceptionHandling.cpp
+++ /dev/null
@@ -1,149 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @file ExceptionHandling.cpp
- * @brief Implements interfaces and utilities used for managed exception handling.
- */
-
-#include "Lower.h"
-#include "ExceptionHandling.h"
-#include "NcgAot.h"
-#include "Scheduler.h"
-#include "Singleton.h"
-
-ExceptionHandlingRestoreState::ExceptionHandlingRestoreState(void) :
-        uniqueStreamId(0), lastLabelGenerated(NULL) {
-    // For now, there's nothing else we need to do in constructor
-}
-
-ExceptionHandlingRestoreState::~ExceptionHandlingRestoreState(void) {
-    this->reset();
-}
-
-void ExceptionHandlingRestoreState::reset(void) {
-    this->streams.clear();
-    this->targets.clear();
-    this->lastLabelGenerated = NULL;
-    this->uniqueStreamId = 0;
-
-    // We must free the labels we inserted
-    freeShortMap();
-}
-
-char * ExceptionHandlingRestoreState::getUniqueLabel(void) {
-    // Allocate a label
-    char * label = static_cast<char *>(dvmCompilerNew(LABEL_SIZE, false));
-
-    // Give it a unique name
-    snprintf(label, LABEL_SIZE, "exception_restore_state_%d",
-            this->uniqueStreamId);
-
-    // Ensure future ids will be unique
-    this->uniqueStreamId++;
-
-    // Save label generated
-    this->lastLabelGenerated = label;
-
-    return label;
-}
-
-void ExceptionHandlingRestoreState::createExceptionHandlingStream(
-        char * beginningOfStream, char * endOfStream,
-        const char * targetLabel) {
-    // Just converting some pointers to unsigned ints to do some math.
-    unsigned int beginning = reinterpret_cast<unsigned int>(beginningOfStream);
-    unsigned int end = reinterpret_cast<unsigned int>(endOfStream);
-    size_t lengthOfTargetLabel = strlen(targetLabel);
-
-    // Developer needs to ensure that this doesn't happen
-    assert(end >= beginning);
-
-    // Calculate size of exception handling instructions
-    size_t sizeOfStream = end - beginning;
-
-    // Create the new stream using compiler arena
-    char * newStream = static_cast<char *>(dvmCompilerNew(sizeOfStream, false));
-
-    // Copy instructions to the new stream
-    memcpy(newStream, beginningOfStream, sizeOfStream);
-
-    // Reset stream pointer now
-    stream = beginningOfStream;
-
-    // Add new stream to list of exception handling stream
-    this->streams.push_back(std::make_pair(newStream, sizeOfStream));
-
-    // Create a copy of the targetLabel because we cannot assume it won't be destroyed
-    // before we use it. Ensure that it can fit the entire old label, that is all zeros
-    // on allocation, and that it has room for the terminating null.
-    char * targetLabelCopy = static_cast<char *>(dvmCompilerNew(
-            lengthOfTargetLabel + 1, true));
-
-    // Copy string to our label copy. Allocation already ensures null termination
-    strncpy(targetLabelCopy, targetLabel, lengthOfTargetLabel);
-
-    // Save the name of own label and name of target label so we know
-    // where to generate jump to
-    this->targets.push_back(std::make_pair(this->lastLabelGenerated, targetLabelCopy));
-}
-
-void ExceptionHandlingRestoreState::dumpAllExceptionHandlingRestoreState(void) {
-    // Flush scheduler queue before copying to stream
-    if (gDvmJit.scheduling)
-        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
-
-    // Go through each saved restore state
-    for (unsigned int i = 0; i < this->streams.size(); ++i) {
-        size_t sizeOfExceptionRestore = this->streams[i].second;
-
-        // Ensure that we won't overfill the code cache
-        if (gDvmJit.codeCacheByteUsed + (stream - streamStart)
-                + sizeOfExceptionRestore + CODE_CACHE_PADDING
-                > gDvmJit.codeCacheSize) {
-            gDvmJit.codeCacheFull = true;
-            ALOGI("JIT_INFO: Code cache full while dumping exception handling restore state");
-            SET_JIT_ERROR(kJitErrorCodeCacheFull);
-            this->reset();
-            return;
-        }
-
-        char * label = this->targets[i].first;
-        char * targetLabel = this->targets[i].second;
-
-        // JIT verbosity
-        if (dump_x86_inst)
-            ALOGD("LOWER %s @%p", label, stream);
-
-        // Insert label exception_restore_state_# where # is the unique identifier
-        if (insertLabel(label, true) == -1) {
-            this->reset();
-            return;
-        }
-
-        // Copy to instruction stream
-        memcpy(stream, this->streams[i].first, sizeOfExceptionRestore);
-
-        // After the copy, we still need to update stream pointer
-        stream = stream + sizeOfExceptionRestore;
-
-        // Jump to the target error label
-        unconditional_jump_global_API(targetLabel, false);
-    }
-
-    // Since we dumped to code stream, we can clear out the data structures
-    this->reset();
-}
diff --git a/vm/compiler/codegen/x86/ExceptionHandling.h b/vm/compiler/codegen/x86/ExceptionHandling.h
deleted file mode 100644
index dafc750..0000000
--- a/vm/compiler/codegen/x86/ExceptionHandling.h
+++ /dev/null
@@ -1,107 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @file ExceptionHandling.h
- * @brief Defines interfaces and implements utilities used for managed exception handling.
- */
-
-#include <vector>
-#include <utility> // for std::pair
-
-/**
- * @brief Used to defer committing instructions for exception handling
- * restore state before punting to interpreter or common exception handler
- */
-class ExceptionHandlingRestoreState {
-private:
-    /**
-     * @brief List of streams created for exception handling restore state
-     * along with their sizes.
-     * @details This is a list of stream pointers and their corresponding sizes.
-     */
-    std::vector<std::pair<char *, size_t> > streams;
-
-    /**
-     * @brief For each exception handling stream, contains a pair of stream's
-     * name and its target.
-     */
-    std::vector<std::pair<char *, char *> > targets;
-
-    /**
-     * @brief Counter to ensure some uniqueness for labels generated.
-     */
-    unsigned int uniqueStreamId;
-
-    /**
-     * @brief Keeps track of last label generated.
-     */
-    char * lastLabelGenerated;
-
-    // Declare the copy constructor and the equal operator as private to
-    // prevent copying
-    ExceptionHandlingRestoreState(ExceptionHandlingRestoreState const&);
-    void operator=(ExceptionHandlingRestoreState const&);
-
-public:
-    /**
-     * @brief Default constructor
-     */
-    ExceptionHandlingRestoreState(void);
-
-    /**
-     * @brief Default destructor
-     */
-    ~ExceptionHandlingRestoreState(void);
-
-    /**
-     * @brief Generates a label which will be used to tag
-     * exception handling restore state.
-     * @details Does not guarantee uniqueness across instances of this
-     * class. Does not guarantee uniqueness after
-     * dumpAllExceptionHandlingRestoreState is called.
-     * @return label for exception handling restore state
-     */
-    char * getUniqueLabel(void);
-
-    /**
-     * @brief Creates stream for exception handling and copies all
-     * instructions for the restore state to this stream.
-     * @details It uses the last label generated as the label for this
-     * stream. It resets stream pointer to be beginningOfStream.
-     * @param beginningOfStream Pointer to stream before exception handling
-     * restore state was dumped.
-     * @param endOfStream Pointer to stream after exception handling
-     * restore state was dumped.
-     * @param targetLabel label of target error
-     */
-    void createExceptionHandlingStream(char * beginningOfStream,
-            char * endOfStream, const char * targetLabel);
-
-    /**
-     * @brief Copies all of the buffered exception handling restore states
-     * to the instruction stream.
-     * @details After dumping each of the exception handling restore states to the
-     * stream, it generates a jump to the error name label (which ends up punting
-     * to interpreter).
-     */
-    void dumpAllExceptionHandlingRestoreState(void);
-
-    /**
-     * @brief Resets state of instance.
-     */
-    void reset(void);
-};
diff --git a/vm/compiler/codegen/x86/InstructionGeneration.cpp b/vm/compiler/codegen/x86/InstructionGeneration.cpp
deleted file mode 100644
index c6edaad..0000000
--- a/vm/compiler/codegen/x86/InstructionGeneration.cpp
+++ /dev/null
@@ -1,344 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "Dalvik.h"
-#include "InstructionGeneration.h"
-#include "libdex/DexOpcodes.h"
-#include "compiler/Compiler.h"
-#include "compiler/CompilerIR.h"
-#include "interp/Jit.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "NcgAot.h"
-#include "compiler/codegen/CompilerCodegen.h"
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-/**
- * @brief Generate a Null check
- * @param cUnit the CompilationUnit
- * @param mir the MIR instruction
- */
-void genHoistedNullCheck (CompilationUnit *cUnit, MIR *mir)
-{
-    /*
-     * NOTE: these synthesized blocks don't have ssa names assigned
-     * for Dalvik registers.  However, because they dominate the following
-     * blocks we can simply use the Dalvik name w/ subscript 0 as the
-     * ssa name.
-     */
-    /* Assign array in virtual register to P_GPR_1 */
-    get_virtual_reg(mir->dalvikInsn.vA, OpndSize_32, P_GPR_1, true);
-    export_pc();
-    compare_imm_reg(OpndSize_32, 0, P_GPR_1, true);
-    condJumpToBasicBlock(stream, Condition_E, cUnit->exceptionBlockId);
-}
-
-/**
- * @brief Generate a Bound check:
-      vA arrayReg
-      arg[0] -> determines whether it is a constant or a register
-      arg[1] -> register or constant
-
-      is idx < 0 || idx >= array.length ?
- * @param cUnit the CompilationUnit
- * @param mir the MIR instruction
- */
-void genHoistedBoundCheck (CompilationUnit *cUnit, MIR *mir)
-{
-    /* Assign array in virtual register to P_GPR_1 */
-    get_virtual_reg(mir->dalvikInsn.vA, OpndSize_32, P_GPR_1, true);
-
-    if (mir->dalvikInsn.arg[0] == MIR_BOUND_CHECK_REG)
-    {
-        //Ok let us get the index in P_GPR_2
-        get_virtual_reg(mir->dalvikInsn.arg[1], OpndSize_32, P_GPR_2, true);
-    }
-    else
-    {
-        //Move the constant to P_GPR_2
-        move_imm_to_reg(OpndSize_32, mir->dalvikInsn.arg[1], P_GPR_2, true);
-    }
-    export_pc();
-
-    //Compare array length with index value
-    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), P_GPR_1, true, P_GPR_2, true);
-    //Jump to exception block if array.length <= index
-    condJumpToBasicBlock(stream, Condition_LE, cUnit->exceptionBlockId);
-
-    //Now, compare to 0
-    compare_imm_reg(OpndSize_32, 0, P_GPR_2, true);
-    //Jump to exception if index < 0
-    condJumpToBasicBlock(stream, Condition_L, cUnit->exceptionBlockId);
-}
-
-//use O0 code generator for hoisted checks outside of the loop
-/*
- * vA = arrayReg;
- * vB = idxReg;
- * vC = endConditionReg;
- * arg[0] = maxC
- * arg[1] = minC
- * arg[2] = loopBranchConditionCode
- */
-void genHoistedChecksForCountUpLoop(CompilationUnit *cUnit, MIR *mir)
-{
-    /*
-     * NOTE: these synthesized blocks don't have ssa names assigned
-     * for Dalvik registers.  However, because they dominate the following
-     * blocks we can simply use the Dalvik name w/ subscript 0 as the
-     * ssa name.
-     */
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    const int maxC = dInsn->arg[0];
-
-    //First do the null check
-    genHoistedNullCheck (cUnit, mir);
-
-    /* assign index in virtual register to P_GPR_2 */
-    get_virtual_reg(mir->dalvikInsn.vC, OpndSize_32, P_GPR_2, true);
-
-    int delta = maxC;
-    /*
-     * If the loop end condition is ">=" instead of ">", then the largest value
-     * of the index is "endCondition - 1".
-     */
-    if (dInsn->arg[2] == OP_IF_GE) {
-        delta--;
-    }
-
-    if (delta < 0) { //+delta
-        //if P_GPR_2 is mapped to a VR, we can't do this
-        alu_binary_imm_reg(OpndSize_32, sub_opc, -delta, P_GPR_2, true);
-    } else if(delta > 0) {
-        alu_binary_imm_reg(OpndSize_32, add_opc, delta, P_GPR_2, true);
-    }
-    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), P_GPR_1, true, P_GPR_2, true);
-    condJumpToBasicBlock(stream, Condition_NC, cUnit->exceptionBlockId);
-}
-
-/*
- * vA = arrayReg;
- * vB = idxReg;
- * vC = endConditionReg;
- * arg[0] = maxC
- * arg[1] = minC
- * arg[2] = loopBranchConditionCode
- */
-void genHoistedChecksForCountDownLoop(CompilationUnit *cUnit, MIR *mir)
-{
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    const int maxC = dInsn->arg[0];
-
-    //First do the null check
-    genHoistedNullCheck (cUnit, mir);
-
-    /* assign index in virtual register to P_GPR_2 */
-    get_virtual_reg(mir->dalvikInsn.vB, OpndSize_32, P_GPR_2, true);
-
-    if (maxC < 0) {
-        //if P_GPR_2 is mapped to a VR, we can't do this
-        alu_binary_imm_reg(OpndSize_32, sub_opc, -maxC, P_GPR_2, true);
-    } else if(maxC > 0) {
-        alu_binary_imm_reg(OpndSize_32, add_opc, maxC, P_GPR_2, true);
-    }
-    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), P_GPR_1, true, P_GPR_2, true);
-    condJumpToBasicBlock(stream, Condition_NC, cUnit->exceptionBlockId);
-
-}
-
-#undef P_GPR_1
-#undef P_GPR_2
-
-/*
- * vA = idxReg;
- * vB = minC;
- */
-#define P_GPR_1 PhysicalReg_ECX
-void genHoistedLowerBoundCheck(CompilationUnit *cUnit, MIR *mir)
-{
-    DecodedInstruction *dInsn = &mir->dalvikInsn;
-    const int minC = dInsn->vB;
-    get_virtual_reg(mir->dalvikInsn.vA, OpndSize_32, P_GPR_1, true); //array
-    export_pc();
-    compare_imm_reg(OpndSize_32, -minC, P_GPR_1, true);
-    condJumpToBasicBlock(stream, Condition_C, cUnit->exceptionBlockId);
-}
-#undef P_GPR_1
-
-#ifdef WITH_JIT_INLINING_PHASE2
-void genValidationForPredictedInline(CompilationUnit *cUnit, MIR *mir)
-{
-    CallsiteInfo *callsiteInfo = mir->meta.callsiteInfo;
-    if(gDvm.executionMode == kExecutionModeNcgO0) {
-        get_virtual_reg(mir->dalvikInsn.vC, OpndSize_32, PhysicalReg_EBX, true);
-        move_imm_to_reg(OpndSize_32, (int) callsiteInfo->clazz, PhysicalReg_ECX, true);
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EBX, true);
-        export_pc(); //use %edx
-        conditional_jump_global_API(, Condition_E, "common_errNullObject", false);
-        move_mem_to_reg(OpndSize_32, offObject_clazz, PhysicalReg_EBX, true, PhysicalReg_EAX, true);
-        compare_reg_reg(PhysicalReg_ECX, true, PhysicalReg_EAX, true);
-    } else {
-        get_virtual_reg(mir->dalvikInsn.vC, OpndSize_32, 5, false);
-        move_imm_to_reg(OpndSize_32, (int) callsiteInfo->clazz, 4, false);
-        nullCheck(5, false, 1, mir->dalvikInsn.vC);
-        move_mem_to_reg(OpndSize_32, offObject_clazz, 5, false, 6, false);
-        compare_reg_reg(4, false, 6, false);
-    }
-
-    //immdiate will be updated later in genLandingPadForMispredictedCallee
-    streamMisPred = stream;
-    callsiteInfo->misPredBranchOver = (LIR*)conditional_jump_int(Condition_NE, 0, OpndSize_8);
-}
-#endif
-
-/**
- * @brief Uses heuristics to determine whether a registerize request should be satisfied.
- * @param physicalType The backend physical type for the registerize request
- * @return Returns true if the registerize request should be satisfied.
- */
-static bool shouldGenerateRegisterize (LowOpndRegType physicalType)
-{
-    std::set<PhysicalReg> freeGPs, freeXMMs;
-
-    //Get the free registers available
-    findFreeRegisters (freeGPs, true, false);
-    findFreeRegisters (freeXMMs, false, true);
-
-    //If we want to registerize into a GP and we have no more, then reject this request
-    if (freeGPs.size () == 0 && physicalType == LowOpndRegType_gp)
-    {
-        return false;
-    }
-
-    //If we want to registerize into an XMM and we have no more, then reject this request
-    if (freeXMMs.size () == 0 && (physicalType == LowOpndRegType_ss || physicalType == LowOpndRegType_xmm))
-    {
-        return false;
-    }
-
-    //We accept the registerize request if we get here
-    return true;
-}
-
-bool genRegisterize (CompilationUnit *cUnit, BasicBlock_O1 *bb, MIR *mir)
-{
-    //Get the virtual register which is vA
-    int vR = mir->dalvikInsn.vA;
-
-    //Get the class from vB, it determines which instruction to use for the move
-    RegisterClass regClass = static_cast<RegisterClass> (mir->dalvikInsn.vB);
-
-    LowOpndRegType physicalType = LowOpndRegType_invalid;
-
-    // We want to figure out the mapping between the register class and the backend physical type
-    if (regClass == kCoreReg)
-    {
-        physicalType = LowOpndRegType_gp;
-    }
-    else if (regClass == kSFPReg)
-    {
-        physicalType = LowOpndRegType_ss;
-    }
-    else if (regClass == kDFPReg)
-    {
-        physicalType = LowOpndRegType_xmm;
-    }
-
-    //If we haven't determined a proper backend type, we reject this case
-    if (physicalType == LowOpndRegType_invalid)
-    {
-        ALOGI ("JIT_INFO: genRegisterize is requesting an unsupported regClass %d", regClass);
-        SET_JIT_ERROR (kJitErrorUnsupportedBytecode);
-        return false;
-    }
-
-    //We haven't registerized yet so we mark it as false for now until we actually do it
-    bool registerized = false;
-
-    //Look for this virtual register in the compile table
-    CompileTable::const_iterator vrPtr = compileTable.findVirtualRegister (vR, physicalType);
-
-    //We should already have this virtual register in the compile table because it is part of
-    //uses of this extended MIR. However, if we don't, then simply ignore the registerize request.
-    if (vrPtr != compileTable.end())
-    {
-        //Get the compile entry reference
-        const CompileTableEntry &compileEntry = *vrPtr;
-
-        //We check if it is already in physical register so we don't reload if not needed.
-        if (compileEntry.inPhysicalRegister () == false)
-        {
-            //We might want to load it in physical register so check the heuristics
-            if (shouldGenerateRegisterize (physicalType) == true)
-            {
-                //What is the size of this virtual register?
-                OpndSize size = compileEntry.getSize ();
-
-                //Define the temporary we will load into
-                const int temp = 1;
-
-                //Now we want to do the actual loading of this virtual register. We do this by using a trick
-                //to load the virtual register into a temp. And then to make sure the load happens we alias
-                //the virtual register to that temp
-                if (physicalType == LowOpndRegType_gp)
-                {
-                    get_virtual_reg (vR, size, temp, false);
-                    set_virtual_reg (vR, size, temp, false);
-                    registerized = true;
-                }
-                else if (physicalType == LowOpndRegType_ss)
-                {
-                    get_VR_ss (vR, temp, false);
-                    set_VR_ss (vR, temp, false);
-                    registerized = true;
-                }
-                else if (physicalType == LowOpndRegType_xmm)
-                {
-                    get_VR_sd (vR, temp, false);
-                    set_VR_sd (vR, temp, false);
-                    registerized = true;
-                }
-            }
-        }
-        else
-        {
-            //This is already in physical register so we mark it as having been registerized
-            registerized = true;
-        }
-    }
-
-    //If we don't satisfy this registerize request then we should make this part of the writeback requests
-    if (registerized == false)
-    {
-        //We have a wide virtual register if its backend type is xmm
-        bool isWideVr = (physicalType == LowOpndRegType_xmm);
-
-        BitVector *writebacks = bb->requestWriteBack;
-
-        //Put this VR in this block's writeback requests
-        dvmSetBit (writebacks, vR);
-
-        //If it is wide, we make sure the high VR also makes it in the writeback requests
-        if (isWideVr == true)
-        {
-            dvmSetBit (writebacks, vR + 1);
-        }
-    }
-
-    //If we get here, everything was handled
-    return true;
-}
diff --git a/vm/compiler/codegen/x86/InstructionGeneration.h b/vm/compiler/codegen/x86/InstructionGeneration.h
deleted file mode 100644
index da71a61..0000000
--- a/vm/compiler/codegen/x86/InstructionGeneration.h
+++ /dev/null
@@ -1,97 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef H_DALVIK_INSTRUCTIONGENERATION
-#define H_DALVIK_INSTRUCTIONGENERATION
-
-#include "Dalvik.h"
-
-//Forward declarations
-class BasicBlock_O1;
-
-/**
- * @brief Generate a Null check
- * @param cUnit the CompilationUnit
- * @param mir the MIR instruction
- */
-void genHoistedNullCheck (CompilationUnit *cUnit, MIR *mir);
-
-/**
- * @brief Generate a Bound check:
-      vA arrayReg
-      arg[0] -> determines whether it is a constant or a register
-      arg[1] -> register or constant
-
-      is idx < 0 || idx >= array.length ?
- * @param cUnit the CompilationUnit
- * @param mir the MIR instruction
- */
-void genHoistedBoundCheck (CompilationUnit *cUnit, MIR *mir);
-
-//use O0 code generator for hoisted checks outside of the loop
-/**
- * @brief Generate the null and upper bound check for a count up loop
- * vA = arrayReg;
- * vB = idxReg;
- * vC = endConditionReg;
- * arg[0] = maxC
- * arg[1] = minC
- * arg[2] = loopBranchConditionCode
- * @param cUnit the CompilationUnit
- * @param mir the MIR instruction
- */
-void genHoistedChecksForCountUpLoop(CompilationUnit *cUnit, MIR *mir);
-
-/**
- * @brief Generate the null and upper bound check for a count down loop
- * vA = arrayReg;
- * vB = idxReg;
- * vC = endConditionReg;
- * arg[0] = maxC
- * arg[1] = minC
- * arg[2] = loopBranchConditionCode
- * @param cUnit the CompilationUnit
- * @param mir the MIR instruction
- */
-void genHoistedChecksForCountDownLoop(CompilationUnit *cUnit, MIR *mir);
-
-/**
- * @brief Generate the lower bound check
- * vA = arrayReg;
- * vB = minimum constant used for the array;
- * @param cUnit the CompilationUnit
- * @param mir the MIR instruction
- */
-void genHoistedLowerBoundCheck(CompilationUnit *cUnit, MIR *mir);
-
-/**
- * @brief Generate the validation for a predicted inline
- * vC actual class
- * @param cUnit the CompilationUnit
- * @param mir the MIR instruction
- */
-void genValidationForPredictedInline(CompilationUnit *cUnit, MIR *mir);
-
-/**
- * @brief Generate native code for the registerize extended instruction
- * @details vA of the mir has the register to set in a physical register
- * @param cUnit The Compilation Unit
- * @param bb The basic block that contains the request
- * @param mir the MIR instruction representing the registerization request
- * @return Returns whether or not it successfully handled the request
- */
-bool genRegisterize (CompilationUnit *cUnit, BasicBlock_O1 *bb, MIR *mir);
-#endif
diff --git a/vm/compiler/codegen/x86/Lower.cpp b/vm/compiler/codegen/x86/Lower.cpp
deleted file mode 100644
index 73d22f0..0000000
--- a/vm/compiler/codegen/x86/Lower.cpp
+++ /dev/null
@@ -1,1255 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file Lower.cpp
-    \brief This file implements the high-level wrapper for lowering
-
-*/
-
-#include "CompilationUnit.h"
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include <math.h>
-#include <sys/mman.h>
-#include "Translator.h"
-#include "Lower.h"
-#include "enc_wrapper.h"
-#include "vm/mterp/Mterp.h"
-#include "NcgHelper.h"
-#include "libdex/DexCatch.h"
-#include "compiler/CompilerIR.h"
-#if defined VTUNE_DALVIK
-#include "compiler/JitProfiling.h"
-#endif
-#include "Singleton.h"
-#include "ExceptionHandling.h"
-#include "compiler/Dataflow.h"
-
-//statistics for optimization
-int num_removed_nullCheck;
-
-PhysicalReg scratchRegs[4];
-
-LowOp* ops[BUFFER_SIZE];
-LowOp* op;
-u2* rPC; //PC pointer to bytecode
-int offsetPC/*offset in bytecode*/, offsetNCG/*byte offset in native code*/;
-int ncg_rPC;
-//! map from PC in bytecode to PC in native code
-int mapFromBCtoNCG[BYTECODE_SIZE_PER_METHOD]; //initially mapped to -1
-char* streamStart = NULL; //start of the Pure CodeItem?, not include the global symbols
-char* streamCode = NULL; //start of the Pure CodeItem?, not include the global symbols
-char* streamMethodStart; //start of the method
-char* stream; //current stream pointer
-Method* currentMethod = NULL;
-int currentExceptionBlockIdx = -1;
-BasicBlock* traceCurrentBB = NULL;
-JitMode traceMode = kJitTrace;
-CompilationUnit_O1 *gCompilationUnit;
-
-int common_invokeArgsDone(ArgsDoneType);
-
-//data section of .ia32:
-char globalData[128];
-
-char strClassCastException[] = "Ljava/lang/ClassCastException;";
-char strInstantiationError[] = "Ljava/lang/InstantiationError;";
-char strInternalError[] = "Ljava/lang/InternalError;";
-char strFilledNewArrayNotImpl[] = "filled-new-array only implemented for 'int'";
-char strArithmeticException[] = "Ljava/lang/ArithmeticException;";
-char strArrayIndexException[] = "Ljava/lang/ArrayIndexOutOfBoundsException;";
-char strArrayStoreException[] = "Ljava/lang/ArrayStoreException;";
-char strDivideByZero[] = "divide by zero";
-char strNegativeArraySizeException[] = "Ljava/lang/NegativeArraySizeException;";
-char strNoSuchMethodError[] = "Ljava/lang/NoSuchMethodError;";
-char strNullPointerException[] = "Ljava/lang/NullPointerException;";
-char strStringIndexOutOfBoundsException[] = "Ljava/lang/StringIndexOutOfBoundsException;";
-
-int LstrClassCastExceptionPtr, LstrInstantiationErrorPtr, LstrInternalError, LstrFilledNewArrayNotImpl;
-int LstrArithmeticException, LstrArrayIndexException, LstrArrayStoreException, LstrStringIndexOutOfBoundsException;
-int LstrDivideByZero, LstrNegativeArraySizeException, LstrNoSuchMethodError, LstrNullPointerException;
-int LdoubNeg, LvaluePosInfLong, LvalueNegInfLong, LvalueNanLong, LshiftMask, Lvalue64, L64bits, LintMax, LintMin;
-
-void initConstDataSec() {
-    char* tmpPtr = globalData;
-
-    LdoubNeg = (int)tmpPtr;
-    *((u4*)tmpPtr) = 0x00000000;
-    tmpPtr += sizeof(u4);
-    *((u4*)tmpPtr) = 0x80000000;
-    tmpPtr += sizeof(u4);
-
-    // 16 byte aligned
-    tmpPtr = align(tmpPtr, 16);
-    LvaluePosInfLong = (int)tmpPtr;
-    *((u4*)tmpPtr) = 0xFFFFFFFF;
-    tmpPtr += sizeof(u4);
-    *((u4*)tmpPtr) = 0x7FFFFFFF;
-    tmpPtr += sizeof(u4);
-
-    LvalueNegInfLong = (int)tmpPtr;
-    *((u4*)tmpPtr) = 0x00000000;
-    tmpPtr += sizeof(u4);
-    *((u4*)tmpPtr) = 0x80000000;
-    tmpPtr += sizeof(u4);
-
-    LvalueNanLong = (int)tmpPtr;
-    *((u4*)tmpPtr) = 0;
-    tmpPtr += sizeof(u4);
-    *((u4*)tmpPtr) = 0;
-    tmpPtr += sizeof(u4);
-
-    LshiftMask = (int)tmpPtr;
-    *((u4*)tmpPtr) = 0x3f;
-    tmpPtr += sizeof(u4);
-    *((u4*)tmpPtr) = 0;
-    tmpPtr += sizeof(u4);
-
-    Lvalue64 = (int)tmpPtr;
-    *((u4*)tmpPtr) = 0x40;
-    tmpPtr += sizeof(u4);
-    *((u4*)tmpPtr) = 0;
-    tmpPtr += sizeof(u4);
-
-    L64bits = (int)tmpPtr;
-    *((u4*)tmpPtr) = 0xFFFFFFFF;
-    tmpPtr += sizeof(u4);
-    *((u4*)tmpPtr) = 0xFFFFFFFF;
-    tmpPtr += sizeof(u4);
-
-    LintMin = (int)tmpPtr;
-    *((u4*)tmpPtr) = 0x80000000;
-    tmpPtr += sizeof(u4);
-
-    LintMax = (int)tmpPtr;
-    *((u4*)tmpPtr) = 0x7FFFFFFF;
-    tmpPtr += sizeof(u4);
-
-    LstrClassCastExceptionPtr = (int)strClassCastException;
-    LstrInstantiationErrorPtr = (int)strInstantiationError;
-    LstrInternalError = (int)strInternalError;
-    LstrFilledNewArrayNotImpl = (int)strFilledNewArrayNotImpl;
-    LstrArithmeticException = (int)strArithmeticException;
-    LstrArrayIndexException = (int)strArrayIndexException;
-    LstrArrayStoreException = (int)strArrayStoreException;
-    LstrDivideByZero = (int)strDivideByZero;
-    LstrNegativeArraySizeException = (int)strNegativeArraySizeException;
-    LstrNoSuchMethodError = (int)strNoSuchMethodError;
-    LstrNullPointerException = (int)strNullPointerException;
-    LstrStringIndexOutOfBoundsException = (int)strStringIndexOutOfBoundsException;
-}
-
-//declarations of functions used in this file
-int spill_reg(int reg, bool isPhysical);
-int unspill_reg(int reg, bool isPhysical);
-
-int const_string_resolve();
-int sget_sput_resolve();
-int new_instance_needinit();
-int new_instance_abstract();
-int invoke_virtual_resolve();
-int invoke_direct_resolve();
-int invoke_static_resolve();
-int filled_new_array_notimpl();
-int resolve_class2(
-                   int startLR/*logical register index*/, bool isPhysical, int indexReg/*const pool index*/,
-                   bool indexPhysical,
-                   int thirdArg);
-int resolve_method2(
-                    int startLR/*logical register index*/, bool isPhysical, int indexReg/*const pool index*/,
-                    bool indexPhysical,
-                    int thirdArg/*VIRTUAL*/);
-int resolve_inst_field2(
-                        int startLR/*logical register index*/, bool isPhysical,
-                        int indexReg/*const pool index*/,
-                        bool indexPhysical);
-int resolve_static_field2(
-                          int startLR/*logical register index*/, bool isPhysical,
-                          int indexReg/*const pool index*/,
-                          bool indexPhysical);
-
-int invokeMethodNoRange_1_helper();
-int invokeMethodNoRange_2_helper();
-int invokeMethodNoRange_3_helper();
-int invokeMethodNoRange_4_helper();
-int invokeMethodNoRange_5_helper();
-int invokeMethodRange_helper();
-
-int invoke_virtual_helper();
-int invoke_virtual_quick_helper();
-int invoke_static_helper();
-int invoke_direct_helper();
-int new_instance_helper();
-int sget_sput_helper(int flag);
-int aput_obj_helper();
-int aget_helper(int flag);
-int aput_helper(int flag);
-int monitor_enter_helper();
-int monitor_exit_helper();
-int throw_helper();
-int const_string_helper();
-int array_length_helper();
-int invoke_super_helper();
-int invoke_interface_helper();
-int iget_iput_helper(int flag);
-int check_cast_helper(bool instance);
-int new_array_helper();
-
-/*!
-\brief dump helper functions
-
-*/
-int performCGWorklist() {
-    int retCode = 0;
-    filled_new_array_notimpl();
-    freeShortMap();
-    const_string_resolve();
-    freeShortMap();
-
-    resolve_class2(PhysicalReg_EAX, true, PhysicalReg_EAX, true, 0);
-    freeShortMap();
-    resolve_method2(PhysicalReg_EAX, true, PhysicalReg_EAX, true, METHOD_VIRTUAL);
-    freeShortMap();
-    resolve_method2(PhysicalReg_EAX, true, PhysicalReg_EAX, true, METHOD_DIRECT);
-    freeShortMap();
-    resolve_method2(PhysicalReg_EAX, true, PhysicalReg_EAX, true, METHOD_STATIC);
-    freeShortMap();
-    resolve_inst_field2(PhysicalReg_EAX, true, PhysicalReg_EAX, true);
-    freeShortMap();
-    resolve_static_field2(PhysicalReg_EAX, true, PhysicalReg_EAX, true);
-    freeShortMap();
-    throw_exception_message(PhysicalReg_ECX, PhysicalReg_EAX, true, PhysicalReg_Null, true);
-    freeShortMap();
-    throw_exception(PhysicalReg_ECX, PhysicalReg_EAX, PhysicalReg_Null, true);
-    freeShortMap();
-    retCode = new_instance_needinit();
-    freeShortMap();
-    return retCode;
-}
-
-int aput_object_count;
-int common_periodicChecks_entry();
-int common_periodicChecks4();
-
-#if 0 /* Commented out because it is dead code. If re-enabling, this needs to be updated
-         to take MIR as parameter */
-/*!
-\brief for debugging purpose, dump the sequence of native code for each bytecode
-
-*/
-int ncgMethodFake(Method* method) {
-    //to measure code size expansion, no need to patch up labels
-    methodDataWorklist = NULL;
-    globalShortWorklist = NULL;
-    globalNCGWorklist = NULL;
-    streamMethodStart = stream;
-
-    //initialize mapFromBCtoNCG
-    memset(&mapFromBCtoNCG[0], -1, BYTECODE_SIZE_PER_METHOD * sizeof(mapFromBCtoNCG[0]));
-    unsigned int i;
-    u2* rStart = (u2*)malloc(5*sizeof(u2));
-    if(rStart == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at ncgMethodFake");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    rPC = rStart;
-    method->insns = rStart;
-    for(i = 0; i < 5; i++) *rPC++ = 0;
-    for(i = 0; i < 256; i++) {
-        rPC = rStart;
-        //modify the opcode
-        char* tmp = (char*)rStart;
-        *tmp++ = i;
-        *tmp = i;
-        inst = FETCH(0);
-        char* tmpStart = stream;
-        lowerByteCodeCanThrowCheck(method); //use inst, rPC, method, modify rPC
-        int size_in_u2 = rPC - rStart;
-        if(stream - tmpStart  > 0)
-            ALOGI("LOWER bytecode %x size in u2: %d ncg size in byte: %d", i, size_in_u2, stream - tmpStart);
-    }
-    exit(0);
-}
-#endif
-
-bool existATryBlock(Method* method, int startPC, int endPC) {
-    const DexCode* pCode = dvmGetMethodCode(method);
-    u4 triesSize = pCode->triesSize;
-    const DexTry* pTries = dexGetTries(pCode);
-    unsigned int i;
-    for (i = 0; i < triesSize; i++) {
-        const DexTry* pTry = &pTries[i];
-        u4 start = pTry->startAddr; //offsetPC
-        u4 end = start + pTry->insnCount;
-        //if [start, end] overlaps with [startPC, endPC] returns true
-        if((int)end < startPC || (int)start > endPC) { //no overlap
-        } else {
-            return true;
-        }
-    }
-    return false;
-}
-
-int mm_bytecode_size = 0;
-int mm_ncg_size = 0;
-int mm_relocation_size = 0;
-int mm_map_size = 0;
-void resetCodeSize() {
-    mm_bytecode_size = 0;
-    mm_ncg_size = 0;
-    mm_relocation_size = 0;
-    mm_map_size = 0;
-}
-
-bool bytecodeIsRemoved(const Method* method, u4 bytecodeOffset) {
-    if(gDvm.executionMode == kExecutionModeNcgO0) return false;
-    u4 ncgOff = mapFromBCtoNCG[bytecodeOffset];
-    int k = bytecodeOffset+1;
-    u2 insnsSize = dvmGetMethodInsnsSize(method);
-    while(k < insnsSize) {
-        if(mapFromBCtoNCG[k] < 0) {
-            k++;
-            continue;
-        }
-        if(mapFromBCtoNCG[k] == (int)ncgOff) return true;
-        return false;
-    }
-    return false;
-}
-
-int invoke_super_nsm();
-void init_common(const char* curFileName, DvmDex *pDvmDex, bool forNCG); //forward declaration
-void initGlobalMethods(); //forward declaration
-
-//called once when compiler thread starts up
-void initJIT(const char* curFileName, DvmDex *pDvmDex) {
-    init_common(curFileName, pDvmDex, false);
-}
-
-void init_common(const char* curFileName, DvmDex *pDvmDex, bool forNCG) {
-    if(!gDvm.constInit) {
-        globalMapNum = 0;
-        globalMap = NULL;
-        initConstDataSec();
-        gDvm.constInit = true;
-    }
-
-    //for initJIT: stream is already set
-    if(!gDvm.commonInit) {
-        initGlobalMethods();
-        gDvm.commonInit = true;
-    }
-}
-
-void initGlobalMethods() {
-    bool old_dump_x86_inst = dump_x86_inst;
-    bool old_scheduling = gDvmJit.scheduling;
-    dump_x86_inst = false; // Enable this to debug common section
-
-    //! \warning Scheduling should be turned off when creating common section
-    //! because it relies on the fact the register allocation has already been
-    //! done (either via register allocator or via hardcoded registers). But,
-    //! when we get to this point, the execution mode is Jit instead of either
-    //! NcgO1 or NcgO0, which leads to the unintended consequence that NcgO0
-    //! path is taken, but logical registers are used instead of physical
-    //! registers and thus relies on encoder to do the mapping, which the
-    //! scheduler cannot predict for dependency graph creation.
-    //! \todo The reason "logical" registers are used is because variable
-    //! isScratchPhysical is set to false even when a physical register is
-    //! used. This issue should be addressed at some point.
-    gDvmJit.scheduling = false;
-
-    // generate native code for function ncgGetEIP
-    if (insertLabel("ncgGetEIP", false) == -1)
-        return;
-    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EDX, true);
-    x86_return();
-
-    //generate code for common labels
-    //jumps within a helper function is treated as short labels
-    globalShortMap = NULL;
-    common_periodicChecks_entry();
-    freeShortMap();
-    common_periodicChecks4();
-    freeShortMap();
-
-    if(dump_x86_inst)
-        ALOGI("ArgsDone_Normal start");
-    common_invokeArgsDone(ArgsDone_Normal);
-    freeShortMap();
-    if(dump_x86_inst)
-        ALOGI("ArgsDone_Native start");
-    common_invokeArgsDone(ArgsDone_Native);
-    freeShortMap();
-    if(dump_x86_inst)
-        ALOGI("ArgsDone_Full start");
-    common_invokeArgsDone(ArgsDone_Full);
-    if(dump_x86_inst)
-        ALOGI("ArgsDone_Full end");
-    freeShortMap();
-
-    common_backwardBranch();
-    freeShortMap();
-    common_exceptionThrown();
-    freeShortMap();
-    common_errNullObject();
-    freeShortMap();
-    common_errArrayIndex();
-    freeShortMap();
-    common_errArrayStore();
-    freeShortMap();
-    common_errNegArraySize();
-    freeShortMap();
-    common_errNoSuchMethod();
-    freeShortMap();
-    common_errDivideByZero();
-    freeShortMap();
-    common_gotoBail();
-    freeShortMap();
-    common_gotoBail_0();
-    freeShortMap();
-    invoke_super_nsm();
-    freeShortMap();
-
-    performCGWorklist(); //generate code for helper functions
-    performLabelWorklist(); //it is likely that the common labels will jump to other common labels
-
-    gDvmJit.scheduling = old_scheduling;
-    dump_x86_inst = old_dump_x86_inst;
-}
-
-ExecutionMode origMode;
-
-/**
- * @brief Lowers bytecode to native code
- * @param method parent method of trace
- * @param mir bytecode representation
- * @param dalvikPC the program counter of the instruction
- * @return true when NOT handled and false when it IS handled
- */
-bool lowerByteCodeJit(const Method* method, const MIR * mir, const u2 * dalvikPC) {
-    int retCode = lowerByteCodeCanThrowCheck(method, mir, dalvikPC);
-    freeShortMap();
-    if(retCode >= 0) return false; //handled
-    return true; //not handled
-}
-
-void startOfBasicBlock(BasicBlock* bb) {
-    traceCurrentBB = bb;
-    if(gDvm.executionMode == kExecutionModeNcgO0) {
-        isScratchPhysical = true;
-    } else {
-        isScratchPhysical = false;
-    }
-}
-
-void startOfTrace(const Method* method, int exceptionBlockId,
-                  CompilationUnit_O1 *cUnit) {
-
-    //Set the global compilation
-    gCompilationUnit = cUnit;
-
-    origMode = gDvm.executionMode;
-    gDvm.executionMode = kExecutionModeNcgO1;
-    if(gDvm.executionMode == kExecutionModeNcgO0) {
-        isScratchPhysical = true;
-    } else {
-        isScratchPhysical = false;
-    }
-    currentMethod = (Method*)method;
-    currentExceptionBlockIdx = exceptionBlockId;
-    methodDataWorklist = NULL;
-    globalShortWorklist = NULL;
-    globalNCGWorklist = NULL;
-    singletonPtr<ExceptionHandlingRestoreState>()->reset();
-
-    streamMethodStart = stream;
-    //initialize mapFromBCtoNCG
-    memset(&mapFromBCtoNCG[0], -1, BYTECODE_SIZE_PER_METHOD * sizeof(mapFromBCtoNCG[0]));
-    if(gDvm.executionMode == kExecutionModeNcgO1)
-        startOfTraceO1(method, exceptionBlockId, cUnit);
-}
-
-/**
- * @brief Used to free the data structures in basic blocks that were used by backend
- * @param basicBlocks The list of all basic blocks in current cUnit
- */
-static void freeCFG (GrowableList &basicBlocks)
-{
-    //Create and initialize the basic block iterator
-    GrowableListIterator iter;
-    dvmGrowableListIteratorInit (&basicBlocks, &iter);
-
-    //Get the first basic block provided by iterator
-    BasicBlock_O1 *bb = reinterpret_cast<BasicBlock_O1 *> (dvmGrowableListIteratorNext (&iter));
-
-    while (bb != 0)
-    {
-        //Call the BasicBlock_O1 clear function
-        bb->freeIt ();
-
-        //We want to move on to next basic block
-        bb = reinterpret_cast<BasicBlock_O1 *> (dvmGrowableListIteratorNext (&iter));
-    }
-}
-
-void performWorklistWork (void)
-{
-    performLabelWorklist ();
-    performNCGWorklist (); //handle forward jump (GOTO, IF)
-    performDataWorklist (); //handle SWITCH & FILL_ARRAY_DATA
-    performChainingWorklist ();
-}
-
-void endOfTrace (CompilationUnit *cUnit) {
-    freeLabelWorklist ();
-    freeNCGWorklist ();
-    freeDataWorklist ();
-    freeChainingWorklist ();
-
-    //Now we want to free anything in BasicBlock that we used during backend but was not
-    //allocated using the arena
-    freeCFG (cUnit->blockList);
-
-    //Restore the execution mode as the ME expects it
-    gDvm.executionMode = origMode;
-
-    //Reset the global compilation unit
-    gCompilationUnit = 0;
-}
-
-int lowerByteCodeCanThrowCheck(const Method* method, const MIR * mir, const u2 * dalvikPC) {
-    bool delay_requested = false;
-
-    int flags = dvmCompilerGetOpcodeFlags (mir->dalvikInsn.opcode);
-
-    // Delay free VRs if we potentially can exit to interpreter
-    // We do not activate delay if VRs state is not changed
-    if ((flags & kInstrCanThrow) != 0)
-    {
-        int dfAttributes = dvmCompilerDataFlowAttributes[mir->dalvikInsn.opcode];
-
-        if ( (dfAttributes & DF_IS_CALL) == 0) { // Not applicable to calls
-            int mirOptFlags = mir->OptimizationFlags;
-
-            // Avoid delay if we null/range check optimized
-            if ( (dfAttributes & DF_HAS_NR_CHECKS) != 0 ) {
-                // Both null check and range check applicable
-
-                if( (mirOptFlags & MIR_IGNORE_NULL_CHECK) == 0 ) {
-                    // Null check is not optimized, request delay
-                    if(requestVRFreeDelayAll(VRDELAY_CAN_THROW) == true) {
-                        delay_requested = true;
-                    }
-                }
-
-                if( (mirOptFlags & MIR_IGNORE_RANGE_CHECK) == 0 ) {
-                    // Range check is not optimized, put additional request delay
-                    if(requestVRFreeDelayAll(VRDELAY_CAN_THROW) == true) {
-                        delay_requested = true;
-                    }
-                }
-            } else if ( (dfAttributes & DF_HAS_OBJECT_CHECKS) != 0 ) {
-                // Only null check applicable to opcode
-
-                if( (mirOptFlags & MIR_IGNORE_NULL_CHECK) == 0 ) {
-                    // Null check is not optimized, request delay
-                    if(requestVRFreeDelayAll(VRDELAY_CAN_THROW) == true) {
-                        delay_requested = true;
-                    }
-                }
-            } else {
-                // Can exit to interpreter but have no null/range checks
-                if(requestVRFreeDelayAll(VRDELAY_CAN_THROW) == true) {
-                    delay_requested = true;
-                }
-            }
-        }
-    }
-
-    int retCode = lowerByteCode(method, mir, dalvikPC);
-
-    if(delay_requested == true) {
-        bool state_changed = cancelVRFreeDelayRequestAll(VRDELAY_CAN_THROW);
-        if(state_changed==true) {
-            // Not optimized case (delay was not canceled inside bytecode generation)
-            ALOGI("JIT_INFO: VRDELAY_CAN_THROW cancel was not optimized for bytecode=0x%x",
-                  mir->dalvikInsn.opcode);
-            // Release all remaining VRDELAY_CAN_THROW requests
-            do {
-                state_changed = cancelVRFreeDelayRequestAll(VRDELAY_CAN_THROW);
-            } while (state_changed == true);
-        }
-    }
-    return retCode;
-}
-
-/**
- * @brief Generates native code for the bytecode.
- * @details May update code stream.
- * @param method parent method of trace
- * @param mir bytecode representation
- * @param dalvikPC the program counter of the instruction
- * @return 0 or greater when handled
- */
-int lowerByteCode(const Method* method, const MIR * mir, const u2 * dalvikPC) {
-    /* offsetPC is used in O1 code generator, where it is defined as the sequence number
-       use a local version to avoid overwriting */
-    int offsetPC = mir->offset; //! \warning When doing method inlining, offsetPC
-                                //! will be the same for the invoke and the inlined
-                                //! bytecode. This WILL break mapping from BC to NCG
-                                //! if more than one bytecode is inlined.
-
-    if (dump_x86_inst == true)
-    {
-        const int maxDecodedLen = 256;
-        char decodedString[maxDecodedLen];
-
-        //We want to decode the current instruction but we pass a null cUnit because we don't
-        //care to have any ssa information printed.
-        dvmCompilerExtendedDisassembler (0, mir, &(mir->dalvikInsn), decodedString, maxDecodedLen);
-
-        ALOGI ("LOWER %s with offsetPC %x offsetNCG %x @%p\n", decodedString, offsetPC,
-                stream - streamMethodStart, stream);
-    }
-
-    //update mapFromBCtoNCG
-    offsetNCG = stream - streamMethodStart;
-    if(offsetPC >= BYTECODE_SIZE_PER_METHOD) {
-        ALOGI("JIT_INFO: offsetPC %d exceeds BYTECODE_SIZE_PER_METHOD", offsetPC);
-        SET_JIT_ERROR(kJitErrorTraceFormation);
-        return -1;
-    }
-    mapFromBCtoNCG[offsetPC] = offsetNCG;
-#if defined(ENABLE_TRACING) && defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC, mapFromBCtoNCG[offsetPC], 1);
-#endif
-    //return number of LowOps generated
-    switch (mir->dalvikInsn.opcode) {
-    case OP_NOP:
-        return op_nop(mir);
-    case OP_MOVE:
-    case OP_MOVE_OBJECT:
-        return op_move(mir);
-    case OP_MOVE_FROM16:
-    case OP_MOVE_OBJECT_FROM16:
-        return op_move_from16(mir);
-    case OP_MOVE_16:
-    case OP_MOVE_OBJECT_16:
-        return op_move_16(mir);
-    case OP_MOVE_WIDE:
-        return op_move_wide(mir);
-    case OP_MOVE_WIDE_FROM16:
-        return op_move_wide_from16(mir);
-    case OP_MOVE_WIDE_16:
-        return op_move_wide_16(mir);
-    case OP_MOVE_RESULT:
-    case OP_MOVE_RESULT_OBJECT:
-        return op_move_result(mir);
-    case OP_MOVE_RESULT_WIDE:
-        return op_move_result_wide(mir);
-    case OP_MOVE_EXCEPTION:
-        return op_move_exception(mir);
-    case OP_RETURN_VOID:
-    case OP_RETURN_VOID_BARRIER:
-        return op_return_void(mir);
-    case OP_RETURN:
-    case OP_RETURN_OBJECT:
-        return op_return(mir);
-    case OP_RETURN_WIDE:
-        return op_return_wide(mir);
-    case OP_CONST_4:
-        return op_const_4(mir);
-    case OP_CONST_16:
-        return op_const_16(mir);
-    case OP_CONST:
-        return op_const(mir);
-    case OP_CONST_HIGH16:
-        return op_const_high16(mir);
-    case OP_CONST_WIDE_16:
-        return op_const_wide_16(mir);
-    case OP_CONST_WIDE_32:
-        return op_const_wide_32(mir);
-    case OP_CONST_WIDE:
-        return op_const_wide(mir);
-    case OP_CONST_WIDE_HIGH16:
-        return op_const_wide_high16(mir);
-    case OP_CONST_STRING:
-        return op_const_string(mir);
-    case OP_CONST_STRING_JUMBO:
-        return op_const_string_jumbo(mir);
-    case OP_CONST_CLASS:
-        return op_const_class(mir);
-    case OP_MONITOR_ENTER:
-        return op_monitor_enter(mir);
-    case OP_MONITOR_EXIT:
-        return op_monitor_exit(mir);
-    case OP_CHECK_CAST:
-        return op_check_cast(mir);
-    case OP_INSTANCE_OF:
-        return op_instance_of(mir);
-    case OP_ARRAY_LENGTH:
-        return op_array_length(mir);
-    case OP_NEW_INSTANCE:
-        return op_new_instance(mir);
-    case OP_NEW_ARRAY:
-        return op_new_array(mir);
-    case OP_FILLED_NEW_ARRAY:
-        return op_filled_new_array(mir);
-    case OP_FILLED_NEW_ARRAY_RANGE:
-        return op_filled_new_array_range(mir);
-    case OP_FILL_ARRAY_DATA:
-        return op_fill_array_data(mir, dalvikPC);
-    case OP_THROW:
-        return op_throw(mir);
-    case OP_THROW_VERIFICATION_ERROR:
-        return op_throw_verification_error(mir);
-    case OP_GOTO:
-        return op_goto(mir);
-    case OP_GOTO_16:
-        return op_goto_16(mir);
-    case OP_GOTO_32:
-        return op_goto_32(mir);
-    case OP_PACKED_SWITCH:
-        return op_packed_switch(mir, dalvikPC);
-    case OP_SPARSE_SWITCH:
-        return op_sparse_switch(mir, dalvikPC);
-    case OP_CMPL_FLOAT:
-        return op_cmpl_float(mir);
-    case OP_CMPG_FLOAT:
-        return op_cmpg_float(mir);
-    case OP_CMPL_DOUBLE:
-        return op_cmpl_double(mir);
-    case OP_CMPG_DOUBLE:
-        return op_cmpg_double(mir);
-    case OP_CMP_LONG:
-        return op_cmp_long(mir);
-    case OP_IF_EQ:
-        return op_if_eq(mir);
-    case OP_IF_NE:
-        return op_if_ne(mir);
-    case OP_IF_LT:
-        return op_if_lt(mir);
-    case OP_IF_GE:
-        return op_if_ge(mir);
-    case OP_IF_GT:
-        return op_if_gt(mir);
-    case OP_IF_LE:
-        return op_if_le(mir);
-    case OP_IF_EQZ:
-        return op_if_eqz(mir);
-    case OP_IF_NEZ:
-        return op_if_nez(mir);
-    case OP_IF_LTZ:
-        return op_if_ltz(mir);
-    case OP_IF_GEZ:
-        return op_if_gez(mir);
-    case OP_IF_GTZ:
-        return op_if_gtz(mir);
-    case OP_IF_LEZ:
-        return op_if_lez(mir);
-    case OP_AGET:
-        return op_aget(mir);
-    case OP_AGET_WIDE:
-        return op_aget_wide(mir);
-    case OP_AGET_OBJECT:
-        return op_aget_object(mir);
-    case OP_AGET_BOOLEAN:
-        return op_aget_boolean(mir);
-    case OP_AGET_BYTE:
-        return op_aget_byte(mir);
-    case OP_AGET_CHAR:
-        return op_aget_char(mir);
-    case OP_AGET_SHORT:
-        return op_aget_short(mir);
-    case OP_APUT:
-        return op_aput(mir);
-    case OP_APUT_WIDE:
-        return op_aput_wide(mir);
-    case OP_APUT_OBJECT:
-        return op_aput_object(mir);
-    case OP_APUT_BOOLEAN:
-        return op_aput_boolean(mir);
-    case OP_APUT_BYTE:
-        return op_aput_byte(mir);
-    case OP_APUT_CHAR:
-        return op_aput_char(mir);
-    case OP_APUT_SHORT:
-        return op_aput_short(mir);
-    case OP_IGET:
-    case OP_IGET_VOLATILE:
-        return op_iget(mir);
-    case OP_IGET_WIDE:
-        return op_iget_wide(mir, false /*isVolatile*/);
-    case OP_IGET_WIDE_VOLATILE:
-        return op_iget_wide(mir, true /*isVolatile*/);
-    case OP_IGET_OBJECT:
-    case OP_IGET_OBJECT_VOLATILE:
-        return op_iget_object(mir);
-    case OP_IGET_BOOLEAN:
-        return op_iget_boolean(mir);
-    case OP_IGET_BYTE:
-        return op_iget_byte(mir);
-    case OP_IGET_CHAR:
-        return op_iget_char(mir);
-    case OP_IGET_SHORT:
-        return op_iget_short(mir);
-    case OP_IPUT:
-    case OP_IPUT_VOLATILE:
-        return op_iput(mir);
-    case OP_IPUT_WIDE:
-        return op_iput_wide(mir, false /*isVolatile*/);
-    case OP_IPUT_WIDE_VOLATILE:
-        return op_iput_wide(mir, true /*isVolatile*/);
-    case OP_IPUT_OBJECT:
-    case OP_IPUT_OBJECT_VOLATILE:
-        return op_iput_object(mir);
-    case OP_IPUT_BOOLEAN:
-        return op_iput_boolean(mir);
-    case OP_IPUT_BYTE:
-        return op_iput_byte(mir);
-    case OP_IPUT_CHAR:
-        return op_iput_char(mir);
-    case OP_IPUT_SHORT:
-        return op_iput_short(mir);
-    case OP_SGET:
-    case OP_SGET_VOLATILE:
-        return op_sget(mir);
-    case OP_SGET_WIDE:
-        return op_sget_wide(mir, false /*isVolatile*/);
-    case OP_SGET_WIDE_VOLATILE:
-        return op_sget_wide(mir, true /*isVolatile*/);
-    case OP_SGET_OBJECT:
-    case OP_SGET_OBJECT_VOLATILE:
-        return op_sget_object(mir);
-    case OP_SGET_BOOLEAN:
-        return op_sget_boolean(mir);
-    case OP_SGET_BYTE:
-        return op_sget_byte(mir);
-    case OP_SGET_CHAR:
-        return op_sget_char(mir);
-    case OP_SGET_SHORT:
-        return op_sget_short(mir);
-    case OP_SPUT:
-    case OP_SPUT_VOLATILE:
-        return op_sput(mir, false /*isObj*/);
-    case OP_SPUT_WIDE:
-        return op_sput_wide(mir, false /*isVolatile*/);
-    case OP_SPUT_WIDE_VOLATILE:
-        return op_sput_wide(mir, true /*isVolatile*/);
-    case OP_SPUT_OBJECT:
-    case OP_SPUT_OBJECT_VOLATILE:
-        return op_sput_object(mir);
-    case OP_SPUT_BOOLEAN:
-        return op_sput_boolean(mir);
-    case OP_SPUT_BYTE:
-        return op_sput_byte(mir);
-    case OP_SPUT_CHAR:
-        return op_sput_char(mir);
-    case OP_SPUT_SHORT:
-        return op_sput_short(mir);
-    case OP_INVOKE_VIRTUAL:
-        return op_invoke_virtual(mir);
-    case OP_INVOKE_SUPER:
-        return op_invoke_super(mir);
-    case OP_INVOKE_DIRECT:
-        return op_invoke_direct(mir);
-    case OP_INVOKE_STATIC:
-        return op_invoke_static(mir);
-    case OP_INVOKE_INTERFACE:
-        return op_invoke_interface(mir);
-    case OP_INVOKE_VIRTUAL_RANGE:
-        return op_invoke_virtual_range(mir);
-    case OP_INVOKE_SUPER_RANGE:
-        return op_invoke_super_range(mir);
-    case OP_INVOKE_DIRECT_RANGE:
-        return op_invoke_direct_range(mir);
-    case OP_INVOKE_STATIC_RANGE:
-        return op_invoke_static_range(mir);
-    case OP_INVOKE_INTERFACE_RANGE:
-        return op_invoke_interface_range(mir);
-    case OP_NEG_INT:
-        return op_neg_int(mir);
-    case OP_NOT_INT:
-        return op_not_int(mir);
-    case OP_NEG_LONG:
-        return op_neg_long(mir);
-    case OP_NOT_LONG:
-        return op_not_long(mir);
-    case OP_NEG_FLOAT:
-        return op_neg_float(mir);
-    case OP_NEG_DOUBLE:
-        return op_neg_double(mir);
-    case OP_INT_TO_LONG:
-        return op_int_to_long(mir);
-    case OP_INT_TO_FLOAT:
-        return op_int_to_float(mir);
-    case OP_INT_TO_DOUBLE:
-        return op_int_to_double(mir);
-    case OP_LONG_TO_INT:
-        return op_long_to_int(mir);
-    case OP_LONG_TO_FLOAT:
-        return op_long_to_float(mir);
-    case OP_LONG_TO_DOUBLE:
-        return op_long_to_double(mir);
-    case OP_FLOAT_TO_INT:
-        return op_float_to_int(mir);
-    case OP_FLOAT_TO_LONG:
-        return op_float_to_long(mir);
-    case OP_FLOAT_TO_DOUBLE:
-        return op_float_to_double(mir);
-    case OP_DOUBLE_TO_INT:
-        return op_double_to_int(mir);
-    case OP_DOUBLE_TO_LONG:
-        return op_double_to_long(mir);
-    case OP_DOUBLE_TO_FLOAT:
-        return op_double_to_float(mir);
-    case OP_INT_TO_BYTE:
-        return op_int_to_byte(mir);
-    case OP_INT_TO_CHAR:
-        return op_int_to_char(mir);
-    case OP_INT_TO_SHORT:
-        return op_int_to_short(mir);
-    case OP_ADD_INT:
-        return op_add_int(mir);
-    case OP_SUB_INT:
-        return op_sub_int(mir);
-    case OP_MUL_INT:
-        return op_mul_int(mir);
-    case OP_DIV_INT:
-        return op_div_int(mir);
-    case OP_REM_INT:
-        return op_rem_int(mir);
-    case OP_AND_INT:
-        return op_and_int(mir);
-    case OP_OR_INT:
-        return op_or_int(mir);
-    case OP_XOR_INT:
-        return op_xor_int(mir);
-    case OP_SHL_INT:
-        return op_shl_int(mir);
-    case OP_SHR_INT:
-        return op_shr_int(mir);
-    case OP_USHR_INT:
-        return op_ushr_int(mir);
-    case OP_ADD_LONG:
-        return op_add_long(mir);
-    case OP_SUB_LONG:
-        return op_sub_long(mir);
-    case OP_MUL_LONG:
-        return op_mul_long(mir);
-    case OP_DIV_LONG:
-        return op_div_long(mir);
-    case OP_REM_LONG:
-        return op_rem_long(mir);
-    case OP_AND_LONG:
-        return op_and_long(mir);
-    case OP_OR_LONG:
-        return op_or_long(mir);
-    case OP_XOR_LONG:
-        return op_xor_long(mir);
-    case OP_SHL_LONG:
-        return op_shl_long(mir);
-    case OP_SHR_LONG:
-        return op_shr_long(mir);
-    case OP_USHR_LONG:
-        return op_ushr_long(mir);
-    case OP_ADD_FLOAT:
-        return op_add_float(mir);
-    case OP_SUB_FLOAT:
-        return op_sub_float(mir);
-    case OP_MUL_FLOAT:
-        return op_mul_float(mir);
-    case OP_DIV_FLOAT:
-        return op_div_float(mir);
-    case OP_REM_FLOAT:
-        return op_rem_float(mir);
-    case OP_ADD_DOUBLE:
-        return op_add_double(mir);
-    case OP_SUB_DOUBLE:
-        return op_sub_double(mir);
-    case OP_MUL_DOUBLE:
-        return op_mul_double(mir);
-    case OP_DIV_DOUBLE:
-        return op_div_double(mir);
-    case OP_REM_DOUBLE:
-        return op_rem_double(mir);
-    case OP_ADD_INT_2ADDR:
-        return op_add_int_2addr(mir);
-    case OP_SUB_INT_2ADDR:
-        return op_sub_int_2addr(mir);
-    case OP_MUL_INT_2ADDR:
-        return op_mul_int_2addr(mir);
-    case OP_DIV_INT_2ADDR:
-        return op_div_int_2addr(mir);
-    case OP_REM_INT_2ADDR:
-        return op_rem_int_2addr(mir);
-    case OP_AND_INT_2ADDR:
-        return op_and_int_2addr(mir);
-    case OP_OR_INT_2ADDR:
-        return op_or_int_2addr(mir);
-    case OP_XOR_INT_2ADDR:
-        return op_xor_int_2addr(mir);
-    case OP_SHL_INT_2ADDR:
-        return op_shl_int_2addr(mir);
-    case OP_SHR_INT_2ADDR:
-        return op_shr_int_2addr(mir);
-    case OP_USHR_INT_2ADDR:
-        return op_ushr_int_2addr(mir);
-    case OP_ADD_LONG_2ADDR:
-        return op_add_long_2addr(mir);
-    case OP_SUB_LONG_2ADDR:
-        return op_sub_long_2addr(mir);
-    case OP_MUL_LONG_2ADDR:
-        return op_mul_long_2addr(mir);
-    case OP_DIV_LONG_2ADDR:
-        return op_div_long_2addr(mir);
-    case OP_REM_LONG_2ADDR:
-        return op_rem_long_2addr(mir);
-    case OP_AND_LONG_2ADDR:
-        return op_and_long_2addr(mir);
-    case OP_OR_LONG_2ADDR:
-        return op_or_long_2addr(mir);
-    case OP_XOR_LONG_2ADDR:
-        return op_xor_long_2addr(mir);
-    case OP_SHL_LONG_2ADDR:
-        return op_shl_long_2addr(mir);
-    case OP_SHR_LONG_2ADDR:
-        return op_shr_long_2addr(mir);
-    case OP_USHR_LONG_2ADDR:
-        return op_ushr_long_2addr(mir);
-    case OP_ADD_FLOAT_2ADDR:
-        return op_add_float_2addr(mir);
-    case OP_SUB_FLOAT_2ADDR:
-        return op_sub_float_2addr(mir);
-    case OP_MUL_FLOAT_2ADDR:
-        return op_mul_float_2addr(mir);
-    case OP_DIV_FLOAT_2ADDR:
-        return op_div_float_2addr(mir);
-    case OP_REM_FLOAT_2ADDR:
-        return op_rem_float_2addr(mir);
-    case OP_ADD_DOUBLE_2ADDR:
-        return op_add_double_2addr(mir);
-    case OP_SUB_DOUBLE_2ADDR:
-        return op_sub_double_2addr(mir);
-    case OP_MUL_DOUBLE_2ADDR:
-        return op_mul_double_2addr(mir);
-    case OP_DIV_DOUBLE_2ADDR:
-        return op_div_double_2addr(mir);
-    case OP_REM_DOUBLE_2ADDR:
-        return op_rem_double_2addr(mir);
-    case OP_ADD_INT_LIT16:
-        return op_add_int_lit16(mir);
-    case OP_RSUB_INT:
-        return op_rsub_int(mir);
-    case OP_MUL_INT_LIT16:
-        return op_mul_int_lit16(mir);
-    case OP_DIV_INT_LIT16:
-        return op_div_int_lit16(mir);
-    case OP_REM_INT_LIT16:
-        return op_rem_int_lit16(mir);
-    case OP_AND_INT_LIT16:
-        return op_and_int_lit16(mir);
-    case OP_OR_INT_LIT16:
-        return op_or_int_lit16(mir);
-    case OP_XOR_INT_LIT16:
-        return op_xor_int_lit16(mir);
-    case OP_ADD_INT_LIT8:
-        return op_add_int_lit8(mir);
-    case OP_RSUB_INT_LIT8:
-        return op_rsub_int_lit8(mir);
-    case OP_MUL_INT_LIT8:
-        return op_mul_int_lit8(mir);
-    case OP_DIV_INT_LIT8:
-        return op_div_int_lit8(mir);
-    case OP_REM_INT_LIT8:
-        return op_rem_int_lit8(mir);
-    case OP_AND_INT_LIT8:
-        return op_and_int_lit8(mir);
-    case OP_OR_INT_LIT8:
-        return op_or_int_lit8(mir);
-    case OP_XOR_INT_LIT8:
-        return op_xor_int_lit8(mir);
-    case OP_SHL_INT_LIT8:
-        return op_shl_int_lit8(mir);
-    case OP_SHR_INT_LIT8:
-        return op_shr_int_lit8(mir);
-    case OP_USHR_INT_LIT8:
-        return op_ushr_int_lit8(mir);
-    case OP_EXECUTE_INLINE:
-        return op_execute_inline(mir, false /*isRange*/);
-    case OP_EXECUTE_INLINE_RANGE:
-        return op_execute_inline(mir, true /*isRange*/);
-//  case OP_INVOKE_OBJECT_INIT_RANGE:
-//      return op_invoke_object_init_range();
-    case OP_IGET_QUICK:
-        return op_iget_quick(mir);
-    case OP_IGET_WIDE_QUICK:
-        return op_iget_wide_quick(mir);
-    case OP_IGET_OBJECT_QUICK:
-        return op_iget_object_quick(mir);
-    case OP_IPUT_QUICK:
-        return op_iput_quick(mir);
-    case OP_IPUT_WIDE_QUICK:
-        return op_iput_wide_quick(mir);
-    case OP_IPUT_OBJECT_QUICK:
-        return op_iput_object_quick(mir);
-    case OP_INVOKE_VIRTUAL_QUICK:
-        return op_invoke_virtual_quick(mir);
-    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
-        return op_invoke_virtual_quick_range(mir);
-    case OP_INVOKE_SUPER_QUICK:
-        return op_invoke_super_quick(mir);
-    case OP_INVOKE_SUPER_QUICK_RANGE:
-        return op_invoke_super_quick_range(mir);
-    default:
-        ALOGI("JIT_INFO: JIT does not support bytecode %s\n",
-                dexGetOpcodeName(mir->dalvikInsn.opcode));
-        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
-        assert(false && "All opcodes should be supported.");
-        break;
-    }
-    return -1;
-}
-
-int op_nop(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_NOP);
-    return 0;
-}
-
-#if defined VTUNE_DALVIK
-/**
- * Send the label information (size, start_address and name) to VTune
- */
-void sendLabelInfoToVTune(int startStreamPtr, int endStreamPtr, const char* labelName) {
-    if (endStreamPtr == startStreamPtr) {
-        return;
-    }
-
-    iJIT_Method_Load jitMethod;
-    memset(&jitMethod, 0, sizeof(iJIT_Method_Load));
-    jitMethod.method_id = iJIT_GetNewMethodID();
-    jitMethod.method_name = const_cast<char *>(labelName);
-    jitMethod.method_load_address = (void *)startStreamPtr;
-    jitMethod.method_size = endStreamPtr-startStreamPtr;
-    int res = iJIT_NotifyEvent(iJVM_EVENT_TYPE_METHOD_LOAD_FINISHED, (void*)&jitMethod);
-    if (gDvmJit.printMe == true) {
-        if (res != 0) {
-            ALOGD("JIT API: a trace of %s method was written successfully address: id=%u, address=%p, size=%d."
-                    , labelName, jitMethod.method_id, jitMethod.method_load_address, jitMethod.method_size);
-        } else {
-            ALOGD("JIT API: failed to write a trace of %s method address: id=%u, address=%p, size=%d."
-                    , labelName, jitMethod.method_id, jitMethod.method_load_address, jitMethod.method_size);
-        }
-    }
-}
-#endif
-
-int getLabelOffset (unsigned int blockId) {
-    //Paranoid
-    if (gCompilationUnit == 0) {
-        //We can't do much except reporting an error
-        ALOGI("JIT_INFO: getLabelOffset has  null gCompilationUnit");
-        SET_JIT_ERROR(kJitErrorTraceFormation);
-        return -1;
-    }
-
-    //Get the BasicBlock
-    BasicBlock *bb = (BasicBlock *) dvmGrowableListGetElement(&gCompilationUnit->blockList, blockId);
-
-    //Transform into a BasicBlock_O1
-    BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (bb);
-
-    //Paranoid
-    if (bbO1 == 0 || bbO1->label == 0) {
-        //We can't do much except reporting an error
-        ALOGI("JIT_INFO: getLabelOffset has invalid basic block");
-        SET_JIT_ERROR(kJitErrorInvalidBBId);
-        return -1;
-    }
-
-    //Now return the label
-    return bbO1->label->lop.generic.offset;
-}
-
-
-/**
- * @brief Calculate magic number and shift for a given divisor
- * @param divisor divisor number for calculation
- * @param magic hold calculated magic number
- * @param shift hold calculated shift
- * @return void
- */
-void calculateMagicAndShift(int divisor, int* magic, int* shift) {
-    //It does not make sense to calculate magic and shift for zero divisor
-    assert (divisor != 0);
-
-    int p = 31;
-    unsigned abs_d, abs_nc, delta, quotient1, remainder1, quotient2, remainder2, tmp;
-    const unsigned two31 = 1 << p;
-
-    /* According to H.S.Warren's Hacker's Delight Chapter 10 and
-       T,Grablund, P.L.Montogomery's Division by invariant integers using multiplication
-       The magic number M and shift S can be calculated in the following way:
-       Let nc be the most positive value of numerator(n) such that nc = kd - 1, where divisor(d) >=2
-       Let nc be the most negative value of numerator(n) such that nc = kd + 1, where divisor(d) <= -2
-       Thus nc can be calculated like:
-       nc = 2^31 + 2^31 % d - 1, where d >= 2
-       nc = -2^31 + (2^31 + 1) % d, where d >= 2.
-
-       So the shift p is the smallest p satisfying
-       2^p > nc * (d - 2^p % d), where d >= 2
-       2^p > nc * (d + 2^p % d), where d <= -2.
-
-       the magic number M is calcuated by
-       M = (2^p + d - 2^p % d) / d, where d >= 2
-       M = (2^p - d - 2^p % d) / d, where d <= -2.
-
-       Notice that p is always bigger than or equal to 32, so we just return 32-p as the shift number S. */
-
-    // Initialize
-    abs_d = abs(divisor);
-    tmp = two31 + ((unsigned)divisor >> 31);
-    abs_nc = tmp - 1 - tmp % abs_d;
-    quotient1 = two31 / abs_nc;
-    remainder1 = two31 % abs_nc;
-    quotient2 = two31 / abs_d;
-    remainder2 = two31 % abs_d;
-
-    // To avoid handling both positive and negative divisor, Hacker's Delight introduces a method to handle
-    // these 2 cases together to avoid duplication.
-    do {
-        p++;
-        quotient1 = 2 * quotient1;
-        remainder1 = 2 * remainder1;
-        if (remainder1 >= abs_nc){
-            quotient1++;
-            remainder1 = remainder1 - abs_nc;
-        }
-        quotient2 = 2 * quotient2;
-        remainder2 = 2 * remainder2;
-        if (remainder2 >= abs_d){
-            quotient2++;
-            remainder2 = remainder2 - abs_d;
-        }
-        delta = abs_d - remainder2;
-    }while (quotient1 < delta || (quotient1 == delta && remainder1 == 0));
-
-    *magic = (divisor > 0) ? (quotient2 + 1) : (-quotient2 - 1);
-    *shift = p - 32;
-}
diff --git a/vm/compiler/codegen/x86/Lower.h b/vm/compiler/codegen/x86/Lower.h
deleted file mode 100644
index 44c8923..0000000
--- a/vm/compiler/codegen/x86/Lower.h
+++ /dev/null
@@ -1,1453 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file Lower.h
-    \brief A header file to define interface between lowering, register allocator, and scheduling
-*/
-
-#ifndef _DALVIK_LOWER
-#define _DALVIK_LOWER
-
-#define CODE_CACHE_PADDING 1024 //code space for a single bytecode
-// comment out for phase 1 porting
-#define PREDICTED_CHAINING
-#define JIT_CHAIN
-
-#define NCG_O1
-//compilaton flags used by NCG O1
-#define DUMP_EXCEPTION //to measure performance, required to have correct exception handling
-/*! multiple versions for hardcoded registers */
-#define HARDREG_OPT
-#define CFG_OPT
-/*! remove redundant move ops when accessing virtual registers */
-#define MOVE_OPT
-/*! remove redundant spill of virtual registers */
-#define SPILL_OPT
-#define XFER_OPT
-/*! use live range analysis to allocate registers */
-#define LIVERANGE_OPT
-/*! remove redundant null check */
-#define NULLCHECK_OPT
-//#define BOUNDCHECK_OPT
-#define CALL_FIX
-#define NATIVE_FIX
-#define INVOKE_FIX //optimization
-#define GETVR_FIX //optimization
-
-#include "CodegenErrors.h"
-#include "Dalvik.h"
-#include "enc_wrapper.h"
-#include "AnalysisO1.h"
-#include "CompileTable.h"
-#include "compiler/CompilerIR.h"
-
-//compilation flags for debugging
-//#define DEBUG_INFO
-//#define DEBUG_CALL_STACK
-//#define DEBUG_IGET_OBJ
-//#define DEBUG_NCG_CODE_SIZE
-//#define DEBUG_NCG
-//#define DEBUG_NCG_1
-//#define DEBUG_LOADING
-//#define USE_INTERPRETER
-//#define DEBUG_EACH_BYTECODE
-
-/*! registers for functions are hardcoded */
-#define HARDCODE_REG_CALL
-#define HARDCODE_REG_SHARE
-#define HARDCODE_REG_HELPER
-
-#define PhysicalReg_FP PhysicalReg_EDI
-#define PhysicalReg_Glue PhysicalReg_EBP
-
-//COPIED from interp/InterpDefs.h
-#define FETCH(_offset) (rPC[(_offset)])
-#define INST_INST(_inst) ((_inst) & 0xff)
-#define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
-#define INST_B(_inst)       ((_inst) >> 12)
-#define INST_AA(_inst)      ((_inst) >> 8)
-
-#define offEBP_self 8
-#define offEBP_spill -56
-#define offThread_jniLocal_nextEntry 168
-
-// Definitions must be consistent with vm/mterp/x86/header.S
-#define FRAME_SIZE     124
-
-typedef enum ArgsDoneType {
-    ArgsDone_Normal = 0,
-    ArgsDone_Native,
-    ArgsDone_Full
-} ArgsDoneType;
-
-/*! An enum type
-    to list bytecodes for AGET, APUT
-*/
-typedef enum ArrayAccess {
-    AGET, AGET_WIDE, AGET_CHAR, AGET_SHORT, AGET_BOOLEAN, AGET_BYTE,
-    APUT, APUT_WIDE, APUT_CHAR, APUT_SHORT, APUT_BOOLEAN, APUT_BYTE
-} ArrayAccess;
-/*! An enum type
-    to list bytecodes for IGET, IPUT
-*/
-typedef enum InstanceAccess {
-    IGET, IGET_WIDE, IPUT, IPUT_WIDE
-} InstanceAccess;
-/*! An enum type
-    to list bytecodes for SGET, SPUT
-*/
-typedef enum StaticAccess {
-    SGET, SGET_WIDE, SPUT, SPUT_WIDE
-} StaticAccess;
-
-typedef enum JmpCall_type {
-    JmpCall_uncond = 1,
-    JmpCall_cond,
-    JmpCall_reg, //jump reg32
-    JmpCall_call
-} JmpCall_type;
-
-//! \enum AtomOpCode
-//! \brief Pseudo-mnemonics for Atom
-//! \details Initially included to be in sync with ArmOpCode which specifies
-//! additional pseudo mnemonics for use during codegen, but it has
-//! diverted. Although there are references to this everywhere,
-//! very little of this is actually used for functionality.
-//! \todo Either refactor to match ArmOpCode or remove dependency on this.
-enum AtomOpCode {
-    ATOM_PSEUDO_CHAINING_CELL_BACKWARD_BRANCH = -15,
-    ATOM_NORMAL_ALU = -14,
-    ATOM_PSEUDO_ENTRY_BLOCK = -13,
-    ATOM_PSEUDO_EXIT_BLOCK = -12,
-    ATOM_PSEUDO_TARGET_LABEL = -11,
-    ATOM_PSEUDO_CHAINING_CELL_HOT = -10,
-    ATOM_PSEUDO_CHAINING_CELL_INVOKE_PREDICTED = -9,
-    ATOM_PSEUDO_CHAINING_CELL_INVOKE_SINGLETON = -8,
-    ATOM_PSEUDO_CHAINING_CELL_NORMAL = -7,
-    ATOM_PSEUDO_DALVIK_BYTECODE_BOUNDARY = -6,
-    ATOM_PSEUDO_ALIGN4 = -5,
-    ATOM_PSEUDO_PC_RECONSTRUCTION_CELL = -4,
-    ATOM_PSEUDO_PC_RECONSTRUCTION_BLOCK_LABEL = -3,
-    ATOM_PSEUDO_EH_BLOCK_LABEL = -2,
-    ATOM_PSEUDO_NORMAL_BLOCK_LABEL = -1,
-    ATOM_NORMAL,
-};
-
-//! \enum LowOpndType
-//! \brief Defines types of operands that a LowOp can have.
-//! \details The Imm, Mem, and Reg variants correspond literally to what
-//! the final encoded x86 instruction will have. The others are used for
-//! additional behavior needed before the x86 encoding.
-//! \see LowOp
-enum LowOpndType {
-    //! \brief Immediate
-    LowOpndType_Imm,
-    //! \brief Register
-    LowOpndType_Reg,
-    //! \brief Memory access
-    LowOpndType_Mem,
-    //! \brief Used for jumps to labels
-    LowOpndType_Label,
-    //! \brief Used for jumps to other blocks
-    LowOpndType_BlockId,
-    //! \brief Used for chaining
-    LowOpndType_Chain
-};
-
-//! \enum LowOpndDefUse
-//! \brief Defines type of usage that a LowOpnd can have.
-//! \see LowOpnd
-enum LowOpndDefUse {
-    //! \brief Definition
-    LowOpndDefUse_Def,
-    //! \brief Usage
-    LowOpndDefUse_Use,
-    //! \brief Usage and Definition
-    LowOpndDefUse_UseDef
-};
-
-//! \enum MemoryAccessType
-//! \brief Classifies type of memory access.
-enum MemoryAccessType {
-    //! \brief access Dalvik virtual register
-    MemoryAccess_VR,
-    //! \brief access spill region
-    MemoryAccess_SPILL,
-    //! \brief unclassified memory access
-    MemoryAccess_Unknown,
-    //! \brief access to read-only constant section
-    MemoryAccess_Constants,
-};
-
-//! \enum UseDefEntryType
-//! \brief Defines types of resources on which there can be a dependency.
-enum UseDefEntryType {
-    //! \brief Control flags, EFLAGS register
-    UseDefType_Ctrl,
-    //! \brief Floating-point stack
-    //! \details This is a very generic resource for x87 operations and
-    //! doesn't break down different possible resources like control word,
-    //! status word, FPU flags, etc. All of x87 resources fall into this
-    //! type of resource.
-    UseDefType_Float,
-    //! \brief Dalvik virtual register. Corresponds to MemoryAccess_VR
-    UseDefType_MemVR,
-    //! \brief Spill region. Corresponds to MemoryAccess_SPILL
-    UseDefType_MemSpill,
-    //! \brief Unclassified memory access. Corresponds to MemoryAccess_Unknown
-    //! \details No memory disambiguation will be done with unknown accesses
-    UseDefType_MemUnknown,
-    //! \brief Register
-    UseDefType_Reg
-};
-
-//! \enum DependencyType
-//! \brief Defines types of dependencies on a resource.
-enum DependencyType {
-    //! \brief Read after Write
-    Dependency_RAW,
-    //! \brief Write after Write
-    Dependency_WAW,
-    //! \brief Write after Read
-    Dependency_WAR,
-};
-
-//! \enum LatencyBetweenNativeInstructions
-//! \brief Defines reasons for what causes pipeline stalls
-//! between two instructions.
-//! \warning Make sure that if adding new reasons here,
-//! the scheduler needs updated with the actual latency value.
-//! \see mapLatencyReasonToValue
-enum LatencyBetweenNativeInstructions {
-    //! \brief No latency between the two instructions
-    Latency_None = 0,
-    //! \brief Stall in address generation phase of pipeline
-    //! when register is not available.
-    Latency_Agen_stall,
-    //! \brief Stall when a memory load is blocked by a store
-    //! and there is no store forwarding.
-    Latency_Load_blocked_by_store,
-    //! \brief Stall due to cache miss during load from memory
-    Latency_Memory_Load,
-};
-
-//! \brief Defines a relationship between a resource and its producer.
-struct UseDefProducerEntry {
-    //! \brief Resource type on which there is a dependency.
-    UseDefEntryType type;
-    //! \brief Virtual or physical register this resource is
-    //! associated with.
-    //! \details When physical, this is of enum type PhysicalReg.
-    //! When VR, this is the virtual register number.
-    //! When there is no register related dependency, this is
-    //! negative.
-    int regNum;
-    //! \brief Corresponds to LowOp::slotId to keep track of producer.
-    unsigned int producerSlot;
-};
-
-//! \brief Defines a relationship between a resource and its users.
-struct UseDefUserEntry {
-    //! \brief Resource type on which there is a dependency.
-    UseDefEntryType type;
-    //! \brief Virtual or physical register this resource is
-    //! associated with.
-    //! \details When physical, this is of enum type PhysicalReg.
-    //! When VR, this is the virtual register number.
-    //! When there is no register related dependency, this is
-    //! negative.
-    int regNum;
-    //! \brief A list of LowOp::slotId to keep track of all users
-    //! of this resource.
-    std::vector<unsigned int> useSlotsList;
-};
-
-//! \brief Holds information on the data dependencies
-struct DependencyInformation {
-    //! \brief Type of data hazard
-    DependencyType dataHazard;
-    //! \brief Holds the LowOp::slotId of the LIR that causes this
-    //! data dependence.
-    unsigned int lowopSlotId;
-    //! \brief Description for what causes the edge latency
-    //! \see LatencyBetweenNativeInstructions
-    LatencyBetweenNativeInstructions causeOfEdgeLatency;
-    //! \brief Holds latency information for edges in the
-    //! dependency graph, not execute to execute latency for the
-    //! instructions.
-    int edgeLatency;
-};
-
-//! \brief Holds general information about an operand.
-struct LowOpnd {
-    //! \brief Classification of operand.
-    LowOpndType type;
-    //! \brief Size of operand.
-    OpndSize size;
-    //! \brief Usage, definition, or both of operand.
-    LowOpndDefUse defuse;
-};
-
-//! \brief Holds information about a register operand.
-struct LowOpndReg {
-    //! \brief Classification on type of register.
-    LowOpndRegType regType;
-    //! \brief Register number, either logical or physical.
-    int regNum;
-    //! \brief When false, register is logical.
-    bool isPhysical;
-};
-
-//! \brief Holds information about an immediate operand.
-struct LowOpndImm {
-    //! \brief Value of the immediate.
-    s4 value;
-};
-
-//! \brief Holds information about an immediate operand where the immediate
-//! has not been generated yet.
-struct LowOpndBlock {
-    //! \brief Holds id of MIR level basic block.
-    s4 value;
-    //! \brief Whether the immediate needs to be aligned within 16-bytes
-    bool immediateNeedsAligned;
-};
-
-//! \brief Defines maximum length of string holding label name.
-#define LABEL_SIZE 256
-
-//! \brief Holds information about an immediate operand where the immediate
-//! has not been generated yet from label.
-struct LowOpndLabel {
-    //! \brief Name of the label for which to generate immediate.
-    char label[LABEL_SIZE];
-    //! \brief This is true when label is short term distance from caller
-    //! and an 8-bit operand is sufficient.
-    bool isLocal;
-};
-
-//! \brief Holds information about a memory operand.
-struct LowOpndMem {
-    //! \brief Displacement
-    LowOpndImm m_disp;
-    //! \brief Scaling
-    LowOpndImm m_scale;
-    //! \brief Index Register
-    LowOpndReg m_index;
-    //! \brief Base Register
-    LowOpndReg m_base;
-    //! \brief If true, must use the scaling value.
-    bool hasScale;
-    //! \brief Defines type of memory access.
-    MemoryAccessType mType;
-    //! \brief If positive, this represents the VR number
-    int index;
-};
-
-//! \brief Data structure for an x86 LIR.
-//! \todo Decouple fields used for scheduling from this struct.
-//! is a good idea if using it throughout the trace JIT and never
-//! actually passing it for scheduling.
-struct LowOp {
-    //! \brief Holds general LIR information (Google's implementation)
-    //! \warning Only offset information is used for x86 and the other
-    //! fields are not valid except in LowOpBlockLabel.
-    LIR generic;
-    //! \brief x86 mnemonic for instruction
-    Mnemonic opCode;
-    //! \brief x86 pseudo-mnemonic
-    AtomOpCode opCode2;
-    //! \brief Destination operand
-    //! \details This is not used when there are only 0 or 1 operands.
-    LowOpnd opndDest;
-    //! \brief Source operand
-    //! \details This is used when there is a single operand.
-    LowOpnd opndSrc;
-    //! \brief Holds number of operands for this LIR (0, 1, or 2)
-    unsigned short numOperands;
-    //! \brief Logical timestamp for ordering.
-    //! \details This value should uniquely identify an LIR and also
-    //! provide natural ordering depending on when it was requested.
-    //! This is used during scheduling to hold original order for the
-    //! native basic block.
-    unsigned int slotId;
-    //! \brief Logical time for when the LIR is ready.
-    //! \details This field is used only for scheduling.
-    int readyTime;
-    //! \brief Cycle in which LIR is scheduled for issue.
-    //! \details This field is used only for scheduling.
-    int scheduledTime;
-    //! \brief Execute to execute time for this instruction.
-    //! \details This field is used only for scheduling.
-    //! \see MachineModelEntry::executeToExecuteLatency
-    int instructionLatency;
-    //! \brief Issue port for this instruction.
-    //! \details This field is used only for scheduling.
-    //! \see MachineModelEntry::issuePortType
-    int portType;
-    //! \brief Weight of longest path in dependency graph from
-    //! current instruction to end of the basic block.
-    //! \details This field is used only for scheduling.
-    int longestPath;
-};
-
-//! \brief Specialized LowOp with known label operand but
-//! whose offset immediate is not known yet.
-struct LowOpLabel : LowOp {
-    //! \brief Label operand whose immediate has not yet been
-    //! generated.
-    LowOpndLabel labelOpnd;
-};
-
-//! \brief Specialized LowOp for use with block operand whose id
-//! is known but the offset immediate has not been generated yet.
-struct LowOpBlock : LowOp {
-    //! \brief Non-generated immediate operand
-    LowOpndBlock blockIdOpnd;
-};
-
-//! \brief Specialized LowOp which is only used with
-//! pseudo-mnemonic.
-//! \see AtomOpCode
-struct LowOpBlockLabel {
-    //! \todo Does not use inheritance like the other LowOp
-    //! data structures because of a git merge issue. In future,
-    //! this can be safely updated.
-    LowOp lop;
-};
-
-//! \brief Specialized LowOp with an immediate operand.
-struct LowOpImm : LowOp {
-    //! \brief Immediate
-    LowOpndImm immOpnd;
-};
-
-//! \brief Specialized LowOp with a memory operand.
-struct LowOpMem : LowOp {
-    //! \brief Memory Operand
-    LowOpndMem memOpnd;
-};
-
-//! \brief Specialized LowOp with register operand.
-struct LowOpReg : LowOp {
-    //! \brief Register
-    LowOpndReg regOpnd;
-};
-
-//! \brief Specialized LowOp for immediate to register.
-struct LowOpImmReg : LowOp {
-    //! \brief Immediate as source.
-    LowOpndImm immSrc;
-    //! \brief Register as destination.
-    LowOpndReg regDest;
-};
-
-//! \brief Specialized LowOp for register to register.
-struct LowOpRegReg : LowOp {
-    //! \brief Register as source.
-    LowOpndReg regSrc;
-    //! \brief Register as destination.
-    LowOpndReg regDest;
-};
-
-//! \brief Specialized LowOp for memory to register.
-struct LowOpMemReg : LowOp {
-    //! \brief Memory as source.
-    LowOpndMem memSrc;
-    //! \brief Register as destination.
-    LowOpndReg regDest;
-   //! \brief ptr to data structure containing 64 bit constants
-    ConstInfo *constLink;
-};
-
-//! \brief Specialized LowOp for immediate to memory.
-struct LowOpImmMem : LowOp {
-    //! \brief Immediate as source.
-    LowOpndImm immSrc;
-    //! \brief Memory as destination.
-    LowOpndMem memDest;
-};
-
-//! \brief Specialized LowOp for register to memory.
-struct LowOpRegMem : LowOp {
-    //! \brief Register as source.
-    LowOpndReg regSrc;
-    //! \brief Memory as destination.
-    LowOpndMem memDest;
-};
-
-/*!
-\brief data structure for labels used when lowering a method
-
-four label maps are defined: globalMap globalShortMap globalWorklist globalShortWorklist
-globalMap: global labels where codePtr points to the label
-           freeLabelMap called in clearNCG
-globalWorklist: global labels where codePtr points to an instruciton using the label
-  standalone NCG -------
-                accessed by insertLabelWorklist & performLabelWorklist
-  code cache ------
-                inserted by performLabelWorklist(false),
-                handled & cleared by generateRelocation in NcgFile.c
-globalShortMap: local labels where codePtr points to the label
-                freeShortMap called after generation of one bytecode
-globalShortWorklist: local labels where codePtr points to an instruction using the label
-                accessed by insertShortWorklist & insertLabel
-definition of local label: life time of the label is within a bytecode or within a helper function
-extra label maps are used by code cache:
-  globalDataWorklist VMAPIWorklist
-*/
-typedef struct LabelMap {
-  char label[LABEL_SIZE];
-  char* codePtr; //code corresponding to the label or code that uses the label
-  struct LabelMap* nextItem;
-  OpndSize size;
-  uint  addend;
-} LabelMap;
-/*!
-\brief data structure to handle forward jump (GOTO, IF)
-
-accessed by insertNCGWorklist & performNCGWorklist
-*/
-typedef struct NCGWorklist {
-  //when WITH_JIT, relativePC stores the target basic block id
-  s4 relativePC; //relative offset in bytecode
-  int offsetPC;  //PC in bytecode
-  int offsetNCG; //PC in native code
-  char* codePtr; //code for native jump instruction
-  struct NCGWorklist* nextItem;
-  OpndSize size;
-}NCGWorklist;
-/*!
-\brief data structure to handle SWITCH & FILL_ARRAY_DATA
-
-two data worklist are defined: globalDataWorklist (used by code cache) & methodDataWorklist
-methodDataWorklist is accessed by insertDataWorklist & performDataWorklist
-*/
-typedef struct DataWorklist {
-  s4 relativePC; //relative offset in bytecode to access the data
-  int offsetPC;  //PC in bytecode
-  int offsetNCG; //PC in native code
-  char* codePtr; //code for native instruction add_imm_reg imm, %edx
-  char* codePtr2;//code for native instruction add_reg_reg %eax, %edx for SWITCH
-                 //                            add_imm_reg imm, %edx for FILL_ARRAY_DATA
-  struct DataWorklist* nextItem;
-}DataWorklist;
-#ifdef ENABLE_TRACING
-typedef struct MapWorklist {
-  u4 offsetPC;
-  u4 offsetNCG;
-  int isStartOfPC; //1 --> true 0 --> false
-  struct MapWorklist* nextItem;
-} MapWorklist;
-#endif
-
-#define BUFFER_SIZE 1024 //# of Low Ops buffered
-//the following three numbers are hardcoded, please CHECK
-#define BYTECODE_SIZE_PER_METHOD 81920
-#define NATIVE_SIZE_PER_DEX 19000000 //FIXME for core.jar: 16M --> 18M for O1
-#define NATIVE_SIZE_FOR_VM_STUBS 100000
-#define MAX_HANDLER_OFFSET 1024 //maximal number of handler offsets
-
-extern int LstrClassCastExceptionPtr, LstrInstantiationErrorPtr, LstrInternalError, LstrFilledNewArrayNotImpl;
-extern int LstrArithmeticException, LstrArrayIndexException, LstrArrayStoreException, LstrStringIndexOutOfBoundsException;
-extern int LstrDivideByZero, LstrNegativeArraySizeException, LstrNoSuchMethodError, LstrNullPointerException;
-extern int LdoubNeg, LvaluePosInfLong, LvalueNegInfLong, LvalueNanLong, LshiftMask, Lvalue64, L64bits, LintMax, LintMin;
-
-extern LabelMap* globalMap;
-extern LabelMap* globalShortMap;
-extern LabelMap* globalWorklist;
-extern LabelMap* globalShortWorklist;
-extern NCGWorklist* globalNCGWorklist;
-extern DataWorklist* methodDataWorklist;
-#ifdef ENABLE_TRACING
-extern MapWorklist* methodMapWorklist;
-#endif
-extern PhysicalReg scratchRegs[4];
-
-#define C_SCRATCH_1 scratchRegs[0]
-#define C_SCRATCH_2 scratchRegs[1]
-#define C_SCRATCH_3 scratchRegs[2] //scratch reg inside callee
-
-extern LowOp* ops[BUFFER_SIZE];
-extern bool isScratchPhysical;
-extern u2* rPC;
-extern int offsetPC;
-extern int offsetNCG;
-extern int mapFromBCtoNCG[BYTECODE_SIZE_PER_METHOD];
-extern char* streamStart;
-
-extern char* streamCode;
-
-extern char* streamMethodStart; //start of the method
-extern char* stream; //current stream pointer
-
-extern Method* currentMethod;
-extern int currentExceptionBlockIdx;
-
-extern int globalMapNum;
-extern int globalWorklistNum;
-extern int globalDataWorklistNum;
-extern int globalPCWorklistNum;
-extern int chainingWorklistNum;
-extern int VMAPIWorklistNum;
-
-extern LabelMap* globalDataWorklist;
-extern LabelMap* globalPCWorklist;
-extern LabelMap* chainingWorklist;
-extern LabelMap* VMAPIWorklist;
-
-extern int ncgClassNum;
-extern int ncgMethodNum;
-
-// Global pointer to the current CompilationUnit
-class CompilationUnit_O1;
-extern CompilationUnit_O1 *gCompilationUnit;
-
-bool existATryBlock(Method* method, int startPC, int endPC);
-// interface between register allocator & lowering
-extern int num_removed_nullCheck;
-
-//Allocate a register
-int registerAlloc(int type, int reg, bool isPhysical, bool updateRef, bool isDest = false);
-//Allocate a register trying to alias a virtual register with a temporary
-int registerAllocMove(int reg, int type, bool isPhysical, int srcReg, bool isDest = false);
-
-int checkVirtualReg(int reg, LowOpndRegType type, int updateRef); //returns the physical register
-int updateRefCount(int reg, LowOpndRegType type);
-int updateRefCount2(int reg, int type, bool isPhysical);
-int spillVirtualReg(int vrNum, LowOpndRegType type, bool updateTable);
-int checkTempReg(int reg, int type, bool isPhysical, int vA);
-bool checkTempReg2(int reg, int type, bool isPhysical, int physicalRegForVR, int vB);
-int freeReg(bool writeBackAllVRs);
-int nextVersionOfHardReg(PhysicalReg pReg, int refCount);
-int updateVirtualReg(int reg, LowOpndRegType type);
-int setVRNullCheck(int regNum, OpndSize size);
-bool isVRNullCheck(int regNum, OpndSize size);
-void setVRBoundCheck(int vr_array, int vr_index);
-bool isVRBoundCheck(int vr_array, int vr_index);
-int requestVRFreeDelay(int regNum, u4 reason);
-int cancelVRFreeDelayRequest(int regNum, u4 reason);
-
-// Update delay flag for all VRs, stored in physical registers
-bool requestVRFreeDelayAll(u4 reason);
-bool cancelVRFreeDelayRequestAll(u4 reason);
-
-bool getVRFreeDelayRequested(int regNum);
-
-//Update the virtual register use information
-void updateVRAtUse(int reg, LowOpndRegType pType, int regAll);
-int touchEcx();
-int touchEax();
-int touchEdx();
-int beforeCall(const char* target);
-int afterCall(const char* target);
-void startBranch();
-void endBranch();
-void rememberState(int);
-void goToState(int);
-void transferToState(int);
-
-//Handle virtual register writebacks
-int handleRegistersEndOfBB(bool syncChildren);
-
-//Call to reset certain flags before generating native code
-void startNativeCode(int num, int type);
-//Call to reset certain flags after generating native code
-void endNativeCode(void);
-
-#define XMM_1 PhysicalReg_XMM0
-#define XMM_2 PhysicalReg_XMM1
-#define XMM_3 PhysicalReg_XMM2
-#define XMM_4 PhysicalReg_XMM3
-
-/////////////////////////////////////////////////////////////////////////////////
-//LR[reg] = disp + PR[base_reg] or disp + LR[base_reg]
-void load_effective_addr(int disp, int base_reg, bool isBasePhysical,
-                          int reg, bool isPhysical);
-void load_effective_addr_scale(int base_reg, bool isBasePhysical,
-                                int index_reg, bool isIndexPhysical, int scale,
-                                int reg, bool isPhysical);
-//! lea reg, [base_reg + index_reg*scale + disp]
-void load_effective_addr_scale_disp(int base_reg, bool isBasePhysical, int disp,
-                int index_reg, bool isIndexPhysical, int scale,
-                int reg, bool isPhysical);
-//! Loads a 16-bit value into the x87 FPU control word. Typically used to
-//! establish or change the FPU's operational mode. Can cause exceptions to
-//! be thrown if not cleared beforehand.
-void load_fpu_cw(int disp, int base_reg, bool isBasePhysical);
-void store_fpu_cw(bool checkException, int disp, int base_reg, bool isBasePhysical);
-void convert_integer(OpndSize srcSize, OpndSize dstSize);
-void convert_int_to_fp(int srcReg, bool isSrcPhysical, int destReg, bool isDestPhysical, bool isDouble);
-void load_fp_stack(LowOp* op, OpndSize size, int disp, int base_reg, bool isBasePhysical);
-void load_int_fp_stack(OpndSize size, int disp, int base_reg, bool isBasePhysical);
-void load_int_fp_stack_imm(OpndSize size, int imm);
-void store_fp_stack(LowOp* op, bool pop, OpndSize size, int disp, int base_reg, bool isBasePhysical);
-void store_int_fp_stack(LowOp* op, bool pop, OpndSize size, int disp, int base_reg, bool isBasePhysical);
-
-void load_fp_stack_VR(OpndSize size, int vA);
-void load_int_fp_stack_VR(OpndSize size, int vA);
-void store_fp_stack_VR(bool pop, OpndSize size, int vA);
-void store_int_fp_stack_VR(bool pop, OpndSize size, int vA);
-void compare_VR_ss_reg(int vA, int reg, bool isPhysical);
-void compare_VR_sd_reg(int vA, int reg, bool isPhysical);
-void fpu_VR(ALU_Opcode opc, OpndSize size, int vA);
-void compare_reg_mem(LowOp* op, OpndSize size, int reg, bool isPhysical,
-                           int disp, int base_reg, bool isBasePhysical);
-void compare_mem_reg(OpndSize size,
-                           int disp, int base_reg, bool isBasePhysical,
-                           int reg, bool isPhysical);
-void compare_VR_reg(OpndSize size,
-                           int vA,
-                           int reg, bool isPhysical);
-void compare_imm_reg(OpndSize size, int imm,
-                           int reg, bool isPhysical);
-void compare_imm_mem(OpndSize size, int imm,
-                           int disp, int base_reg, bool isBasePhysical);
-void compare_imm_VR(OpndSize size, int imm,
-                           int vA);
-void compare_reg_reg(int reg1, bool isPhysical1,
-                           int reg2, bool isPhysical2);
-void compare_reg_reg_16(int reg1, bool isPhysical1,
-                         int reg2, bool isPhysical2);
-void compare_ss_mem_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
-                              int reg, bool isPhysical);
-void compare_ss_reg_with_reg(LowOp* op, int reg1, bool isPhysical1,
-                              int reg2, bool isPhysical2);
-void compare_sd_mem_with_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
-                              int reg, bool isPhysical);
-void compare_sd_reg_with_reg(LowOp* op, int reg1, bool isPhysical1,
-                              int reg2, bool isPhysical2);
-void compare_fp_stack(bool pop, int reg, bool isDouble);
-void test_imm_reg(OpndSize size, int imm, int reg, bool isPhysical);
-void test_imm_mem(OpndSize size, int imm, int disp, int reg, bool isPhysical);
-
-void conditional_move_reg_to_reg(OpndSize size, ConditionCode cc, int reg1, bool isPhysical1, int reg, bool isPhysical);
-void move_ss_mem_to_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
-                        int reg, bool isPhysical);
-void move_ss_reg_to_mem(LowOp* op, int reg, bool isPhysical,
-                         int disp, int base_reg, bool isBasePhysical);
-LowOpMemReg* move_ss_mem_to_reg_noalloc(int disp, int base_reg, bool isBasePhysical,
-                         MemoryAccessType mType, int mIndex,
-                         int reg, bool isPhysical);
-LowOpRegMem* move_ss_reg_to_mem_noalloc(int reg, bool isPhysical,
-                         int disp, int base_reg, bool isBasePhysical,
-                         MemoryAccessType mType, int mIndex);
-void move_sd_mem_to_reg(int disp, int base_reg, bool isBasePhysical,
-                         int reg, bool isPhysical);
-void move_sd_reg_to_mem(LowOp* op, int reg, bool isPhysical,
-                         int disp, int base_reg, bool isBasePhysical);
-
-void conditional_jump(ConditionCode cc, const char* target, bool isShortTerm);
-void unconditional_jump(const char* target, bool isShortTerm);
-void conditional_jump_int(ConditionCode cc, int target, OpndSize size);
-void unconditional_jump_int(int target, OpndSize size);
-void conditional_jump_block(ConditionCode cc, int targetBlockId, bool immediateNeedsAligned = false);
-void unconditional_jump_block(int targetBlockId, bool immediateNeedsAligned = false);
-void unconditional_jump_reg(int reg, bool isPhysical);
-void unconditional_jump_rel32(void * target);
-void call(const char* target);
-void call_reg(int reg, bool isPhysical);
-void call_reg_noalloc(int reg, bool isPhysical);
-void call_mem(int disp, int reg, bool isPhysical);
-void x86_return();
-
-void alu_unary_reg(OpndSize size, ALU_Opcode opc, int reg, bool isPhysical);
-void alu_unary_mem(LowOp* op, OpndSize size, ALU_Opcode opc, int disp, int base_reg, bool isBasePhysical);
-
-void alu_binary_imm_mem(OpndSize size, ALU_Opcode opc,
-                         int imm, int disp, int base_reg, bool isBasePhysical);
-void alu_binary_imm_reg(OpndSize size, ALU_Opcode opc, int imm, int reg, bool isPhysical);
-//Operate on a VR with another VR and an immediate
-bool alu_imm_to_VR(OpndSize size, ALU_Opcode opc,
-                         int srcVR, int destVR, int imm, int tempReg, bool isTempPhysical, const MIR * mir);
-void alu_binary_mem_reg(OpndSize size, ALU_Opcode opc,
-                         int disp, int base_reg, bool isBasePhysical,
-                         int reg, bool isPhysical);
-void alu_binary_VR_reg(OpndSize size, ALU_Opcode opc, int vA, int reg, bool isPhysical);
-void alu_sd_binary_VR_reg(ALU_Opcode opc, int vA, int reg, bool isPhysical, bool isSD);
-void alu_binary_reg_reg(OpndSize size, ALU_Opcode opc,
-                         int reg1, bool isPhysical1,
-                         int reg2, bool isPhysical2);
-void alu_binary_reg_mem(OpndSize size, ALU_Opcode opc,
-                         int reg, bool isPhysical,
-                         int disp, int base_reg, bool isBasePhysical);
-
-void fpu_mem(LowOp* op, ALU_Opcode opc, OpndSize size, int disp, int base_reg, bool isBasePhysical);
-void alu_ss_binary_reg_reg(ALU_Opcode opc, int reg, bool isPhysical,
-                            int reg2, bool isPhysical2);
-void alu_sd_binary_reg_reg(ALU_Opcode opc, int reg, bool isPhysical,
-                            int reg2, bool isPhysical2);
-
-void push_mem_to_stack(OpndSize size, int disp, int base_reg, bool isBasePhysical);
-void push_reg_to_stack(OpndSize size, int reg, bool isPhysical);
-
-// create a new record for a 64 bit constant
-void addNewToConstList(struct ConstInfo** listPtr, int constL, int constH, int reg, bool align);
-// save address of memory location to be patched
-bool saveAddrToConstList(struct ConstInfo** listPtr, int constL, int constH, int reg, char* patchAddr, int offset);
-// access address of global constants
-int getGlobalDataAddr(const char* dataName);
-
-//returns the pointer to end of the native code
-void move_reg_to_mem(OpndSize size,
-                      int reg, bool isPhysical,
-                      int disp, int base_reg, bool isBasePhysical);
-LowOpMemReg* move_mem_to_reg(OpndSize size,
-                      int disp, int base_reg, bool isBasePhysical,
-                      int reg, bool isPhysical);
-void movez_mem_to_reg(OpndSize size,
-                      int disp, int base_reg, bool isBasePhysical,
-                      int reg, bool isPhysical);
-void movez_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical,
-                      int reg2, bool isPhysical2);
-void moves_mem_to_reg(LowOp* op, OpndSize size,
-                      int disp, int base_reg, bool isBasePhysical,
-                      int reg, bool isPhysical);
-void movez_mem_disp_scale_to_reg(OpndSize size,
-                      int base_reg, bool isBasePhysical,
-                      int disp, int index_reg, bool isIndexPhysical, int scale,
-                      int reg, bool isPhysical);
-void moves_mem_disp_scale_to_reg(OpndSize size,
-                      int base_reg, bool isBasePhysical,
-                      int disp, int index_reg, bool isIndexPhysical, int scale,
-                      int reg, bool isPhysical);
-
-//! \brief Performs MOVSX reg, reg2
-//!
-//! \details Sign extends reg and moves to reg2
-//! Size of destination register is fixed at 32-bits
-//! \param size of the source operand
-//! \param reg source operand
-//! \param isPhysical if reg is a physical register
-//! \param reg2 destination register
-//! \param isPhysical2 if reg2 is a physical register
-void moves_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical,
-                      int reg2, bool isPhysical2);
-void move_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical,
-                      int reg2, bool isPhysical2);
-void move_reg_to_reg_noalloc(OpndSize size,
-                      int reg, bool isPhysical,
-                      int reg2, bool isPhysical2);
-void move_mem_scale_to_reg(OpndSize size,
-                            int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
-                            int reg, bool isPhysical);
-void move_mem_disp_scale_to_reg(OpndSize size,
-                int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                int reg, bool isPhysical);
-void move_reg_to_mem_scale(OpndSize size,
-                            int reg, bool isPhysical,
-                            int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale);
-void move_reg_to_mem_disp_scale(OpndSize size,
-                            int reg, bool isPhysical,
-                            int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale);
-void move_imm_to_mem(OpndSize size, int imm,
-                      int disp, int base_reg, bool isBasePhysical);
-void set_VR_to_imm(int vA, OpndSize size, int imm);
-void set_VR_to_imm_noalloc(int vA, OpndSize size, int imm);
-void set_VR_to_imm_noupdateref(LowOp* op, int vA, OpndSize size, int imm);
-void move_imm_to_reg(OpndSize size, int imm, int reg, bool isPhysical);
-void move_imm_to_reg_noalloc(OpndSize size, int imm, int reg, bool isPhysical);
-void compareAndExchange(OpndSize size,
-             int reg, bool isPhysical,
-             int disp, int base_reg, bool isBasePhysical);
-
-//LR[reg] = VR[vB]
-//or
-//PR[reg] = VR[vB]
-void get_virtual_reg(int vB, OpndSize size, int reg, bool isPhysical);
-void get_virtual_reg_noalloc(int vB, OpndSize size, int reg, bool isPhysical);
-//VR[v] = LR[reg]
-//or
-//VR[v] = PR[reg]
-void set_virtual_reg(int vA, OpndSize size, int reg, bool isPhysical);
-void set_virtual_reg_noalloc(int vA, OpndSize size, int reg, bool isPhysical);
-void get_VR_ss(int vB, int reg, bool isPhysical);
-void set_VR_ss(int vA, int reg, bool isPhysical);
-void get_VR_sd(int vB, int reg, bool isPhysical);
-void set_VR_sd(int vA, int reg, bool isPhysical);
-
-int spill_reg(int reg, bool isPhysical);
-int unspill_reg(int reg, bool isPhysical);
-
-void move_reg_to_mem_noalloc(OpndSize size,
-                      int reg, bool isPhysical,
-                      int disp, int base_reg, bool isBasePhysical,
-                      MemoryAccessType mType, int mIndex);
-LowOpMemReg* move_mem_to_reg_noalloc(OpndSize size,
-                      int disp, int base_reg, bool isBasePhysical,
-                      MemoryAccessType mType, int mIndex,
-                      int reg, bool isPhysical);
-
-//////////////////////////////////////////////////////////////
-int insertLabel(const char* label, bool checkDup);
-int export_pc();
-int simpleNullCheck(int reg, bool isPhysical, int vr);
-int nullCheck(int reg, bool isPhysical, int exceptionNum, int vr);
-int handlePotentialException(
-                             ConditionCode code_excep, ConditionCode code_okay,
-                             int exceptionNum, const char* errName);
-int get_currentpc(int reg, bool isPhysical);
-int get_self_pointer(int reg, bool isPhysical);
-int get_res_strings(int reg, bool isPhysical);
-int get_res_classes(int reg, bool isPhysical);
-int get_res_fields(int reg, bool isPhysical);
-int get_res_methods(int reg, bool isPhysical);
-int get_glue_method_class(int reg, bool isPhysical);
-int get_glue_method(int reg, bool isPhysical);
-int get_suspendCount(int reg, bool isPhysical);
-int get_return_value(OpndSize size, int reg, bool isPhysical);
-int set_return_value(OpndSize size, int reg, bool isPhysical);
-void set_return_value(OpndSize size, int sourceReg, bool isSourcePhysical,
-        int scratchRegForSelfThread, int isScratchPhysical);
-int clear_exception();
-int get_exception(int reg, bool isPhysical);
-int set_exception(int reg, bool isPhysical);
-int savearea_from_fp(int reg, bool isPhysical);
-
-int call_moddi3();
-int call_divdi3();
-int call_fmod();
-int call_fmodf();
-int call_dvmFindCatchBlock();
-int call_dvmThrowVerificationError();
-int call_dvmAllocObject();
-int call_dvmAllocArrayByClass();
-int call_dvmResolveMethod();
-int call_dvmResolveClass();
-int call_dvmInstanceofNonTrivial();
-int call_dvmThrow();
-int call_dvmThrowWithMessage();
-int call_dvmCheckSuspendPending();
-int call_dvmLockObject();
-int call_dvmUnlockObject();
-int call_dvmInitClass();
-int call_dvmAllocPrimitiveArray();
-int call_dvmInterpHandleFillArrayData();
-int call_dvmNcgHandlePackedSwitch();
-int call_dvmNcgHandleSparseSwitch();
-int call_dvmJitHandlePackedSwitch();
-int call_dvmJitHandleSparseSwitch();
-int call_dvmJitToInterpTraceSelectNoChain();
-int call_dvmJitToPatchPredictedChain();
-void call_dvmJitToInterpNormal();
-/** @brief helper function to call dvmJitToInterpBackwardBranch */
-void call_dvmJitToInterpBackwardBranch();
-void call_dvmJitToInterpTraceSelect();
-int call_dvmQuasiAtomicSwap64();
-int call_dvmQuasiAtomicRead64();
-int call_dvmCanPutArrayElement();
-int call_dvmFindInterfaceMethodInCache();
-int call_dvmHandleStackOverflow();
-int call_dvmResolveString();
-int call_dvmResolveInstField();
-int call_dvmResolveStaticField();
-#ifdef WITH_SELF_VERIFICATION
-int call_selfVerificationLoad(void);
-int call_selfVerificationStore(void);
-int call_selfVerificationLoadDoubleword(void);
-int call_selfVerificationStoreDoubleword(void);
-#endif
-
-//labels and branches
-//shared branch to resolve class: 2 specialized versions
-//OPTION 1: call & ret
-//OPTION 2: store jump back label in a fixed register or memory
-//jump to .class_resolve, then jump back
-//OPTION 3: share translator code
-/* global variables: ncg_rPC */
-int resolve_class(
-                  int startLR/*logical register index*/, bool isPhysical, int tmp/*const pool index*/,
-                  int thirdArg);
-/* EXPORT_PC; movl exceptionPtr, -8(%esp); movl descriptor, -4(%esp); lea; call; lea; jmp */
-int throw_exception_message(int exceptionPtr, int obj_reg, bool isPhysical,
-                            int startLR/*logical register index*/, bool startPhysical);
-/* EXPORT_PC; movl exceptionPtr, -8(%esp); movl imm, -4(%esp); lea; call; lea; jmp */
-int throw_exception(int exceptionPtr, int imm,
-                    int startLR/*logical register index*/, bool startPhysical);
-
-void freeShortMap();
-int insertDataWorklist(s4 relativePC, char* codePtr1);
-#ifdef ENABLE_TRACING
-int insertMapWorklist(s4 BCOffset, s4 NCGOffset, int isStartOfPC);
-#endif
-int performNCGWorklist();
-int performDataWorklist();
-void performLabelWorklist();
-void performMethodLabelWorklist();
-void freeLabelMap();
-void performSharedWorklist();
-void performChainingWorklist();
-void freeNCGWorklist();
-void freeDataWorklist();
-void freeLabelWorklist();
-/** @brief search chainingWorklist to return instruction offset address in move instruction */
-char* searchChainingWorklist(unsigned int blockId);
-/** @brief search globalNCGWorklist to find the jmp/jcc offset address */
-char* searchNCGWorklist(int blockId);
-/** @brief search globalWorklist to find the jmp/jcc offset address */
-char* searchLabelWorklist(char* label);
-void freeChainingWorklist();
-
-int common_backwardBranch();
-int common_exceptionThrown();
-int common_errNullObject();
-int common_errArrayIndex();
-int common_errArrayStore();
-int common_errNegArraySize();
-int common_errNoSuchMethod();
-int common_errDivideByZero();
-int common_periodicChecks_entry();
-int common_periodicChecks4();
-int common_gotoBail(void);
-int common_gotoBail_0(void);
-int common_errStringIndexOutOfBounds();
-
-#if defined VTUNE_DALVIK
-void sendLabelInfoToVTune(int startStreamPtr, int endStreamPtr, const char* labelName);
-#endif
-
-// Delay VRs freeing if bytecode can throw exception, then call lowerByteCode
-int lowerByteCodeCanThrowCheck(const Method* method, const MIR * mir, const u2 * dalvikPC);
-//lower a bytecode
-int lowerByteCode(const Method* method, const MIR * mir, const u2 * dalvikPC);
-
-int op_nop(const MIR * mir);
-int op_move(const MIR * mir);
-int op_move_from16(const MIR * mir);
-int op_move_16(const MIR * mir);
-int op_move_wide(const MIR * mir);
-int op_move_wide_from16(const MIR * mir);
-int op_move_wide_16(const MIR * mir);
-int op_move_result(const MIR * mir);
-int op_move_result_wide(const MIR * mir);
-int op_move_exception(const MIR * mir);
-
-int op_return_void(const MIR * mir);
-int op_return(const MIR * mir);
-int op_return_wide(const MIR * mir);
-int op_const_4(const MIR * mir);
-int op_const_16(const MIR * mir);
-int op_const(const MIR * mir);
-int op_const_high16(const MIR * mir);
-int op_const_wide_16(const MIR * mir);
-int op_const_wide_32(const MIR * mir);
-int op_const_wide(const MIR * mir);
-int op_const_wide_high16(const MIR * mir);
-int op_const_string(const MIR * mir);
-int op_const_string_jumbo(const MIR * mir);
-int op_const_class(const MIR * mir);
-int op_monitor_enter(const MIR * mir);
-int op_monitor_exit(const MIR * mir);
-int op_check_cast(const MIR * mir);
-int op_instance_of(const MIR * mir);
-
-int op_array_length(const MIR * mir);
-int op_new_instance(const MIR * mir);
-int op_new_array(const MIR * mir);
-int op_filled_new_array(const MIR * mir);
-int op_filled_new_array_range(const MIR * mir);
-int op_fill_array_data(const MIR * mir, const u2 * dalvikPC);
-int op_throw(const MIR * mir);
-int op_throw_verification_error(const MIR * mir);
-int op_goto(const MIR * mir);
-int op_goto_16(const MIR * mir);
-int op_goto_32(const MIR * mir);
-int op_packed_switch(const MIR * mir, const u2 * dalvikPC);
-int op_sparse_switch(const MIR * mir, const u2 * dalvikPC);
-int op_if_ge(const MIR * mir);
-int op_aget(const MIR * mir);
-int op_aget_wide(const MIR * mir);
-int op_aget_object(const MIR * mir);
-int op_aget_boolean(const MIR * mir);
-int op_aget_byte(const MIR * mir);
-int op_aget_char(const MIR * mir);
-int op_aget_short(const MIR * mir);
-int op_aput(const MIR * mir);
-int op_aput_wide(const MIR * mir);
-int op_aput_object(const MIR * mir);
-int op_aput_boolean(const MIR * mir);
-int op_aput_byte(const MIR * mir);
-int op_aput_char(const MIR * mir);
-int op_aput_short(const MIR * mir);
-int op_iget(const MIR * mir);
-int op_iget_wide(const MIR * mir, bool isVolatile);
-int op_iget_object(const MIR * mir);
-int op_iget_boolean(const MIR * mir);
-int op_iget_byte(const MIR * mir);
-int op_iget_char(const MIR * mir);
-int op_iget_short(const MIR * mir);
-int op_iput(const MIR * mir);
-int op_iput_wide(const MIR * mir, bool isVolatile);
-int op_iput_object(const MIR * mir);
-int op_iput_boolean(const MIR * mir);
-int op_iput_byte(const MIR * mir);
-int op_iput_char(const MIR * mir);
-int op_iput_short(const MIR * mir);
-int op_sget(const MIR * mir);
-int op_sget_wide(const MIR * mir, bool isVolatile);
-int op_sget_object(const MIR * mir);
-int op_sget_boolean(const MIR * mir);
-int op_sget_byte(const MIR * mir);
-int op_sget_char(const MIR * mir);
-int op_sget_short(const MIR * mir);
-int op_sput(const MIR * mir, bool isObj);
-int op_sput_wide(const MIR * mir, bool isVolatile);
-int op_sput_object(const MIR * mir);
-int op_sput_boolean(const MIR * mir);
-int op_sput_byte(const MIR * mir);
-int op_sput_char(const MIR * mir);
-int op_sput_short(const MIR * mir);
-int op_invoke_virtual(const MIR * mir);
-int op_invoke_super(const MIR * mir);
-int op_invoke_direct(const MIR * mir);
-int op_invoke_static(const MIR * mir);
-int op_invoke_interface(const MIR * mir);
-int op_invoke_virtual_range(const MIR * mir);
-int op_invoke_super_range(const MIR * mir);
-int op_invoke_direct_range(const MIR * mir);
-int op_invoke_static_range(const MIR * mir);
-int op_invoke_interface_range(const MIR * mir);
-int op_int_to_long(const MIR * mir);
-int op_add_long_2addr(const MIR * mir);
-int op_add_int_lit8(const MIR * mir);
-int op_cmpl_float(const MIR * mir);
-int op_cmpg_float(const MIR * mir);
-int op_cmpl_double(const MIR * mir);
-int op_cmpg_double(const MIR * mir);
-int op_cmp_long(const MIR * mir);
-int op_if_eq(const MIR * mir);
-int op_if_ne(const MIR * mir);
-int op_if_lt(const MIR * mir);
-int op_if_gt(const MIR * mir);
-int op_if_le(const MIR * mir);
-int op_if_eqz(const MIR * mir);
-int op_if_nez(const MIR * mir);
-int op_if_ltz(const MIR * mir);
-int op_if_gez(const MIR * mir);
-int op_if_gtz(const MIR * mir);
-int op_if_lez(const MIR * mir);
-int op_neg_int(const MIR * mir);
-int op_not_int(const MIR * mir);
-int op_neg_long(const MIR * mir);
-int op_not_long(const MIR * mir);
-int op_neg_float(const MIR * mir);
-int op_neg_double(const MIR * mir);
-int op_int_to_float(const MIR * mir);
-int op_int_to_double(const MIR * mir);
-int op_long_to_int(const MIR * mir);
-int op_long_to_float(const MIR * mir);
-int op_long_to_double(const MIR * mir);
-int op_float_to_int(const MIR * mir);
-int op_float_to_long(const MIR * mir);
-int op_float_to_double(const MIR * mir);
-int op_double_to_int(const MIR * mir);
-int op_double_to_long(const MIR * mir);
-int op_double_to_float(const MIR * mir);
-int op_int_to_byte(const MIR * mir);
-int op_int_to_char(const MIR * mir);
-int op_int_to_short(const MIR * mir);
-int op_add_int(const MIR * mir);
-int op_sub_int(const MIR * mir);
-int op_mul_int(const MIR * mir);
-int op_div_int(const MIR * mir);
-int op_rem_int(const MIR * mir);
-int op_and_int(const MIR * mir);
-int op_or_int(const MIR * mir);
-int op_xor_int(const MIR * mir);
-int op_shl_int(const MIR * mir);
-int op_shr_int(const MIR * mir);
-int op_ushr_int(const MIR * mir);
-int op_add_long(const MIR * mir);
-int op_sub_long(const MIR * mir);
-int op_mul_long(const MIR * mir);
-int op_div_long(const MIR * mir);
-int op_rem_long(const MIR * mir);
-int op_and_long(const MIR * mir);
-int op_or_long(const MIR * mir);
-int op_xor_long(const MIR * mir);
-int op_shl_long(const MIR * mir);
-int op_shr_long(const MIR * mir);
-int op_ushr_long(const MIR * mir);
-int op_add_float(const MIR * mir);
-int op_sub_float(const MIR * mir);
-int op_mul_float(const MIR * mir);
-int op_div_float(const MIR * mir);
-int op_rem_float(const MIR * mir);
-int op_add_double(const MIR * mir);
-int op_sub_double(const MIR * mir);
-int op_mul_double(const MIR * mir);
-int op_div_double(const MIR * mir);
-int op_rem_double(const MIR * mir);
-int op_add_int_2addr(const MIR * mir);
-int op_sub_int_2addr(const MIR * mir);
-int op_mul_int_2addr(const MIR * mir);
-int op_div_int_2addr(const MIR * mir);
-int op_rem_int_2addr(const MIR * mir);
-int op_and_int_2addr(const MIR * mir);
-int op_or_int_2addr(const MIR * mir);
-int op_xor_int_2addr(const MIR * mir);
-int op_shl_int_2addr(const MIR * mir);
-int op_shr_int_2addr(const MIR * mir);
-int op_ushr_int_2addr(const MIR * mir);
-int op_sub_long_2addr(const MIR * mir);
-int op_mul_long_2addr(const MIR * mir);
-int op_div_long_2addr(const MIR * mir);
-int op_rem_long_2addr(const MIR * mir);
-int op_and_long_2addr(const MIR * mir);
-int op_or_long_2addr(const MIR * mir);
-int op_xor_long_2addr(const MIR * mir);
-int op_shl_long_2addr(const MIR * mir);
-int op_shr_long_2addr(const MIR * mir);
-int op_ushr_long_2addr(const MIR * mir);
-int op_add_float_2addr(const MIR * mir);
-int op_sub_float_2addr(const MIR * mir);
-int op_mul_float_2addr(const MIR * mir);
-int op_div_float_2addr(const MIR * mir);
-int op_rem_float_2addr(const MIR * mir);
-int op_add_double_2addr(const MIR * mir);
-int op_sub_double_2addr(const MIR * mir);
-int op_mul_double_2addr(const MIR * mir);
-int op_div_double_2addr(const MIR * mir);
-int op_rem_double_2addr(const MIR * mir);
-int op_add_int_lit16(const MIR * mir);
-int op_rsub_int(const MIR * mir);
-int op_mul_int_lit16(const MIR * mir);
-int op_div_int_lit16(const MIR * mir);
-int op_rem_int_lit16(const MIR * mir);
-int op_and_int_lit16(const MIR * mir);
-int op_or_int_lit16(const MIR * mir);
-int op_xor_int_lit16(const MIR * mir);
-int op_rsub_int_lit8(const MIR * mir);
-int op_mul_int_lit8(const MIR * mir);
-int op_div_int_lit8(const MIR * mir);
-int op_rem_int_lit8(const MIR * mir);
-int op_and_int_lit8(const MIR * mir);
-int op_or_int_lit8(const MIR * mir);
-int op_xor_int_lit8(const MIR * mir);
-int op_shl_int_lit8(const MIR * mir);
-int op_shr_int_lit8(const MIR * mir);
-int op_ushr_int_lit8(const MIR * mir);
-int op_execute_inline(const MIR * mir, bool isRange);
-int op_invoke_direct_empty(const MIR * mir);
-int op_iget_quick(const MIR * mir);
-int op_iget_wide_quick(const MIR * mir);
-int op_iget_object_quick(const MIR * mir);
-int op_iput_quick(const MIR * mir);
-int op_iput_wide_quick(const MIR * mir);
-int op_iput_object_quick(const MIR * mir);
-int op_invoke_virtual_quick(const MIR * mir);
-int op_invoke_virtual_quick_range(const MIR * mir);
-int op_invoke_super_quick(const MIR * mir);
-int op_invoke_super_quick_range(const MIR * mir);
-
-///////////////////////////////////////////////
-void set_reg_opnd(LowOpndReg* op_reg, int reg, bool isPhysical, LowOpndRegType type);
-void set_mem_opnd(LowOpndMem* mem, int disp, int base, bool isPhysical);
-void set_mem_opnd_scale(LowOpndMem* mem, int base, bool isPhysical, int disp, int index, bool indexPhysical, int scale);
-LowOpImm* dump_imm(Mnemonic m, OpndSize size, int imm);
-void dump_imm_update(int imm, char* codePtr, bool updateSecondOperand);
-LowOpBlock* dump_blockid_imm(Mnemonic m, int targetBlockId,
-        bool immediateNeedsAligned);
-LowOpMem* dump_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
-               int disp, int base_reg, bool isBasePhysical);
-LowOpReg* dump_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-               int reg, bool isPhysical, LowOpndRegType type);
-LowOpReg* dump_reg_noalloc(Mnemonic m, OpndSize size,
-               int reg, bool isPhysical, LowOpndRegType type);
-LowOpImmMem* dump_imm_mem_noalloc(Mnemonic m, OpndSize size,
-                           int imm,
-                           int disp, int base_reg, bool isBasePhysical,
-                           MemoryAccessType mType, int mIndex);
-LowOpRegReg* dump_reg_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int reg, bool isPhysical,
-                   int reg2, bool isPhysical2, LowOpndRegType type);
-LowOpRegReg* dump_reg_reg_diff_types(Mnemonic m, AtomOpCode m2, OpndSize srcSize,
-                   int srcReg, int isSrcPhysical, LowOpndRegType srcType,
-                   OpndSize destSize, int destReg, int isDestPhysical,
-                   LowOpndRegType destType);
-LowOpRegReg* dump_movez_reg_reg(Mnemonic m, OpndSize size,
-                        int reg, bool isPhysical,
-                        int reg2, bool isPhysical2);
-LowOpMemReg* dump_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int disp, int base_reg, bool isBasePhysical,
-                   MemoryAccessType mType, int mIndex,
-                   int reg, bool isPhysical, LowOpndRegType type);
-LowOpMemReg* dump_mem_reg_noalloc(Mnemonic m, OpndSize size,
-                           int disp, int base_reg, bool isBasePhysical,
-                           MemoryAccessType mType, int mIndex,
-                           int reg, bool isPhysical, LowOpndRegType type);
-LowOpMemReg* dump_mem_scale_reg(Mnemonic m, OpndSize size,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         int reg, bool isPhysical, LowOpndRegType type);
-LowOpRegMem* dump_reg_mem_scale(Mnemonic m, OpndSize size,
-                         int reg, bool isPhysical,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         LowOpndRegType type);
-LowOpRegMem* dump_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int reg, bool isPhysical,
-                   int disp, int base_reg, bool isBasePhysical,
-                   MemoryAccessType mType, int mIndex, LowOpndRegType type);
-LowOpRegMem* dump_reg_mem_noalloc(Mnemonic m, OpndSize size,
-                           int reg, bool isPhysical,
-                           int disp, int base_reg, bool isBasePhysical,
-                           MemoryAccessType mType, int mIndex, LowOpndRegType type);
-LowOpImmReg* dump_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int imm, int reg, bool isPhysical, LowOpndRegType type, bool chaining);
-/**
- * @brief generate a x86 instruction that takes one immediate and one physical reg operand
- * @param m opcode mnemonic
- * @param size width of the operand
- * @param imm immediate value
- * @param reg register number
- * @param isPhysical TRUE if reg is a physical register, false otherwise
- * @param type register type
- * @return return a LowOp for immediate to register if scheduling is on, otherwise, return NULL
- */
-LowOpImmReg* dump_imm_reg_noalloc(Mnemonic m, OpndSize size, int imm, int reg,
-                   bool isPhysical, LowOpndRegType type);
-LowOpImmMem* dump_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int imm,
-                   int disp, int base_reg, bool isBasePhysical,
-                   MemoryAccessType mType, int mIndex, bool chaining);
-LowOpRegMem* dump_fp_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
-                  int disp, int base_reg, bool isBasePhysical,
-                  MemoryAccessType mType, int mIndex);
-LowOpMemReg* dump_mem_fp(Mnemonic m, AtomOpCode m2, OpndSize size,
-                  int disp, int base_reg, bool isBasePhysical,
-                  MemoryAccessType mType, int mIndex,
-                  int reg);
-LowOpLabel* dump_label(Mnemonic m, OpndSize size, int imm,
-               const char* label, bool isLocal);
-
-unsigned getJmpCallInstSize(OpndSize size, JmpCall_type type);
-bool lowerByteCodeJit(const Method* method, const u2* codePtr, MIR* mir);
-#if defined(WITH_JIT)
-bool lowerByteCodeJit(const Method* method, const MIR * mir, const u2 * dalvikPC);
-void startOfBasicBlock(struct BasicBlock* bb);
-extern struct BasicBlock* traceCurrentBB;
-extern JitMode traceMode;
-
-//Forward declarations
-class CompilationUnit_O1;
-class BasicBlock_O1;
-
-//Start of a trace call to reset certain elements
-void startOfTrace(const Method* method, int, CompilationUnit_O1*);
-
-//End of a trace call to reset certain elements
-void endOfTrace (CompilationUnit *cUnit);
-
-//Initiates all worklists to do their work
-void performWorklistWork (void);
-
-/**
- * @brief Generates a conditional jump to taken child of current BB being generated.
- * @details Implements semantics of "if" bytecode.
- * @param takenCondition The condition for the taken branch
- * @return Returns value >= 0 when successful and negative otherwise.
- */
-int generateConditionalJumpToTakenBlock (ConditionCode takenCondition);
-
-LowOp* jumpToBasicBlock(char* instAddr, int targetId, bool targetIsChainingCell = false);
-LowOp* condJumpToBasicBlock(char* instAddr, ConditionCode cc, int targetId, bool immediateNeedsAligned = false);
-bool jumpToException(const char* target);
-int codeGenBasicBlockJit(const Method* method, BasicBlock* bb);
-void endOfBasicBlock(struct BasicBlock* bb);
-
-//Used to generate native code the extended MIRs
-bool handleExtendedMIR (CompilationUnit *cUnit, BasicBlock_O1 *bb, MIR *mir);
-
-int insertChainingWorklist(int bbId, char * codeStart);
-void startOfTraceO1(const Method* method, int exceptionBlockId, CompilationUnit *cUnit);
-/** @brief search globalMap to find the entry for the given label */
-char* findCodeForLabel(const char* label);
-/* Find a label offset given a BasicBlock index */
-int getLabelOffset (unsigned int bbIdx);
-#endif
-int isPowerOfTwo(int imm);
-
-/** @brief calculate the magic number and shift for a given divisor
-    @details For a division by a signed integer constant, we can always
-     find a magic number M and a shift S. Thus we can transform the div
-     operation to a serial of multiplies, adds, shifts. This function
-     is used to calcuate the magic number and shift for a given divisor.
-     For the detailed desrciption and proof of
-     this optimization, please refer to "Hacker's Delight", Henry S.
-     Warren, Jr., chapter 10.
-    @param divisor the given divisor we need to calculate
-    @param magic pointer to hold the magic number
-    @param shift pointer to hold the shift
-*/
-void calculateMagicAndShift(int divisor, int* magic, int* shift);
-
-void move_chain_to_mem(OpndSize size, int imm,
-                        int disp, int base_reg, bool isBasePhysical);
-void move_chain_to_reg(OpndSize size, int imm, int reg, bool isPhysical);
-
-bool isInMemory(int regNum, OpndSize size);
-int touchEbx();
-int boundCheck(int vr_array, int reg_array, bool isPhysical_array,
-               int vr_index, int reg_index, bool isPhysical_index,
-               int exceptionNum);
-int getRelativeOffset(const char* target, bool isShortTerm, JmpCall_type type, bool* unknown,
-                      OpndSize* immSize);
-int getRelativeNCG(s4 tmp, JmpCall_type type, bool* unknown, OpndSize* size);
-void freeAtomMem();
-OpndSize estOpndSizeFromImm(int target);
-
-//Preprocess a BasicBlock before being lowered
-int preprocessingBB (CompilationUnit *cUnit, BasicBlock *bb);
-/** @brief align the relative offset of jmp/jcc and movl within 16B */
-void alignOffset(int cond);
-
-/** @brief align a pointer to n-bytes aligned */
-char* align(char* addr, int n);
-bool doesJumpToBBNeedAlignment(BasicBlock * bb);
-
-/**
- * @brief Architecture specific BasicBlock creator
- * @details Initializes x86 specific BasicBlock fields
- * @return newly created BasicBlock
- */
-BasicBlock *x86StandAloneArchSpecificNewBB (void);
-
-/**
- * @brief Architecture specific BasicBlock printing
- * @param cUnit the CompilationUnit
- * @param bb the BasicBlock
- * @param file the File in which to dump the BasicBlock
- * @param beforeMIRs is this call performed before generating the dumps for the MIRs
- */
-void x86StandAloneArchSpecificDumpBB (CompilationUnit *cUnit, BasicBlock *bb, FILE *file, bool beforeMIRs);
-
-/**
- * @brief Handle the invoke label
- * @param value the form of the arguments
- * @return the section label's name
- */
-const char *dvmCompilerHandleInvokeArgsHeader (int value);
-
-void pushCallerSavedRegs(void);
-void popCallerSavedRegs(void);
-
-//Print the emitted code
-void printEmittedCodeBlock(unsigned char *startAddr, unsigned char *endAddr);
-
-#endif
diff --git a/vm/compiler/codegen/x86/LowerAlu.cpp b/vm/compiler/codegen/x86/LowerAlu.cpp
deleted file mode 100644
index 8678217..0000000
--- a/vm/compiler/codegen/x86/LowerAlu.cpp
+++ /dev/null
@@ -1,2557 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file LowerAlu.cpp
-    \brief This file lowers ALU bytecodes.
-*/
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "NcgAot.h"
-#include "enc_wrapper.h"
-
-/////////////////////////////////////////////
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecode neg-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_neg_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_NEG_INT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    alu_unary_reg(OpndSize_32, neg_opc, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode not-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_not_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_NOT_INT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    alu_unary_reg(OpndSize_32, not_opc, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-#undef P_GPR_1
-
-/**
- * @brief Generate native code for bytecode neg-long
- * @details Implementation uses XMM registers
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_neg_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_NEG_LONG);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_64, 1, false);
-    alu_binary_reg_reg(OpndSize_64, xor_opc, 2, false, 2, false);
-    alu_binary_reg_reg(OpndSize_64, sub_opc, 1, false, 2, false);
-    set_virtual_reg(vA, OpndSize_64, 2, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode not-long
- * @details Implementation uses XMM registers
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_not_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_NOT_LONG);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_64, 1, false);
-    load_global_data_API("64bits", OpndSize_64, 2, false);
-    alu_binary_reg_reg(OpndSize_64, andn_opc, 2, false, 1, false);
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 0;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-/**
- * @brief Generate native code for bytecode neg-float
- * @details Implementation uses general purpose registers
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_neg_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_NEG_FLOAT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    alu_binary_imm_reg(OpndSize_32, add_opc, 0x80000000, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-#undef P_GPR_1
-
-/**
- * @brief Generate native code for bytecode neg-double
- * @details Implementation uses XMM registers
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_neg_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_NEG_DOUBLE);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_64, 1, false);
-    load_global_data_API("doubNeg", OpndSize_64, 2, false);
-    alu_binary_reg_reg(OpndSize_64, xor_opc, 2, false, 1, false);
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode int-to-long
- * @details Implementation uses native instruction cdq
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_int_to_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INT_TO_LONG);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, PhysicalReg_EAX, true);
-    convert_integer(OpndSize_32, OpndSize_64);
-    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-    set_virtual_reg(vA+1, OpndSize_32, PhysicalReg_EDX, true);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode int-to-float
- * @details Implementation uses FP stack
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_int_to_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INT_TO_FLOAT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    load_int_fp_stack_VR(OpndSize_32, vB); //fildl
-    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode int-to-double
- * @details Implementation uses FP stack
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_int_to_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INT_TO_DOUBLE);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    convert_int_to_fp(1, false, 2, false, true /* isDouble */);
-    set_virtual_reg(vA, OpndSize_64, 2, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode long-to-float
- * @details Implementation uses FP stack
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_long_to_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_LONG_TO_FLOAT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    load_int_fp_stack_VR(OpndSize_64, vB); //fildll
-    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode long-to-double
- * @details Implementation uses FP stack
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_long_to_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_LONG_TO_DOUBLE);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    load_int_fp_stack_VR(OpndSize_64, vB); //fildll
-    store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode float-to-double
- * @details Implementation uses FP stack
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_float_to_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_FLOAT_TO_DOUBLE);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    load_fp_stack_VR(OpndSize_32, vB); //flds
-    store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode double-to-float
- * @details Implementation uses FP stack
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_double_to_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DOUBLE_TO_FLOAT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    load_fp_stack_VR(OpndSize_64, vB); //fldl
-    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
-    return 0;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-/**
- * @brief Generate native code for bytecode long-to-int
- * @details Implementation uses general purpose registers
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_long_to_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_LONG_TO_INT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-#undef P_GPR_1
-
-//! common code to convert a float or double to integer
-
-//! It uses FP stack
-int common_fp_to_int(bool isDouble, int vA, int vB) {
-    if(isDouble) {
-        load_fp_stack_VR(OpndSize_64, vB); //fldl
-    }
-    else {
-        load_fp_stack_VR(OpndSize_32, vB); //flds
-    }
-
-    load_fp_stack_global_data_API("intMax", OpndSize_32);
-    load_fp_stack_global_data_API("intMin", OpndSize_32);
-
-    //ST(0) ST(1) ST(2) --> LintMin LintMax value
-    compare_fp_stack(true, 2, false/*isDouble*/); //ST(2)
-    //ST(0) ST(1) --> LintMax value
-    conditional_jump(Condition_AE, ".float_to_int_negInf", true);
-    rememberState(1);
-    compare_fp_stack(true, 1, false/*isDouble*/); //ST(1)
-    //ST(0) --> value
-    rememberState(2);
-    conditional_jump(Condition_C, ".float_to_int_nanInf", true);
-    //fnstcw, orw, fldcw, xorw
-    load_effective_addr(-2, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    store_fpu_cw(false/*checkException*/, 0, PhysicalReg_ESP, true);
-    alu_binary_imm_mem(OpndSize_16, or_opc, 0xc00, 0, PhysicalReg_ESP, true);
-    load_fpu_cw(0, PhysicalReg_ESP, true);
-    alu_binary_imm_mem(OpndSize_16, xor_opc, 0xc00, 0, PhysicalReg_ESP, true);
-    store_int_fp_stack_VR(true/*pop*/, OpndSize_32, vA); //fistpl
-    //fldcw
-    load_fpu_cw(0, PhysicalReg_ESP, true);
-    load_effective_addr(2, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    rememberState(3);
-    unconditional_jump(".float_to_int_okay", true);
-    if (insertLabel(".float_to_int_nanInf", true) == -1)
-        return -1;
-    conditional_jump(Condition_NP, ".float_to_int_posInf", true);
-    //fstps CHECK
-    goToState(2);
-    store_fp_stack_VR(true, OpndSize_32, vA);
-    set_VR_to_imm(vA, OpndSize_32, 0);
-    transferToState(3);
-    unconditional_jump(".float_to_int_okay", true);
-    if (insertLabel(".float_to_int_posInf", true) == -1)
-        return -1;
-    //fstps CHECK
-    goToState(2);
-    store_fp_stack_VR(true, OpndSize_32, vA);
-    set_VR_to_imm(vA, OpndSize_32, 0x7fffffff);
-    transferToState(3);
-    unconditional_jump(".float_to_int_okay", true);
-    if (insertLabel(".float_to_int_negInf", true) == -1)
-        return -1;
-    goToState(1);
-    //fstps CHECK
-    store_fp_stack_VR(true, OpndSize_32, vA);
-    store_fp_stack_VR(true, OpndSize_32, vA);
-    set_VR_to_imm(vA, OpndSize_32, 0x80000000);
-    transferToState(3);
-    if (insertLabel(".float_to_int_okay", true) == -1)
-        return -1;
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode float-to-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_float_to_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_FLOAT_TO_INT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    int retval = common_fp_to_int(false, vA, vB);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode double-to-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_double_to_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DOUBLE_TO_INT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    int retval = common_fp_to_int(true, vA, vB);
-    return retval;
-}
-
-//! common code to convert float or double to long
-
-//! It uses FP stack
-int common_fp_to_long(bool isDouble, int vA, int vB) {
-    if(isDouble) {
-        load_fp_stack_VR(OpndSize_64, vB); //fldl
-    }
-    else {
-        load_fp_stack_VR(OpndSize_32, vB); //flds
-    }
-
-    //Check if it is the special Negative Infinity value
-    load_fp_stack_global_data_API("valueNegInfLong", OpndSize_64);
-    //Stack status: ST(0) ST(1) --> LlongMin value
-    compare_fp_stack(true, 1, false/*isDouble*/); // Pops ST(1)
-    conditional_jump(Condition_AE, ".float_to_long_negInf", true);
-    rememberState(1);
-
-    //Check if it is the special Positive Infinity value
-    load_fp_stack_global_data_API("valuePosInfLong", OpndSize_64);
-    //Stack status: ST(0) ST(1) --> LlongMax value
-    compare_fp_stack(true, 1, false/*isDouble*/); // Pops ST(1)
-    rememberState(2);
-    conditional_jump(Condition_C, ".float_to_long_nanInf", true);
-
-    //Normal Case
-    //We want to truncate to 0 for conversion. That will be rounding mode 0x11
-    load_effective_addr(-2, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    store_fpu_cw(false/*checkException*/, 0, PhysicalReg_ESP, true);
-    //Change control word to rounding mode 11:
-    alu_binary_imm_mem(OpndSize_16, or_opc, 0xc00, 0, PhysicalReg_ESP, true);
-    //Load the control word
-    load_fpu_cw(0, PhysicalReg_ESP, true);
-    //Reset the control word
-    alu_binary_imm_mem(OpndSize_16, xor_opc, 0xc00, 0, PhysicalReg_ESP, true);
-    //Perform the actual conversion
-    store_int_fp_stack_VR(true/*pop*/, OpndSize_64, vA); //fistpll
-    // Restore the original control word
-    load_fpu_cw(0, PhysicalReg_ESP, true);
-    load_effective_addr(2, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    rememberState(3);
-    /* NOTE: We do not need to pop out the original value we pushed
-     * since load_fpu_cw above already clears the stack for
-     * normal values.
-     */
-    unconditional_jump(".float_to_long_okay", true);
-
-    //We can be here for positive infinity or NaN. Check parity bit
-    if (insertLabel(".float_to_long_nanInf", true) == -1)
-        return -1;
-    conditional_jump(Condition_NP, ".float_to_long_posInf", true);
-    goToState(2);
-    //Save corresponding Long NaN value
-    load_global_data_API("valueNanLong", OpndSize_64, 1, false);
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    transferToState(3);
-    //Pop out the original value we pushed
-    compare_fp_stack(true, 0, false/*isDouble*/); //ST(0)
-    unconditional_jump(".float_to_long_okay", true);
-
-    if (insertLabel(".float_to_long_posInf", true) == -1)
-        return -1;
-    goToState(2);
-    //Save corresponding Long Positive Infinity value
-    load_global_data_API("valuePosInfLong", OpndSize_64, 2, false);
-    set_virtual_reg(vA, OpndSize_64, 2, false);
-    transferToState(3);
-    //Pop out the original value we pushed
-    compare_fp_stack(true, 0, false/*isDouble*/); //ST(0)
-    unconditional_jump(".float_to_long_okay", true);
-
-    if (insertLabel(".float_to_long_negInf", true) == -1)
-        return -1;
-    //fstpl
-    goToState(1);
-    //Load corresponding Long Negative Infinity value
-    load_global_data_API("valueNegInfLong", OpndSize_64, 3, false);
-    set_virtual_reg(vA, OpndSize_64, 3, false);
-    transferToState(3);
-    //Pop out the original value we pushed
-    compare_fp_stack(true, 0, false/*isDouble*/); //ST(0)
-
-    if (insertLabel(".float_to_long_okay", true) == -1)
-        return -1;
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode float-to-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_float_to_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_FLOAT_TO_LONG);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    int retval = common_fp_to_long(false, vA, vB);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode double-to-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_double_to_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DOUBLE_TO_LONG);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    int retval = common_fp_to_long(true, vA, vB);
-    return retval;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-/**
- * @brief Generate native code for bytecode int-to-byte
- * @details Implementation uses general purpose registers
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_int_to_byte(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INT_TO_BYTE);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    moves_reg_to_reg(OpndSize_8, 1, false, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode int-to-char
- * @details Implementation uses general purpose registers
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_int_to_char(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INT_TO_CHAR);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    alu_binary_imm_reg(OpndSize_32, sal_opc, 16, 1, false);
-    alu_binary_imm_reg(OpndSize_32, shr_opc, 16, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode int-to-short
- * @details Implementation uses general purpose registers
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_int_to_short(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INT_TO_SHORT);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    moves_reg_to_reg(OpndSize_16, 1, false, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-//! common code to handle integer ALU ops
-
-//! It uses GPR
-int common_alu_int(ALU_Opcode opc, int vA, int v1, int v2) { //except div and rem
-    get_virtual_reg(v1, OpndSize_32, 1, false);
-    //in encoder, reg is first operand, which is the destination
-    //gpr_1 op v2(rFP) --> gpr_1
-    //shift only works with reg cl, v2 should be in %ecx
-    alu_binary_VR_reg(OpndSize_32, opc, v2, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-#undef P_GPR_1
-#define P_GPR_1 PhysicalReg_EBX
-//! common code to handle integer shift ops
-
-//! It uses GPR
-int common_shift_int(ALU_Opcode opc, int vA, int v1, int v2) {
-    get_virtual_reg(v2, OpndSize_32, PhysicalReg_ECX, true);
-    get_virtual_reg(v1, OpndSize_32, 1, false);
-    //in encoder, reg2 is first operand, which is the destination
-    //gpr_1 op v2(rFP) --> gpr_1
-    //shift only works with reg cl, v2 should be in %ecx
-    alu_binary_reg_reg(OpndSize_32, opc, PhysicalReg_ECX, true, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-#undef p_GPR_1
-
-/**
- * @brief Generate native code for bytecode add-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_int(add_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode sub-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sub_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SUB_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_int(sub_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode mul-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_int(imul_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode and-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_and_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AND_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_int(and_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode or-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_or_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_OR_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_int(or_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode xor-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_xor_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_XOR_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_int(xor_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode shl-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shl_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHL_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_shift_int(shl_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode shr-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shr_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHR_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_shift_int(sar_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode ushr-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_ushr_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_USHR_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_shift_int(shr_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode add-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_INT_2ADDR);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = vA;
-    v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_int(add_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode sub-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sub_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SUB_INT_2ADDR);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = vA;
-    v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_int(sub_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode mul-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_INT_2ADDR);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = vA;
-    v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_int(imul_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode and-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_and_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AND_INT_2ADDR);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = vA;
-    v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_int(and_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode or-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_or_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_OR_INT_2ADDR);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = vA;
-    v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_int(or_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode xor-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_xor_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_XOR_INT_2ADDR);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = vA;
-    v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_int(xor_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode shl-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shl_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHL_INT_2ADDR);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = vA;
-    v2 = mir->dalvikInsn.vB;
-    int retval = common_shift_int(shl_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode shr-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shr_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHR_INT_2ADDR);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = vA;
-    v2 = mir->dalvikInsn.vB;
-    int retval = common_shift_int(sar_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode ushr-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_ushr_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_USHR_INT_2ADDR);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = vA;
-    v2 = mir->dalvikInsn.vB;
-    int retval = common_shift_int(shr_opc, vA, v1, v2);
-    return retval;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-/**
- * @brief common code to handle integer DIV & REM, it used GPR
- *  If the divisor is a constant at compiler time, use the algorithm from "Hacker's Delight", Henry S.
- *  Warren, Jr., chapter 10. to simplify the code.
- *  The special case: when op0 == minint && op1 == -1, return 0 for isRem, return 0x80000000 for isDiv
- *  There are four merge points in the control flow for this bytecode
- *  make sure the reg. alloc. state is the same at merge points by calling transferToState
- * @param isRem true for REM, false for DIV
- * @param vA the destination VR
- * @param v1 the source VR for numerator
- * @param v2 the source VR for divisor
- * @return value >= 0 when handled
- */
-int common_div_rem_int(bool isRem, int vA, int v1, int v2) {
-    get_virtual_reg(v1, OpndSize_32, PhysicalReg_EAX, true);
-    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EDX, true);
-    get_virtual_reg(v2, OpndSize_32, 2, false);
-
-    // Handle the div 0 case
-    compare_imm_reg(OpndSize_32, 0, 2, false);
-    handlePotentialException(
-                                       Condition_E, Condition_NE,
-                                       1, "common_errDivideByZero");
-
-    //Check if numerator is 0
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    rememberState(2);
-    conditional_jump(Condition_Z, ".common_div_rem_int_divdone", true);
-
-    transferToState(1);
-
-    // Handle the case where the divisor is a constant at compile time
-    int divisor[2];
-    int isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, divisor, false);
-    // For now we just use the generic code generation for division by -1
-    if (isConst > 0 && divisor[0] != -1) {
-        if (divisor[0] == 0) {
-            // Division by zero: we don't need to generate the code at all.
-            transferToState(2);
-            return 0;
-        } else if (divisor[0] != 1) { // There is nothing to do with division by 1
-            int magic;
-            int shift;
-            calculateMagicAndShift(divisor[0], &magic, &shift);
-
-            // According to H.S.Warren's Hacker's Delight Chapter 10 and
-            // T,Grablund, P.L.Montogomery's Division by invariant integers using multiplication
-            // For a integer divided by a constant,
-            // we can always find a magic number M and a shift S. Thus,
-            // For d >= 2,
-            //     int(n/d) = floor(n/d) = floor(M*n/2^S), while n > 0
-            //     int(n/d) = ceil(n/d) = floor(M*n/2^S) +1, while n < 0.
-            // For d <= -2,
-            //     int(n/d) = ceil(n/d) = floor(M*n/2^S) +1 , while n > 0
-            //     int(n/d) = floor(n/d) = floor(M*n/2^S), while n < 0.
-            // We implement this algorithm in the following way:
-            // 1. multiply magic number m and numerator n, get the higher 32bit result in EDX
-            // 2. if divisor > 0 and magic < 0, add numerator to EDX
-            //    if divisor < 0 and magic > 0, sub numerator to EDX
-            // 3. if S !=0, SAR S bits for EDX
-            // 4. add 1 to EDX if EDX < 0
-            // 5. Thus, EDX is the quotient
-
-            // mov %eax, %tmp1
-            // mov magic, %tmp2
-            // imul %tmp2
-
-            move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 1, false);
-            move_imm_to_reg(OpndSize_32, magic, 2, false);
-            alu_unary_reg(OpndSize_32, imul_opc, 2, false);
-            // v2>0 && M<0
-            if (divisor[0] > 0 && magic < 0){
-                alu_binary_reg_reg(OpndSize_32, add_opc, 1, false, PhysicalReg_EDX, true);
-            }else if (divisor[0] < 0 && magic > 0){
-            // v2<0 && M>0
-                alu_binary_reg_reg(OpndSize_32, sub_opc, 1, false, PhysicalReg_EDX, true);
-            }
-            // sarl shift, %edx
-            if (shift != 0)
-                alu_binary_imm_reg(OpndSize_32, sar_opc, shift, PhysicalReg_EDX, true);
-            // mov %edx, %tmp2
-            // shrl 31, %edx
-            // add %tmp2, %edx
-            move_reg_to_reg(OpndSize_32, PhysicalReg_EDX, true, 2, false);
-            alu_binary_imm_reg(OpndSize_32, shr_opc, 31, PhysicalReg_EDX, true);
-            alu_binary_reg_reg(OpndSize_32, add_opc, 2, false, PhysicalReg_EDX, true);
-
-            if (isRem == true) {
-                 //mov %edx, %eax
-                 //mov v2, %tmp2
-                 //imul %tmp2
-                 //sub %eax, %tmp1
-                 //mov %tmp1, edx
-                move_reg_to_reg(OpndSize_32, PhysicalReg_EDX, true, PhysicalReg_EAX, true);
-                move_imm_to_reg(OpndSize_32, divisor[0], 2, false);
-                alu_unary_reg(OpndSize_32, imul_opc, 2, false);
-                alu_binary_reg_reg(OpndSize_32, sub_opc, PhysicalReg_EAX, true, 1, false);
-                move_reg_to_reg(OpndSize_32, 1, false, PhysicalReg_EDX, true);
-            }
-        }
-    } else { //It is a general case. Both divisor and numerator are variables.
- 
-        //Find out Numerator | Denominator
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 3, false);
-        alu_binary_reg_reg(OpndSize_32, or_opc, 2, false, 3, false);
-
-        //If both arguments are less than 8-bits (and positive), do 8-bit divide
-        test_imm_reg(OpndSize_32, 0xFFFFFF00, 3, false);
-
-        rememberState(3);
-        conditional_jump(Condition_Z, ".common_div_rem_int_8", true);
-
-        //If both arguments are less than 16-bits (and positive), do 16-bit divide
-        test_imm_reg(OpndSize_32, 0xFFFF0000, 3, false);
-        conditional_jump(Condition_Z, ".common_div_rem_int_16", true);
-
-        //Handle special cases:
-        //0x80000000 / -1 should result in a quotient of 0x80000000
-        //and a remainder of 0.
-        //Check for -1:
-        compare_imm_reg(OpndSize_32, -1, 2, false);
-        rememberState(4);
-        conditional_jump(Condition_NE, ".common_div_rem_int_32", true);
-        //Check for 0x80000000 (MinInt)
-        compare_imm_reg(OpndSize_32, 0x80000000, PhysicalReg_EAX, true);
-        //Special case, no division is needed.
-        //We set the quotient to 0x800000000 (EAX is already that),
-        //and remainder to 0
-        transferToState(2);
-        conditional_jump(Condition_E, ".common_div_rem_int_divdone", true);
-
-
-        goToState(4);
-        if (insertLabel(".common_div_rem_int_32", true) == -1) //merge point
-            return -1;
-        convert_integer(OpndSize_32, OpndSize_64); //cdq
-        //idiv: dividend in edx:eax; quotient in eax; remainder in edx
-        alu_unary_reg(OpndSize_32, idiv_opc, 2, false);
-        transferToState(2);
-        unconditional_jump(".common_div_rem_int_divdone", true);
-
-        //Do 8-bit unsigned divide:
-        //div: dividend in ax; quotient in al; remainder in ah
-        //We are forced to use a hard-coded register, since the register allocator
-        //can allocate a register not capable of 8-bit operation, like ESI,
-        //which will cause undefined behaviour.
-        goToState(3);
-        if (insertLabel(".common_div_rem_int_8", true) == -1)
-            return -1;
-        move_reg_to_reg(OpndSize_32, 2, false, 4, false);
-        alu_unary_reg(OpndSize_8, div_opc, 4, false);
-        if (isRem) {
-            move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_EDX, true);
-            alu_binary_imm_reg(OpndSize_32, shr_opc, 8, PhysicalReg_EDX, true);
-        } else
-            alu_binary_imm_reg(OpndSize_32, and_opc, 0x000000FF, PhysicalReg_EAX, true);
-        transferToState(2);
-        unconditional_jump(".common_div_rem_int_divdone", true);
-
-        //Do 16-bit divide:
-        //div: dividend in dx:ax; quotient in ax; remainder in dx
-        goToState(3);
-        if (insertLabel(".common_div_rem_int_16", true) == -1)
-            return -1;
-        alu_unary_reg(OpndSize_16, div_opc, 2, false);
-    }
-
-    transferToState(2);
-    if (insertLabel(".common_div_rem_int_divdone", true) == -1)
-        return -1;
-    if(isRem)
-        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EDX, true);
-    else //divide: quotient in %eax
-        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-
-    return 0;
-}
-#undef P_GPR_1
-
-/**
- * @brief Generate native code for bytecode div-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_div_rem_int(false, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode rem-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_INT);
-    int vA, v1, v2;
-    vA = mir->dalvikInsn.vA;
-    v1 = mir->dalvikInsn.vB;
-    v2 = mir->dalvikInsn.vC;
-    int retval = common_div_rem_int(true, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode div-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_INT_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_div_rem_int(false, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode rem-int/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_int_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_INT_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_div_rem_int(true, vA, v1, v2);
-    return retval;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-/**
- * @brief Common function to handle alu operations involving literals
- * @param opc The Opcode to perform
- * @param vA The destination VR
- * @param vB The source VR
- * @param imm The literal value
- * @return value >= 0 when handled
- */
-int common_alu_int_lit(ALU_Opcode opc, int vA, int vB, s2 imm) { //except div and rem
-    // For add and sub, try if we can operate directly on VRs
-    if ((opc == add_opc) || (opc == sub_opc)) {
-        bool success = alu_imm_to_VR(OpndSize_32, opc, vB, vA, imm, 1, false, NULL);
-        //If succeeded, we are done
-        if (success == true) {
-            return 0;
-        }
-        //Otherwise, go the normal path
-    }
-
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    alu_binary_imm_reg(OpndSize_32, opc, imm, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-
-//! calls common_alu_int_lit
-int common_shift_int_lit(ALU_Opcode opc, int vA, int vB, s2 imm) {
-    return common_alu_int_lit(opc, vA, vB, imm);
-}
-#undef p_GPR_1
-
-/**
- * @brief Generate native code for bytecode add-int/lit16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_int_lit16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_INT_LIT16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_alu_int_lit(add_opc, vA, vB, literal);
-    return retval;
-}
-
-int alu_rsub_int(ALU_Opcode opc, int vA, s2 imm, int vB) {
-    move_imm_to_reg(OpndSize_32, imm, 2, false);
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    alu_binary_reg_reg(OpndSize_32, opc, 1, false, 2, false);
-    set_virtual_reg(vA, OpndSize_32, 2, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode rsub-int
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rsub_int(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_RSUB_INT);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = alu_rsub_int(sub_opc, vA, literal, vB);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode mul-int/lit16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_int_lit16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_INT_LIT16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_alu_int_lit(imul_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode and-int/lit16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_and_int_lit16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AND_INT_LIT16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_alu_int_lit(and_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode or-int/lit16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_or_int_lit16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_OR_INT_LIT16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_alu_int_lit(or_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode xor-int/lit16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_xor_int_lit16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_XOR_INT_LIT16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_alu_int_lit(xor_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode add-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-
-    //try if we can operate directly on VRs
-    bool success = alu_imm_to_VR(OpndSize_32, add_opc, vB, vA, literal, 1, false, mir);
-
-    //If succeeded, we are done
-    if (success == true) {
-        return 0;
-    }
-
-    //Otherwise, go the normal path
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    alu_binary_imm_reg(OpndSize_32, add_opc, literal, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode rsub-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rsub_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_RSUB_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = alu_rsub_int(sub_opc, vA, literal, vB);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode mul-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_alu_int_lit(imul_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode and-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_and_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AND_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_alu_int_lit(and_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode or-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_or_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_OR_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_alu_int_lit(or_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode xor-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_xor_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_XOR_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_alu_int_lit(xor_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode shl-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shl_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHL_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_shift_int_lit(shl_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode shr-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shr_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHR_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_shift_int_lit(sar_opc, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode ushr-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_ushr_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_USHR_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_shift_int_lit(shr_opc, vA, vB, literal);
-    return retval;
-}
-
-int isPowerOfTwo(int imm) {
-    int i;
-    for(i = 1; i < 17; i++) {
-        if(imm == (1 << i)) return i;
-    }
-    return -1;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-int div_lit_strength_reduction(int vA, int vB, s2 imm) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        //strength reduction for div by 2,4,8,...
-        int power = isPowerOfTwo(imm);
-        if(power < 1) return 0;
-        //tmp2 is not updated, so it can share with vB
-        get_virtual_reg(vB, OpndSize_32, 2, false);
-        //if imm is 2, power will be 1
-        if(power == 1) {
-            /* mov tmp1, tmp2
-               shrl $31, tmp1
-               addl tmp2, tmp1
-               sarl $1, tmp1 */
-            move_reg_to_reg(OpndSize_32, 2, false, 1, false);
-            alu_binary_imm_reg(OpndSize_32, shr_opc, 31, 1, false);
-            alu_binary_reg_reg(OpndSize_32, add_opc, 2, false, 1, false);
-            alu_binary_imm_reg(OpndSize_32, sar_opc, 1, 1, false);
-            set_virtual_reg(vA, OpndSize_32, 1, false);
-            return 1;
-        }
-        //power > 1
-        /* mov tmp1, tmp2
-           sarl $power-1, tmp1
-           shrl 32-$power, tmp1
-           addl tmp2, tmp1
-           sarl $power, tmp1 */
-        move_reg_to_reg(OpndSize_32, 2, false, 1, false);
-        alu_binary_imm_reg(OpndSize_32, sar_opc, power-1, 1, false);
-        alu_binary_imm_reg(OpndSize_32, shr_opc, 32-power, 1, false);
-        alu_binary_reg_reg(OpndSize_32, add_opc, 2, false, 1, false);
-        alu_binary_imm_reg(OpndSize_32, sar_opc, power, 1, false);
-        set_virtual_reg(vA, OpndSize_32, 1, false);
-        return 1;
-    }
-    return 0;
-}
-
-////////// throws exception!!!
-//! common code to handle integer DIV & REM with literal
-
-//! It uses GPR
-int common_div_rem_int_lit(bool isRem, int vA, int vB, s2 imm) {
-    if(!isRem) {
-        int retCode = div_lit_strength_reduction(vA, vB, imm);
-        if(retCode > 0) return 0;
-    }
-    if(imm == 0) {
-        export_pc(); //use %edx
-#ifdef DEBUG_EXCEPTION
-        ALOGI("EXTRA code to handle exception");
-#endif
-        beforeCall("exception"); //dump GG, GL VRs
-        unconditional_jump_global_API(
-                          "common_errDivideByZero", false);
-
-        return 0;
-    }
-    get_virtual_reg(vB, OpndSize_32, PhysicalReg_EAX, true);
-    //check against -1 for DIV_INT??
-    if(imm == -1) {
-        compare_imm_reg(OpndSize_32, 0x80000000, PhysicalReg_EAX, true);
-        conditional_jump(Condition_E, ".div_rem_int_lit_special", true);
-        rememberState(1);
-    }
-    move_imm_to_reg(OpndSize_32, imm, 2, false);
-    convert_integer(OpndSize_32, OpndSize_64); //cdq
-    //idiv: dividend in edx:eax; quotient in eax; remainder in edx
-    alu_unary_reg(OpndSize_32, idiv_opc, 2, false);
-    if(isRem)
-        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EDX, true);
-    else
-        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-
-    if(imm == -1) {
-        unconditional_jump(".div_rem_int_lit_okay", true);
-        rememberState(2);
-
-        if (insertLabel(".div_rem_int_lit_special", true) == -1)
-            return -1;
-        goToState(1);
-        if(isRem)
-            set_VR_to_imm(vA, OpndSize_32, 0);
-        else
-            set_VR_to_imm(vA, OpndSize_32, 0x80000000);
-        transferToState(2);
-    }
-
-    if (insertLabel(".div_rem_int_lit_okay", true) == -1)
-        return -1; //merge point 2
-    return 0;
-}
-#undef P_GPR_1
-
-/**
- * @brief Generate native code for bytecode div-int/lit16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_int_lit16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_INT_LIT16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_div_rem_int_lit(false, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode rem-int/lit16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_int_lit16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_INT_LIT16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_div_rem_int_lit(true, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode div-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_div_rem_int_lit(false, vA, vB, literal);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode rem-int/lit8
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_int_lit8(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_INT_LIT8);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    s2 literal = mir->dalvikInsn.vC;
-    int retval = common_div_rem_int_lit(true, vA, vB, literal);
-    return retval;
-}
-//! common code to hanle long ALU ops
-
-//! It uses XMM
-//!all logical operations and sub operation of long type (no add mul div or rem)
-int common_alu_long(ALU_Opcode opc, int vA, int v1, int v2) {
-    int value[2];
-    int isConst = isVirtualRegConstant(v2, LowOpndRegType_xmm, value, false);
-
-    get_virtual_reg(v1, OpndSize_64, 1, false);
-    if (isConst == 3) {                                           //operate on constants stored in code stream
-        alu_binary_VR_reg(OpndSize_64, opc, v2, 1, false);        //opc const, XMM
-    } else {
-        get_virtual_reg(v2, OpndSize_64, 2, false);               //operate on XMM registers
-        alu_binary_reg_reg(OpndSize_64, opc, 2, false, 1, false); //opc XMM, XMM
-    }
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 0;
-}
-
-//! Use general purpose registers during the lowering for add-long and add-long/2addr
-int common_add_long(int vA, int v1, int v2) {
-    get_virtual_reg(v1, OpndSize_32, 1, false);
-    get_virtual_reg(v1+1, OpndSize_32, 2, false);
-    alu_binary_VR_reg(OpndSize_32, add_opc, v2, 1, false);
-    alu_binary_VR_reg(OpndSize_32, adc_opc, (v2+1), 2, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    set_virtual_reg(vA+1, OpndSize_32, 2, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode add-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_add_long(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode sub-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sub_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SUB_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_long(sub_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode and-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_and_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AND_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_long(and_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode or-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_or_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_OR_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_long(or_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode xor-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_xor_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_XOR_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_long(xor_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode add-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_add_long(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode sub-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sub_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SUB_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_long(sub_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode and-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_and_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AND_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_long(and_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode or-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_or_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_OR_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_long(or_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode xor-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_xor_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_XOR_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_long(xor_opc, vA, v1, v2);
-    return retval;
-}
-
-//signed vs unsigned imul and mul?
-//! common code to handle multiplication of long
-
-//! It uses GPR
-int common_mul_long(int vA, int v1, int v2) {
-    get_virtual_reg(v2, OpndSize_32, 1, false);
-    move_reg_to_reg(OpndSize_32, 1, false, PhysicalReg_EAX, true);
-    //imul: 2L * 1H update temporary 1
-    alu_binary_VR_reg(OpndSize_32, imul_opc, (v1+1), 1, false);
-    get_virtual_reg(v1, OpndSize_32, 3, false);
-    move_reg_to_reg(OpndSize_32, 3, false, 2, false);
-    //imul: 1L * 2H
-    alu_binary_VR_reg(OpndSize_32, imul_opc, (v2+1), 2, false);
-    alu_binary_reg_reg(OpndSize_32, add_opc, 2, false, 1, false);
-    alu_unary_reg(OpndSize_32, mul_opc, 3, false);
-    alu_binary_reg_reg(OpndSize_32, add_opc, PhysicalReg_EDX, true, 1, false);
-    set_virtual_reg(vA+1, OpndSize_32, 1, false);
-    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-    return 0;
-}
-
-//! common code to handle multiplication when multiplicands of long type are the same
-
-//! It uses GPR
-int common_mul_long_square(int vA, int v1) {
-    get_virtual_reg(v1, OpndSize_32, 1, false);
-    move_reg_to_reg(OpndSize_32, 1, false, PhysicalReg_EAX, true);
-    move_reg_to_reg(OpndSize_32,1, false, PhysicalReg_EDX, true);
-    //imul: 1L * 1H update temporary 1.
-    //same as 2L * 1H or 1L * 2H, thus eliminating need for second imul.
-    alu_binary_VR_reg(OpndSize_32, imul_opc, (v1+1), 1, false);
-    alu_binary_reg_reg(OpndSize_32, add_opc, 1, false, 1, false);
-    alu_unary_reg(OpndSize_32, mul_opc, PhysicalReg_EDX, true);
-    alu_binary_reg_reg(OpndSize_32, add_opc, PhysicalReg_EDX, true, 1, false);
-    set_virtual_reg(vA+1, OpndSize_32, 1, false);
-    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode mul-long
- * @details when multiplicands are same, use special case for square
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval;
-    if (v1 != v2){
-      retval = common_mul_long(vA, v1, v2);
-    }
-    else{
-      retval = common_mul_long_square(vA, v1);
-    }
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode mul-long/2addr
- * @details when multiplicands are same, use special case for square
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval;
-    if (v1 != v2){
-      retval = common_mul_long(vA, v1, v2);
-    }
-    else{
-      retval = common_mul_long_square(vA, v1);
-    }
-    return retval;
-}
-
-//! common code to handle DIV & REM of long
-
-//! It uses GPR & XMM; and calls call_moddi3 & call_divdi3
-int common_div_rem_long(bool isRem, int vA, int v1, int v2) {
-    get_virtual_reg(v2, OpndSize_32, 1, false);
-    get_virtual_reg(v2+1, OpndSize_32, 2, false);
-    //save to native stack before changing register 1, esp-8 is unused area
-    move_reg_to_mem(OpndSize_32, 1, false, 8-16, PhysicalReg_ESP, true);
-    alu_binary_reg_reg(OpndSize_32, or_opc, 2, false, 1, false);
-
-    handlePotentialException(
-                                       Condition_E, Condition_NE,
-                                       1, "common_errDivideByZero");
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 2, false, 12, PhysicalReg_ESP, true);
-    get_virtual_reg(v1, OpndSize_64, 1, false);
-    move_reg_to_mem(OpndSize_64, 1, false, 0, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    nextVersionOfHardReg(PhysicalReg_EDX, 2); //next version has 2 refs
-    if(isRem)
-        call_moddi3();
-    else
-        call_divdi3();
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    set_virtual_reg(vA+1, OpndSize_32,PhysicalReg_EDX, true);
-    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode div-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_div_rem_long(false, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode rem-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_div_rem_long(true, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode div-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_div_rem_long(false, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode rem-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_div_rem_long(true, vA, v1, v2);
-    return retval;
-}
-
-//! common code to handle SHL long
-
-//! It uses XMM
-int common_shl_long(int vA, int v1, int v2) {
-    get_VR_ss(v2, 2, false);
-    get_virtual_reg(v1, OpndSize_64, 1, false);
-
-    int value[2];
-    int isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, value, false); //do not update refCount
-    if (isConst == 3) {                          // case where shift amount is available
-        int shiftImm = (0x3f) & (value[0]); // compute masked shift amount statically
-        alu_binary_imm_reg(OpndSize_64, sll_opc, shiftImm, 1, false);
-    } else {                                // case where shift count to be read from VR
-        load_global_data_API("shiftMask", OpndSize_64, 3, false);
-        alu_binary_reg_reg(OpndSize_64, and_opc, 3, false, 2, false);
-        alu_binary_reg_reg(OpndSize_64, sll_opc, 2, false, 1, false);
-    }
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 0;
-}
-
-//! common code to handle SHR long
-
-//! It uses XMM
-int common_shr_long(int vA, int v1, int v2) {
-    get_VR_ss(v2, 2, false);
-
-    load_global_data_API("shiftMask", OpndSize_64, 3, false);
-
-    get_virtual_reg(v1, OpndSize_64, 1, false);
-    alu_binary_reg_reg(OpndSize_64, and_opc, 3, false, 2, false);
-    alu_binary_reg_reg(OpndSize_64, srl_opc, 2, false, 1, false);
-    compare_imm_VR(OpndSize_32, 0, (v1+1));
-    conditional_jump(Condition_GE, ".common_shr_long_special", true);
-    rememberState(1);
-
-    load_global_data_API("value64", OpndSize_64, 4, false);
-
-    alu_binary_reg_reg(OpndSize_64, sub_opc, 2, false, 4, false);
-
-    load_global_data_API("64bits", OpndSize_64, 5, false);
-
-    alu_binary_reg_reg(OpndSize_64, sll_opc, 4, false, 5, false);
-    alu_binary_reg_reg(OpndSize_64, or_opc, 5, false, 1, false);
-    rememberState(2);
-    //check whether the target is next instruction TODO
-    unconditional_jump(".common_shr_long_done", true);
-
-    if (insertLabel(".common_shr_long_special", true) == -1)
-        return -1;
-    goToState(1);
-    transferToState(2);
-    if (insertLabel(".common_shr_long_done", true) == -1)
-        return -1;
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 0;
-}
-
-//! common code to handle USHR long
-
-//! It uses XMM
-int common_ushr_long(int vA, int v1, int v2) {
-    get_VR_sd(v1, 1, false);
-    get_VR_ss(v2, 2, false);
-
-    int value[2];
-    int isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, value, false); //do not update refCount
-    if (isConst == 3) {                     // case where shift amount is available
-        int shiftImm = (0x3f) & (value[0]); // compute masked shift amount statically
-        alu_binary_imm_reg(OpndSize_64, srl_opc, shiftImm, 1, false);
-    } else {                                // case where shift count to be read from VR
-        load_sd_global_data_API("shiftMask", 3, false);
-        alu_binary_reg_reg(OpndSize_64, and_opc, 3, false, 2, false);
-        alu_binary_reg_reg(OpndSize_64, srl_opc, 2, false, 1, false);
-    }
-    set_VR_sd(vA, 1, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode shl-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shl_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHL_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_shl_long(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode shl-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shl_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHL_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_shl_long(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode shr-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shr_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHR_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_shr_long(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode shr-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_shr_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SHR_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_shr_long(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode ushr-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_ushr_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_USHR_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_ushr_long(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode ushr-long/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_ushr_long_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_USHR_LONG_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_ushr_long(vA, v1, v2);
-    return retval;
-}
-#define USE_MEM_OPERAND
-///////////////////////////////////////////
-//! common code to handle ALU of floats
-
-//! It uses XMM
-int common_alu_float(ALU_Opcode opc, int vA, int v1, int v2) {//add, sub, mul
-    get_VR_ss(v1, 1, false);
-#ifdef USE_MEM_OPERAND
-    alu_sd_binary_VR_reg(opc, v2, 1, false, false/*isSD*/);
-#else
-    get_VR_ss(v2, 2, false);
-    alu_ss_binary_reg_reg(opc, 2, false, 1, false);
-#endif
-    set_VR_ss(vA, 1, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode add-float
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_FLOAT);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_float(add_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode sub-float
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sub_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SUB_FLOAT);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_float(sub_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode mul-float
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_FLOAT);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_float(mul_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode add-float/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_float_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_FLOAT_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_float(add_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode sub-float/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sub_float_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SUB_FLOAT_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_float(sub_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode mul-float/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_float_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_FLOAT_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_float(mul_opc, vA, v1, v2);
-    return retval;
-}
-//! common code to handle DIV of float
-
-//! It uses FP stack
-int common_div_float(int vA, int v1, int v2) {
-    load_fp_stack_VR(OpndSize_32, v1); //flds
-    fpu_VR(div_opc, OpndSize_32, v2);
-    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode div-float
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_FLOAT);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_float(div_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode div-float/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_float_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_FLOAT_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_float(div_opc, vA, v1, v2);
-    return retval;
-}
-//! common code to handle DIV of double
-
-//! It uses XMM
-int common_alu_double(ALU_Opcode opc, int vA, int v1, int v2) {//add, sub, mul
-    get_VR_sd(v1, 1, false);
-#ifdef USE_MEM_OPERAND
-    alu_sd_binary_VR_reg(opc, v2, 1, false, true /*isSD*/);
-#else
-    get_VR_sd(v2, 2, false);
-    alu_sd_binary_reg_reg(opc, 2, false, 1, false);
-#endif
-    set_VR_sd(vA, 1, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode add-double
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_DOUBLE);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_double(add_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode sub-double
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sub_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SUB_DOUBLE);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_double(sub_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode mul-double
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_DOUBLE);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_double(mul_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode add-double/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_add_double_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ADD_DOUBLE_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_double(add_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode sub-double/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sub_double_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SUB_DOUBLE_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_double(sub_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode mul-double/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_mul_double_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MUL_DOUBLE_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_double(mul_opc, vA, v1, v2);
-    return retval;
-}
-//! common code to handle DIV of double
-
-//! It uses FP stack
-int common_div_double(int vA, int v1, int v2) {
-    load_fp_stack_VR(OpndSize_64, v1); //fldl
-    fpu_VR(div_opc, OpndSize_64, v2); //fdivl
-    store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode div-double
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_DOUBLE);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_alu_double(div_opc, vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode div-double/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_div_double_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_DIV_DOUBLE_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_alu_double(div_opc, vA, v1, v2);
-    return retval;
-}
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-//! common code to handle REM of float
-
-//! It uses GPR & calls call_fmodf
-int common_rem_float(int vA, int v1, int v2) {
-    get_virtual_reg(v1, OpndSize_32, 1, false);
-    get_virtual_reg(v2, OpndSize_32, 2, false);
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 1, false, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 2, false, 4, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    call_fmodf(); //(float x, float y) return float
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-
-/**
- * @brief Generate native code for bytecode rem-float
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_FLOAT);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_rem_float(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode rem-float/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_float_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_FLOAT_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_rem_float(vA, v1, v2);
-    return retval;
-}
-//! common code to handle REM of double
-
-//! It uses XMM & calls call_fmod
-int common_rem_double(int vA, int v1, int v2) {
-    get_virtual_reg(v1, OpndSize_64, 1, false);
-    get_virtual_reg(v2, OpndSize_64, 2, false);
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_64, 1, false, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_64, 2, false, 8, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    call_fmod(); //(long double x, long double y) return double
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode rem-double
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_DOUBLE);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    int retval = common_rem_double(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode rem-double/2addr
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_rem_double_2addr(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_REM_DOUBLE_2ADDR);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = vA;
-    int v2 = mir->dalvikInsn.vB;
-    int retval = common_rem_double(vA, v1, v2);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode cmpl-float
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_cmpl_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CMPL_FLOAT);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    get_VR_ss(v1, 1, false); //xmm
-    move_imm_to_reg(OpndSize_32, 0, 1, false);
-    move_imm_to_reg(OpndSize_32, 1, 2, false);
-    move_imm_to_reg(OpndSize_32, 0xffffffff, 3, false);
-    compare_VR_ss_reg(v2, 1, false);
-    //default: 0xffffffff??
-    move_imm_to_reg(OpndSize_32,
-                                 0xffffffff, 4, false);
-    //ORDER of cmov matters !!! (Z,P,A)
-    //finalNaN: unordered 0xffffffff
-    conditional_move_reg_to_reg(OpndSize_32, Condition_Z,
-                                             1, false, 4, false);
-    conditional_move_reg_to_reg(OpndSize_32, Condition_P,
-                                             3, false, 4, false);
-    conditional_move_reg_to_reg(OpndSize_32, Condition_A,
-                                             2, false, 4, false);
-    set_virtual_reg(vA, OpndSize_32, 4, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode "cmpg-float vAA, vBB, vCC
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_cmpg_float(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CMPG_FLOAT);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-
-    //Operands are reversed here. Comparing vCC and vBB
-    get_VR_ss(v2, 1, false);
-    compare_VR_ss_reg(v1, 1, false);
-
-    rememberState(1);
-
-    //if vCC > vBB, jump to label ".cmp_float_less"
-    conditional_jump(Condition_A, ".cmp_float_less", true);
-
-    //if vCC < vBB, jump to label ".cmp_float_greater". Handles < and NaN
-    conditional_jump(Condition_B, ".cmp_float_greater", true);
-
-    //if vCC = vBB, move 0 to vAA
-    set_VR_to_imm(vA, OpndSize_32, 0);
-
-    rememberState(2);
-    unconditional_jump(".cmp_float_done", true);
-
-    // if vCC < vBB, i.e (if vBB > vCC) or if one of the operand is a NaN,  move +1 to vAA
-    if (insertLabel(".cmp_float_greater", true) == -1)
-       return -1;
-    goToState(1);
-    set_VR_to_imm(vA, OpndSize_32, 1);
-    transferToState(2);
-    unconditional_jump(".cmp_float_done", true);
-
-    // if vCC > vBB, i.e (if vBB < vCC), move -1 to vAA
-    if (insertLabel(".cmp_float_less", true) == -1)
-       return -1;
-    goToState(1);
-    set_VR_to_imm(vA, OpndSize_32, 0xffffffff);
-    transferToState(2);
-
-    //cmpg_float handling over
-    if (insertLabel(".cmp_float_done", true) == -1)
-       return -1;
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode cmpl-double
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_cmpl_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CMPL_DOUBLE);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    get_VR_sd(v1, 1, false);
-    compare_VR_sd_reg(v2, 1, false);
-    move_imm_to_reg(OpndSize_32, 0, 1, false);
-    move_imm_to_reg(OpndSize_32, 1, 2, false);
-    move_imm_to_reg(OpndSize_32, 0xffffffff, 3, false);
-
-    //default: 0xffffffff??
-    move_imm_to_reg(OpndSize_32, 0xffffffff, 4, false);
-    conditional_move_reg_to_reg(OpndSize_32, Condition_Z,
-                                             1, false, 4, false);
-    conditional_move_reg_to_reg(OpndSize_32, Condition_P,
-                                             3, false, 4, false);
-    conditional_move_reg_to_reg(OpndSize_32, Condition_A,
-                                             2, false, 4, false);
-    set_virtual_reg(vA, OpndSize_32, 4, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode cmpg-double vAA, vBB, vCC
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_cmpg_double(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CMPG_DOUBLE);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-
-    //Operands are reversed here. Comparing vCC and vBB
-    get_VR_sd(v2, 1, false);
-    compare_VR_sd_reg(v1, 1, false);
-
-    rememberState(1);
-
-    //if vCC > vBB, jump to label ".cmp_double_less"
-    conditional_jump(Condition_A, ".cmp_double_less", true);
-
-    //if vCC < vBB, jump to label ".cmp_double_greater". Handles < and NaN
-    conditional_jump(Condition_B, ".cmp_double_greater", true);
-
-    //if vCC = vBB, move 0 to vAA
-    set_VR_to_imm(vA, OpndSize_32, 0);
-
-    rememberState(2);
-    unconditional_jump(".cmp_double_done", true);
-
-    // if vCC < vBB, i.e (if vBB > vCC) or if one of the operand is a NaN, move +1 to vAA
-    if (insertLabel(".cmp_double_greater", true) == -1)
-       return -1;
-    goToState(1);
-    set_VR_to_imm(vA, OpndSize_32, 1);
-    transferToState(2);
-    unconditional_jump(".cmp_double_done", true);
-
-    // if vCC > vBB, i.e (if vBB < vCC), move -1 to vAA
-    if (insertLabel(".cmp_double_less", true) == -1)
-       return -1;
-    goToState(1);
-    set_VR_to_imm(vA, OpndSize_32, 0xffffffff);
-    transferToState(2);
-
-    //cmpg_double handling over
-    if (insertLabel(".cmp_double_done", true) == -1)
-       return -1;
-    return 0;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_ESI
-#define P_SCRATCH_1 PhysicalReg_EDX
-#define P_SCRATCH_2 PhysicalReg_EAX
-
-/**
- * @brief Generate native code for bytecode cmp-long
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_cmp_long(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CMP_LONG);
-    int vA = mir->dalvikInsn.vA;
-    int v1 = mir->dalvikInsn.vB;
-    int v2 = mir->dalvikInsn.vC;
-    get_virtual_reg(v1+1, OpndSize_32, 2, false);
-
-    //Compare higher 32 bits
-    compare_VR_reg(OpndSize_32,v2+1, 2, false);
-    rememberState(1);
-    //If equal on higher 32 bits, goto compare of lower 32 bits
-    conditional_jump(Condition_E, ".cmp_long_higher_32b_equal", true);
-    //If less on higher 32 bits, it is less on 64 bits
-    conditional_jump(Condition_L, ".cmp_long_higher_32b_less", true);
-    //If greater on higher 32 bits, it is greater on 64 bits
-    set_VR_to_imm(vA, OpndSize_32, 1);
-    rememberState(2);
-    unconditional_jump(".cmp_long_done", true);
-
-    //If higher 32 bits are equal, compare lower 32 bits
-    if (insertLabel(".cmp_long_higher_32b_equal",true) == -1)
-       return -1;
-    goToState(1);
-    get_virtual_reg(v1, OpndSize_32, 1, false);
-
-    //Compare lower 32 bits
-    compare_VR_reg(OpndSize_32, v2, 1, false);
-    rememberState(3);
-    //Less on lower 32 bits
-    conditional_jump(Condition_B, ".cmp_long_lower_32b_less", true);
-    //Equal on lower 32 bits
-    conditional_jump(Condition_E, ".cmp_long_lower_32b_equal", true);
-    //Greater on lower 32 bits
-    set_VR_to_imm(vA, OpndSize_32, 1);
-    transferToState(2);
-    unconditional_jump(".cmp_long_done", true);
-
-    if (insertLabel(".cmp_long_higher_32b_less", true) == -1)
-       return -1;
-    goToState(1);
-    set_VR_to_imm(vA, OpndSize_32, 0xffffffff);
-    transferToState(2);
-    unconditional_jump(".cmp_long_done", true);
-
-    if (insertLabel(".cmp_long_lower_32b_less", true) == -1)
-       return -1;
-    goToState(3);
-    set_VR_to_imm(vA, OpndSize_32, 0xffffffff);
-    transferToState(2);
-    unconditional_jump(".cmp_long_done", true);
-
-    if (insertLabel(".cmp_long_lower_32b_equal", true) == -1)
-       return -1;
-    goToState(3);
-    set_VR_to_imm(vA, OpndSize_32, 0);
-    transferToState(2);
-
-    if (insertLabel(".cmp_long_done", true) == -1)
-       return -1;
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
diff --git a/vm/compiler/codegen/x86/LowerConst.cpp b/vm/compiler/codegen/x86/LowerConst.cpp
deleted file mode 100644
index 0fe1860..0000000
--- a/vm/compiler/codegen/x86/LowerConst.cpp
+++ /dev/null
@@ -1,240 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file LowerConst.cpp
-    \brief This file lowers the following bytecodes: CONST_XXX
-
-    Functions are called from the lowered native sequence:
-    1> const_string_resolve
-       INPUT: const pool index in %eax
-       OUTPUT: resolved string in %eax
-       The only register that is still live after this function is ebx
-    2> class_resolve
-       INPUT: const pool index in %eax
-       OUTPUT: resolved class in %eax
-       The only register that is still live after this function is ebx
-*/
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "NcgAot.h"
-#include "enc_wrapper.h"
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-
-//! LOWER bytecode CONST_STRING without usage of helper function
-
-//! It calls const_string_resolve (%ebx is live across the call)
-//! Since the register allocator does not handle control flow within the lowered native sequence,
-//!   we define an interface between the lowering module and register allocator:
-//!     rememberState, gotoState, transferToState
-//!   to make sure at the control flow merge point the state of registers is the same
-int const_string_common_nohelper(u4 tmp, int vA) {
-    /* for trace-based JIT, the string is already resolved since this code has been executed */
-    void *strPtr = (void*)
-              (currentMethod->clazz->pDvmDex->pResStrings[tmp]);
-    assert(strPtr != NULL);
-    set_VR_to_imm(vA, OpndSize_32, (int) strPtr );
-    return 0;
-}
-//! dispatcher to select either const_string_common_helper or const_string_common_nohelper
-
-//!
-int const_string_common(u4 tmp, int vA) {
-    return const_string_common_nohelper(tmp, vA);
-}
-#undef P_GPR_1
-#undef P_GPR_2
-
-/**
- * @brief Generate native code for bytecode const/4
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_4(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_4);
-    int vA = mir->dalvikInsn.vA;
-    s4 tmp = mir->dalvikInsn.vB;
-    set_VR_to_imm(vA, OpndSize_32, tmp);
-    return 1;
-}
-
-/**
- * @brief Generate native code for bytecode const/16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_16);
-    u2 BBBB = mir->dalvikInsn.vB;
-    int vA = mir->dalvikInsn.vA;
-    set_VR_to_imm(vA, OpndSize_32, (s2)BBBB);
-    return 1;
-}
-
-/**
- * @brief Generate native code for bytecode const
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-    set_VR_to_imm(vA, OpndSize_32, (s4)tmp);
-    return 1;
-}
-
-/**
- * @brief Generate native code for bytecode const/high16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_high16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_HIGH16);
-    int vA = mir->dalvikInsn.vA;
-    u2 tmp = mir->dalvikInsn.vB;
-    set_VR_to_imm(vA, OpndSize_32, ((s4)tmp)<<16);
-    return 1;
-}
-
-/**
- * @brief Generate native code for bytecode const-wide/16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_wide_16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE_16);
-    int vA = mir->dalvikInsn.vA;
-    u2 tmp = mir->dalvikInsn.vB;
-    set_VR_to_imm(vA, OpndSize_32, (s2)tmp);
-    set_VR_to_imm(vA+1, OpndSize_32, ((s2)tmp)>>31);
-    return 2;
-}
-
-/**
- * @brief Generate native code for bytecode const-wide/32
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_wide_32(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE_32);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-    set_VR_to_imm(vA, OpndSize_32, (s4)tmp);
-    set_VR_to_imm(vA+1, OpndSize_32, ((s4)tmp)>>31);
-    return 2;
-}
-
-/**
- * @brief Generate native code for bytecode const-wide
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_wide(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE);
-    int vA = mir->dalvikInsn.vA;
-    u8 tmp = mir->dalvikInsn.vB_wide;
-    set_VR_to_imm(vA, OpndSize_32, (s4)tmp);
-    set_VR_to_imm(vA+1, OpndSize_32, (s4)(tmp >> 32));
-    return 2;
-}
-
-/**
- * @brief Generate native code for bytecode const-wide/high16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_wide_high16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE_HIGH16);
-    int vA = mir->dalvikInsn.vA;
-    u2 tmp = mir->dalvikInsn.vB;
-    set_VR_to_imm(vA, OpndSize_32, 0);
-    set_VR_to_imm(vA+1, OpndSize_32, ((s4)tmp)<<16);
-    return 2;
-}
-
-/**
- * @brief Generate native code for bytecode const-string
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_string(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_STRING);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-    int retval = const_string_common(tmp, vA);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode const-string/jumbo
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_string_jumbo(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_STRING_JUMBO);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-    int retval = const_string_common(tmp, vA);
-    return retval;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-/**
- * @brief Generate native code for bytecode const-class
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_const_class(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CONST_CLASS);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-#if !defined(WITH_JIT)
-    // It calls class_resolve (%ebx is live across the call)
-    // Since the register allocator does not handle control flow within the lowered native sequence,
-    //   we define an interface between the lowering module and register allocator:
-    //     rememberState, gotoState, transferToState
-    //   to make sure at the control flow merge point the state of registers is the same
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-    get_res_classes(3, false);
-    move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_NE, ".const_class_resolved", true);
-    rememberState(1);
-    export_pc();
-    move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
-    call_helper_API(".class_resolve");
-    transferToState(1);
-    if (insertLabel(".const_class_resolved", true) == -1)
-        return -1;
-    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-#else
-    /* for trace-based JIT, the class is already resolved since this code has been executed */
-    void *classPtr = (void*)
-       (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
-    assert(classPtr != NULL);
-    set_VR_to_imm(vA, OpndSize_32, (int) classPtr );
-#endif
-
-    return 0;
-}
-
-#undef P_GPR_1
-
diff --git a/vm/compiler/codegen/x86/LowerGetPut.cpp b/vm/compiler/codegen/x86/LowerGetPut.cpp
deleted file mode 100644
index e5b8f1d..0000000
--- a/vm/compiler/codegen/x86/LowerGetPut.cpp
+++ /dev/null
@@ -1,1873 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file compiler/codegen/x86/LowerGetPut.cpp
-    \brief This file lowers the following bytecodes: XGET|PUT_XXX
-*/
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "NcgAot.h"
-#include "enc_wrapper.h"
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_ESI
-#define P_GPR_4 PhysicalReg_EDX
-
-/**
- * @brief Common function for generating native code for aget variants
- * @details Includes null check and bound check.
- * @param flag
- * @param vA destination VR
- * @param vref VR holding reference
- * @param vindex VR holding index
- * @param mirOptFlags optimization flags for current bytecode
- * @return value >= 0 when handled
- */
-int aget_common_nohelper(ArrayAccess flag, int vA, int vref, int vindex, int mirOptFlags) {
-    ////////////////////////////
-    // Request VR free delays before register allocation for the temporaries
-    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK))
-        requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
-    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
-        requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
-        requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
-    }
-
-    get_virtual_reg(vref, OpndSize_32, 1, false); //array
-    get_virtual_reg(vindex, OpndSize_32, 2, false); //index
-
-    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK)) {
-        //last argument is the exception number for this bytecode
-        nullCheck(1, false, 1, vref); //maybe optimized away, if not, call
-        cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
-    } else {
-        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
-    }
-
-    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
-        boundCheck(vref, 1, false,
-                             vindex, 2, false,
-                             2);
-        cancelVRFreeDelayRequest(vref,VRDELAY_BOUNDCHECK);
-        cancelVRFreeDelayRequest(vindex,VRDELAY_BOUNDCHECK);
-    } else {
-        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
-        updateRefCount2(2, LowOpndRegType_gp, false); //update reference count for tmp2
-    }
-
-    if(flag == AGET) {
-#ifndef WITH_SELF_VERIFICATION
-        move_mem_disp_scale_to_reg(OpndSize_32, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 4, false);
-#else
-        // Load address into temp 5
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 5, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 5 to (esp)
-        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-        // Mov opnd size to 4(esp)
-        move_imm_to_mem(OpndSize_32, int(OpndSize_32), 4, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationLoad();
-        // Restore ESP
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move result of self verification load into temp4
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-    else if(flag == AGET_WIDE) {
-#ifndef WITH_SELF_VERIFICATION
-        move_mem_disp_scale_to_reg(OpndSize_64, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 8, 1, false);
-#else
-        // Load address into temp 5 (scale of 8 due to opnd size 64), temp 1 is base gp
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 8, 5, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 5 to (esp)
-        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationLoadDoubleword();
-        // Restore ESP
-        load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move result of self verification load into temp 1(XMM)
-        move_reg_to_reg(OpndSize_64, PhysicalReg_XMM7, true, 1, false);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-    else if(flag == AGET_CHAR) {
-#ifndef WITH_SELF_VERIFICATION
-        movez_mem_disp_scale_to_reg(OpndSize_16, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 4, false);
-#else
-        // Load address into temp 5
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 5, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 5 to (esp) //address
-        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-        // Mov opnd size to 4(esp)  // op_size
-        move_imm_to_mem(OpndSize_32, int(OpndSize_16), 4, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationLoad();
-        // Restore ESP
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move result of self verification load into temp 4
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-    else if(flag == AGET_SHORT) {
-#ifndef WITH_SELF_VERIFICATION
-        moves_mem_disp_scale_to_reg(OpndSize_16, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 4, false);
-#else
-        // Load address into temp 5
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 5, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 5 to (esp) //address
-        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-        // Mov opnd size to 4(esp)  // op_size
-        move_imm_to_mem(OpndSize_32, int(0x22), 4, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationLoad();
-        // Restore ESP
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move result of self verification load into temp 4
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-    else if(flag == AGET_BOOLEAN) {
-
-#ifndef WITH_SELF_VERIFICATION
-        movez_mem_disp_scale_to_reg(OpndSize_8, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 4, false);
-#else
-        // Load address into temp 5
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 5, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 5 to (esp) //address
-        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-        // Mov opnd size to 4(esp)  // op_size
-        move_imm_to_mem(OpndSize_32, int(OpndSize_8), 4, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationLoad();
-        // Restore ESP
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move result of self verification load into temp 4
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-    else if(flag == AGET_BYTE) {
-#ifndef WITH_SELF_VERIFICATION
-        moves_mem_disp_scale_to_reg(OpndSize_8, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 4, false);
-#else
-        // Load address into temp 5
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 5, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 5 to (esp) //address
-        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-        // Mov opnd size to 4(esp)  // op_size
-        move_imm_to_mem(OpndSize_32, int(0x11), 4, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationLoad();
-        // Restore ESP
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move result of self verification load into temp 4
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-
-    if(flag == AGET_WIDE) {
-        set_virtual_reg(vA, OpndSize_64, 1, false);
-    }
-    else {
-        set_virtual_reg(vA, OpndSize_32, 4, false);
-    }
-    //////////////////////////////////
-    return 0;
-}
-#if 0 /* Code is deprecated. If reenabled, needs additional parameter
-         for optimization flags*/
-//! wrapper to call either aget_common_helper or aget_common_nohelper
-
-//!
-int aget_common(int flag, int vA, int vref, int vindex) {
-    return aget_common_nohelper(flag, vA, vref, vindex);
-}
-#endif
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef P_GPR_4
-
-/**
- * @brief Generate native code for bytecode aget and aget-object
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aget(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AGET
-            || mir->dalvikInsn.opcode == OP_AGET_OBJECT);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aget_common_nohelper(AGET, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aget-wide
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aget_wide(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AGET_WIDE);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aget_common_nohelper(AGET_WIDE, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aget-object
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aget_object(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AGET_OBJECT);
-    return op_aget(mir);
-}
-
-/**
- * @brief Generate native code for bytecode aget-boolean
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aget_boolean(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AGET_BOOLEAN);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aget_common_nohelper(AGET_BOOLEAN, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aget-byte
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aget_byte(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AGET_BYTE);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aget_common_nohelper(AGET_BYTE, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aget-char
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aget_char(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AGET_CHAR);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aget_common_nohelper(AGET_CHAR, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aget-short
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aget_short(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_AGET_SHORT);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aget_common_nohelper(AGET_SHORT, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_ESI
-#define P_GPR_4 PhysicalReg_EDX
-
-/**
- * @brief Common function for generating native code for aput variants
- * @details Includes null check and bound check.
- * @param flag
- * @param vA destination VR
- * @param vref VR holding reference
- * @param vindex VR holding index
- * @param mirOptFlags optimization flags for current bytecode
- * @return value >= 0 when handled
- */
-int aput_common_nohelper(ArrayAccess flag, int vA, int vref, int vindex, int mirOptFlags) {
-    //////////////////////////////////////
-    // Request VR free delays before register allocation for the temporaries.
-    // No need to request delay for vA since it will be transferred to temporary
-    // after the null check and bound check.
-    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK))
-        requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
-    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
-        requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
-        requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
-    }
-
-    get_virtual_reg(vref, OpndSize_32, 1, false); //array
-    get_virtual_reg(vindex, OpndSize_32, 2, false); //index
-
-    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK)) {
-        //last argument is the exception number for this bytecode
-        nullCheck(1, false, 1, vref); //maybe optimized away, if not, call
-        cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
-    } else {
-        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
-    }
-
-    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
-        boundCheck(vref, 1, false,
-                             vindex, 2, false,
-                             2);
-        cancelVRFreeDelayRequest(vref,VRDELAY_BOUNDCHECK);
-        cancelVRFreeDelayRequest(vindex,VRDELAY_BOUNDCHECK);
-    } else {
-        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
-        updateRefCount2(2, LowOpndRegType_gp, false); //update reference count for tmp2
-    }
-
-    if(flag == APUT_WIDE) {
-        get_virtual_reg(vA, OpndSize_64, 1, false);
-    }
-    else {
-        get_virtual_reg(vA, OpndSize_32, 4, false);
-    }
-    if(flag == APUT) {
-#ifndef WITH_SELF_VERIFICATION
-        move_reg_to_mem_disp_scale(OpndSize_32, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4);
-#else
-        // Load address into temp 5
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 5, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 5 namely the address to (esp)
-        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-        // Store value from temp 4 namely the data to 4(esp)
-        move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);
-        // Mov opnd size to 8(esp)
-        move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationStore();
-        // Restore ESP
-        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    } else if(flag == APUT_WIDE) {
-#ifndef WITH_SELF_VERIFICATION
-        move_reg_to_mem_disp_scale(OpndSize_64, 1, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 8);
-#else
-        // Load address into temp 4
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 8, 4, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 4 namely the address to (esp)
-        move_reg_to_mem(OpndSize_32, 4, false, 0, PhysicalReg_ESP, true);
-        // Store value from temp 1(XMM) namely the data to 4(esp)
-        move_reg_to_mem(OpndSize_64, 1, false, 4, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationStoreDoubleword();
-        // Restore ESP
-        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-    else if(flag == APUT_CHAR || flag == APUT_SHORT) {
-#ifndef WITH_SELF_VERIFICATION
-        move_reg_to_mem_disp_scale(OpndSize_16, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2);
-#else
-        // Load address into temp 5
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 5, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 5 namely the address to (esp)
-        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-        // Store value from temp 4 namely the data to 4(esp)
-        move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);
-        // Mov opnd size to 8(esp)
-        move_imm_to_mem(OpndSize_32, int(OpndSize_16), 8, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationStore();
-        // Restore ESP
-        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-    else if(flag == APUT_BOOLEAN || flag == APUT_BYTE) {
-#ifndef WITH_SELF_VERIFICATION
-        move_reg_to_mem_disp_scale(OpndSize_8, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1);
-#else
-        // Load address into temp 5
-        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 5, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 5 namely the address to (esp)
-        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-        // Store value from temp 4 namely the data to 4(esp)
-        move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);
-        // Mov opnd size to 8(esp)
-        move_imm_to_mem(OpndSize_32, int(OpndSize_8), 8, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationStore();
-        // Restore ESP
-        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-    //////////////////////////////////
-    return 0;
-}
-#if 0 /* Code is deprecated. If reenabled, needs additional parameter
-         for optimization flags*/
-//! wrapper to call either aput_common_helper or aput_common_nohelper
-
-//!
-int aput_common(int flag, int vA, int vref, int vindex) {
-    return aput_common_nohelper(flag, vA, vref, vindex);
-}
-#endif
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef P_GPR_4
-
-/**
- * @brief Generate native code for bytecode aput
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aput(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_APUT);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aput_common_nohelper(APUT, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aput-wide
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aput_wide(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_APUT_WIDE);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aput_common_nohelper(APUT_WIDE, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aput-boolean
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aput_boolean(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_APUT_BOOLEAN);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aput_common_nohelper(APUT_BOOLEAN, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aput-byte
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aput_byte(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_APUT_BYTE);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aput_common_nohelper(APUT_BYTE, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aput-char
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aput_char(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_APUT_CHAR);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aput_common_nohelper(APUT_CHAR, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode aput-short
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aput_short(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_APUT_SHORT);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-    int retval = aput_common_nohelper(APUT_SHORT, vA, vref, vindex,
-            mir->OptimizationFlags);
-    return retval;
-}
-
-#define P_GPR_1 PhysicalReg_EBX //callee-saved valid after CanPutArray
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_ESI //callee-saved
-#define P_SCRATCH_1 PhysicalReg_EDX
-#define P_SCRATCH_2 PhysicalReg_EAX
-#define P_SCRATCH_3 PhysicalReg_EDX
-
-void markCard_notNull(int tgtAddrReg, int scratchReg, bool isPhysical);
-
-/**
- * @brief Generate native code for bytecode aput-object
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_aput_object(const MIR * mir) { //type checking
-    assert(mir->dalvikInsn.opcode == OP_APUT_OBJECT);
-    int vA = mir->dalvikInsn.vA;
-    int vref = mir->dalvikInsn.vB;
-    int vindex = mir->dalvikInsn.vC;
-#ifdef INC_NCG_O0
-    if(gDvm.helper_switch[6]) {
-        export_pc(); //use %edx
-        move_imm_to_reg(OpndSize_32, vA, P_SCRATCH_1, true);
-        move_imm_to_reg(OpndSize_32, vref, P_SCRATCH_2, true);
-        move_imm_to_reg(OpndSize_32, vindex, P_GPR_2, true);
-
-        spillVirtualReg(vref, LowOpndRegType_gp, true);
-        spillVirtualReg(vindex, LowOpndRegType_gp, true);
-        spillVirtualReg(vA, LowOpndRegType_gp, true);
-        call_helper_API(".aput_obj_helper");
-    }
-    else
-#endif
-    {
-        ///////////////////////////
-        // Request VR free delays before register allocation for the temporaries
-        // No need to request delay for vA since it will be transferred to temporary
-        // after the null check and bound check.
-        if(!(mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK))
-            requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
-        if(!(mir->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
-            requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
-            requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
-        }
-
-        get_virtual_reg(vref, OpndSize_32, 1, false); //array
-        if(!(mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK)) {
-            nullCheck(1, false, 1, vref); //maybe optimized away, if not, call
-            cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
-        } else {
-            updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
-        }
-
-        get_virtual_reg(vindex, OpndSize_32, 2, false); //index
-        if(!(mir->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
-            boundCheck(vref, 1, false, vindex, 2, false, 2);
-            cancelVRFreeDelayRequest(vref,VRDELAY_BOUNDCHECK);
-            cancelVRFreeDelayRequest(vindex,VRDELAY_BOUNDCHECK);
-        } else {
-            updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
-            updateRefCount2(2, LowOpndRegType_gp, false); //update reference count for tmp2
-        }
-
-    get_virtual_reg(vA, OpndSize_32, 4, false);
-    compare_imm_reg(OpndSize_32, 0, 4, false);
-    conditional_jump(Condition_E, ".aput_object_skip_check", true);
-    rememberState(1);
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), 4, false, 5, false);
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), 1, false, 6, false);
-    move_reg_to_mem(OpndSize_32, 6, false, 4, PhysicalReg_ESP, true);
-
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    call_dvmCanPutArrayElement(); //scratch??
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump_global_API(Condition_E, "common_errArrayStore", false);
-
-#ifndef WITH_SELF_VERIFICATION
-    //NOTE: "2, false" is live through function call
-    move_reg_to_mem_disp_scale(OpndSize_32, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4);
-#else
-    // lea to temp 7, temp 4 contains the data
-    load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 7, false);
-    pushCallerSavedRegs();
-    // make space on the stack and push 3 args (address, data, operand size)
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 7, false, 0, PhysicalReg_ESP, true);  // address
-    move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);  //data
-    move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    call_selfVerificationStore();
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    popCallerSavedRegs();
-#endif
-    markCard_notNull(1, 11, false);
-    rememberState(2);
-    ////TODO NCG O1 + code cache
-    unconditional_jump(".aput_object_after_check", true);
-
-    if (insertLabel(".aput_object_skip_check", true) == -1)
-        return -1;
-    goToState(1);
-#ifndef WITH_SELF_VERIFICATION
-    //NOTE: "2, false" is live through function call
-    move_reg_to_mem_disp_scale(OpndSize_32, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4);
-#else
-    // lea to temp 7, temp 4 contains the data
-    load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 7, false);
-    pushCallerSavedRegs();
-    // make space on the stack and push 3 args (address, data, operand size)
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 7, false, 0, PhysicalReg_ESP, true); //address
-    move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true); //data
-    move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    call_selfVerificationStore();
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    popCallerSavedRegs();
-#endif
-    transferToState(2);
-    if (insertLabel(".aput_object_after_check", true) == -1)
-        return -1;
-    ///////////////////////////////
-  }
-  return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef P_SCRATCH_1
-#undef P_SCRATCH_2
-#undef P_SCRATCH_3
-
-//////////////////////////////////////////
-#define P_GPR_1 PhysicalReg_ECX
-#define P_GPR_2 PhysicalReg_EBX //should be callee-saved to avoid overwritten by inst_field_resolve
-#define P_GPR_3 PhysicalReg_ESI
-#define P_SCRATCH_1 PhysicalReg_EDX
-
-/*
-   movl offThread_cardTable(self), scratchReg
-   compare_imm_reg 0, valReg (testl valReg, valReg)
-   je .markCard_skip
-   shrl $GC_CARD_SHIFT, tgtAddrReg
-   movb %, (scratchReg, tgtAddrReg)
-   NOTE: scratchReg can be accessed with the corresponding byte
-         tgtAddrReg will be updated
-   for O1, update the corresponding reference count
-*/
-void markCard(int valReg, int tgtAddrReg, bool targetPhysical, int scratchReg, bool isPhysical) {
-   get_self_pointer(PhysicalReg_SCRATCH_6, isScratchPhysical);
-   move_mem_to_reg(OpndSize_32, offsetof(Thread, cardTable), PhysicalReg_SCRATCH_6, isScratchPhysical, scratchReg, isPhysical);
-   compare_imm_reg(OpndSize_32, 0, valReg, isPhysical);
-   conditional_jump(Condition_E, ".markCard_skip", true);
-   alu_binary_imm_reg(OpndSize_32, shr_opc, GC_CARD_SHIFT, tgtAddrReg, targetPhysical);
-   move_reg_to_mem_disp_scale(OpndSize_8, scratchReg, isPhysical, scratchReg, isPhysical, 0, tgtAddrReg, targetPhysical, 1);
-   if (insertLabel(".markCard_skip", true) == -1) {
-       return;
-   }
-}
-
-void markCard_notNull(int tgtAddrReg, int scratchReg, bool isPhysical) {
-   get_self_pointer(PhysicalReg_SCRATCH_2, isScratchPhysical);
-   move_mem_to_reg(OpndSize_32, offsetof(Thread, cardTable), PhysicalReg_SCRATCH_2, isScratchPhysical, scratchReg, isPhysical);
-   alu_binary_imm_reg(OpndSize_32, shr_opc, GC_CARD_SHIFT, tgtAddrReg, isPhysical);
-   move_reg_to_mem_disp_scale(OpndSize_8, scratchReg, isPhysical, scratchReg, isPhysical, 0, tgtAddrReg, isPhysical, 1);
-}
-
-void markCard_filled(int tgtAddrReg, bool isTgtPhysical, int scratchReg, bool isScratchPhysical) {
-   get_self_pointer(PhysicalReg_SCRATCH_2, false/*isPhysical*/);
-   move_mem_to_reg(OpndSize_32, offsetof(Thread, cardTable), PhysicalReg_SCRATCH_2, isScratchPhysical, scratchReg, isScratchPhysical);
-   alu_binary_imm_reg(OpndSize_32, shr_opc, GC_CARD_SHIFT, tgtAddrReg, isTgtPhysical);
-   move_reg_to_mem_disp_scale(OpndSize_8, scratchReg, isScratchPhysical, scratchReg, isScratchPhysical, 0, tgtAddrReg, isTgtPhysical, 1);
-}
-
-/**
- * @brief Common function for generating native code for iget and iput variants
- * @details Includes null check
- * @param referenceIndex instance field index
- * @param flag type of instance access
- * @param vA value register
- * @param vB object register
- * @param isObj true iff mnemonic is object variant
- * @param isVolatile iff mnemonic is volatile variant
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int iget_iput_common_nohelper(u2 referenceIndex, InstanceAccess flag, int vA,
-        int vB, bool isObj, bool isVolatile, const MIR * mir) {
-#if !defined(WITH_JIT)
-    ///////////////////////////////
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-    get_res_fields(3, false);
-    //move_mem_to_reg(OpndSize_32, referenceIndex*4, 3, false, 4, false);
-    compare_imm_mem(OpndSize_32, 0, referenceIndex*4, 3, false);
-    move_mem_to_reg(OpndSize_32, referenceIndex*4, 3, false, PhysicalReg_EAX, true);
-    /*********************************
-    compare_imm_reg(OpndSize_32, 0, 4, false);
-    **********************************/
-    export_pc(); //use %edx
-    conditional_jump(Condition_NE, ".iget_iput_resolved", true);
-    rememberState(1);
-    move_imm_to_reg(OpndSize_32, referenceIndex, PhysicalReg_EAX, true);
-    call_helper_API(".inst_field_resolve");
-    transferToState(1);
-    if (insertLabel(".iget_iput_resolved", true) == -1)
-        return -1;
-#else
-
-    const Method *method =
-            (mir->OptimizationFlags & MIR_CALLEE) ?
-                    mir->meta.calleeMethod : currentMethod;
-    InstField *pInstField =
-            (InstField *) method->clazz->pDvmDex->pResFields[referenceIndex];
-
-    int fieldOffset;
-
-    assert(pInstField != NULL);
-    fieldOffset = pInstField->byteOffset;
-    move_imm_to_reg(OpndSize_32, fieldOffset, 8, false);
-#endif
-
-    // Request VR delay before transfer to temporary. Only vB needs delay.
-    // vA will have non-zero reference count since transfer to temporary for
-    // it happens after null check, thus no delay is needed.
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        requestVRFreeDelay(vB,VRDELAY_NULLCHECK);
-    }
-    get_virtual_reg(vB, OpndSize_32, 7, false);
-    //If we can't ignore the NULL check
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        nullCheck(7, false, 2, vB); //maybe optimized away, if not, call
-        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
-    }
-
-#if !defined(WITH_JIT)
-    move_mem_to_reg(OpndSize_32, offInstField_byteOffset, PhysicalReg_EAX, true, 8, false); //byte offest
-#endif
-    if(flag == IGET) {
-#ifndef WITH_SELF_VERIFICATION
-        move_mem_scale_to_reg(OpndSize_32, 7, false, 8, false, 1, 9, false);
-        set_virtual_reg(vA, OpndSize_32, 9, false);
-#else
-        // Load address into temp reg 10
-        load_effective_addr_scale(7, false, 8, false, 1, 10, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp reg 10 to (esp)
-        move_reg_to_mem(OpndSize_32, 10, false, 0, PhysicalReg_ESP, true);
-        // Mov opnd size to 4(esp)
-        move_imm_to_mem(OpndSize_32, int(OpndSize_32), 4, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationLoad();
-        // Restore ESP
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move result of self verification load into tenp9
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 9, false);
-        // pop caller saved registers
-        popCallerSavedRegs();
-        set_virtual_reg(vA, OpndSize_32, 9, false);
-#endif
-
-#ifdef DEBUG_IGET_OBJ
-        if(isObj > 0) {
-            pushAllRegs();
-            load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            move_reg_to_mem(OpndSize_32, 9, false, 12, PhysicalReg_ESP, true); //field
-            move_reg_to_mem(OpndSize_32, 7, false, 8, PhysicalReg_ESP, true); //object
-            move_imm_to_mem(OpndSize_32, referenceIndex, 4, PhysicalReg_ESP, true); //field
-            move_imm_to_mem(OpndSize_32, 0, 0, PhysicalReg_ESP, true); //iget
-            call_dvmDebugIgetIput();
-            load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            popAllRegs();
-        }
-#endif
-    } else if(flag == IGET_WIDE) {
-#ifndef WITH_SELF_VERIFICATION
-        if(isVolatile) {
-            /* call dvmQuasiAtomicRead64(addr) */
-            load_effective_addr(fieldOffset, 7, false, 9, false);
-            move_reg_to_mem(OpndSize_32, 9, false, -4, PhysicalReg_ESP, true); //1st argument
-            load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            nextVersionOfHardReg(PhysicalReg_EAX, 2);
-            nextVersionOfHardReg(PhysicalReg_EDX, 2);
-            scratchRegs[0] = PhysicalReg_SCRATCH_3;
-            call_dvmQuasiAtomicRead64();
-            load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            //memory content in %edx, %eax
-            set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-            set_virtual_reg(vA+1, OpndSize_32, PhysicalReg_EDX, true);
-        } else {
-            move_mem_scale_to_reg(OpndSize_64, 7, false, 8, false, 1, 1, false); //access field
-            set_virtual_reg(vA, OpndSize_64, 1, false);
-        }
-#else
-        // Load address into temp 10
-        if(isVolatile) {
-            load_effective_addr(fieldOffset, 7, false, 10, false);
-        } else {
-            load_effective_addr_scale(7, false, 8, false, 1, 10, false);
-        }
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 10 to (esp)
-        move_reg_to_mem(OpndSize_32, 10, false, 0, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_5;
-        // Load from shadow heap
-        call_selfVerificationLoadDoubleword();
-        // Restore ESP
-        load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move result of self verification load into temp 1(XMM)
-        move_reg_to_reg(OpndSize_64, PhysicalReg_XMM7, true, 1, false);
-        // pop caller saved registers
-        popCallerSavedRegs();
-        set_virtual_reg(vA, OpndSize_64, 1, false);
-#endif
-    } else if(flag == IPUT) {
-        get_virtual_reg(vA, OpndSize_32, 9, false);
-#ifndef WITH_SELF_VERIFICATION
-        move_reg_to_mem_scale(OpndSize_32, 9, false, 7, false, 8, false, 1); //access field
-#else
-        // Load address into temp 10; reg temp 9 will contain the data
-        load_effective_addr_scale(7, false, 8, false, 1, 10, false);
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 10 namely the address to (esp)
-        move_reg_to_mem(OpndSize_32, 10, false, 0, PhysicalReg_ESP, true);
-        // Store value from temp 9 namely the data to 4(esp)
-        move_reg_to_mem(OpndSize_32, 9, false, 4, PhysicalReg_ESP, true);
-        // Mov opnd size to 8(esp)
-        move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_1;
-        // Load from shadow heap
-        call_selfVerificationStore();
-        // Restore ESP
-        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-        if(isObj) {
-            markCard(9, 7, false, 11, false);
-        }
-    } else if(flag == IPUT_WIDE) {
-        get_virtual_reg(vA, OpndSize_64, 1, false);
-#ifndef WITH_SELF_VERIFICATION
-        if(isVolatile) {
-            /* call dvmQuasiAtomicSwap64(val, addr) */
-            load_effective_addr(fieldOffset, 7, false, 9, false);
-            move_reg_to_mem(OpndSize_32, 9, false, -4, PhysicalReg_ESP, true); //2nd argument
-            move_reg_to_mem(OpndSize_64, 1, false, -12, PhysicalReg_ESP, true); //1st argument
-            load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            scratchRegs[0] = PhysicalReg_SCRATCH_3;
-            call_dvmQuasiAtomicSwap64();
-            load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        }
-        else {
-            move_reg_to_mem_scale(OpndSize_64, 1, false, 7, false, 8, false, 1);
-        }
-#else
-        //TODO: handle the volatile type correctly..
-        // Load address into temp 10
-        if(isVolatile) {
-            load_effective_addr(fieldOffset, 7, false, 10, false);
-        } else {
-            load_effective_addr_scale(7, false, 8, false, 1, 10, false);
-        }
-        // push caller saved registers
-        pushCallerSavedRegs();
-        // Set up arguments
-        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // Move value in temp 10 namely the address to (esp)
-        move_reg_to_mem(OpndSize_32, 10, false, 0, PhysicalReg_ESP, true);
-        // Store value from temp 1(XMM) namely the data to 4(esp)
-        move_reg_to_mem(OpndSize_64, 1, false, 4, PhysicalReg_ESP, true);
-        // In order to call, the scratch reg must be set
-        scratchRegs[0] = PhysicalReg_SCRATCH_5;
-        // Load from shadow heap
-        call_selfVerificationStoreDoubleword();
-        // Restore ESP
-        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        // pop caller saved registers
-        popCallerSavedRegs();
-#endif
-    }
-    ///////////////////////////
-    return 0;
-}
-
-#if 0 /* Code is deprecated. If reenabled, needs additional parameter
-         for optimization flags*/
-//! wrapper to call either iget_iput_common_helper or iget_iput_common_nohelper
-
-//!
-int iget_iput_common(int tmp, int flag, int vA, int vB, int isObj, bool isVolatile) {
-    return iget_iput_common_nohelper(tmp, flag, vA, vB, isObj, isVolatile);
-}
-#endif
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef P_SCRATCH_1
-
-/**
- * @brief Generate native code for bytecodes iget, iget-boolean,
- * iget-byte, iget-char, iget-short, and iget/volatile
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iget(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IGET
-            || mir->dalvikInsn.opcode == OP_IGET_BOOLEAN
-            || mir->dalvikInsn.opcode == OP_IGET_BYTE
-            || mir->dalvikInsn.opcode == OP_IGET_CHAR
-            || mir->dalvikInsn.opcode == OP_IGET_SHORT
-            || mir->dalvikInsn.opcode == OP_IGET_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    u2 referenceIndex = mir->dalvikInsn.vC;
-    int retval = iget_iput_common_nohelper(referenceIndex, IGET, vA, vB, false,
-            false, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecodes iget-wide and iget-wide/volatile
- * @param mir bytecode representation
- * @param isVolatile is the iget a volatile access or not?
- * @return value >= 0 when handled
- */
-int op_iget_wide(const MIR * mir, bool isVolatile) {
-    assert(mir->dalvikInsn.opcode == OP_IGET_WIDE
-            || mir->dalvikInsn.opcode == OP_IGET_WIDE_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    u2 referenceIndex = mir->dalvikInsn.vC;
-    int retval = iget_iput_common_nohelper(referenceIndex, IGET_WIDE, vA, vB,
-            false, isVolatile, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecodes iget-object
- * and iget-object/volatile
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iget_object(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IGET_OBJECT
-            || mir->dalvikInsn.opcode == OP_IGET_OBJECT_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    u2 referenceIndex = mir->dalvikInsn.vC;
-    int retval = iget_iput_common_nohelper(referenceIndex, IGET, vA, vB, true,
-            false, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode iget-boolean
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iget_boolean(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IGET_BOOLEAN);
-    return op_iget(mir);
-}
-
-/**
- * @brief Generate native code for bytecode iget-byte
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iget_byte(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IGET_BYTE);
-    return op_iget(mir);
-}
-
-/**
- * @brief Generate native code for bytecode iget-char
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iget_char(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IGET_CHAR);
-    return op_iget(mir);
-}
-
-/**
- * @brief Generate native code for bytecode iget-short
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iget_short(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IGET_SHORT);
-    return op_iget(mir);
-}
-
-/**
- * @brief Generate native code for bytecodes iput, iput-boolean,
- * iput-byte, iput-char, iput-short, and iput/volatile
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iput(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT
-            || mir->dalvikInsn.opcode == OP_IPUT_BOOLEAN
-            || mir->dalvikInsn.opcode == OP_IPUT_BYTE
-            || mir->dalvikInsn.opcode == OP_IPUT_CHAR
-            || mir->dalvikInsn.opcode == OP_IPUT_SHORT
-            || mir->dalvikInsn.opcode == OP_IPUT_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    u2 referenceIndex = mir->dalvikInsn.vC;
-    int retval = iget_iput_common_nohelper(referenceIndex, IPUT, vA, vB, false,
-            false, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecodes iput-wide and iput-wide/volatile
- * @param mir bytecode representation
- * @param isVolatile is the iput a volatile access or not?
- * @return value >= 0 when handled
- */
-int op_iput_wide(const MIR * mir, bool isVolatile) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT_WIDE
-            || mir->dalvikInsn.opcode == OP_IPUT_WIDE_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    u2 referenceIndex = mir->dalvikInsn.vC;
-    int retval = iget_iput_common_nohelper(referenceIndex, IPUT_WIDE, vA, vB,
-            false, isVolatile, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecodes iput-object and iput-object/volatile
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iput_object(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT_OBJECT
-            || mir->dalvikInsn.opcode == OP_IPUT_OBJECT_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    u2 referenceIndex = mir->dalvikInsn.vC;
-    int retval = iget_iput_common_nohelper(referenceIndex, IPUT, vA, vB, true,
-            false, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode iput-boolean
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iput_boolean(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT_BOOLEAN);
-    return op_iput(mir);
-}
-
-/**
- * @brief Generate native code for bytecode iput-byte
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iput_byte(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT_BYTE);
-    return op_iput(mir);
-}
-
-/**
- * @brief Generate native code for bytecode iput-char
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iput_char(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT_CHAR);
-    return op_iput(mir);
-}
-
-/**
- * @brief Generate native code for bytecode iput-short
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iput_short(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT_SHORT);
-    return op_iput(mir);
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_EDX //used by helper only
-
-/**
- * @brief Common function for generating native code for sget and sput variants
- * @details Includes null check
- * @param flag type of static access
- * @param vA value register
- * @param referenceIndex static field index
- * @param isObj true iff mnemonic is object variant
- * @param isVolatile iff mnemonic is volatile variant
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int sget_sput_common(StaticAccess flag, int vA, u2 referenceIndex, bool isObj,
-        bool isVolatile, const MIR * mir) {
-#ifdef INC_NCG_O0
-    if(gDvm.helper_switch[5]) {
-        return sget_sput_common_helper(flag, vA, referenceIndex, isObj);
-    }
-    else
-#endif
-    {
-        //call assembly static_field_resolve
-        //no exception
-        //glue: get_res_fields
-        //hard-coded: eax (one version?)
-        //////////////////////////////////////////
-#if !defined(WITH_JIT)
-        scratchRegs[2] = PhysicalReg_EDX; scratchRegs[3] = PhysicalReg_Null;
-        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-        get_res_fields(3, false);
-        move_mem_to_reg(OpndSize_32, referenceIndex*4, 3, false, PhysicalReg_EAX, true);
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //InstanceField
-        conditional_jump(Condition_NE, ".sget_sput_resolved", true);
-        rememberState(1);
-        move_imm_to_reg(OpndSize_32, referenceIndex, PhysicalReg_EAX, true);
-
-        export_pc(); //use %edx
-        call_helper_API(".static_field_resolve");
-        transferToState(1);
-        if (insertLabel(".sget_sput_resolved", true) == -1)
-            return -1;
-#else
-
-        const Method *method =
-                (mir->OptimizationFlags & MIR_CALLEE) ?
-                        mir->meta.calleeMethod : currentMethod;
-        void *fieldPtr =
-                (void*) (method->clazz->pDvmDex->pResFields[referenceIndex]);
-
-        /* Usually, fieldPtr should not be null. The interpreter should resolve
-         * it before we come here, or not allow this opcode in a trace. However,
-         * we can be in a loop trace and this opcode might have been picked up
-         * by exhaustTrace. Sending a -1 here will terminate the loop formation
-         * and fall back to normal trace, which will not have this opcode.
-         */
-        if (!fieldPtr) {
-            ALOGI("JIT_INFO: Unresolved fieldPtr at sget_sput_common");
-            SET_JIT_ERROR(kJitErrorUnresolvedField);
-            return -1;
-        }
-
-    move_imm_to_reg(OpndSize_32, (int)fieldPtr, PhysicalReg_EAX, true);
-#endif
-    if(flag == SGET) {
-#ifndef WITH_SELF_VERIFICATION
-        move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 7, false); //access field
-        set_virtual_reg(vA, OpndSize_32, 7, false);
-#else
-            // Load address into temp reg 8
-            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 8, false);
-            // push caller saved registers
-            pushCallerSavedRegs();
-            // Set up arguments
-            load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            // Move value in temp reg 8 to (esp)
-            move_reg_to_mem(OpndSize_32, 8, false, 0, PhysicalReg_ESP, true);
-            // Mov opnd size to 4(esp)
-            move_imm_to_mem(OpndSize_32, int(OpndSize_32), 4, PhysicalReg_ESP, true);
-            // In order to call, the scratch reg must be set
-            scratchRegs[0] = PhysicalReg_SCRATCH_5;
-            // Load from shadow heap
-            call_selfVerificationLoad();
-            // Restore ESP
-            load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            // Move result of self verification load into temp 7
-            move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 7, false);
-            // pop caller saved registers
-            popCallerSavedRegs();
-            set_virtual_reg(vA, OpndSize_32, 7, false);
-#endif
-    } else if(flag == SGET_WIDE) {
-#ifndef WITH_SELF_VERIFICATION
-        if(isVolatile) {
-            /* call dvmQuasiAtomicRead64(addr) */
-            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 9, false);
-            move_reg_to_mem(OpndSize_32, 9, false, -4, PhysicalReg_ESP, true); //1st argument
-            load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            nextVersionOfHardReg(PhysicalReg_EAX, 2);
-            nextVersionOfHardReg(PhysicalReg_EDX, 2);
-            scratchRegs[0] = PhysicalReg_SCRATCH_3;
-            call_dvmQuasiAtomicRead64();
-            load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            //memory content in %edx, %eax
-            set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-            set_virtual_reg(vA+1, OpndSize_32, PhysicalReg_EDX, true);
-        }
-        else {
-            move_mem_to_reg(OpndSize_64, OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 1, false); //access field
-            set_virtual_reg(vA, OpndSize_64, 1, false);
-        }
-#else
-            // TODO: the volatile 64 bit type is not handled;
-            // write a C function to only return the mapped shadow address(Read)
-            // Load address into temp 4
-            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 4, false);
-            // push caller saved registers
-            pushCallerSavedRegs();
-            // Set up arguments
-            load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            // Move value in temp 4 (address) to 0(esp)
-            move_reg_to_mem(OpndSize_32, 4, false, 0, PhysicalReg_ESP, true);
-            // In order to call, the scratch reg must be set
-            scratchRegs[0] = PhysicalReg_SCRATCH_5;
-            // Load from shadow heap
-            call_selfVerificationLoadDoubleword();
-            // Restore ESP
-            load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            // Move result of self verification load from XMM7 to temp 1(XMM)
-            move_reg_to_reg(OpndSize_64, PhysicalReg_XMM7, true, 1, false);
-            // pop caller saved registers
-            popCallerSavedRegs();
-            set_virtual_reg(vA, OpndSize_64, 1, false);
-#endif
-    } else if(flag == SPUT) {
-        get_virtual_reg(vA, OpndSize_32, 7, false);
-#ifndef WITH_SELF_VERIFICATION
-        move_reg_to_mem(OpndSize_32, 7, false, OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true); //access field
-#else
-            // Load address into temp 8; reg temp 7 will contain the data
-            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 8, false);
-            // push caller saved registers
-            pushCallerSavedRegs();
-            // Set up arguments
-            load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            // Move value in temp 8 namely the address to (esp)
-            move_reg_to_mem(OpndSize_32, 8, false, 0, PhysicalReg_ESP, true);
-            // Store value from temp 7 namely the data to 4(esp)
-            move_reg_to_mem(OpndSize_32, 7, false, 4, PhysicalReg_ESP, true);
-            // Mov opnd size to 8(esp)
-            move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
-            // In order to call, the scratch reg must be set
-            scratchRegs[0] = PhysicalReg_SCRATCH_5;
-            // Load from shadow heap
-            call_selfVerificationStore();
-            // Restore ESP
-            load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            // pop caller saved registers
-            popCallerSavedRegs();
-#endif
-        if(isObj) {
-            /* get clazz object, then use clazz object to mark card */
-            move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Field, clazz), PhysicalReg_EAX, true, 12, false);
-            markCard(7/*valReg*/, 12, false, 11, false);
-        }
-    } else if(flag == SPUT_WIDE) {
-        get_virtual_reg(vA, OpndSize_64, 1, false);
-#ifndef WITH_SELF_VERIFICATION
-        if(isVolatile) {
-            /* call dvmQuasiAtomicSwap64(val, addr) */
-            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 9, false);
-            move_reg_to_mem(OpndSize_32, 9, false, -4, PhysicalReg_ESP, true); //2nd argument
-            move_reg_to_mem(OpndSize_64, 1, false, -12, PhysicalReg_ESP, true); //1st argument
-            load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            scratchRegs[0] = PhysicalReg_SCRATCH_3;
-            call_dvmQuasiAtomicSwap64();
-            load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        }
-        else {
-            move_reg_to_mem(OpndSize_64, 1, false, OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true); //access field
-        }
-#else
-            // Load address into temp 4; reg temp 1 will contain the data
-            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 4, false);
-            // push caller saved registers
-            pushCallerSavedRegs();
-            // Set up arguments
-            load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            // Move value in temp 4 namely the address to (esp)
-            move_reg_to_mem(OpndSize_32, 4, false, 0, PhysicalReg_ESP, true);
-            // Store value from temp 1(XMM) namely the data to 4(esp)
-            move_reg_to_mem(OpndSize_64, 1, false, 4, PhysicalReg_ESP, true);
-            // In order to call, the scratch reg must be set
-            scratchRegs[0] = PhysicalReg_SCRATCH_5;
-            // Load from shadow heap
-            call_selfVerificationStoreDoubleword();
-            // Restore ESP
-            load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-            // pop caller saved registers
-            popCallerSavedRegs();
-#endif
-        }
-    //////////////////////////////////////////////
-  }
-  return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-
-/**
- * @brief Generate native code for bytecodes sget, sget-boolean, sget-byte, sget-char, sget-object, sget-short, sget/volatile and sget-object/volatile
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sget(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SGET
-            || mir->dalvikInsn.opcode == OP_SGET_BOOLEAN
-            || mir->dalvikInsn.opcode == OP_SGET_BYTE
-            || mir->dalvikInsn.opcode == OP_SGET_CHAR
-            || mir->dalvikInsn.opcode == OP_SGET_OBJECT
-            || mir->dalvikInsn.opcode == OP_SGET_SHORT
-            || mir->dalvikInsn.opcode == OP_SGET_VOLATILE
-            || mir->dalvikInsn.opcode == OP_SGET_OBJECT_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    u2 referenceIndex = mir->dalvikInsn.vB;
-    int retval = sget_sput_common(SGET, vA, referenceIndex, false, false, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecodes sget-wide and sget-wide/volatile
- * @param mir bytecode representation
- * @param isVolatile is the sget a volatile access or not?
- * @return value >= 0 when handled
- */
-int op_sget_wide(const MIR * mir, bool isVolatile) {
-    assert(mir->dalvikInsn.opcode == OP_SGET_WIDE
-            || mir->dalvikInsn.opcode == OP_SGET_WIDE_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    u2 referenceIndex = mir->dalvikInsn.vB;
-    int retval = sget_sput_common(SGET_WIDE, vA, referenceIndex, false,
-            isVolatile, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecodes sget-object and sget-object/volatile
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sget_object(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SGET_OBJECT
-            || mir->dalvikInsn.opcode == OP_SGET_OBJECT_VOLATILE);
-    return op_sget(mir);
-}
-
-/**
- * @brief Generate native code for bytecode sget-boolean
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sget_boolean(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SGET_BOOLEAN);
-    return op_sget(mir);
-}
-
-/**
- * @brief Generate native code for bytecode sget-byte
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sget_byte(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SGET_BYTE);
-    return op_sget(mir);
-}
-
-/**
- * @brief Generate native code for bytecode sget-char
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sget_char(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SGET_CHAR);
-    return op_sget(mir);
-}
-
-/**
- * @brief Generate native code for bytecode sget-short
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sget_short(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SGET_SHORT);
-    return op_sget(mir);
-}
-
-/**
- * @brief Generate native code for bytecodes sput, sput-boolean,
- * sput-byte, sput-char, sput-object, sput-short, sput/volatile and sput-object/volatile
- * @param mir bytecode representation
- * @param isObj is the store an object?
- * @return value >= 0 when handled
- */
-int op_sput(const MIR * mir, bool isObj) {
-    assert(mir->dalvikInsn.opcode == OP_SPUT
-            || mir->dalvikInsn.opcode == OP_SPUT_BOOLEAN
-            || mir->dalvikInsn.opcode == OP_SPUT_BYTE
-            || mir->dalvikInsn.opcode == OP_SPUT_CHAR
-            || mir->dalvikInsn.opcode == OP_SPUT_OBJECT
-            || mir->dalvikInsn.opcode == OP_SPUT_SHORT
-            || mir->dalvikInsn.opcode == OP_SPUT_VOLATILE
-            || mir->dalvikInsn.opcode == OP_SPUT_OBJECT_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    u2 referenceIndex = mir->dalvikInsn.vB;
-    int retval = sget_sput_common(SPUT, vA, referenceIndex, isObj, false, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecodes sput-wide
- * and sput-wide/volatile
- * @param mir bytecode representation
- * @param isVolatile is the sput a volatile access or not?
- * @return value >= 0 when handled
- */
-int op_sput_wide(const MIR * mir, bool isVolatile) {
-    assert(mir->dalvikInsn.opcode == OP_SPUT_WIDE
-            || mir->dalvikInsn.opcode == OP_SPUT_WIDE_VOLATILE);
-    int vA = mir->dalvikInsn.vA;
-    u2 referenceIndex = mir->dalvikInsn.vB;
-    int retval = sget_sput_common(SPUT_WIDE, vA, referenceIndex, false,
-            isVolatile, mir);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecodes sput-object and
- * sput-object/volatile
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sput_object(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SPUT_OBJECT
-            || mir->dalvikInsn.opcode == OP_SPUT_OBJECT_VOLATILE);
-    return op_sput(mir, true /*isObj*/);
-}
-
-/**
- * @brief Generate native code for bytecode sput-boolean
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sput_boolean(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SPUT_BOOLEAN);
-    return op_sput(mir, false /*isObj*/);
-}
-
-/**
- * @brief Generate native code for bytecode sput-byte
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sput_byte(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SPUT_BYTE);
-    return op_sput(mir, false /*isObj*/);
-}
-
-/**
- * @brief Generate native code for bytecode sput-char
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sput_char(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SPUT_CHAR);
-    return op_sput(mir, false /*isObj*/);
-}
-
-/**
- * @brief Generate native code for bytecode sput-short
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_sput_short(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_SPUT_SHORT);
-    return op_sput(mir, false /*isObj*/);
-}
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-
-/**
- * @brief Generate native code for bytecodes iget-quick and iget-object-quick
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iget_quick(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IGET_QUICK
-            || mir->dalvikInsn.opcode == OP_IGET_OBJECT_QUICK);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB; //object
-    u2 fieldByteOffset = mir->dalvikInsn.vC;
-
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
-    }
-
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-
-    //If we can't ignore the NULL check
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
-        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
-    }
-
-#ifndef WITH_SELF_VERIFICATION
-    move_mem_to_reg(OpndSize_32, fieldByteOffset, 1, false, 2, false);
-    set_virtual_reg(vA, OpndSize_32, 2, false);
-#else
-    // Load address into temp reg 3
-    load_effective_addr(fieldByteOffset, 1, false, 3, false);
-    // push caller saved registers
-    pushCallerSavedRegs();
-    // Set up arguments
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    // Move value in temp reg 3 to (esp)
-    move_reg_to_mem(OpndSize_32, 3, false, 0, PhysicalReg_ESP, true);
-    // Mov opnd size to 4(esp)
-    move_imm_to_mem(OpndSize_32, int(OpndSize_32), 4, PhysicalReg_ESP, true);
-    // In order to call, the scratch reg must be set
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    // Load from shadow heap
-    call_selfVerificationLoad();
-    // Restore ESP
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    // Move result of self verification load into temp 2
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 2, false);
-    // pop caller saved registers
-    popCallerSavedRegs();
-    set_virtual_reg(vA, OpndSize_32, 2, false);
-#endif
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecode
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iget_wide_quick(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IGET_WIDE_QUICK);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB; //object
-    u2 fieldByteOffset = mir->dalvikInsn.vC;
-
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
-    }
-
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-
-    //If we can't ignore the NULL check
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
-        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
-    }
-
-#ifndef WITH_SELF_VERIFICATION
-    move_mem_to_reg(OpndSize_64, fieldByteOffset, 1, false, 1, false);
-#else
-    // Load address into temp 3
-    load_effective_addr(fieldByteOffset, 1, false, 3, false);
-    // push caller saved registers
-    pushCallerSavedRegs();
-    // Set up arguments
-    load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    // Move value in temp 3 (address) to 0(esp)
-    move_reg_to_mem(OpndSize_32, 3, false, 0, PhysicalReg_ESP, true);
-    // In order to call, the scratch reg must be set
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    // Load from shadow heap
-    call_selfVerificationLoadDoubleword();
-    // Restore ESP
-    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    // Move result of self verification load from XMM7 to temp 1(XMM)
-    move_reg_to_reg(OpndSize_64, PhysicalReg_XMM7, true, 1, false);
-    // pop caller saved registers
-    popCallerSavedRegs();
-#endif
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 0;
-}
-#undef P_GPR_1
-
-/**
- * @brief Generate native code for bytecode
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iget_object_quick(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IGET_OBJECT_QUICK);
-    return op_iget_quick(mir);
-}
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-
-/**
- *
- * @param mir
- * @param isObj
- * @return
- */
-int iput_quick_common(const MIR * mir, bool isObj) {
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB; //object
-    u2 fieldByteOffset = mir->dalvikInsn.vC;
-
-    // Request VR delay before transfer to temporary. Only vB needs delay.
-    // vA will have non-zero reference count since transfer to temporary for
-    // it happens after null check, thus no delay is needed.
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        requestVRFreeDelay(vB,VRDELAY_NULLCHECK);
-    }
-
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-
-    //If we can't ignore the NULL check
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
-        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
-    }
-
-    get_virtual_reg(vA, OpndSize_32, 2, false);
-#ifndef WITH_SELF_VERIFICATION
-    move_reg_to_mem(OpndSize_32, 2, false, fieldByteOffset, 1, false);
-#else
-    // Load address into temp 3; reg temp 2 will contain the data
-    load_effective_addr(fieldByteOffset, 1, false, 3, false);
-    // push caller saved registers
-    pushCallerSavedRegs();
-    // Set up arguments
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    // Move value in temp 3 namely the address to (esp)
-    move_reg_to_mem(OpndSize_32, 3, false, 0, PhysicalReg_ESP, true);
-    // Store value from temp 2 namely the data to 4(esp)
-    move_reg_to_mem(OpndSize_32, 2, false, 4, PhysicalReg_ESP, true);
-    // Mov opnd size to 8(esp)
-    move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
-    // In order to call, the scratch reg must be set
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    // Load from shadow heap
-    call_selfVerificationStore();
-    // Restore ESP
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    // pop caller saved registers
-    popCallerSavedRegs();
-#endif
-    if(isObj) {
-        markCard(2/*valReg*/, 1, false, 11, false);
-    }
-    return 0;
-}
-/**
- * @brief Generate native code for bytecode
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iput_quick(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT_QUICK);
-    return iput_quick_common(mir, false /*isObj*/);
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecode
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iput_wide_quick(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT_WIDE_QUICK);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB; //object
-    u2 fieldByteOffset = mir->dalvikInsn.vC;
-
-    // Request VR delay before transfer to temporary. Only vB needs delay.
-    // vA will have non-zero reference count since transfer to temporary for
-    // it happens after null check, thus no delay is needed.
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        requestVRFreeDelay(vB,VRDELAY_NULLCHECK);
-    }
-
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-
-    //If we can't ignore the NULL check
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
-        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
-    }
-
-    get_virtual_reg(vA, OpndSize_64, 1, false);
-
-#ifndef WITH_SELF_VERIFICATION
-    move_reg_to_mem(OpndSize_64, 1, false, fieldByteOffset, 1, false);
-#else
-    // Load address into temp 3; reg temp 1 will contain the data
-    load_effective_addr(fieldByteOffset, 1, false, 3, false);
-    // push caller saved registers
-    pushCallerSavedRegs();
-    // Set up arguments
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    // Move value in temp 3 namely the address to (esp)
-    move_reg_to_mem(OpndSize_32, 3, false, 0, PhysicalReg_ESP, true);
-    // Store value from temp 1(XMM) namely the data to 4(esp)
-    move_reg_to_mem(OpndSize_64, 1, false, 4, PhysicalReg_ESP, true);
-    // In order to call, the scratch reg must be set
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    // Load from shadow heap
-    call_selfVerificationStoreDoubleword();
-    // Restore ESP
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    // pop caller saved registers
-    popCallerSavedRegs();
-#endif
-    return 0;
-}
-#undef P_GPR_1
-
-/**
- * @brief Generate native code for bytecode
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_iput_object_quick(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IPUT_OBJECT_QUICK);
-    return iput_quick_common(mir, true /*isObj*/);
-}
-
diff --git a/vm/compiler/codegen/x86/LowerHelper.cpp b/vm/compiler/codegen/x86/LowerHelper.cpp
deleted file mode 100644
index 1716771..0000000
--- a/vm/compiler/codegen/x86/LowerHelper.cpp
+++ /dev/null
@@ -1,4277 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file LowerHelper.cpp
-    \brief This file implements helper functions for lowering
-
-With NCG O0: all registers are hard-coded ;
-With NCG O1: the lowering module will use variables that will be allocated to a physical register by the register allocator.
-
-register types: FS 32-bit or 64-bit;
-                XMM: SS(32-bit) SD (64-bit);
-                GPR: 8-bit, 16-bit, 32-bit;
-LowOpndRegType tells whether it is gpr, xmm or fs;
-OpndSize can be OpndSize_8, OpndSize_16, OpndSize_32, OpndSize_64
-
-A single native instruction can use multiple physical registers.
-  we can't call freeReg in the middle of emitting a native instruction,
-  since it may free the physical register used by an operand and cause two operands being allocated to the same physical register.
-
-When allocating a physical register for an operand, we can't spill the operands that are already allocated. To avoid that, we call startNativeCode before each native instruction, it resets the spill information to true for each physical register;
-  when a physical register is allocated, we set its corresponding flag to false;
-  at end of each native instruction, call endNativeCode to also reset the flags to true.
-*/
-
-#include "CompilationUnit.h"
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "NcgAot.h"
-#include "enc_wrapper.h"
-#include "vm/mterp/Mterp.h"
-#include "vm/mterp/common/FindInterface.h"
-#include "NcgHelper.h"
-#include <math.h>
-#include "interp/InterpState.h"
-#include "Scheduler.h"
-#include "Singleton.h"
-#include "ExceptionHandling.h"
-#include "compiler/Dataflow.h"
-
-extern "C" int64_t __divdi3(int64_t, int64_t);
-extern "C" int64_t __moddi3(int64_t, int64_t);
-bool isScratchPhysical;
-
-//4 tables are defined: GPR integer ALU ops, ALU ops in FPU, SSE 32-bit, SSE 64-bit
-//the index to the table is the opcode
-//add_opc,    or_opc,     adc_opc,    sbb_opc,
-//and_opc,    sub_opc,    xor_opc,    cmp_opc,
-//mul_opc,    imul_opc,   div_opc,    idiv_opc,
-//sll_opc,    srl_opc,    sra, (SSE)
-//shl_opc,    shr_opc,    sal_opc,    sar_opc, //integer shift
-//neg_opc,    not_opc,    andn_opc, (SSE)
-//n_alu
-//!mnemonic for integer ALU operations
-const  Mnemonic map_of_alu_opcode_2_mnemonic[] = {
-    Mnemonic_ADD,  Mnemonic_OR,   Mnemonic_ADC,  Mnemonic_SBB,
-    Mnemonic_AND,  Mnemonic_SUB,  Mnemonic_XOR,  Mnemonic_CMP,
-    Mnemonic_MUL,  Mnemonic_IMUL, Mnemonic_DIV,  Mnemonic_IDIV,
-    Mnemonic_Null, Mnemonic_Null, Mnemonic_Null,
-    Mnemonic_SHL,  Mnemonic_SHR,  Mnemonic_SAL,  Mnemonic_SAR,
-    Mnemonic_NEG,  Mnemonic_NOT,  Mnemonic_Null,
-    Mnemonic_Null
-};
-//!mnemonic for ALU operations in FPU
-const  Mnemonic map_of_fpu_opcode_2_mnemonic[] = {
-    Mnemonic_FADD,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_Null,  Mnemonic_FSUB,  Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_FMUL,  Mnemonic_Null,  Mnemonic_FDIV,  Mnemonic_Null,
-    Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_Null
-};
-//!mnemonic for SSE 32-bit
-const  Mnemonic map_of_sse_opcode_2_mnemonic[] = {
-    Mnemonic_ADDSD,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_Null,   Mnemonic_SUBSD, Mnemonic_XORPD, Mnemonic_Null,
-    Mnemonic_MULSD,  Mnemonic_Null,  Mnemonic_DIVSD,  Mnemonic_Null,
-    Mnemonic_Null,   Mnemonic_Null,
-    Mnemonic_Null,   Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_Null,   Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_Null
-};
-//!mnemonic for SSE 64-bit integer
-const  Mnemonic map_of_64_opcode_2_mnemonic[] = {
-    Mnemonic_PADDQ, Mnemonic_POR,   Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_PAND,  Mnemonic_PSUBQ, Mnemonic_PXOR,  Mnemonic_Null,
-    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_PSLLQ, Mnemonic_PSRLQ, Mnemonic_Null,
-    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
-    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_PANDN,
-    Mnemonic_Null
-};
-
-//! \brief Simplifies update of LowOpndReg fields.
-void set_reg_opnd(LowOpndReg* op_reg, int reg, bool isPhysical,
-        LowOpndRegType type) {
-    op_reg->regType = type;
-    op_reg->regNum = reg;
-    op_reg->isPhysical = isPhysical;
-}
-
-//! \brief Simplifies update of LowOpndMem fields when only base and
-//! displacement is used.
-void set_mem_opnd(LowOpndMem* mem, int disp, int base, bool isPhysical) {
-    mem->m_disp.value = disp;
-    mem->hasScale = false;
-    mem->m_base.regType = LowOpndRegType_gp;
-    mem->m_base.regNum = base;
-    mem->m_base.isPhysical = isPhysical;
-}
-
-//! \brief Simplifies update of LowOpndMem fields when base, displacement, index,
-//! and scaling is used.
-void set_mem_opnd_scale(LowOpndMem* mem, int base, bool isPhysical, int disp,
-        int index, bool indexPhysical, int scale) {
-    mem->hasScale = true;
-    mem->m_base.regType = LowOpndRegType_gp;
-    mem->m_base.regNum = base;
-    mem->m_base.isPhysical = isPhysical;
-    mem->m_index.regNum = index;
-    mem->m_index.isPhysical = indexPhysical;
-    mem->m_disp.value = disp;
-    mem->m_scale.value = scale;
-}
-
-//! \brief Return either LowOpndRegType_xmm or LowOpndRegType_gp
-//! depending on operand size.
-//! \param size
-inline LowOpndRegType getTypeFromIntSize(OpndSize size) {
-    return size == OpndSize_64 ? LowOpndRegType_xmm : LowOpndRegType_gp;
-}
-
-//! \brief Thin layer over encoder that makes scheduling decision and
-//! is used for dumping instruction whose immediate is a target label.
-//! \param m x86 mnemonic
-//! \param size operand size
-//! \param imm When scheduling is disabled, this is the actual immediate.
-//! When scheduling is enabled, this is 0 because immediate has not been
-//! generated yet.
-//! \param label name of label for which we need to generate immediate for
-//! using the label address.
-//! \param isLocal Used to hint the distance from this instruction to label.
-//! When this is true, it means that 8 bits should be enough.
-inline LowOpLabel* lower_label(Mnemonic m, OpndSize size, int imm,
-        const char* label, bool isLocal) {
-    if (!gDvmJit.scheduling) {
-        stream = encoder_imm(m, size, imm, stream);
-        return NULL;
-    }
-    LowOpLabel * op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpLabel>();
-    op->opCode = m;
-    op->opCode2 = ATOM_NORMAL;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Label;
-    op->numOperands = 1;
-    snprintf(op->labelOpnd.label, LABEL_SIZE, "%s", label);
-    op->labelOpnd.isLocal = isLocal;
-    singletonPtr<Scheduler>()->updateUseDefInformation_imm(op);
-    return op;
-}
-
-//! \brief Interface to encoder.
-LowOpLabel* dump_label(Mnemonic m, OpndSize size, int imm, const char* label,
-        bool isLocal) {
-    return lower_label(m, size, imm, label, isLocal);
-}
-
-//! Used for dumping an instruction with a single immediate to the code stream
-//! but the immediate is not yet known because the target MIR block still needs
-//! code generated for it. This is only valid when scheduling is on.
-//! \pre Instruction scheduling must be enabled
-//! \param m x86 mnemonic
-//! \param targetBlockId id of the MIR block
-//! \param immediateNeedsAligned if the immediate in the instruction need to be aligned within 16B
-LowOpBlock* dump_blockid_imm(Mnemonic m, int targetBlockId,
-        bool immediateNeedsAligned) {
-    assert(gDvmJit.scheduling && "Scheduling must be turned on before "
-                "calling dump_blockid_imm");
-    LowOpBlock* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpBlock>();
-    op->opCode = m;
-    op->opCode2 = ATOM_NORMAL;
-    op->opndSrc.type = LowOpndType_BlockId;
-    op->numOperands = 1;
-    op->blockIdOpnd.value = targetBlockId;
-    op->blockIdOpnd.immediateNeedsAligned = immediateNeedsAligned;
-    singletonPtr<Scheduler>()->updateUseDefInformation_imm(op);
-    return op;
-}
-
-//! \brief Thin layer over encoder that makes scheduling decision and
-//! is used for dumping instruction with a known immediate.
-//! \param m x86 mnemonic
-//! \param size operand size
-//! \param imm immediate
-LowOpImm* lower_imm(Mnemonic m, OpndSize size, int imm) {
-    if (!gDvmJit.scheduling) {
-        stream = encoder_imm(m, size, imm, stream);
-        return NULL;
-    }
-    LowOpImm* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpImm>();
-    op->opCode = m;
-    op->opCode2 = ATOM_NORMAL;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Imm;
-    op->numOperands = 1;
-    op->immOpnd.value = imm;
-    singletonPtr<Scheduler>()->updateUseDefInformation_imm(op);
-    return op;
-}
-
-//! \brief Interface to encoder.
-LowOpImm* dump_imm(Mnemonic m, OpndSize size, int imm) {
-    return lower_imm(m, size, imm);
-}
-
-//! \brief Used to update the immediate of an instruction already in the
-//! code stream.
-//! \warning This assumes that the instruction to update is already in the
-//! code stream. If it is not, the VM will abort.
-//! \param imm new immediate to use
-//! \param codePtr pointer to location in code stream where the instruction
-//! whose immediate needs updated
-//! \param updateSecondOperand This is true when second operand needs updated
-void dump_imm_update(int imm, char* codePtr, bool updateSecondOperand) {
-    // These encoder call do not need to go through scheduler since they need
-    // to be dumped at a specific location in code stream.
-    if(updateSecondOperand)
-        encoder_update_imm_rm(imm, codePtr);
-    else // update first operand
-        encoder_update_imm(imm, codePtr);
-}
-
-//! \brief Thin layer over encoder that makes scheduling decision and
-//! is used for dumping instruction with a single memory operand.
-//! \param m x86 mnemonic
-//! \param m2 Atom pseudo-mnemonic
-//! \param size operand size
-//! \param disp displacement offset
-//! \param base_reg physical register (PhysicalReg type) or a logical register
-//! \param isBasePhysical notes if base_reg is a physical register. It must
-//! be true when scheduling is enabled or else VM will abort.
-LowOpMem* lower_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
-        int base_reg, bool isBasePhysical) {
-    if (!gDvmJit.scheduling) {
-        stream = encoder_mem(m, size, disp, base_reg, isBasePhysical, stream);
-        return NULL;
-    }
-
-    if (!isBasePhysical) {
-        ALOGI("JIT_INFO: Base register not physical in lower_mem");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-    LowOpMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpMem>();
-
-    op->opCode = m;
-    op->opCode2 = m2;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Mem;
-    op->numOperands = 1;
-    op->memOpnd.mType = MemoryAccess_Unknown;
-    op->memOpnd.index = -1;
-    set_mem_opnd(&(op->memOpnd), disp, base_reg, isBasePhysical);
-    singletonPtr<Scheduler>()->updateUseDefInformation_mem(op);
-    return op;
-}
-
-//! \brief Interface to encoder which includes register allocation
-//! decision.
-//! \details With NCG O1, call freeReg to free up physical registers,
-//! then call registerAlloc to allocate a physical register for memory base
-LowOpMem* dump_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
-        int base_reg, bool isBasePhysical) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(false);
-        //type of the base is gpr
-        int regAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true, true);
-        return lower_mem(m, m2, size, disp, regAll, true /*isBasePhysical*/);
-    } else {
-        return lower_mem(m, m2, size, disp, base_reg, isBasePhysical);
-    }
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes a single reg operand
-LowOpReg* lower_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
-        LowOpndRegType type, bool isPhysical) {
-    if (!gDvmJit.scheduling) {
-        stream = encoder_reg(m, size, reg, isPhysical, type, stream);
-        return NULL;
-    }
-
-    if (!isPhysical) {
-        ALOGI("JIT_INFO: Register not physical at lower_reg");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-    LowOpReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpReg>();
-
-    op->opCode = m;
-    op->opCode2 = m2;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Reg;
-    op->numOperands = 1;
-    set_reg_opnd(&(op->regOpnd), reg, isPhysical, type);
-    singletonPtr<Scheduler>()->updateUseDefInformation_reg(op);
-    return op;
-}
-
-//!With NCG O1, wecall freeReg to free up physical registers, then call registerAlloc to allocate a physical register for the single operand
-LowOpReg* dump_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
-        bool isPhysical, LowOpndRegType type) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(false);
-        if (m == Mnemonic_MUL || m == Mnemonic_IMUL || m == Mnemonic_DIV
-                || m == Mnemonic_IDIV) {
-            //these four instructions use eax & edx implicitly
-            touchEax();
-            touchEdx();
-        }
-        int regAll = registerAlloc(type, reg, isPhysical, true);
-        return lower_reg(m, m2, size, regAll, type, true /*isPhysical*/);
-    } else {
-        return lower_reg(m, m2, size, reg, type, isPhysical);
-    }
-}
-
-LowOpReg* dump_reg_noalloc(Mnemonic m, OpndSize size, int reg, bool isPhysical,
-        LowOpndRegType type) {
-    return lower_reg(m, ATOM_NORMAL, size, reg, type, true /*isPhysical*/);
-}
-
-//! \brief Update fields of LowOp to generate an instruction with
-//! two register operands
-//!
-//! \details For MOVZX and MOVSX, allows source and destination
-//! operand sizes to be different, and fixes type to general purpose.
-//! \param m x86 mnemonic
-//! \param m2 Atom pseudo-mnemonic
-//! \param size operand size
-//! \param regSrc source register
-//! \param isPhysical if regSrc is a physical register
-//! \param regDest destination register
-//! \param isPhysical2 if regDest is a physical register
-//! \param type the register type. For MOVSX and MOVZX, type is fixed
-//! as general purpose
-//! \return a LowOp corresponding to the reg-reg operation
-LowOpRegReg* lower_reg_to_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int regSrc,
-        bool isPhysical, int regDest, bool isPhysical2, LowOpndRegType type) {
-
-    OpndSize srcSize = size;
-    OpndSize destSize = size;
-    LowOpndRegType srcType = type;
-    LowOpndRegType destType = type;
-
-    //We may need to override the default size and type if src and dest can be
-    //of different size / type, as follows:
-
-    //For MOVSX and MOVZX, fix the destination size and type to 32-bit and GP
-    //respectively. Note that this is a rigid requirement, and for now won't
-    //allow, for example, MOVSX Sz8, Sz16
-    if (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX) {
-        destSize = OpndSize_32;
-    }
-    //For CVTSI2SD or CVTSI2SS, the source needs to be fixed at 32-bit GP
-    else if (m == Mnemonic_CVTSI2SD || m == Mnemonic_CVTSI2SS) {
-        srcSize = OpndSize_32;
-        srcType = LowOpndRegType_gp;
-    }
-
-    if (!gDvmJit.scheduling) {
-        if (m == Mnemonic_FUCOMP || m == Mnemonic_FUCOM) {
-            stream = encoder_compare_fp_stack(m == Mnemonic_FUCOMP, regSrc - regDest,
-                    size == OpndSize_64, stream);
-        } else {
-            stream = encoder_reg_reg_diff_sizes(m, srcSize, regSrc, isPhysical, destSize,
-                    regDest, isPhysical2, destType, stream);
-        }
-        return NULL;
-    }
-
-    if (!isPhysical && !isPhysical2) {
-        ALOGI("JIT_INFO: Registers not physical at lower_reg_to_reg");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-
-    LowOpRegReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpRegReg>();
-
-    op->opCode = m;
-    op->opCode2 = m2;
-    op->opndDest.size = destSize;
-    op->opndDest.type = LowOpndType_Reg;
-    op->opndSrc.size = srcSize;
-    op->opndSrc.type = LowOpndType_Reg;
-    op->numOperands = 2;
-    set_reg_opnd(&(op->regDest), regDest, isPhysical2, destType);
-    set_reg_opnd(&(op->regSrc), regSrc, isPhysical, srcType);
-    singletonPtr<Scheduler>()->updateUseDefInformation_reg_to_reg(op);
-
-    return op;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes two reg operands
-
-//!Here, both registers are physical
-LowOpRegReg* dump_reg_reg_noalloc(Mnemonic m, OpndSize size, int reg,
-        bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
-    return lower_reg_to_reg(m, ATOM_NORMAL, size, reg, true /*isPhysical*/, reg2,
-            true /*isPhysical2*/, type);
-}
-
-//! \brief Check if we have a MOV instruction which can be redundant
-//!
-//! \details Checks if the Mnemonic is a MOV which can possibly be
-//! optimized. For example, MOVSX %ax, %eax cannot be optimized, while
-//! MOV %eax, %eax is a NOP, and can be treated as such.
-//! \param m Mnemonic to check for
-//! \return whether the move can possibly be optimized away
-inline bool isMoveOptimizable(Mnemonic m) {
-    return (m == Mnemonic_MOV || m == Mnemonic_MOVQ || m == Mnemonic_MOVSS
-            || m == Mnemonic_MOVSD);
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes two reg operands
-
-//!here dst reg is already allocated to a physical reg
-//! we should not spill the physical register for dst when allocating for src
-LowOpRegReg* dump_reg_reg_noalloc_dst(Mnemonic m, OpndSize size, int reg,
-        bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        //TODO should mark reg2 as written
-        int regAll = registerAlloc(type, reg, isPhysical, true);
-        /* remove move from one register to the same register */
-        if (isMoveOptimizable(m) && regAll == reg2)
-            return NULL;
-        return lower_reg_to_reg(m, ATOM_NORMAL, size, regAll, true /*isPhysical*/,
-                reg2, true /*isPhysical2*/, type);
-    } else {
-        return lower_reg_to_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg2,
-                isPhysical2, type);
-    }
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes two reg operands
-
-//!here src reg is already allocated to a physical reg
-LowOpRegReg* dump_reg_reg_noalloc_src(Mnemonic m, AtomOpCode m2, OpndSize size,
-        int reg, bool isPhysical, int reg2, bool isPhysical2,
-        LowOpndRegType type) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        int regAll2;
-        if(isMoveOptimizable(m) && checkTempReg2(reg2, type, isPhysical2, reg, -1)) { //dst reg is logical
-            //only from get_virtual_reg_all
-            regAll2 = registerAllocMove(reg2, type, isPhysical2, reg, true);
-        } else {
-            regAll2 = registerAlloc(type, reg2, isPhysical2, true, true);
-            return lower_reg_to_reg(m, m2, size, reg, true /*isPhysical*/, regAll2,
-                    true /*isPhysical2*/, type);
-        }
-    } else {
-        return lower_reg_to_reg(m, m2, size, reg, isPhysical, reg2, isPhysical2,
-                type);
-    }
-    return NULL;
-}
-
-//! \brief Wrapper around lower_reg_to_reg with reg allocation
-//! \details Allocates both registers, checks for optimizations etc,
-//! and calls lower_reg_to_reg
-//! \param m The mnemonic
-//! \param m2 The ATOM mnemonic type
-//! \param srcSize Size of the source operand
-//! \param srcReg The source register itself
-//! \param isSrcPhysical Whether source is physical
-//! \param srcType The type of source register
-//! \param destSize Size of the destination operand
-//! \param destReg The destination register itself
-//! \param isDestPhysical Whether destination is physical
-//! \param destType The type of destination register
-//! \return The generated LowOp
-LowOpRegReg* dump_reg_reg_diff_types(Mnemonic m, AtomOpCode m2, OpndSize srcSize,
-        int srcReg, int isSrcPhysical, LowOpndRegType srcType, OpndSize destSize,
-        int destReg, int isDestPhysical, LowOpndRegType destType) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        startNativeCode(-1, -1);
-        //reg is source if m is MOV
-        freeReg(false);
-        int regAll = registerAlloc(srcType, srcReg, isSrcPhysical, true);
-        int regAll2;
-        LowOpRegReg* op = NULL;
-#ifdef MOVE_OPT2
-        if(isMoveOptimizable(m) &&
-                ((reg != PhysicalReg_EDI && srcReg != PhysicalReg_ESP && srcReg != PhysicalReg_EBP) || (!isSrcPhysical)) &&
-                isDestPhysical == false) { //dst reg is logical
-            //called from move_reg_to_reg
-            regAll2 = registerAllocMove(regDest, destType, isDestPhysical, regAll, true);
-        } else {
-#endif
-        //Do not spill regAll
-        gCompilationUnit->setCanSpillRegister (regAll, false);
-
-        regAll2 = registerAlloc(destType, destReg, isDestPhysical, true, true);
-
-        // NOTE: The use of (destSize, destType) as THE (size, type) can be confusing. In most
-        // cases, we are using this function through dump_reg_reg, so the (size, type) doesn't
-        // matter. For MOVSX and MOVZX, the size passed to dump_reg_reg is the srcSize (8 or 16),
-        // so destSize is technically the srcSize, (type is gpr) and we override destSize inside
-        // lower_reg_to_reg to 32. For CVTSI2SS and CVTSI2SD, the destSize is 64-bit, and we
-        // override the srcSize inside lower_reg_to_reg.
-        op = lower_reg_to_reg(m, m2, destSize, regAll, true /*isPhysical*/, regAll2,
-                true /*isPhysical2*/, destType);
-#ifdef MOVE_OPT2
-    }
-#endif
-        endNativeCode();
-        return op;
-    } else {
-        return lower_reg_to_reg(m, m2, destSize, srcReg, isSrcPhysical, destReg, isDestPhysical,
-                destType);
-    }
-    return NULL;
-}
-
-//! \brief Wrapper around dump_reg_reg_diff_types assuming sizes and types are same
-//! \param m The mnemonic
-//! \param m2 The ATOM mnemonic type
-//! \param size Size of the source and destination operands
-//! \param reg The source register
-//! \param isPhysical Whether source is physical
-//! \param reg2 The destination register
-//! \param isPhysical2 Whether destination is physical
-//! \param type The type of operation
-//! \return The generated LowOp
-LowOpRegReg* dump_reg_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
-        bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
-    return dump_reg_reg_diff_types(m, m2, size, reg, isPhysical, type, size,
-            reg2, isPhysical2, type);
-}
-
-LowOpMemReg* lower_mem_to_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
-        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
-        int reg, bool isPhysical, LowOpndRegType type, struct ConstInfo** listPtr) {
-    bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
-    OpndSize overridden_size = isMovzs ? OpndSize_32 : size;
-    LowOpndRegType overridden_type = isMovzs ? LowOpndRegType_gp : type;
-    if (!gDvmJit.scheduling) {
-        stream = encoder_mem_to_reg_diff_sizes(m, size, disp, base_reg, isBasePhysical,
-                overridden_size, reg, isPhysical, overridden_type, stream);
-        return NULL;
-    }
-
-    if (!isBasePhysical && !isPhysical) {
-        ALOGI("JIT_INFO: Base register or operand register not physical in lower_mem_to_reg");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-
-    LowOpMemReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpMemReg>();
-    if (listPtr != NULL) {
-        op->constLink = *listPtr;
-    } else {
-        op->constLink = NULL;
-    }
-
-    op->opCode = m;
-    op->opCode2 = m2;
-    op->opndDest.size = overridden_size;
-    op->opndDest.type = LowOpndType_Reg;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Mem;
-    op->numOperands = 2;
-    set_reg_opnd(&(op->regDest), reg, isPhysical, overridden_type);
-    set_mem_opnd(&(op->memSrc), disp, base_reg, isBasePhysical);
-    op->memSrc.mType = mType;
-    op->memSrc.index = mIndex;
-    singletonPtr<Scheduler>()->updateUseDefInformation_mem_to_reg(op);
-    return op;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
-
-//!Here, operands are already allocated to physical registers
-LowOpMemReg* dump_mem_reg_noalloc(Mnemonic m, OpndSize size, int disp,
-        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
-        int reg, bool isPhysical, LowOpndRegType type) {
-    return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, base_reg,
-            true /*isBasePhysical*/, mType, mIndex, reg, true /*isPhysical*/,
-            type, NULL);
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
-
-//!Here, memory operand is already allocated to physical register
-LowOpMemReg* dump_mem_reg_noalloc_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
-        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
-        int mIndex, int reg, bool isPhysical, LowOpndRegType type) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        int regAll = registerAlloc(type, reg, isPhysical, true, true);
-        return lower_mem_to_reg(m, m2, size, disp, base_reg,
-                true /*isBasePhysical*/, mType, mIndex, regAll,
-                true /*isPhysical*/, type, NULL);
-    } else {
-        return lower_mem_to_reg(m, m2, size, disp, base_reg, isBasePhysical, mType,
-                mIndex, reg, isPhysical, type, NULL);
-    }
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
-
-//!
-LowOpMemReg* dump_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
-        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
-        int reg, bool isPhysical, LowOpndRegType type, struct ConstInfo** listPtr) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        startNativeCode(-1, -1);
-        freeReg(false);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true);
-        //it is okay to use the same physical register
-        if (isMoveOptimizable(m)) {
-            freeReg(false);
-        } else {
-            //Do not spill baseAll
-            gCompilationUnit->setCanSpillRegister (baseAll, false);
-        }
-        int regAll = registerAlloc(type, reg, isPhysical, true, true);
-        endNativeCode();
-        return lower_mem_to_reg(m, m2, size, disp, baseAll,
-                true /*isBasePhysical*/, mType, mIndex, regAll,
-                true /*isPhysical*/, type, listPtr);
-    } else {
-        return lower_mem_to_reg(m, m2, size, disp, base_reg, isBasePhysical, mType,
-                mIndex, reg, isPhysical, type, NULL);
-    }
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
-
-//!
-LowOpMemReg* dump_moves_mem_reg(Mnemonic m, OpndSize size,
-                         int disp, int base_reg, bool isBasePhysical,
-             int reg, bool isPhysical) {
-#if 0 /* Commented out because it is dead code. If re-enabling, this needs to be updated
-         to work with instruction scheduling and cannot call encoder directly. Please see
-         dump_movez_mem_reg for an example */
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        startNativeCode(-1, -1);
-        freeReg(true);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
-
-        //Do not spill baseAll
-        gCompilationUnit->setCanSpillRegister (baseAll, false);
-
-        int regAll = registerAlloc(LowOpndRegType_gp, reg, isPhysical, true);
-        endNativeCode();
-        return lower_mem_reg(m, ATOM_NORMAL, size, disp, baseAll, MemoryAccess_Unknown, -1,
-            regAll, LowOpndRegType_gp, true/*moves*/);
-    } else {
-        stream = encoder_moves_mem_to_reg(size, disp, base_reg, isBasePhysical, reg, isPhysical, stream);
-    }
-#endif
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
-
-//!
-LowOpMemReg* dump_movez_mem_reg(Mnemonic m, OpndSize size, int disp,
-        int base_reg, bool isBasePhysical, int reg, bool isPhysical) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        startNativeCode(-1, -1);
-        freeReg(false);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true);
-
-        //Do not spill baseAll
-        gCompilationUnit->setCanSpillRegister (baseAll, false);
-
-        int regAll = registerAlloc(LowOpndRegType_gp, reg, isPhysical, true, true);
-        endNativeCode();
-        return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, baseAll,
-                true /*isBasePhysical*/, MemoryAccess_Unknown, -1, regAll,
-                true /*isPhysical*/, LowOpndRegType_gp, NULL);
-    } else {
-        return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, base_reg,
-                isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical,
-                LowOpndRegType_gp, NULL);
-    }
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one reg operand
-
-//!
-LowOpRegReg* dump_movez_reg_reg(Mnemonic m, OpndSize size,
-             int reg, bool isPhysical,
-             int reg2, bool isPhysical2) {
-#if 0 /* Commented out because it is dead code. If re-enabling, this needs to be updated
-         to work with instruction scheduling and cannot call encoder directly. Please see
-         dump_movez_mem_reg for an example */
-    LowOpRegReg* op = (LowOpRegReg*)atomNew(sizeof(LowOpRegReg));
-    op->lop.opCode = m;
-    op->lop.opnd1.size = OpndSize_32;
-    op->lop.opnd1.type = LowOpndType_Reg;
-    op->lop.opnd2.size = size;
-    op->lop.opnd2.type = LowOpndType_Reg;
-    set_reg_opnd(&(op->regOpnd1), reg2, isPhysical2, LowOpndRegType_gp);
-    set_reg_opnd(&(op->regOpnd2), reg, isPhysical, LowOpndRegType_gp);
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        startNativeCode(-1, -1);
-        //reg is source if m is MOV
-        freeReg(true);
-        int regAll = registerAlloc(LowOpndRegType_gp, reg, isPhysical, true);
-
-        //Do not spill regAll
-        gCompilationUnit->setCanSpillRegister (regAll, false);
-
-        int regAll2 = registerAlloc(LowOpndRegType_gp, reg2, isPhysical2, true);
-        stream = encoder_movez_reg_to_reg(size, regAll, true, regAll2, true,
-                                          LowOpndRegType_gp, stream);
-        endNativeCode();
-    }
-    else {
-        stream = encoder_movez_reg_to_reg(size, reg, isPhysical, reg2,
-                                        isPhysical2, LowOpndRegType_gp, stream);
-    }
-#endif
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
-
-//!
-LowOpMemReg* lower_mem_scale_to_reg(Mnemonic m, OpndSize size, int base_reg,
-        bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical,
-        int scale, int reg, bool isPhysical, LowOpndRegType type) {
-    bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
-    OpndSize overridden_size = isMovzs ? OpndSize_32 : size;
-    LowOpndRegType overridden_type = isMovzs ? LowOpndRegType_gp : type;
-    if (!gDvmJit.scheduling) {
-        stream = encoder_mem_disp_scale_to_reg_diff_sizes(m, size, base_reg, isBasePhysical,
-                disp, index_reg, isIndexPhysical, scale, overridden_size, reg,
-                isPhysical, overridden_type, stream);
-        return NULL;
-    }
-
-    if (!isBasePhysical && !isIndexPhysical && !isPhysical) {
-        ALOGI("JIT_INFO: Base, index or operand register not physical at lower_mem_scale_to_reg");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-    LowOpMemReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpMemReg>();
-
-    op->opCode = m;
-    op->opCode2 = ATOM_NORMAL;
-    op->opndDest.size = overridden_size;
-    op->opndDest.type = LowOpndType_Reg;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Mem;
-    op->numOperands = 2;
-    op->memSrc.mType = MemoryAccess_Unknown;
-    op->memSrc.index = -1;
-    set_reg_opnd(&(op->regDest), reg, isPhysical, overridden_type);
-    set_mem_opnd_scale(&(op->memSrc), base_reg, isBasePhysical, disp,
-            index_reg, isIndexPhysical, scale);
-    singletonPtr<Scheduler>()->updateUseDefInformation_mem_to_reg(op);
-    return op;
-}
-
-LowOpMemReg* dump_mem_scale_reg(Mnemonic m, OpndSize size, int base_reg,
-        bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical,
-        int scale, int reg, bool isPhysical, LowOpndRegType type) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        startNativeCode(-1, -1);
-        freeReg(false);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true);
-
-        //Do not spill baseAll
-        gCompilationUnit->setCanSpillRegister (baseAll, false);
-
-        int indexAll = registerAlloc(LowOpndRegType_gp, index_reg,
-                isIndexPhysical, true);
-        if (isMoveOptimizable(m)) {
-            freeReg(false);
-
-            //We can now spill base
-            gCompilationUnit->setCanSpillRegister (baseAll, true);
-        } else {
-            //Do not spill indexAll
-            gCompilationUnit->setCanSpillRegister (indexAll, false);
-        }
-        bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
-        int regAll = registerAlloc(isMovzs ? LowOpndRegType_gp : type, reg,
-                isPhysical, true, true);
-        endNativeCode();
-        return lower_mem_scale_to_reg(m, size, baseAll, true /*isBasePhysical*/,
-                disp, indexAll, true /*isIndexPhysical*/, scale, regAll,
-                true /*isPhysical*/, type);
-    } else {
-        return lower_mem_scale_to_reg(m, size, base_reg, isBasePhysical, disp,
-                index_reg, isIndexPhysical, scale, reg, isPhysical, type);
-    }
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
-
-//!
-LowOpRegMem* lower_reg_to_mem_scale(Mnemonic m, OpndSize size, int reg,
-        bool isPhysical, int base_reg, bool isBasePhysical, int disp,
-        int index_reg, bool isIndexPhysical, int scale, LowOpndRegType type) {
-    if (!gDvmJit.scheduling) {
-        stream = encoder_reg_mem_disp_scale(m, size, reg, isPhysical, base_reg,
-                isBasePhysical, disp, index_reg, isIndexPhysical, scale, type,
-                stream);
-        return NULL;
-    }
-
-    if (!isBasePhysical && !isIndexPhysical && !isPhysical) {
-        ALOGI("JIT_INFO: Base, index or operand register not physical in lower_reg_to_mem_scale");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-    LowOpRegMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpRegMem>();
-
-    op->opCode = m;
-    op->opCode2 = ATOM_NORMAL;
-    op->opndDest.size = size;
-    op->opndDest.type = LowOpndType_Mem;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Reg;
-    op->numOperands = 2;
-    op->memDest.mType = MemoryAccess_Unknown;
-    op->memDest.index = -1;
-    set_reg_opnd(&(op->regSrc), reg, isPhysical, type);
-    set_mem_opnd_scale(&(op->memDest), base_reg, isBasePhysical, disp,
-            index_reg, isIndexPhysical, scale);
-    singletonPtr<Scheduler>()->updateUseDefInformation_reg_to_mem(op);
-    return op;
-}
-
-LowOpRegMem* dump_reg_mem_scale(Mnemonic m, OpndSize size, int reg,
-        bool isPhysical, int base_reg, bool isBasePhysical, int disp,
-        int index_reg, bool isIndexPhysical, int scale, LowOpndRegType type) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        startNativeCode(-1, -1);
-        freeReg(false);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true);
-
-        //Do not spill baseAll
-        gCompilationUnit->setCanSpillRegister (baseAll, false);
-
-        int indexAll = registerAlloc(LowOpndRegType_gp, index_reg,
-                isIndexPhysical, true);
-
-        //Do not spill indexAll
-        gCompilationUnit->setCanSpillRegister (indexAll, false);
-
-        int regAll = registerAlloc(type, reg, isPhysical, true, true);
-        endNativeCode();
-        return lower_reg_to_mem_scale(m, size, regAll, true /*isPhysical*/,
-                baseAll, true /*isBasePhysical*/, disp, indexAll,
-                true /*isIndexPhysical*/, scale, type);
-    } else {
-        return lower_reg_to_mem_scale(m, size, reg, isPhysical, base_reg,
-                isBasePhysical, disp, index_reg, isIndexPhysical, scale, type);
-    }
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
-
-//!Here operands are already allocated
-LowOpRegMem* lower_reg_to_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
-        bool isPhysical, int disp, int base_reg, bool isBasePhysical,
-        MemoryAccessType mType, int mIndex, LowOpndRegType type) {
-    if (!gDvmJit.scheduling) {
-        stream = encoder_reg_mem(m, size, reg, isPhysical, disp, base_reg,
-                isBasePhysical, type, stream);
-        return NULL;
-    }
-
-    if (!isBasePhysical && !isPhysical) {
-        ALOGI("JIT_INFO: Base or operand register not physical in lower_reg_to_mem");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-    LowOpRegMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpRegMem>();
-
-    op->opCode = m;
-    op->opCode2 = m2;
-    op->opndDest.size = size;
-    op->opndDest.type = LowOpndType_Mem;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Reg;
-    op->numOperands = 2;
-    set_reg_opnd(&(op->regSrc), reg, isPhysical, type);
-    set_mem_opnd(&(op->memDest), disp, base_reg, isBasePhysical);
-    op->memDest.mType = mType;
-    op->memDest.index = mIndex;
-    singletonPtr<Scheduler>()->updateUseDefInformation_reg_to_mem(op);
-    return op;
-}
-
-LowOpRegMem* dump_reg_mem_noalloc(Mnemonic m, OpndSize size, int reg,
-        bool isPhysical, int disp, int base_reg, bool isBasePhysical,
-        MemoryAccessType mType, int mIndex, LowOpndRegType type) {
-    return lower_reg_to_mem(m, ATOM_NORMAL, size, reg, true /*isPhysical*/, disp,
-            base_reg, true /*isBasePhysical*/, mType, mIndex, type);
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
-
-//!
-LowOpRegMem* dump_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
-        bool isPhysical, int disp, int base_reg, bool isBasePhysical,
-        MemoryAccessType mType, int mIndex, LowOpndRegType type) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        startNativeCode(-1, -1);
-        freeReg(false);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true);
-
-        //Do not spill baseAll
-        gCompilationUnit->setCanSpillRegister (baseAll, false);
-
-        int regAll = registerAlloc(type, reg, isPhysical, true);
-        endNativeCode();
-        return lower_reg_to_mem(m, m2, size, regAll, true /*isPhysical*/, disp,
-                baseAll, true /*isBasePhysical*/, mType, mIndex, type);
-    } else {
-        return lower_reg_to_mem(m, m2, size, reg, isPhysical, disp, base_reg,
-                isBasePhysical, mType, mIndex, type);
-    }
-    return NULL;
-}
-
-//! \brief Checks if Mnemonic sign extends imm operand
-//! \details Information taken from Atom instruction manual
-//! \param mn Mnemonic to check for
-//! \return whether mn sign extends its imm operand
-bool mnemonicSignExtendsImm(Mnemonic mn) {
-    if ((mn == Mnemonic_ADD) || (mn == Mnemonic_ADC)
-            || (mn == Mnemonic_SUB) || (mn == Mnemonic_SBB)) {
-        return true;
-    }
-    return false;
-}
-
-//! \brief Returns minimum size to fit an imm
-//! \param imm The immediate value to check for
-//! \return the OpndSize befitting the imm
-OpndSize minSizeForImm(int imm) {
-    //Don't care about signed values
-    if (imm < 0)
-        return OpndSize_32;
-    if (imm < 128)
-        return OpndSize_8;
-    if (imm < 32768)
-        return OpndSize_16;
-
-    return OpndSize_32;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one immediate and one reg operand
-
-//!The reg operand is allocated already
-LowOpImmReg* lower_imm_to_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
-        int reg, bool isPhysical, LowOpndRegType type, bool chaining) {
-    // size of opnd1 can be different from size of opnd2:
-    OpndSize overridden_size = size;
-    if (m == Mnemonic_SAL || m == Mnemonic_SHR || m == Mnemonic_SHL
-                    || m == Mnemonic_SAR || m == Mnemonic_ROR) {
-                    overridden_size = OpndSize_8;
-    }
-    else if (mnemonicSignExtendsImm(m)) {
-        overridden_size = minSizeForImm(imm);
-    }
-    if (!gDvmJit.scheduling) {
-        stream = encoder_imm_reg_diff_sizes(m, overridden_size, imm, size, reg, isPhysical, type, stream);
-        return NULL;
-    }
-
-    if (!isPhysical) {
-        ALOGI("JIT_INFO: Operand register not physical in lower_imm_to_reg");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-    LowOpImmReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpImmReg>();
-
-    op->opCode = m;
-    op->opCode2 = m2;
-    op->opndDest.size = size;
-    op->opndDest.type = LowOpndType_Reg;
-    op->numOperands = 2;
-    op->opndSrc.size = overridden_size;
-    op->opndSrc.type = chaining ? LowOpndType_Chain : LowOpndType_Imm;
-    set_reg_opnd(&(op->regDest), reg, isPhysical, type);
-    op->immSrc.value = imm;
-    singletonPtr<Scheduler>()->updateUseDefInformation_imm_to_reg(op);
-    return op;
-}
-
-LowOpImmReg* dump_imm_reg_noalloc(Mnemonic m, OpndSize size, int imm, int reg,
-        bool isPhysical, LowOpndRegType type) {
-    return lower_imm_to_reg(m, ATOM_NORMAL, size, imm, reg, true /*isPhysical*/,
-            type, false);
-}
-
-LowOpImmReg* dump_imm_reg_noalloc_alu(Mnemonic m, OpndSize size, int imm, int reg,
-        bool isPhysical, LowOpndRegType type) {
-    return lower_imm_to_reg(m, ATOM_NORMAL_ALU, size, imm, reg, true /*isPhysical*/,
-            type, false);
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one immediate and one reg operand
-
-//!
-LowOpImmReg* dump_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
-        int reg, bool isPhysical, LowOpndRegType type, bool chaining) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(false);
-        int regAll = registerAlloc(type, reg, isPhysical, true, true);
-        return lower_imm_to_reg(m, m2, size, imm, regAll, true /*isPhysical*/,
-                type, chaining);
-    } else {
-        return lower_imm_to_reg(m, m2, size, imm, reg, isPhysical, type, chaining);
-    }
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one immediate and one mem operand
-
-//!The mem operand is already allocated
-LowOpImmMem* lower_imm_to_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
-        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
-        int mIndex, bool chaining) {
-    if (!gDvmJit.scheduling) {
-        stream = encoder_imm_mem(m, size, imm, disp, base_reg, isBasePhysical,
-                stream);
-        return NULL;
-    }
-
-    if (!isBasePhysical) {
-        ALOGI("JIT_INFO: Base register not physical in lower_imm_to_mem");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-    LowOpImmMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpImmMem>();
-
-    op->opCode = m;
-    op->opCode2 = m2;
-    op->opndDest.size = size;
-    op->opndDest.type = LowOpndType_Mem;
-    op->opndSrc.size = size;
-    op->opndSrc.type = chaining ? LowOpndType_Chain : LowOpndType_Imm;
-    op->numOperands = 2;
-    set_mem_opnd(&(op->memDest), disp, base_reg, isBasePhysical);
-    op->immSrc.value = imm;
-    op->memDest.mType = mType;
-    op->memDest.index = mIndex;
-    singletonPtr<Scheduler>()->updateUseDefInformation_imm_to_mem(op);
-    return op;
-}
-
-LowOpImmMem* dump_imm_mem_noalloc(Mnemonic m, OpndSize size, int imm, int disp,
-        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex) {
-    return lower_imm_to_mem(m, ATOM_NORMAL, size, imm, disp, base_reg,
-            true /*isBasePhysical*/, mType, mIndex, false);
-}
-
-LowOpImmMem* dump_imm_mem_noalloc_alu(Mnemonic m, OpndSize size, int imm, int disp,
-        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex) {
-    return lower_imm_to_mem(m, ATOM_NORMAL_ALU, size, imm, disp, base_reg,
-            true /*isBasePhysical*/, mType, mIndex, false);
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes one immediate and one mem operand
-
-//!
-LowOpImmMem* dump_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
-        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
-        int mIndex, bool chaining) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        /* do not free register if the base is %edi, %esp, or %ebp
-         make sure dump_imm_mem will only generate a single instruction */
-        if (!isBasePhysical
-                || (base_reg != PhysicalReg_EDI && base_reg != PhysicalReg_ESP
-                        && base_reg != PhysicalReg_EBP)) {
-            freeReg(false);
-        }
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true);
-        return lower_imm_to_mem(m, m2, size, imm, disp, baseAll,
-                true /*isBasePhysical*/, mType, mIndex, chaining);
-    } else {
-        return lower_imm_to_mem(m, m2, size, imm, disp, base_reg, isBasePhysical,
-                mType, mIndex, chaining);
-    }
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that uses the FP stack and takes one mem operand
-
-//!
-LowOpRegMem* lower_fp_to_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
-        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
-        int mIndex) {
-    if (!gDvmJit.scheduling) {
-        stream = encoder_fp_mem(m, size, reg, disp, base_reg, isBasePhysical,
-                stream);
-        return NULL;
-    }
-
-    if (!isBasePhysical) {
-        ALOGI("JIT_INFO: Base register not physical in lower_fp_to_mem");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-    LowOpRegMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpRegMem>();
-
-    op->opCode = m;
-    op->opCode2 = m2;
-    op->opndDest.size = size;
-    op->opndDest.type = LowOpndType_Mem;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Reg;
-    op->numOperands = 2;
-    set_reg_opnd(&(op->regSrc), PhysicalReg_ST0 + reg, true,
-            LowOpndRegType_fs);
-    set_mem_opnd(&(op->memDest), disp, base_reg, isBasePhysical);
-    op->memDest.mType = mType;
-    op->memDest.index = mIndex;
-    singletonPtr<Scheduler>()->updateUseDefInformation_fp_to_mem(op);
-    return op;
-}
-
-LowOpRegMem* dump_fp_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
-        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
-        int mIndex) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(false);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true);
-        return lower_fp_to_mem(m, m2, size, reg, disp, baseAll,
-                true /*isBasePhysical*/, mType, mIndex);
-    } else {
-        return lower_fp_to_mem(m, m2, size, reg, disp, base_reg, isBasePhysical,
-                mType, mIndex);
-    }
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that uses the FP stack and takes one mem operand
-
-//!
-LowOpMemReg* lower_mem_to_fp(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
-        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
-        int reg) {
-    if (!gDvmJit.scheduling) {
-        stream = encoder_mem_fp(m, size, disp, base_reg, isBasePhysical, reg,
-                stream);
-        return NULL;
-    }
-
-    if (!isBasePhysical) {
-        ALOGI("JIT_INFO: Base register not physical in lower_mem_to_fp");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return NULL;
-    }
-
-    LowOpMemReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpMemReg>();
-
-    op->opCode = m;
-    op->opCode2 = m2;
-    op->opndDest.size = size;
-    op->opndDest.type = LowOpndType_Reg;
-    op->opndSrc.size = size;
-    op->opndSrc.type = LowOpndType_Mem;
-    op->numOperands = 2;
-    set_reg_opnd(&(op->regDest), PhysicalReg_ST0 + reg, true,
-            LowOpndRegType_fs);
-    set_mem_opnd(&(op->memSrc), disp, base_reg, isBasePhysical);
-    op->memSrc.mType = mType;
-    op->memSrc.index = mIndex;
-    singletonPtr<Scheduler>()->updateUseDefInformation_mem_to_fp(op);
-    return op;
-}
-
-LowOpMemReg* dump_mem_fp(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
-        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
-        int reg) {
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        freeReg(false);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
-                true);
-        return lower_mem_to_fp(m, m2, size, disp, baseAll,
-                true /*isBasePhysical*/, mType, mIndex, reg);
-    } else {
-        return lower_mem_to_fp(m, m2, size, disp, base_reg, isBasePhysical,
-                mType, mIndex, reg);
-    }
-    return NULL;
-}
-///////////////////////////////////////////////////////////////
-///////////////////////////////////////////////////////////////
-//OPERAND ORDER:
-//LowOp same as EncoderBase destination first
-//parameter order of function: src first
-
-////////////////////////////////// IA32 native instructions //////////////
-//! generate a native instruction lea
-
-//!
-void load_effective_addr(int disp, int base_reg, bool isBasePhysical,
-                          int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_LEA;
-    dump_mem_reg(m, ATOM_NORMAL, OpndSize_32, disp, base_reg, isBasePhysical,
-        MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_gp, NULL);
-}
-//! generate a native instruction lea
-
-//! Computes the effective address of the source operand and stores it in the
-//! first operand. (lea reg, [base_reg + index_reg*scale])
-void load_effective_addr_scale(int base_reg, bool isBasePhysical,
-                int index_reg, bool isIndexPhysical, int scale,
-                int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_LEA;
-    dump_mem_scale_reg(m, OpndSize_32,
-                              base_reg, isBasePhysical, 0/*disp*/, index_reg, isIndexPhysical, scale,
-                              reg, isPhysical, LowOpndRegType_gp);
-}
-
-//! lea reg, [base_reg + index_reg*scale + disp]
-void load_effective_addr_scale_disp(int base_reg, bool isBasePhysical, int disp,
-                int index_reg, bool isIndexPhysical, int scale,
-                int reg, bool isPhysical) {
-    dump_mem_scale_reg(Mnemonic_LEA, OpndSize_32, base_reg, isBasePhysical, disp,
-            index_reg, isIndexPhysical, scale, reg, isPhysical,
-            LowOpndRegType_gp);
-}
-//!fldcw
-
-//!
-void load_fpu_cw(int disp, int base_reg, bool isBasePhysical) {
-    Mnemonic m = Mnemonic_FLDCW;
-    dump_mem(m, ATOM_NORMAL, OpndSize_16, disp, base_reg, isBasePhysical);
-}
-//!fnstcw
-
-//!
-void store_fpu_cw(bool checkException, int disp, int base_reg, bool isBasePhysical) {
-    assert(!checkException);
-    Mnemonic m = Mnemonic_FNSTCW;
-    dump_mem(m, ATOM_NORMAL, OpndSize_16, disp, base_reg, isBasePhysical);
-}
-//!cdq
-
-//!
-void convert_integer(OpndSize srcSize, OpndSize dstSize) { //cbw, cwd, cdq
-    assert(srcSize == OpndSize_32 && dstSize == OpndSize_64);
-    Mnemonic m = Mnemonic_CDQ;
-    dump_reg_reg(m, ATOM_NORMAL, OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_EDX, true, LowOpndRegType_gp);
-}
-
-//! \brief Generates the CVTSI2SD and CVTSI2SS opcodes
-//! \details performs cvtsi2** destReg, srcReg
-//! NOTE: Even for cvtsi2ss, the destination is still XMM
-//! and needs to be moved to a GPR.
-//! \param srcReg the src register
-//! \param isSrcPhysical if the srcReg is a physical register
-//! \param destReg the destination register
-//! \param isDestPhysical if destReg is a physical register
-//! \param isDouble if the destination needs to be a double value (float otherwise)
-void convert_int_to_fp(int srcReg, bool isSrcPhysical, int destReg, bool isDestPhysical, bool isDouble) {
-    Mnemonic m = isDouble ? Mnemonic_CVTSI2SD : Mnemonic_CVTSI2SS;
-    dump_reg_reg_diff_types(m, ATOM_NORMAL, OpndSize_32, srcReg, isSrcPhysical, LowOpndRegType_gp,
-            OpndSize_64, destReg, isDestPhysical, LowOpndRegType_xmm);
-}
-
-//!fld: load from memory (float or double) to stack
-
-//!
-void load_fp_stack(LowOp* op, OpndSize size, int disp, int base_reg, bool isBasePhysical) {//fld(s|l)
-    Mnemonic m = Mnemonic_FLD;
-    dump_mem_fp(m, ATOM_NORMAL, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, 0); //ST0
-}
-//! fild: load from memory (int or long) to stack
-
-//!
-void load_int_fp_stack(OpndSize size, int disp, int base_reg, bool isBasePhysical) {//fild(ll|l)
-    Mnemonic m = Mnemonic_FILD;
-    dump_mem_fp(m, ATOM_NORMAL, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, 0); //ST0
-}
-//!fild: load from memory (absolute addr)
-
-//!
-void load_int_fp_stack_imm(OpndSize size, int imm) {//fild(ll|l)
-    return load_int_fp_stack(size, imm, PhysicalReg_Null, true);
-}
-//!fst: store from stack to memory (float or double)
-
-//!
-void store_fp_stack(LowOp* op, bool pop, OpndSize size, int disp, int base_reg, bool isBasePhysical) {//fst(p)(s|l)
-    Mnemonic m = pop ? Mnemonic_FSTP : Mnemonic_FST;
-    dump_fp_mem(m, ATOM_NORMAL, size, 0, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1);
-}
-//!fist: store from stack to memory (int or long)
-
-//!
-void store_int_fp_stack(LowOp* op, bool pop, OpndSize size, int disp, int base_reg, bool isBasePhysical) {//fist(p)(l)
-    Mnemonic m = pop ? Mnemonic_FISTP : Mnemonic_FIST;
-    dump_fp_mem(m, ATOM_NORMAL, size, 0, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1);
-}
-//!cmp reg, mem
-
-//!
-void compare_reg_mem(LowOp* op, OpndSize size, int reg, bool isPhysical,
-              int disp, int base_reg, bool isBasePhysical) {
-    Mnemonic m = Mnemonic_CMP;
-    dump_reg_mem(m, ATOM_NORMAL, size, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, getTypeFromIntSize(size));
-}
-//!cmp mem, reg
-
-//!
-void compare_mem_reg(OpndSize size,
-              int disp, int base_reg, bool isBasePhysical,
-              int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_CMP;
-    dump_mem_reg(m, ATOM_NORMAL, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical, getTypeFromIntSize(size), NULL);
-}
-//! compare a VR with a temporary variable
-
-//!
-void compare_VR_reg_all(OpndSize size,
-             int vA,
-             int reg, bool isPhysical, Mnemonic m) {
-    LowOpndRegType type = getTypeFromIntSize(size);
-    LowOpndRegType pType = type;
-    if(m == Mnemonic_COMISS) {
-        size = OpndSize_32;
-        type = LowOpndRegType_ss;
-        pType = LowOpndRegType_xmm;
-    }
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        int tmpValue[2];
-        int isConst = isVirtualRegConstant(vA, type, tmpValue, true/*updateRefCount*/);
-        if(isConst == 3) {
-            if(m == Mnemonic_COMISS) {
-#ifdef DEBUG_NCG_O1
-                ALOGI("VR is const and SS in compare_VR_reg");
-#endif
-                bool storedAddr = false;
-
-                if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
-                    tmpValue[1] = 0;// set higher 32 bits to zero
-                    // create a new record of a constant
-                    addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, false);
-
-                    // save mem access location in constList
-                    const int offset = 3; // offset is 3 for COMISS
-                    storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
-
-                    ConstInfo* tmpPtr = gCompilationUnit->constListHead;
-                    if (storedAddr == true){
-#ifdef DEBUG_CONST
-                        ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
-                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL,
-                                tmpPtr->valueH, tmpPtr->valueH);
-#endif
-                    } else {
-                        ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
-                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL,
-                                tmpPtr->valueH, tmpPtr->valueH);
-                    }
-                }
-                // Lower mem_reg instruction with constant to be accessed from constant data section
-                if (storedAddr == true) {
-                    int dispAddr =  getGlobalDataAddr("64bits");
-                    dump_mem_reg(m, ATOM_NORMAL, OpndSize_32, dispAddr, PhysicalReg_Null, true,
-                                     MemoryAccess_Constants, vA, reg, isPhysical, pType,
-                                     &(gCompilationUnit->constListHead));
-                } else {
-                    writeBackConstVR(vA, tmpValue[0]);
-                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
-                }
-                return;
-            }
-            else if(size != OpndSize_64) {
-#ifdef DEBUG_NCG_O1
-                ALOGI("VR is const and 32 bits in compare_VR_reg");
-#endif
-                dump_imm_reg(m, ATOM_NORMAL, size, tmpValue[0], reg, isPhysical, pType, false);
-                return;
-            }
-            else if(size == OpndSize_64) {
-#ifdef DEBUG_NCG_O1
-                ALOGI("VR is const and 64 bits in compare_VR_reg");
-#endif
-                bool storedAddr = false;
-
-                if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
-                    // create a new record of a constant
-                    addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, false);
-                    const int offset = 4; // offset is 4 for COMISD
-
-                    // save mem access location in constList
-                    storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
-
-                    ConstInfo* tmpPtr = gCompilationUnit->constListHead;
-                    if (storedAddr == true){
-#ifdef DEBUG_CONST
-                        ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
-                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL,
-                                tmpPtr->valueH, tmpPtr->valueH);
-#endif
-                    } else {
-                        ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
-                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL,
-                                tmpPtr->valueH, tmpPtr->valueH);
-                    }
-                }
-                // Lower mem_reg instruction with constant to be accessed from constant data section
-                if (storedAddr == true) {
-                    int dispAddr =  getGlobalDataAddr("64bits");
-                    dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, dispAddr, PhysicalReg_Null, true,
-                                     MemoryAccess_Constants, vA, reg, isPhysical, LowOpndRegType_xmm,
-                                     &(gCompilationUnit->constListHead));
-                } else {
-                    writeBackConstVR(vA, tmpValue[0]);
-                    writeBackConstVR(vA+1, tmpValue[1]);
-                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
-                                    MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
-                }
-                return;
-            }
-        }
-        if(isConst == 1) writeBackConstVR(vA, tmpValue[0]);
-        if(isConst == 2) writeBackConstVR(vA+1, tmpValue[1]);
-        freeReg(false);
-        int regAll = checkVirtualReg(vA, type, 0/*do not update*/);
-        if(regAll != PhysicalReg_Null) { //do not spill regAll when allocating register for dst
-            startNativeCode(-1, -1);
-
-            //Do not spill regAll
-            gCompilationUnit->setCanSpillRegister (regAll, false);
-
-            dump_reg_reg_noalloc_src(m, ATOM_NORMAL, size, regAll, true, reg, isPhysical, pType);
-            endNativeCode();
-        }
-        else {
-            //virtual register is not allocated to a physical register
-            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
-                MemoryAccess_VR, vA, reg, isPhysical, pType);
-        }
-        updateRefCount(vA, type);
-        return;
-    } else {
-        dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
-            MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
-        return;
-    }
-}
-void compare_VR_reg(OpndSize size,
-             int vA,
-             int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_CMP;
-    return compare_VR_reg_all(size, vA, reg, isPhysical, m);
-}
-void compare_VR_ss_reg(int vA, int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_COMISS;
-    return compare_VR_reg_all(OpndSize_32, vA, reg, isPhysical, m);
-}
-void compare_VR_sd_reg(int vA, int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_COMISD;
-    return compare_VR_reg_all(OpndSize_64, vA, reg, isPhysical, m);
-}
-//!load VR to stack
-
-//!
-void load_fp_stack_VR_all(OpndSize size, int vB, Mnemonic m) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        //can't load from immediate to fp stack
-        int tmpValue[2];
-        int isConst = isVirtualRegConstant(vB, getTypeFromIntSize(size), tmpValue, false/*updateRefCount*/);
-        if(isConst > 0) {
-            if(size != OpndSize_64) {
-#ifdef DEBUG_NCG_O1
-                ALOGI("VR is const and 32 bits in load_fp_stack");
-#endif
-                writeBackConstVR(vB, tmpValue[0]);
-            }
-            else {
-#ifdef DEBUG_NCG_O1
-                ALOGI("VR is const and 64 bits in load_fp_stack_VR");
-#endif
-                if(isConst == 1 || isConst == 3) writeBackConstVR(vB, tmpValue[0]);
-                if(isConst == 2 || isConst == 3) writeBackConstVR(vB+1, tmpValue[1]);
-            }
-        }
-        else { //if VR was updated by a def of gp, a xfer point was inserted
-            //if VR was updated by a def of xmm, a xfer point was inserted
-#if 0
-            int regAll = checkVirtualReg(vB, size, 1);
-            if(regAll != PhysicalReg_Null) //dump from register to memory
-                dump_reg_mem_noalloc(m, size, regAll, true, 4*vB, PhysicalReg_FP, true,
-                    MemoryAccess_VR, vB, getTypeFromIntSize(size));
-#endif
-        }
-        dump_mem_fp(m, ATOM_NORMAL, size, 4*vB, PhysicalReg_FP, true, MemoryAccess_VR, vB, 0);
-    } else {
-        dump_mem_fp(m, ATOM_NORMAL, size, 4*vB, PhysicalReg_FP, true, MemoryAccess_VR, vB, 0);
-    }
-}
-//!load VR(float or double) to stack
-
-//!
-void load_fp_stack_VR(OpndSize size, int vA) {//fld(s|l)
-    Mnemonic m = Mnemonic_FLD;
-    return load_fp_stack_VR_all(size, vA, m);
-}
-//!load VR(int or long) to stack
-
-//!
-void load_int_fp_stack_VR(OpndSize size, int vA) {//fild(ll|l)
-    Mnemonic m = Mnemonic_FILD;
-    return load_fp_stack_VR_all(size, vA, m);
-}
-//!store from stack to VR (float or double)
-
-//!
-void store_fp_stack_VR(bool pop, OpndSize size, int vA) {//fst(p)(s|l)
-    Mnemonic m = pop ? Mnemonic_FSTP : Mnemonic_FST;
-    dump_fp_mem(m, ATOM_NORMAL, size, 0, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        if(size == OpndSize_32)
-            updateVirtualReg(vA, LowOpndRegType_fs_s);
-        else
-            updateVirtualReg(vA, LowOpndRegType_fs);
-    }
-}
-//!store from stack to VR (int or long)
-
-//!
-void store_int_fp_stack_VR(bool pop, OpndSize size, int vA) {//fist(p)(l)
-    Mnemonic m = pop ? Mnemonic_FISTP : Mnemonic_FIST;
-    dump_fp_mem(m, ATOM_NORMAL, size, 0, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        if(size == OpndSize_32)
-            updateVirtualReg(vA, LowOpndRegType_fs_s);
-        else
-            updateVirtualReg(vA, LowOpndRegType_fs);
-    }
-}
-//! ALU ops in FPU, one operand is a VR
-
-//!
-void fpu_VR(ALU_Opcode opc, OpndSize size, int vA) {
-    Mnemonic m = map_of_fpu_opcode_2_mnemonic[opc];
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        int tmpValue[2];
-        int isConst = isVirtualRegConstant(vA, getTypeFromIntSize(size), tmpValue, false/*updateRefCount*/);
-        if(isConst > 0) {
-            if(size != OpndSize_64) {
-                //allocate a register for dst
-                writeBackConstVR(vA, tmpValue[0]);
-            }
-            else {
-                if((isConst == 1 || isConst == 3) && size == OpndSize_64) {
-                    writeBackConstVR(vA, tmpValue[0]);
-                }
-                if((isConst == 2 || isConst == 3) && size == OpndSize_64) {
-                    writeBackConstVR(vA+1, tmpValue[1]);
-                }
-            }
-        }
-        if(!isInMemory(vA, size)) {
-            ALOGI("JIT_INFO: VR not in memory for FPU operation");
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return;
-        }
-        dump_mem_fp(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, 0);
-    } else {
-        dump_mem_fp(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, 0);
-    }
-}
-//! cmp imm reg
-
-//!
-void compare_imm_reg(OpndSize size, int imm,
-              int reg, bool isPhysical) {
-    if(imm == 0) {
-        LowOpndRegType type = getTypeFromIntSize(size);
-        Mnemonic m = Mnemonic_TEST;
-        if(gDvm.executionMode == kExecutionModeNcgO1) {
-            freeReg(false);
-            int regAll = registerAlloc(type, reg, isPhysical, true);
-            lower_reg_to_reg(m, ATOM_NORMAL, size, regAll, true /*isPhysical*/, regAll, true /*isPhysical2*/, type);
-        } else {
-            lower_reg_to_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg, isPhysical, type);
-        }
-        return;
-    }
-    Mnemonic m = Mnemonic_CMP;
-    dump_imm_reg(m, ATOM_NORMAL, size, imm, reg, isPhysical, getTypeFromIntSize(size), false);
-}
-//! cmp imm mem
-
-//!
-void compare_imm_mem(OpndSize size, int imm,
-              int disp, int base_reg, bool isBasePhysical) {
-    Mnemonic m = Mnemonic_CMP;
-    dump_imm_mem(m, ATOM_NORMAL, size, imm, disp,
-                        base_reg, isBasePhysical, MemoryAccess_Unknown, -1, false);
-}
-//! cmp imm VR
-
-//!
-void compare_imm_VR(OpndSize size, int imm,
-             int vA) {
-    Mnemonic m = Mnemonic_CMP;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        if(size != OpndSize_32) {
-            ALOGI("JIT_INFO: Only 32 bits supported in compare_imm_VR");
-            SET_JIT_ERROR(kJitErrorRegAllocFailed);
-            return;
-        }
-        int tmpValue[2];
-        int isConst = isVirtualRegConstant(vA, getTypeFromIntSize(size), tmpValue, false/*updateRefCount*/);
-        if(isConst > 0) {
-            writeBackConstVR(vA, tmpValue[0]);
-        }
-        int regAll = checkVirtualReg(vA, getTypeFromIntSize(size), 0);
-        if(regAll != PhysicalReg_Null)
-            dump_imm_reg_noalloc(m, size, imm, regAll, true, LowOpndRegType_gp);
-        else
-            dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true,
-                MemoryAccess_VR, vA);
-        updateRefCount(vA, getTypeFromIntSize(size));
-    } else {
-        dump_imm_mem(m, ATOM_NORMAL, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, false);
-    }
-}
-//! cmp reg reg
-
-//!
-void compare_reg_reg(int reg1, bool isPhysical1,
-              int reg2, bool isPhysical2) {
-    Mnemonic m = Mnemonic_CMP;
-    dump_reg_reg(m, ATOM_NORMAL, OpndSize_32, reg1, isPhysical1, reg2, isPhysical2, LowOpndRegType_gp);
-}
-void compare_reg_reg_16(int reg1, bool isPhysical1,
-              int reg2, bool isPhysical2) {
-    Mnemonic m = Mnemonic_CMP;
-    dump_reg_reg(m, ATOM_NORMAL, OpndSize_16, reg1, isPhysical1, reg2, isPhysical2, LowOpndRegType_gp);
-}
-
-//! comiss mem reg
-
-//!SSE, XMM: comparison of floating point numbers
-void compare_ss_mem_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
-             int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_COMISS;
-    dump_mem_reg(m, ATOM_NORMAL, OpndSize_32, disp, base_reg, isBasePhysical,
-        MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_xmm, NULL);
-}
-//! comiss reg reg
-
-//!
-void compare_ss_reg_with_reg(LowOp* op, int reg1, bool isPhysical1,
-                  int reg2, bool isPhysical2) {
-    Mnemonic m = Mnemonic_COMISS;
-    dump_reg_reg(m,  ATOM_NORMAL, OpndSize_32, reg1, isPhysical1, reg2, isPhysical2, LowOpndRegType_xmm);
-}
-//! comisd mem reg
-
-//!
-void compare_sd_mem_with_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
-                  int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_COMISD;
-    dump_mem_reg(m, ATOM_NORMAL, OpndSize_64, disp, base_reg, isBasePhysical,
-        MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_xmm, NULL);
-}
-//! comisd reg reg
-
-//!
-void compare_sd_reg_with_reg(LowOp* op, int reg1, bool isPhysical1,
-                  int reg2, bool isPhysical2) {
-    Mnemonic m = Mnemonic_COMISD;
-    dump_reg_reg(m, ATOM_NORMAL, OpndSize_64, reg1, isPhysical1, reg2, isPhysical2, LowOpndRegType_xmm);
-}
-//! fucom[p]
-
-//!
-void compare_fp_stack(bool pop, int reg, bool isDouble) { //compare ST(0) with ST(reg)
-    Mnemonic m = pop ? Mnemonic_FUCOMP : Mnemonic_FUCOM;
-    lower_reg_to_reg(m, ATOM_NORMAL, isDouble ? OpndSize_64 : OpndSize_32,
-                  PhysicalReg_ST0+reg, true /*isPhysical*/, PhysicalReg_ST0, true /*isPhysical2*/, LowOpndRegType_fs);
-}
-
-/*!
-\brief generate a single return instruction
-
-*/
-inline LowOp* lower_return() {
-    if (gDvm.executionMode == kExecutionModeNcgO0 || !gDvmJit.scheduling) {
-        stream = encoder_return(stream);
-        return NULL;
-    }
-    LowOp * op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOp>();
-    op->numOperands = 0;
-    op->opCode = Mnemonic_RET;
-    op->opCode2 = ATOM_NORMAL;
-    singletonPtr<Scheduler>()->updateUseDefInformation(op);
-    return op;
-}
-
-void x86_return() {
-    lower_return();
-}
-
-//!test imm reg
-
-//!
-void test_imm_reg(OpndSize size, int imm, int reg, bool isPhysical) {
-    dump_imm_reg(Mnemonic_TEST, ATOM_NORMAL, size, imm, reg, isPhysical, getTypeFromIntSize(size), false);
-}
-//!test imm mem
-
-//!
-void test_imm_mem(OpndSize size, int imm, int disp, int reg, bool isPhysical) {
-    dump_imm_mem(Mnemonic_TEST, ATOM_NORMAL, size, imm, disp, reg, isPhysical, MemoryAccess_Unknown, -1, false);
-}
-//!alu unary op with one reg operand
-
-//!
-void alu_unary_reg(OpndSize size, ALU_Opcode opc, int reg, bool isPhysical) {
-    Mnemonic m;
-    if(size == OpndSize_64)
-        m = map_of_64_opcode_2_mnemonic[opc];
-    else
-        m = map_of_alu_opcode_2_mnemonic[opc];
-    dump_reg(m, ATOM_NORMAL_ALU, size, reg, isPhysical, getTypeFromIntSize(size));
-}
-//!alu unary op with one mem operand
-
-//!
-void alu_unary_mem(LowOp* op, OpndSize size, ALU_Opcode opc, int disp, int base_reg, bool isBasePhysical) {
-    Mnemonic m;
-    if(size == OpndSize_64)
-        m = map_of_64_opcode_2_mnemonic[opc];
-    else
-        m = map_of_alu_opcode_2_mnemonic[opc];
-    dump_mem(m, ATOM_NORMAL_ALU, size, disp, base_reg, isBasePhysical);
-}
-//!alu binary op with immediate and one mem operand
-
-//!
-void alu_binary_imm_mem(OpndSize size, ALU_Opcode opc, int imm, int disp, int base_reg, bool isBasePhysical) {
-    Mnemonic m;
-    if(size == OpndSize_64)
-        m = map_of_64_opcode_2_mnemonic[opc];
-    else
-        m = map_of_alu_opcode_2_mnemonic[opc];
-    dump_imm_mem(m, ATOM_NORMAL_ALU, size, imm, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, false);
-}
-//!alu binary op with immediate and one reg operand
-
-//!
-void alu_binary_imm_reg(OpndSize size, ALU_Opcode opc, int imm, int reg, bool isPhysical) {
-    Mnemonic m;
-    if(size == OpndSize_64)
-        m = map_of_64_opcode_2_mnemonic[opc];
-    else
-        m = map_of_alu_opcode_2_mnemonic[opc];
-    dump_imm_reg(m, ATOM_NORMAL_ALU, size, imm, reg, isPhysical, getTypeFromIntSize(size), false);
-}
-
-/**
- * @brief Performs get_VR, alu_op and set_VR but with lesser instructions
- * @details Only for 32-bit integers for now
- * @param size The Operand size (Only 32-bit currently)
- * @param opc The alu operation to perform (add or subtract)
- * @param srcVR The source VR to fetch
- * @param destVR The destination VR to set
- * @param imm The literal value to be added to VR value
- * @param tempReg A temporary register
- * @param isTempPhysical Whether the tempReg is physical
- * @param mir current lowered MIR
- * @return whether we were successful. If false, caller needs to perform get_VR, alu_op, set_VR separately
- */
-bool alu_imm_to_VR(OpndSize size, ALU_Opcode opc, int srcVR, int destVR, int imm, int tempReg, bool isTempPhysical, const MIR * mir) {
-    const LowOpndRegType pType = getTypeFromIntSize(size); //gp or xmm
-
-    //We accept only add_opc and sub_opc for now
-    if (opc != add_opc && opc != sub_opc) {
-        return false;
-    }
-
-    //We accept only 32-bit values for now
-    if (size != OpndSize_32) {
-        return false;
-    }
-
-    Mnemonic alu_mn = map_of_alu_opcode_2_mnemonic[opc];
-
-    enum CaseSrc {
-        SRC_IS_CONSTANT,
-        SRC_IN_MEMORY,
-        SRC_IS_ALLOCATED
-    };
-
-    enum CaseDest {
-        DEST_SAME_AS_SRC,
-        DEST_IN_MEMORY,
-        DEST_IS_ALLOCATED
-    };
-
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-
-        /*
-         * We have the following possibilities with the VRs
-         *
-         *  CaseSrc == 1: srcVR is constant
-         *          CaseDest == 1: destVR == srcVR (We do constant += IMM)
-         *          CaseDest == 2: destVR is in Memory (We do MOV IMM + const, MEM)
-         *          CaseDest == 3: destVR is allocated (We do MOV IMM + const, REG)
-         *
-         * CaseSrc == 2: srcVR is in memory
-         *          CaseDest == 1: destVR == srcVR (We do <op> IMM, MEM)
-         *          CaseDest == 2: destVR is in memory (worst case. We return from here and do normal op)
-         *          CaseDest == 3: destVR is allocated (We spill srcVR to same reg, then <op> imm, reg)
-         *
-         * CaseSrc == 3: srcVR is allocated
-         *          CaseDest == 1: destVR == srcVR (We do <op> IMM, REG)
-         *          CaseDest == 2: destVR is in memory (We LEA srcVR plus imm to a temp, and then set destVR to temp)
-         *          CaseDest == 3: destVR is allocated (LEA IMM(srcVR), destVR)
-         *
-         * Now depending on above, we find out the cases, and if needed, find out the const value of src,
-         * and reg allocated to dest and/or src. Memory locations, if needed, are (4*destVRNum/srcVRNum + PhysicalReg_FP)
-         */
-
-        //Initializing
-        CaseSrc caseSrc = SRC_IS_CONSTANT;
-        CaseDest caseDest = DEST_SAME_AS_SRC;
-        int constValSrc = 0;
-        int regDest = -1;
-        int regSrc = -1;
-
-        //Check the case for srcVR
-        int constValue[2];
-        int isConst = isVirtualRegConstant(srcVR, pType, constValue, true/*updateRefCount*/);
-        int tempPhysicalReg = checkVirtualReg(srcVR, pType, 0);
-        if (isConst == 3) {
-            caseSrc = SRC_IS_CONSTANT;
-            constValSrc = constValue[0];
-        }
-        else if (tempPhysicalReg != PhysicalReg_Null) {
-            caseSrc = SRC_IS_ALLOCATED;
-            regSrc = tempPhysicalReg;
-        }
-        else {
-            caseSrc = SRC_IN_MEMORY;
-        }
-
-        //Check the case for destVR
-        if (destVR != srcVR) {
-            tempPhysicalReg = checkVirtualReg(destVR, pType, 0);
-            if (tempPhysicalReg != PhysicalReg_Null) {
-                caseDest = DEST_IS_ALLOCATED;
-                regDest = tempPhysicalReg;
-            }
-            else {
-                caseDest = DEST_IN_MEMORY;
-            }
-        }
-        else {
-            caseDest = DEST_SAME_AS_SRC;
-        }
-
-        int signedImm = (opc == add_opc ? imm : -imm);
-        int finalSum = constValSrc + signedImm;
-
-        //Now handle the cases
-        switch (caseSrc) {
-            case SRC_IS_CONSTANT:
-                if (caseDest == DEST_SAME_AS_SRC) {
-                    //Add or subtract
-                    constValue[0] = finalSum;
-                    constValue[1] = 0; //To be safe
-                    return setVRToConst(destVR, size, constValue);
-                }
-                else if (caseDest == DEST_IN_MEMORY) {
-                    dump_imm_mem_noalloc(Mnemonic_MOV, size, finalSum, 4*destVR, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
-                    return true; //Successfully updated
-                }
-                else if (caseDest == DEST_IS_ALLOCATED) {
-                    dump_imm_reg_noalloc(Mnemonic_MOV, size, finalSum, regDest, true, pType);
-                    updateRefCount(destVR, pType);
-                    updateVirtualReg(destVR, pType);
-                    return true; //Successfully updated
-                }
-                break;
-
-            case SRC_IN_MEMORY:
-                if (caseDest == DEST_SAME_AS_SRC) {
-
-                    // for Silvermont platform
-                    if (strcmp(ARCH_VARIANT, "x86-slm") == 0) {
-
-                        /* Heuristic for inc optimization to avoid store/load REHABQ hazard.
-                           number of adjacent bytecodes which need to be checked for avoiding
-                           store/load REHABQ hazard for increment in memory */
-                        const int incOptMirWindow = 2;
-
-                        // Get SSA representation
-                        SSARepresentation *ssa = mir->ssaRep;
-
-                        // current add/sub mir should only have one def and we
-                        // only care if this def is used
-                        if(ssa != 0 && ssa->numDefs == 1 && ssa->usedNext != 0 &&
-                           ssa->usedNext[0] != 0 && ssa->usedNext[0]->mir != 0) {
-
-                            MIR * mirUse = ssa->usedNext[0]->mir;
-                            MIR * nextMIR = const_cast<MIR*>(mir);
-
-                            // check adjacent mirs window
-                            for (int i = 0; i < incOptMirWindow; i ++) {
-                                if (nextMIR != 0) {
-                                    nextMIR = nextMIR->next;
-
-                                    // if the define variable of mir is used in adjacent mir, return
-                                    // false to avoid add/sub in memory
-                                    if (mirUse == nextMIR) {
-                                        return false;
-                                    }
-                                }
-                            }
-                        }
-
-                        // when we reach here, we can use add/sub on memory directly based
-                        // on the fact that no uses of the mir's def in adjacent mirs window
-                        dump_imm_mem_noalloc_alu(alu_mn, size, imm, 4*destVR, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
-
-                        // Successfully updated
-                        return true;
-                    }
-
-                    // for other platforms
-                    else {
-                        dump_imm_mem_noalloc_alu(alu_mn, size, imm, 4*destVR, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
-
-                        // Successfully updated
-                        return true;
-                    }
-                }
-                else if (caseDest == DEST_IN_MEMORY) {
-                    //We can in no way do better than get_VR, add / sub, set_VR
-                    return false;
-                }
-                else if (caseDest == DEST_IS_ALLOCATED) {
-                    //Load srcVR to regDest, and then add the constant
-                    //Note that with MOVE_OPT on, this is as good as get_VR, add / sub , set_VR
-                    dump_mem_reg_noalloc(Mnemonic_MOV, size, 4*srcVR, PhysicalReg_FP, true, MemoryAccess_VR, srcVR, regDest, true, pType);
-                    dump_imm_reg_noalloc_alu(alu_mn, size, imm, regDest, true, pType);
-                    updateRefCount(destVR, pType);
-                    updateVirtualReg(destVR, pType);
-                    return true; //Successfully updated
-                }
-                break;
-
-            case SRC_IS_ALLOCATED:
-                if (caseDest == DEST_SAME_AS_SRC) {
-                    dump_imm_reg_noalloc_alu(alu_mn, size, imm, regSrc, true, pType);
-                    //We have to reduce refCounts twice. Let's call the VR with
-                    //different names, even though srcVR == destVR
-                    updateRefCount(srcVR, pType);
-                    updateRefCount(destVR, pType);
-                    updateVirtualReg(destVR, pType);
-                    return true; //Successfully updated
-                }
-                else if (caseDest == DEST_IN_MEMORY) {
-                    //We can write regSrc directly to destVR, and then ADD imm, destVR (which is 2 inst). But
-                    //if destVR gets used later, we will load it to a reg anyways. That makes it 3 instructions.
-                    //Instead, let's do LEA imm(regSrc), temp. And assign destVR to temp. Worst case we write
-                    // back destVR soon after, which is still 2 instructions. Best case we get away with just 1.
-                    dump_mem_reg_noalloc_mem(Mnemonic_LEA, ATOM_NORMAL, size, signedImm, regSrc, true, MemoryAccess_Unknown, -1, tempReg, isTempPhysical, pType);
-                    set_virtual_reg(destVR, size, tempReg, isTempPhysical);
-                    updateRefCount(srcVR, pType);
-                    return true; //Successfully updated
-                }
-                else if (caseDest == DEST_IS_ALLOCATED) {
-                    dump_mem_reg_noalloc(Mnemonic_LEA, size, signedImm, regSrc, true, MemoryAccess_Unknown, -1, regDest, true, pType);
-                    //Done with srcVR and destVR
-                    updateRefCount(srcVR, pType);
-                    updateRefCount(destVR, pType);
-                    updateVirtualReg(destVR, pType);
-                    return true; //Successfully updated
-                }
-                break;
-
-            default:
-                return false;
-        }
-    }
-
-    //No optimization for O0
-    return false;
-}
-
-//!alu binary op with one mem operand and one reg operand
-
-//!
-void alu_binary_mem_reg(OpndSize size, ALU_Opcode opc,
-             int disp, int base_reg, bool isBasePhysical,
-             int reg, bool isPhysical) {
-    Mnemonic m;
-    if(size == OpndSize_64)
-        m = map_of_64_opcode_2_mnemonic[opc];
-    else
-        m = map_of_alu_opcode_2_mnemonic[opc];
-    dump_mem_reg(m, ATOM_NORMAL_ALU, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical, getTypeFromIntSize(size), NULL);
-}
-
-void alu_sd_binary_VR_reg(ALU_Opcode opc, int vA, int reg, bool isPhysical, bool isSD) {
-    Mnemonic m;
-    if(isSD) m = map_of_sse_opcode_2_mnemonic[opc];
-    else m = (Mnemonic)(map_of_sse_opcode_2_mnemonic[opc]+1); //from SD to SS
-    OpndSize size = isSD ? OpndSize_64 : OpndSize_32;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        LowOpndRegType type = isSD ? LowOpndRegType_xmm : LowOpndRegType_ss; //type of the mem operand
-        int tmpValue[2];
-        int isConst = isVirtualRegConstant(vA, type, tmpValue,
-                          true/*updateRefCount*/);
-        if(isConst == 3 && !isSD) {            //isConst can be 0 or 3, mem32, use xmm
-            bool storedAddr = false;
-
-            if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
-                tmpValue[1] = 0;// set higher 32 bits to zero
-                // create a new record of a constant
-                addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, false);
-
-                // save mem access location in constList
-                const int offset = 4; // offset is 4 for OPC_(ADD,SUB,MUL,DIV) float operations
-                storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
-
-                ConstInfo* tmpPtr = gCompilationUnit->constListHead;
-                if (storedAddr == true){ // creating constant record and saving address to constant list was successful
-#ifdef DEBUG_CONST
-                    ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
-                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-#endif
-                } else {
-                    ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
-                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-                }
-            }
-            // Lower mem_reg instruction with constant to be accessed from constant data section
-            if (storedAddr == true){
-                int dispAddr =  getGlobalDataAddr("64bits");
-                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_32, dispAddr, PhysicalReg_Null, true,
-                                   MemoryAccess_Constants, vA, reg, isPhysical, LowOpndRegType_xmm,
-                                   &(gCompilationUnit->constListHead));
-            } else {
-                 writeBackConstVR(vA, tmpValue[0]);
-                 dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_32, 4*vA, PhysicalReg_FP, true,
-                                MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
-            }
-            return;
-        }
-        if(isConst == 3 && isSD) {
-            bool storedAddr = false;
-
-            if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
-                // create a new record of a constant
-                addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, false);
-
-                const int offset = 4; // offset is 4 for OPC_(ADD,SUB,MUL,DIV) double operations
-                // save mem access location in constList
-                storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
-
-                ConstInfo* tmpPtr = gCompilationUnit->constListHead;
-                if (storedAddr == true){ // creating constant record and saving address to constant list was successful
-#ifdef DEBUG_CONST
-                    ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
-                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-#endif
-                } else {
-                    ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
-                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-                }
-            }
-            // Lower mem_reg instruction with constant to be accessed from constant data section
-            if (storedAddr == true){
-                int dispAddr =  getGlobalDataAddr("64bits");
-                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, dispAddr, PhysicalReg_Null, true,
-                       MemoryAccess_Constants, vA, reg, isPhysical, LowOpndRegType_xmm, &(gCompilationUnit->constListHead));
-            } else {
-                writeBackConstVR(vA, tmpValue[0]);
-                writeBackConstVR(vA+1, tmpValue[1]);
-                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, 4*vA, PhysicalReg_FP, true,
-                       MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
-            }
-            return;
-        }
-        if(isConst == 1) writeBackConstVR(vA, tmpValue[0]);
-        if(isConst == 2) writeBackConstVR(vA+1, tmpValue[1]);
-        freeReg(false);
-
-        int regAll = checkVirtualReg(vA, type, 0/*do not update refCount*/);
-        if(regAll != PhysicalReg_Null) {
-            startNativeCode(-1, -1); //should we use vA, type
-            //CHECK: callupdateVRAtUse
-
-            //Do not spill regAll
-            gCompilationUnit->setCanSpillRegister (regAll, false);
-
-            dump_reg_reg_noalloc_src(m, ATOM_NORMAL_ALU, size, regAll, true, reg,
-                         isPhysical, LowOpndRegType_xmm);
-            endNativeCode();
-        }
-        else {
-            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
-                         MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm);
-        }
-        updateRefCount(vA, type);
-    }
-    else {
-        dump_mem_reg(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
-                    MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
-    }
-}
-
-//!alu binary op with a VR and one reg operand
-
-//!
-void alu_binary_VR_reg(OpndSize size, ALU_Opcode opc, int vA, int reg, bool isPhysical) {
-    Mnemonic m;
-    if(size == OpndSize_64)
-        m = map_of_64_opcode_2_mnemonic[opc];
-    else
-        m = map_of_alu_opcode_2_mnemonic[opc];
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        int tmpValue[2];
-        int isConst = isVirtualRegConstant(vA, getTypeFromIntSize(size), tmpValue,
-                          true/*updateRefCount*/);
-        if(isConst == 3 && size != OpndSize_64) {
-            //allocate a register for dst
-            dump_imm_reg(m, ATOM_NORMAL_ALU, size, tmpValue[0], reg, isPhysical,
-                       getTypeFromIntSize(size), false);
-            return;
-        }
-        if(isConst == 3 && size == OpndSize_64) {
-            bool storedAddr = false;
-            bool align = false;
-            if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
-
-                // create a new record of a constant
-                if (m == Mnemonic_PADDQ || Mnemonic_PSUBQ || Mnemonic_PAND || Mnemonic_POR || Mnemonic_PXOR) {
-                    align = true;
-                }
-                addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, align);
-                const int offset = 4; // offset is 4 for OPC_(ADD,SUB and logical) long operations
-                // save mem access location in constList
-                storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
-
-                ConstInfo* tmpPtr = gCompilationUnit->constListHead;
-                if (storedAddr == true){ // creating constant record and saving address to constant list was successful
-#ifdef DEBUG_CONST
-                    ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
-                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-#endif
-                } else {
-                    ALOGI("JIT_INFO: Error creating constant failed for regnum %d, valueL %d(%x) valueH %d(%x)",
-                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-                }
-            }
-            // Lower mem_reg instruction with constant to be accessed from constant data section
-            if (storedAddr == true){
-                int dispAddr =  getGlobalDataAddr("64bits");
-                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, dispAddr, PhysicalReg_Null, true,
-                       MemoryAccess_Constants, vA, reg, isPhysical, LowOpndRegType_xmm, &(gCompilationUnit->constListHead));
-            } else {
-                writeBackConstVR(vA, tmpValue[0]);
-                writeBackConstVR(vA+1, tmpValue[1]);
-                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, 4*vA, PhysicalReg_FP, true,
-                       MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
-            }
-            return;
-        }
-        if(isConst == 1) writeBackConstVR(vA, tmpValue[0]);
-        if(isConst == 2) writeBackConstVR(vA+1, tmpValue[1]);
-
-        freeReg(false);
-        int regAll = checkVirtualReg(vA, getTypeFromIntSize(size), 0);
-        if(regAll != PhysicalReg_Null) {
-            startNativeCode(-1, -1);
-
-            //Do not spill regAll
-            gCompilationUnit->setCanSpillRegister (regAll, false);
-
-            dump_reg_reg_noalloc_src(m, ATOM_NORMAL_ALU, size, regAll, true, reg,
-                         isPhysical, getTypeFromIntSize(size));
-            endNativeCode();
-        }
-        else {
-            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
-                MemoryAccess_VR, vA, reg, isPhysical, getTypeFromIntSize(size));
-        }
-        updateRefCount(vA, getTypeFromIntSize(size));
-    }
-    else {
-        dump_mem_reg(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
-            MemoryAccess_VR, vA, reg, isPhysical, getTypeFromIntSize(size), NULL);
-    }
-}
-//!alu binary op with two reg operands
-
-//!
-void alu_binary_reg_reg(OpndSize size, ALU_Opcode opc,
-                         int reg1, bool isPhysical1,
-                         int reg2, bool isPhysical2) {
-    Mnemonic m;
-    if(size == OpndSize_64)
-        m = map_of_64_opcode_2_mnemonic[opc];
-    else
-        m = map_of_alu_opcode_2_mnemonic[opc];
-    dump_reg_reg(m, ATOM_NORMAL_ALU, size, reg1, isPhysical1, reg2, isPhysical2, getTypeFromIntSize(size));
-}
-//!alu binary op with one reg operand and one mem operand
-
-//!
-void alu_binary_reg_mem(OpndSize size, ALU_Opcode opc,
-             int reg, bool isPhysical,
-             int disp, int base_reg, bool isBasePhysical) { //destination is mem!!
-    Mnemonic m;
-    if(size == OpndSize_64)
-        m = map_of_64_opcode_2_mnemonic[opc];
-    else
-        m = map_of_alu_opcode_2_mnemonic[opc];
-    dump_reg_mem(m, ATOM_NORMAL_ALU, size, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, getTypeFromIntSize(size));
-}
-//!FPU ops with one mem operand
-
-//!
-void fpu_mem(LowOp* op, ALU_Opcode opc, OpndSize size, int disp, int base_reg, bool isBasePhysical) {
-    Mnemonic m = map_of_fpu_opcode_2_mnemonic[opc];
-    dump_mem_fp(m, ATOM_NORMAL_ALU, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, 0);
-}
-//!SSE 32-bit ALU
-
-//!
-void alu_ss_binary_reg_reg(ALU_Opcode opc, int reg, bool isPhysical,
-                int reg2, bool isPhysical2) {
-    Mnemonic m = (Mnemonic)(map_of_sse_opcode_2_mnemonic[opc]+1); //from SD to SS
-    dump_reg_reg(m, ATOM_NORMAL_ALU, OpndSize_32, reg, isPhysical, reg2, isPhysical2, LowOpndRegType_xmm);
-}
-//!SSE 64-bit ALU
-
-//!
-void alu_sd_binary_reg_reg(ALU_Opcode opc, int reg, bool isPhysical,
-                int reg2, bool isPhysical2) {
-    Mnemonic m = map_of_sse_opcode_2_mnemonic[opc];
-    dump_reg_reg(m, ATOM_NORMAL_ALU, OpndSize_64, reg, isPhysical, reg2, isPhysical2, LowOpndRegType_xmm);
-}
-//!push reg to native stack
-
-//!
-void push_reg_to_stack(OpndSize size, int reg, bool isPhysical) {
-    dump_reg(Mnemonic_PUSH, ATOM_NORMAL, size, reg, isPhysical, getTypeFromIntSize(size));
-}
-//!push mem to native stack
-
-//!
-void push_mem_to_stack(OpndSize size, int disp, int base_reg, bool isBasePhysical) {
-    dump_mem(Mnemonic_PUSH, ATOM_NORMAL, size, disp, base_reg, isBasePhysical);
-}
-//!move from reg to memory
-
-//!
-void move_reg_to_mem(OpndSize size,
-                      int reg, bool isPhysical,
-                      int disp, int base_reg, bool isBasePhysical) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_reg_mem(m, ATOM_NORMAL, size, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, getTypeFromIntSize(size));
-}
-//!move from reg to memory
-
-//!Operands are already allocated
-void move_reg_to_mem_noalloc(OpndSize size,
-                  int reg, bool isPhysical,
-                  int disp, int base_reg, bool isBasePhysical,
-                  MemoryAccessType mType, int mIndex) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_reg_mem_noalloc(m, size, reg, isPhysical, disp, base_reg, isBasePhysical, mType, mIndex, getTypeFromIntSize(size));
-}
-//!move from memory to reg
-
-//!
-LowOpMemReg* move_mem_to_reg(OpndSize size,
-                      int disp, int base_reg, bool isBasePhysical,
-                      int reg, bool isPhysical) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    return dump_mem_reg(m, ATOM_NORMAL, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical, getTypeFromIntSize(size), NULL);
-}
-//!move from memory to reg
-
-//!Operands are already allocated
-LowOpMemReg* move_mem_to_reg_noalloc(OpndSize size,
-                  int disp, int base_reg, bool isBasePhysical,
-                  MemoryAccessType mType, int mIndex,
-                  int reg, bool isPhysical) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    return dump_mem_reg_noalloc(m, size, disp, base_reg, isBasePhysical, mType, mIndex, reg, isPhysical, getTypeFromIntSize(size));
-}
-//!movss from memory to reg
-
-//!Operands are already allocated
-LowOpMemReg* move_ss_mem_to_reg_noalloc(int disp, int base_reg, bool isBasePhysical,
-                 MemoryAccessType mType, int mIndex,
-                 int reg, bool isPhysical) {
-    return dump_mem_reg_noalloc(Mnemonic_MOVSS, OpndSize_32, disp, base_reg, isBasePhysical, mType, mIndex, reg, isPhysical, LowOpndRegType_xmm);
-}
-//!movss from reg to memory
-
-//!Operands are already allocated
-LowOpRegMem* move_ss_reg_to_mem_noalloc(int reg, bool isPhysical,
-                 int disp, int base_reg, bool isBasePhysical,
-                 MemoryAccessType mType, int mIndex) {
-    return dump_reg_mem_noalloc(Mnemonic_MOVSS, OpndSize_32, reg, isPhysical, disp, base_reg, isBasePhysical, mType, mIndex, LowOpndRegType_xmm);
-}
-//!movzx from memory to reg
-
-//!
-void movez_mem_to_reg(OpndSize size,
-               int disp, int base_reg, bool isBasePhysical,
-               int reg, bool isPhysical) {
-    dump_movez_mem_reg(Mnemonic_MOVZX, size, disp, base_reg, isBasePhysical, reg, isPhysical);
-}
-
-//!movzx from one reg to another reg
-
-//!
-void movez_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical,
-                      int reg2, bool isPhysical2) {
-    Mnemonic m = Mnemonic_MOVZX;
-    dump_movez_reg_reg(m, size, reg, isPhysical, reg2, isPhysical2);
-}
-
-void movez_mem_disp_scale_to_reg(OpndSize size,
-                 int base_reg, bool isBasePhysical,
-                 int disp, int index_reg, bool isIndexPhysical, int scale,
-                 int reg, bool isPhysical) {
-    dump_mem_scale_reg(Mnemonic_MOVZX, size, base_reg, isBasePhysical,
-                 disp, index_reg, isIndexPhysical, scale,
-                 reg, isPhysical, LowOpndRegType_gp);
-}
-void moves_mem_disp_scale_to_reg(OpndSize size,
-                  int base_reg, bool isBasePhysical,
-                  int disp, int index_reg, bool isIndexPhysical, int scale,
-                  int reg, bool isPhysical) {
-    dump_mem_scale_reg(Mnemonic_MOVSX, size, base_reg, isBasePhysical,
-                  disp, index_reg, isIndexPhysical, scale,
-                  reg, isPhysical, LowOpndRegType_gp);
-}
-//!movsx from memory to reg
-
-//!
-void moves_mem_to_reg(LowOp* op, OpndSize size,
-               int disp, int base_reg, bool isBasePhysical,
-               int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_MOVSX;
-    dump_moves_mem_reg(m, size, disp, base_reg, isBasePhysical, reg, isPhysical);
-}
-//!mov from one reg to another reg
-
-//!
-void move_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical,
-                      int reg2, bool isPhysical2) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_reg_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg2, isPhysical2, getTypeFromIntSize(size));
-}
-//!mov from one reg to another reg
-
-//!Sign extends the value. Only 32-bit support.
-void moves_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical,
-                      int reg2, bool isPhysical2) {
-    Mnemonic m = Mnemonic_MOVSX;
-    dump_reg_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg2, isPhysical2, getTypeFromIntSize(size));
-}
-
-//!mov from one reg to another reg
-
-//!Operands are already allocated
-void move_reg_to_reg_noalloc(OpndSize size,
-                  int reg, bool isPhysical,
-                  int reg2, bool isPhysical2) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_reg_reg_noalloc(m, size, reg, isPhysical, reg2, isPhysical2, getTypeFromIntSize(size));
-}
-//!move from memory to reg
-
-//!
-void move_mem_scale_to_reg(OpndSize size,
-                int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
-                int reg, bool isPhysical) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_mem_scale_reg(m, size, base_reg, isBasePhysical, 0/*disp*/, index_reg, isIndexPhysical, scale,
-                              reg, isPhysical, getTypeFromIntSize(size));
-}
-void move_mem_disp_scale_to_reg(OpndSize size,
-                int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                int reg, bool isPhysical) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_mem_scale_reg(m, size, base_reg, isBasePhysical, disp, index_reg, isIndexPhysical, scale,
-                              reg, isPhysical, getTypeFromIntSize(size));
-}
-//!move from reg to memory
-
-//!
-void move_reg_to_mem_scale(OpndSize size,
-                int reg, bool isPhysical,
-                int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_reg_mem_scale(m, size, reg, isPhysical,
-                              base_reg, isBasePhysical, 0/*disp*/, index_reg, isIndexPhysical, scale,
-                              getTypeFromIntSize(size));
-}
-void move_reg_to_mem_disp_scale(OpndSize size,
-                int reg, bool isPhysical,
-                int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_reg_mem_scale(m, size, reg, isPhysical,
-                              base_reg, isBasePhysical, disp, index_reg, isIndexPhysical, scale,
-                              getTypeFromIntSize(size));
-}
-
-void move_chain_to_mem(OpndSize size, int imm,
-                        int disp, int base_reg, bool isBasePhysical) {
-    dump_imm_mem(Mnemonic_MOV, ATOM_NORMAL, size, imm, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, true);
-}
-
-//!move an immediate to memory
-
-//!
-void move_imm_to_mem(OpndSize size, int imm,
-                      int disp, int base_reg, bool isBasePhysical) {
-    assert(size != OpndSize_64);
-    if(size == OpndSize_64) {
-        ALOGI("JIT_INFO: Trying to move 64-bit imm to memory");
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return;
-    }
-    dump_imm_mem(Mnemonic_MOV, ATOM_NORMAL, size, imm, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, false);
-}
-//! set a VR to an immediate
-
-//!
-void set_VR_to_imm(int vA, OpndSize size, int imm) {
-    assert(size != OpndSize_64);
-    if(size == OpndSize_64) {
-        ALOGI("JIT_INFO: Trying to set VR with 64-bit imm");
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return;
-    }
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        int regAll = checkVirtualReg(vA, getTypeFromIntSize(size), 0);
-        if(regAll != PhysicalReg_Null) {
-            dump_imm_reg_noalloc(m, size, imm, regAll, true, LowOpndRegType_gp);
-            updateRefCount(vA, getTypeFromIntSize(size));
-            updateVirtualReg(vA, getTypeFromIntSize(size));
-            return;
-        }
-        //will call freeReg
-        freeReg(false);
-        regAll = registerAlloc(LowOpndRegType_virtual | getTypeFromIntSize(size), vA, false/*dummy*/, true, true);
-        if(regAll == PhysicalReg_Null) {
-            dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
-            return;
-        }
-
-        dump_imm_reg_noalloc(m, size, imm, regAll, true, LowOpndRegType_gp);
-        updateVirtualReg(vA, getTypeFromIntSize(size));
-    }
-    else {
-        dump_imm_mem(m, ATOM_NORMAL, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, false);
-    }
-}
-void set_VR_to_imm_noupdateref(LowOp* op, int vA, OpndSize size, int imm) {
-    return;
-}
-//! set a VR to an immediate
-
-//! Do not allocate a physical register for the VR
-void set_VR_to_imm_noalloc(int vA, OpndSize size, int imm) {
-    assert(size != OpndSize_64);
-    if(size == OpndSize_64) {
-        ALOGI("JIT_INFO: Trying to move 64-bit imm to memory (noalloc)");
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return;
-    }
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
-}
-
-void move_chain_to_reg(OpndSize size, int imm, int reg, bool isPhysical) {
-    dump_imm_reg(Mnemonic_MOV, ATOM_NORMAL, size, imm, reg, isPhysical, LowOpndRegType_gp, true);
-}
-
-//! move an immediate to reg
-
-//!
-void move_imm_to_reg(OpndSize size, int imm, int reg, bool isPhysical) {
-    assert(size != OpndSize_64);
-    if(size == OpndSize_64) {
-        ALOGI("JIT_INFO: Trying to move 64-bit imm to register");
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return;
-    }
-    Mnemonic m = Mnemonic_MOV;
-    dump_imm_reg(m, ATOM_NORMAL, size, imm, reg, isPhysical, LowOpndRegType_gp, false);
-}
-//! move an immediate to reg
-
-//! The operand is already allocated
-void move_imm_to_reg_noalloc(OpndSize size, int imm, int reg, bool isPhysical) {
-    assert(size != OpndSize_64);
-    if(size == OpndSize_64) {
-        ALOGI("JIT_INFO: Trying to move 64-bit imm to register (noalloc)");
-        SET_JIT_ERROR(kJitErrorRegAllocFailed);
-        return;
-    }
-    Mnemonic m = Mnemonic_MOV;
-    dump_imm_reg_noalloc(m, size, imm, reg, isPhysical, LowOpndRegType_gp);
-}
-//!cmov from reg to reg
-
-//!
-void conditional_move_reg_to_reg(OpndSize size, ConditionCode cc, int reg1, bool isPhysical1, int reg, bool isPhysical) {
-    Mnemonic m = (Mnemonic)(Mnemonic_CMOVcc+cc);
-    dump_reg_reg(m, ATOM_NORMAL, size, reg1, isPhysical1, reg, isPhysical, LowOpndRegType_gp);
-}
-//!movss from memory to reg
-
-//!
-void move_ss_mem_to_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
-                         int reg, bool isPhysical) {
-    dump_mem_reg(Mnemonic_MOVSS, ATOM_NORMAL, OpndSize_32, disp, base_reg, isBasePhysical,
-        MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_xmm, NULL);
-}
-//!movss from reg to memory
-
-//!
-void move_ss_reg_to_mem(LowOp* op, int reg, bool isPhysical,
-                         int disp, int base_reg, bool isBasePhysical) {
-    dump_reg_mem(Mnemonic_MOVSS, ATOM_NORMAL, OpndSize_32, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, LowOpndRegType_xmm);
-}
-//!movsd from memory to reg
-
-//!
-void move_sd_mem_to_reg(int disp, int base_reg, bool isBasePhysical,
-                         int reg, bool isPhysical) {
-    dump_mem_reg(Mnemonic_MOVSD, ATOM_NORMAL, OpndSize_64, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_xmm, NULL);
-}
-//!movsd from reg to memory
-
-//!
-void move_sd_reg_to_mem(LowOp* op, int reg, bool isPhysical,
-                         int disp, int base_reg, bool isBasePhysical) {
-    dump_reg_mem(Mnemonic_MOVSD, ATOM_NORMAL, OpndSize_64, reg, isPhysical,
-                        disp, base_reg, isBasePhysical,
-                        MemoryAccess_Unknown, -1, LowOpndRegType_xmm);
-}
-//!load from VR to a temporary
-
-//!
-void get_virtual_reg_all(int vR, OpndSize size, int reg, bool isPhysical, Mnemonic m) {
-    LowOpndRegType type = getTypeFromIntSize(size);
-    LowOpndRegType pType = type;//gp or xmm
-    OpndSize size2 = size;
-    Mnemonic m2 = m;
-    if(m == Mnemonic_MOVSS) {
-        size = OpndSize_32;
-        size2 = OpndSize_64;
-        type = LowOpndRegType_ss;
-        pType = LowOpndRegType_xmm;
-        m2 = Mnemonic_MOVQ; //to move from one xmm register to another
-    }
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        int tmpValue[2];
-        int isConst;
-        isConst = isVirtualRegConstant(vR, type, tmpValue, true/*updateRefCount*/);
-        if(isConst == 3) {
-            if(m == Mnemonic_MOVSS) { //load 32 bits from VR
-                bool storedAddr = false;
-
-                if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
-                    tmpValue[1] = 0;// set higher 32 bits to zero
-                    // create a new record of a constant
-                    addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vR, false);
-
-                    // save mem access location in constList
-                    const int offset = 4; // offset is 4 for MOVSS operations
-                    storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vR, stream, offset);
-
-                    ConstInfo* tmpPtr = gCompilationUnit->constListHead;
-                    if (storedAddr == true){ // creating constant record and saving address to constant list was successful
-#ifdef DEBUG_CONST
-                        ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
-                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-#endif
-                    } else {
-                        ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
-                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-                    }
-                }
-                // Lower mem_reg instruction with constant to be accessed from constant data section
-                if (storedAddr == true){
-                    int dispAddr =  getGlobalDataAddr("64bits");
-                    dump_mem_reg(m, ATOM_NORMAL, size, dispAddr, PhysicalReg_Null, true,
-                                       MemoryAccess_Constants, vR, reg, isPhysical, pType,
-                                       &(gCompilationUnit->constListHead));
-                } else {
-                    //VR is not mapped to a register but in memory
-                    writeBackConstVR(vR, tmpValue[0]);
-                    //temporary reg has "pType" (which is xmm)
-                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
-                                       MemoryAccess_VR, vR, reg, isPhysical, pType, NULL);
-                }
-                return;
-            }
-            else if(m == Mnemonic_MOVSD || size == OpndSize_64) {
-                bool storedAddr = false;
-
-                if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
-                    // create a new record of a constant
-                    addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vR, false);
-
-                    // save mem access location in constList
-                    const int offset = 4; // offset is 4 for MOVSD operations
-                    storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vR, stream, offset);
-
-                    ConstInfo* tmpPtr = gCompilationUnit->constListHead;
-                    if (storedAddr == true){ // creating constant record and saving address to constant list was successful
-#ifdef DEBUG_CONST
-                        ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
-                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-#endif
-                    } else {
-                        ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
-                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
-                    }
-                }
-                // Lower mem_reg instruction with constant to be accessed from constant data section
-                if (storedAddr == true){
-                    int dispAddr =  getGlobalDataAddr("64bits");
-                    dump_mem_reg(m, ATOM_NORMAL, size, dispAddr, PhysicalReg_Null, true,
-                                       MemoryAccess_Constants, vR, reg, isPhysical, pType,
-                                       &(gCompilationUnit->constListHead));
-                } else {
-                    //VR is not mapped to a register but in memory
-                    writeBackConstVR(vR, tmpValue[0]);
-                    writeBackConstVR(vR+1, tmpValue[1]);
-                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
-                                       MemoryAccess_VR, vR, reg, isPhysical, pType, NULL);
-                }
-                return;
-            }
-            else if(size != OpndSize_64) {
-                //VR is not mapped to a register
-                dump_imm_reg(m, ATOM_NORMAL, size, tmpValue[0], reg, isPhysical, pType, false);
-                return;
-            }
-        }
-        if(isConst == 1) writeBackConstVR(vR, tmpValue[0]);
-        if(isConst == 2) writeBackConstVR(vR+1, tmpValue[1]);
-
-        // We want to free any variables no longer in use
-        freeReg(false);
-
-        // Do we have a physical register associated for this VR?
-        int physRegForVR = checkVirtualReg(vR, type, 0);
-
-        // If we do, then let register allocator decide if a new physical
-        // register needs allocated for the temp
-        if(physRegForVR != PhysicalReg_Null) {
-            startNativeCode(vR, type);
-
-            //Do not spill physRegForVR
-            gCompilationUnit->setCanSpillRegister (physRegForVR, false);
-
-            //check XFER_MEM_TO_XMM
-            updateVRAtUse(vR, type, physRegForVR);
-            //temporary reg has "pType"
-            dump_reg_reg_noalloc_src(m2, ATOM_NORMAL, size2, physRegForVR, true, reg, isPhysical, pType);
-            endNativeCode();
-            updateRefCount(vR, type);
-            return;
-        }
-
-        // When we get to this point, we know that we have no physical register
-        // associated with the VR
-        physRegForVR = registerAlloc(LowOpndRegType_virtual | type, vR, false/*dummy*/, false);
-
-        // If we still have no physical register for the VR, then use it as
-        // a memory operand
-        if(physRegForVR == PhysicalReg_Null) {
-            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
-                MemoryAccess_VR, vR, reg, isPhysical, pType);
-            return;
-        }
-
-        // At this point we definitely have a physical register for the VR.
-        // Check to see if the temp can share same physical register.
-        if(checkTempReg2(reg, pType, isPhysical, physRegForVR, vR)) {
-            registerAllocMove(reg, pType, isPhysical, physRegForVR);
-            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
-                MemoryAccess_VR, vR, physRegForVR, true, pType);
-            updateRefCount(vR, type);
-            return;
-        }
-        else {
-            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
-                MemoryAccess_VR, vR, physRegForVR, true, pType);
-            //xmm with 32 bits
-            startNativeCode(vR, type);
-
-            //Do not spill physRegForVR
-            gCompilationUnit->setCanSpillRegister (physRegForVR, false);
-
-            dump_reg_reg_noalloc_src(m2, ATOM_NORMAL, size2, physRegForVR, true, reg, isPhysical, pType);
-            endNativeCode();
-            updateRefCount(vR, type);
-            return;
-        }
-    }
-    else {
-        dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
-            MemoryAccess_VR, vR, reg, isPhysical, pType, NULL);
-    }
-}
-void get_virtual_reg(int vB, OpndSize size, int reg, bool isPhysical) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    return get_virtual_reg_all(vB, size, reg, isPhysical, m);
-}
-void get_virtual_reg_noalloc(int vB, OpndSize size, int reg, bool isPhysical) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_mem_reg_noalloc(m, size, 4*vB, PhysicalReg_FP, true,
-        MemoryAccess_VR, vB, reg, isPhysical, getTypeFromIntSize(size));
-}
-//3 cases: gp, xmm, ss
-//ss: the temporary register is xmm
-//!load from a temporary to a VR
-
-//!
-void set_virtual_reg_all(int vA, OpndSize size, int reg, bool isPhysical, Mnemonic m) {
-    LowOpndRegType type = getTypeFromIntSize(size);
-    LowOpndRegType pType = type;//gp or xmm
-    OpndSize size2 = size;
-    Mnemonic m2 = m;
-    if(m == Mnemonic_MOVSS) {
-        size = OpndSize_32;
-        size2 = OpndSize_64;
-        type = LowOpndRegType_ss;
-        pType = LowOpndRegType_xmm;
-        m2 = Mnemonic_MOVQ;
-    }
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        //3 cases
-        //1: virtual register is already allocated to a physical register
-        //   call dump_reg_reg_noalloc_dst
-        //2: src reg is already allocated, VR is not yet allocated
-        //   allocate VR to the same physical register used by src reg
-        //   [call registerAllocMove]
-        //3: both not yet allocated
-        //   allocate a physical register for the VR
-        //   then call dump_reg_reg_noalloc_dst
-        //may need to convert from gp to xmm or the other way
-        freeReg(false);
-        int regAll = checkVirtualReg(vA, type, 0);
-        if(regAll != PhysicalReg_Null)  { //case 1
-            startNativeCode(-1, -1);
-
-            //Do not spill regAll
-            gCompilationUnit->setCanSpillRegister (regAll, false);
-
-            dump_reg_reg_noalloc_dst(m2, size2, reg, isPhysical, regAll, true, pType); //temporary reg is "pType"
-            endNativeCode();
-            updateRefCount(vA, type);
-            updateVirtualReg(vA, type); //will dump VR to memory, should happen afterwards
-            return;
-        }
-        regAll = checkTempReg(reg, pType, isPhysical, vA); //vA is not used inside
-        if(regAll != PhysicalReg_Null) { //case 2
-            registerAllocMove(vA, LowOpndRegType_virtual | type, false, regAll, true);
-            updateVirtualReg(vA, type); //will dump VR to memory, should happen afterwards
-            return; //next native instruction starts at op
-        }
-        //case 3
-        regAll = registerAlloc(LowOpndRegType_virtual | type, vA, false/*dummy*/, false, true);
-        if(regAll == PhysicalReg_Null) {
-            dump_reg_mem_noalloc(m, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
-                MemoryAccess_VR, vA, pType);
-            return;
-        }
-
-        startNativeCode(-1, -1);
-
-        //Do not spill regAll
-        gCompilationUnit->setCanSpillRegister (regAll, false);
-
-        dump_reg_reg_noalloc_dst(m2, size2, reg, isPhysical, regAll, true, pType);
-        endNativeCode();
-        updateRefCount(vA, type);
-        updateVirtualReg(vA, type);
-    }
-    else {
-        dump_reg_mem(m, ATOM_NORMAL, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
-            MemoryAccess_VR, vA, pType);
-    }
-}
-void set_virtual_reg(int vA, OpndSize size, int reg, bool isPhysical) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    return set_virtual_reg_all(vA, size, reg, isPhysical, m);
-}
-void set_virtual_reg_noalloc(int vA, OpndSize size, int reg, bool isPhysical) {
-    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_reg_mem_noalloc(m, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
-        MemoryAccess_VR, vA, getTypeFromIntSize(size));
-}
-void get_VR_ss(int vB, int reg, bool isPhysical) {
-    return get_virtual_reg_all(vB, OpndSize_64, reg, isPhysical, Mnemonic_MOVSS);
-}
-void set_VR_ss(int vA, int reg, bool isPhysical) {
-    return set_virtual_reg_all(vA, OpndSize_64, reg, isPhysical, Mnemonic_MOVSS);
-}
-void get_VR_sd(int vB, int reg, bool isPhysical) {
-    return get_virtual_reg_all(vB, OpndSize_64, reg, isPhysical, Mnemonic_MOVSD);
-}
-void set_VR_sd(int vA, int reg, bool isPhysical) {
-    return set_virtual_reg_all(vA, OpndSize_64, reg, isPhysical, Mnemonic_MOVSD);
-}
-////////////////////////////////// END: IA32 native instructions //////////////
-
-//! \brief generate native code to perform null check
-//!
-//! \details This function does not export PC
-//! \param reg
-//! \param isPhysical is the reg is physical
-//! \param vr the vr corresponding to reg
-//!
-//! \return -1 if error happened, 0 otherwise
-int simpleNullCheck(int reg, bool isPhysical, int vr) {
-    if(isVRNullCheck(vr, OpndSize_32)) {
-        updateRefCount2(reg, LowOpndRegType_gp, isPhysical);
-        num_removed_nullCheck++;
-        return 0;
-    }
-    compare_imm_reg(OpndSize_32, 0, reg, isPhysical);
-    conditional_jump_global_API(Condition_E, "common_errNullObject", false);
-    int retCode = setVRNullCheck(vr, OpndSize_32);
-    if (retCode < 0)
-        return retCode;
-    return 0;
-}
-
-/* only for O1 code generator */
-int boundCheck(int vr_array, int reg_array, bool isPhysical_array,
-               int vr_index, int reg_index, bool isPhysical_index,
-               int exceptionNum) {
-#ifdef BOUNDCHECK_OPT
-    if(isVRBoundCheck(vr_array, vr_index)) {
-        updateRefCount2(reg_array, LowOpndRegType_gp, isPhysical_array);
-        updateRefCount2(reg_index, LowOpndRegType_gp, isPhysical_index);
-        return 0;
-    }
-#endif
-    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length),
-                    reg_array, isPhysical_array,
-                    reg_index, isPhysical_index);
-
-    char errName[256];
-    sprintf(errName, "common_errArrayIndex");
-    handlePotentialException(
-                                       Condition_NC, Condition_C,
-                                       exceptionNum, errName);
-#ifdef BOUNDCHECK_OPT
-    setVRBoundCheck(vr_array, vr_index);
-#endif
-    return 0;
-}
-
-/**
- * @brief Generates native code to perform null check
- * @param reg temporary or physical register to test
- * @param isPhysical flag to indicate whether parameter reg is physical
- * register
- * @param exceptionNum
- * @param vr virtual register for which the null check is being done
- * @return >= 0 on success
- */
-int nullCheck(int reg, bool isPhysical, int exceptionNum, int vr) {
-    const char * errorName = "common_errNullObject";
-    int retCode = 0;
-
-    //nullCheck optimization is available in O1 mode only
-    if(gDvm.executionMode == kExecutionModeNcgO1 && isVRNullCheck(vr, OpndSize_32)) {
-        updateRefCount2(reg, LowOpndRegType_gp, isPhysical);
-        if(exceptionNum <= 1) {
-            updateRefCount2(PhysicalReg_EDX, LowOpndRegType_gp, true);
-            updateRefCount2(PhysicalReg_EDX, LowOpndRegType_gp, true);
-        }
-        num_removed_nullCheck++;
-        return 0;
-    }
-
-    compare_imm_reg(OpndSize_32, 0, reg, isPhysical);
-
-    // Get a label for exception handling restore state
-    char * newStreamLabel =
-            singletonPtr<ExceptionHandlingRestoreState>()->getUniqueLabel();
-
-    // Since we are not doing the exception handling restore state inline, in case of
-    // ZF=1 we must jump to the BB that restores the state
-    conditional_jump(Condition_E, newStreamLabel, true);
-
-    // We can save stream pointer now since this follows a jump and ensures that
-    // scheduler already flushed stream
-    char * originalStream = stream;
-
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        rememberState(exceptionNum);
-        if (exceptionNum > 1) {
-            nextVersionOfHardReg(PhysicalReg_EDX, 2); //next version has 2 ref count
-        }
-    }
-
-    export_pc(); //use %edx
-
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("exception"); //dump GG, GL VRs
-    }
-
-    // We must flush scheduler queue now before we copy to exception handling
-    // stream.
-    if(gDvmJit.scheduling)
-        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
-
-    // Move all instructions to a deferred stream that will be dumped later
-    singletonPtr<ExceptionHandlingRestoreState>()->createExceptionHandlingStream(
-            originalStream, stream, errorName);
-
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        goToState(exceptionNum);
-        retCode = setVRNullCheck(vr, OpndSize_32);
-        if (retCode < 0)
-            return retCode;
-    }
-
-    return 0;
-}
-
-/**
- * @brief Generates code to handle potential exception
- * @param code_excep Condition code to take exception path
- * @param code_okay Condition code to skip exception
- * @param exceptionNum
- * @param errName Name of exception to handle
- * @return >= 0 on success
- */
-int handlePotentialException(
-                             ConditionCode code_excep, ConditionCode code_okay,
-                             int exceptionNum, const char* errName) {
-    // Get a label for exception handling restore state
-    char * newStreamLabel =
-            singletonPtr<ExceptionHandlingRestoreState>()->getUniqueLabel();
-
-    // Since we are not doing the exception handling restore state inline, in case of
-    // code_excep we must jump to the BB that restores the state
-    conditional_jump(code_excep, newStreamLabel, true);
-
-    // We can save stream pointer now since this follows a jump and ensures that
-    // scheduler already flushed stream
-    char * originalStream = stream;
-
-    if (gDvm.executionMode == kExecutionModeNcgO1) {
-        rememberState(exceptionNum);
-        if (exceptionNum > 1) {
-            nextVersionOfHardReg(PhysicalReg_EDX, 2); //next version has 2 ref count
-        }
-    }
-
-    export_pc(); //use %edx
-
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("exception"); //dump GG, GL VRs
-    }
-
-    if(!strcmp(errName, "common_throw_message")) {
-        move_imm_to_reg(OpndSize_32, LstrInstantiationErrorPtr, PhysicalReg_ECX, true);
-    }
-
-    // We must flush scheduler queue now before we copy to exception handling
-    // stream.
-    if(gDvmJit.scheduling)
-        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
-
-    // Move all instructions to a deferred stream that will be dumped later
-    singletonPtr<ExceptionHandlingRestoreState>()->createExceptionHandlingStream(
-            originalStream, stream, errName);
-
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        goToState(exceptionNum);
-    }
-
-    return 0;
-}
-
-//!generate native code to get the self pointer from glue
-
-//!It uses one scratch register
-int get_self_pointer(int reg, bool isPhysical) {
-    move_mem_to_reg(OpndSize_32, offEBP_self, PhysicalReg_EBP, true, reg, isPhysical);
-    return 0;
-}
-
-int get_res_classes(int reg, bool isPhysical)
-{
-    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
-
-    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.methodClassDex), C_SCRATCH_1, isScratchPhysical, C_SCRATCH_2, isScratchPhysical);
-
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(DvmDex, pResClasses), C_SCRATCH_2, isScratchPhysical, reg, isPhysical);
-
-    return 0;
-}
-
-//!generate native code to get the current class object from glue
-
-//!It uses two scratch registers
-int get_glue_method_class(int reg, bool isPhysical) {
-    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
-    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.method), C_SCRATCH_1, isScratchPhysical, C_SCRATCH_2, isScratchPhysical);
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Method, clazz), C_SCRATCH_2, isScratchPhysical, reg, isPhysical);
-    return 0;
-}
-//!generate native code to get the current method from glue
-
-//!It uses one scratch register
-int get_glue_method(int reg, bool isPhysical) {
-    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
-    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.method), C_SCRATCH_1, isScratchPhysical, reg, isPhysical);
-    return 0;
-}
-
-//!generate native code to get SuspendCount from glue
-
-//!It uses one scratch register
-int get_suspendCount(int reg, bool isPhysical) {
-    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
-    move_mem_to_reg(OpndSize_32, offsetof(Thread, suspendCount), C_SCRATCH_1, isScratchPhysical, reg, isPhysical);
-    return 0;
-}
-
-//!generate native code to get retval from glue
-
-//!It uses one scratch register
-int get_return_value(OpndSize size, int reg, bool isPhysical) {
-    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
-    move_mem_to_reg(size, offsetof(Thread, interpSave.retval), C_SCRATCH_1, isScratchPhysical, reg, isPhysical);
-    return 0;
-}
-
-//!generate native code to set retval in glue
-
-//!It uses one scratch register
-int set_return_value(OpndSize size, int reg, bool isPhysical) {
-    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
-    move_reg_to_mem(size, reg, isPhysical, offsetof(Thread, interpSave.retval), C_SCRATCH_1, isScratchPhysical);
-    return 0;
-}
-
-/**
- * @brief Sets self Thread's retval.
- * @details This needs a scratch register to hold pointer to self.
- * @param size Size of return value
- * @param sourceReg Register that holds the return value.
- * @param isSourcePhysical Flag that determines if the source register is
- * physical or not. For example, the source register can be a temporary.
- * @param scratchRegForSelfThread Scratch register to use for self pointer
- * @param isScratchPhysical Marks whether the scratch register is physical
- * or not.
- * @todo Is retval set as expected for 64-bit? If retval is set as 64 bit
- * but read as 32-bit, is this correct?
- */
-void set_return_value(OpndSize size, int sourceReg, bool isSourcePhysical,
-        int scratchRegForSelfThread, int isScratchPhysical) {
-    // Get self pointer
-    get_self_pointer(scratchRegForSelfThread, isScratchPhysical);
-
-    // Now set Thread.retval with the source register's value
-    move_reg_to_mem(size, sourceReg, isSourcePhysical,
-            offsetof(Thread, interpSave.retval), scratchRegForSelfThread, isScratchPhysical);
-}
-//!generate native code to clear exception object in glue
-
-//!It uses two scratch registers
-int clear_exception() {
-    get_self_pointer(C_SCRATCH_2, isScratchPhysical);
-    move_imm_to_mem(OpndSize_32, 0, offsetof(Thread, exception), C_SCRATCH_2, isScratchPhysical);
-    return 0;
-}
-//!generate native code to get exception object in glue
-
-//!It uses two scratch registers
-int get_exception(int reg, bool isPhysical) {
-    get_self_pointer(C_SCRATCH_2, isScratchPhysical);
-    move_mem_to_reg(OpndSize_32, offsetof(Thread, exception), C_SCRATCH_2, isScratchPhysical, reg, isPhysical);
-    return 0;
-}
-//!generate native code to set exception object in glue
-
-//!It uses two scratch registers
-int set_exception(int reg, bool isPhysical) {
-    get_self_pointer(C_SCRATCH_2, isScratchPhysical);
-    move_reg_to_mem(OpndSize_32, reg, isPhysical, offsetof(Thread, exception), C_SCRATCH_2, isScratchPhysical);
-    return 0;
-}
-
-//! get SaveArea pointer
-
-//!
-int savearea_from_fp(int reg, bool isPhysical) {
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
-    load_effective_addr(-sizeofStackSaveArea, PhysicalReg_FP, true, reg, isPhysical);
-    return 0;
-}
-
-#ifdef DEBUG_CALL_STACK3
-int call_debug_dumpSwitch() {
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = debug_dumpSwitch;
-    callFuncPtr((int)funcPtr, "debug_dumpSwitch");
-    return 0;
-}
-#endif
-
-int call_dvmQuasiAtomicSwap64() {
-    typedef int64_t (*vmHelper)(int64_t, volatile int64_t*);
-    vmHelper funcPtr = dvmQuasiAtomicSwap64;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmQuasiAtomicSwap64");
-        callFuncPtr((int)funcPtr, "dvmQuasiAtomicSwap64");
-        afterCall("dvmQuasiAtomicSwap64");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmQuasiAtomicSwap64");
-    }
-    return 0;
-}
-
-int call_dvmQuasiAtomicRead64() {
-    typedef int64_t (*vmHelper)(volatile const int64_t*);
-    vmHelper funcPtr = dvmQuasiAtomicRead64;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmQuasiAtomiRead64");
-        callFuncPtr((int)funcPtr, "dvmQuasiAtomicRead64");
-        afterCall("dvmQuasiAtomicRead64");
-        touchEax(); //for return value
-        touchEdx();
-    } else {
-        callFuncPtr((int)funcPtr, "dvmQuasiAtomicRead64");
-    }
-    return 0;
-}
-
-int call_dvmJitToInterpPunt() {
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmJitToInterpPunt;
-    callFuncPtr((int)funcPtr, "dvmJitToInterpPunt");
-    return 0;
-}
-
-void call_dvmJitToInterpNormal() {
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmJitToInterpNormal;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmJitToInterpNormal");
-        callFuncPtrImm((int)funcPtr);
-        afterCall("dvmJitToInterpNormal");
-        touchEbx();
-    } else {
-        callFuncPtrImm((int)funcPtr);
-    }
-    return;
-}
-
-/*
- * helper function for generating the call to dvmJitToInterpBackwardBranch
- * This transition to the interpreter is also required for
- * self-verification, in particular, in order to check
- * for control or data divergence for each loop iteration.
- */
-void call_dvmJitToInterpBackwardBranch() {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmJitToInterpBackwardBranch");
-    }
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmJitToInterpBackwardBranch;
-    callFuncPtrImm((int)funcPtr);
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        afterCall("dvmJitToInterpBackwardBranch");
-   }
-   return;
- }
-
-int call_dvmJitToInterpTraceSelectNoChain() {
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmJitToInterpTraceSelectNoChain;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmJitToInterpTraceSelectNoChain");
-        callFuncPtr((int)funcPtr, "dvmJitToInterpTraceSelectNoChain");
-        afterCall("dvmJitToInterpTraceSelectNoChain");
-        touchEbx();
-    } else {
-        callFuncPtr((int)funcPtr, "dvmJitToInterpTraceSelectNoChain");
-    }
-    return 0;
-}
-
-void call_dvmJitToInterpTraceSelect() {
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmJitToInterpTraceSelect;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmJitToInterpTraceSelect");
-        callFuncPtrImm((int)funcPtr);
-        afterCall("dvmJitToInterpTraceSelect");
-        touchEbx();
-    } else {
-        callFuncPtrImm((int)funcPtr);
-    }
-    return;
-}
-
-int call_dvmJitToPatchPredictedChain() {
-    typedef const Method * (*vmHelper)(const Method *method,
-                                       Thread *self,
-                                       PredictedChainingCell *cell,
-                                       const ClassObject *clazz);
-    vmHelper funcPtr = dvmJitToPatchPredictedChain;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmJitToPatchPredictedChain");
-        callFuncPtr((int)funcPtr, "dvmJitToPatchPredictedChain");
-        afterCall("dvmJitToPatchPredictedChain");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmJitToPatchPredictedChain");
-    }
-    return 0;
-}
-
-//!generate native code to call __moddi3
-
-//!
-int call_moddi3() {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("moddi3");
-        callFuncPtr((intptr_t)__moddi3, "__moddi3");
-        afterCall("moddi3");
-    } else {
-        callFuncPtr((intptr_t)__moddi3, "__moddi3");
-    }
-    return 0;
-}
-//!generate native code to call __divdi3
-
-//!
-int call_divdi3() {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("divdi3");
-        callFuncPtr((intptr_t)__divdi3, "__divdi3");
-        afterCall("divdi3");
-    } else {
-        callFuncPtr((intptr_t)__divdi3, "__divdi3");
-    }
-    return 0;
-}
-
-//!generate native code to call fmod
-
-//!
-int call_fmod() {
-    typedef double (*libHelper)(double, double);
-    libHelper funcPtr = fmod;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("fmod");
-        callFuncPtr((int)funcPtr, "fmod");
-        afterCall("fmod");
-    } else {
-        callFuncPtr((int)funcPtr, "fmod");
-    }
-    return 0;
-}
-//!generate native code to call fmodf
-
-//!
-int call_fmodf() {
-    typedef float (*libHelper)(float, float);
-    libHelper funcPtr = fmodf;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("fmodf");
-        callFuncPtr((int)funcPtr, "fmodf");
-        afterCall("fmodf");
-    } else {
-        callFuncPtr((int)funcPtr, "fmodf");
-    }
-    return 0;
-}
-//!generate native code to call dvmFindCatchBlock
-
-//!
-int call_dvmFindCatchBlock() {
-    //int dvmFindCatchBlock(Thread* self, int relPc, Object* exception,
-    //bool doUnroll, void** newFrame)
-    typedef int (*vmHelper)(Thread*, int, Object*, int, void**);
-    vmHelper funcPtr = dvmFindCatchBlock;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmFindCatchBlock");
-        callFuncPtr((int)funcPtr, "dvmFindCatchBlock");
-        afterCall("dvmFindCatchBlock");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmFindCatchBlock");
-    }
-    return 0;
-}
-//!generate native code to call dvmThrowVerificationError
-
-//!
-int call_dvmThrowVerificationError() {
-    typedef void (*vmHelper)(const Method*, int, int);
-    vmHelper funcPtr = dvmThrowVerificationError;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmThrowVerificationError");
-        callFuncPtr((int)funcPtr, "dvmThrowVerificationError");
-        afterCall("dvmThrowVerificationError");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmThrowVerificationError");
-    }
-    return 0;
-}
-
-//!generate native code to call dvmResolveMethod
-
-//!
-int call_dvmResolveMethod() {
-    //Method* dvmResolveMethod(const ClassObject* referrer, u4 methodIdx, MethodType methodType);
-    typedef Method* (*vmHelper)(const ClassObject*, u4, MethodType);
-    vmHelper funcPtr = dvmResolveMethod;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmResolveMethod");
-        callFuncPtr((int)funcPtr, "dvmResolveMethod");
-        afterCall("dvmResolveMethod");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmResolveMethod");
-    }
-    return 0;
-}
-//!generate native code to call dvmResolveClass
-
-//!
-int call_dvmResolveClass() {
-    //ClassObject* dvmResolveClass(const ClassObject* referrer, u4 classIdx, bool fromUnverifiedConstant)
-    typedef ClassObject* (*vmHelper)(const ClassObject*, u4, bool);
-    vmHelper funcPtr = dvmResolveClass;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmResolveClass");
-        callFuncPtr((int)funcPtr, "dvmResolveClass");
-        afterCall("dvmResolveClass");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmResolveClass");
-    }
-    return 0;
-}
-
-//!generate native code to call dvmInstanceofNonTrivial
-
-//!
-int call_dvmInstanceofNonTrivial() {
-    typedef int (*vmHelper)(const ClassObject*, const ClassObject*);
-    vmHelper funcPtr = dvmInstanceofNonTrivial;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmInstanceofNonTrivial");
-        callFuncPtr((int)funcPtr, "dvmInstanceofNonTrivial");
-        afterCall("dvmInstanceofNonTrivial");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmInstanceofNonTrivial");
-    }
-    return 0;
-}
-//!generate native code to call dvmThrowException
-
-//!
-int call_dvmThrow() {
-    typedef void (*vmHelper)(ClassObject* exceptionClass, const char*);
-    vmHelper funcPtr = dvmThrowException;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmThrowException");
-        callFuncPtr((int)funcPtr, "dvmThrowException");
-        afterCall("dvmThrowException");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmThrowException");
-    }
-    return 0;
-}
-//!generate native code to call dvmThrowExceptionWithClassMessage
-
-//!
-int call_dvmThrowWithMessage() {
-    typedef void (*vmHelper)(ClassObject* exceptionClass, const char*);
-    vmHelper funcPtr = dvmThrowExceptionWithClassMessage;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmThrowExceptionWithClassMessage");
-        callFuncPtr((int)funcPtr, "dvmThrowExceptionWithClassMessage");
-        afterCall("dvmThrowExceptionWithClassMessage");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmThrowExceptionWithClassMessage");
-    }
-    return 0;
-}
-//!generate native code to call dvmCheckSuspendPending
-
-//!
-int call_dvmCheckSuspendPending() {
-    typedef bool (*vmHelper)(Thread*);
-    vmHelper funcPtr = dvmCheckSuspendPending;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmCheckSuspendPending");
-        callFuncPtr((int)funcPtr, "dvmCheckSuspendPending");
-        afterCall("dvmCheckSuspendPending");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmCheckSuspendPending");
-    }
-    return 0;
-}
-//!generate native code to call dvmLockObject
-
-//!
-int call_dvmLockObject() {
-    typedef void (*vmHelper)(struct Thread*, struct Object*);
-    vmHelper funcPtr = dvmLockObject;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmLockObject");
-        callFuncPtr((int)funcPtr, "dvmLockObject");
-        afterCall("dvmLockObject");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmLockObject");
-    }
-    return 0;
-}
-//!generate native code to call dvmUnlockObject
-
-//!
-int call_dvmUnlockObject() {
-    typedef bool (*vmHelper)(Thread*, Object*);
-    vmHelper funcPtr = dvmUnlockObject;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmUnlockObject");
-        callFuncPtr((int)funcPtr, "dvmUnlockObject");
-        afterCall("dvmUnlockObject");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmUnlockObject");
-    }
-    return 0;
-}
-//!generate native code to call dvmInitClass
-
-//!
-int call_dvmInitClass() {
-    typedef bool (*vmHelper)(ClassObject*);
-    vmHelper funcPtr = dvmInitClass;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmInitClass");
-        callFuncPtr((int)funcPtr, "dvmInitClass");
-        afterCall("dvmInitClass");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmInitClass");
-    }
-    return 0;
-}
-//!generate native code to call dvmAllocObject
-
-//!
-int call_dvmAllocObject() {
-    typedef Object* (*vmHelper)(ClassObject*, int);
-    vmHelper funcPtr = dvmAllocObject;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmAllocObject");
-        callFuncPtr((int)funcPtr, "dvmAllocObject");
-        afterCall("dvmAllocObject");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmAllocObject");
-    }
-    return 0;
-}
-//!generate native code to call dvmAllocArrayByClass
-
-//!
-int call_dvmAllocArrayByClass() {
-    typedef ArrayObject* (*vmHelper)(ClassObject*, size_t, int);
-    vmHelper funcPtr = dvmAllocArrayByClass;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmAllocArrayByClass");
-        callFuncPtr((int)funcPtr, "dvmAllocArrayByClass");
-        afterCall("dvmAllocArrayByClass");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmAllocArrayByClass");
-    }
-    return 0;
-}
-//!generate native code to call dvmAllocPrimitiveArray
-
-//!
-int call_dvmAllocPrimitiveArray() {
-    typedef ArrayObject* (*vmHelper)(char, size_t, int);
-    vmHelper funcPtr = dvmAllocPrimitiveArray;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmAllocPrimitiveArray");
-        callFuncPtr((int)funcPtr, "dvmAllocPrimitiveArray");
-        afterCall("dvmAllocPrimitiveArray");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmAllocPrimitiveArray");
-    }
-    return 0;
-}
-//!generate native code to call dvmInterpHandleFillArrayData
-
-//!
-int call_dvmInterpHandleFillArrayData() {
-    typedef bool (*vmHelper)(ArrayObject*, const u2*);
-    vmHelper funcPtr = dvmInterpHandleFillArrayData;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmInterpHandleFillArrayData"); //before move_imm_to_reg to avoid spilling C_SCRATCH_1
-        callFuncPtr((int)funcPtr, "dvmInterpHandleFillArrayData");
-        afterCall("dvmInterpHandleFillArrayData");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmInterpHandleFillArrayData");
-    }
-    return 0;
-}
-
-//!generate native code to call dvmNcgHandlePackedSwitch
-
-//!
-int call_dvmNcgHandlePackedSwitch() {
-    typedef s4 (*vmHelper)(const s4*, s4, u2, s4);
-    vmHelper funcPtr = dvmNcgHandlePackedSwitch;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmNcgHandlePackedSwitch");
-        callFuncPtr((int)funcPtr, "dvmNcgHandlePackedSwitch");
-        afterCall("dvmNcgHandlePackedSwitch");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmNcgHandlePackedSwitch");
-    }
-    return 0;
-}
-
-int call_dvmJitHandlePackedSwitch() {
-    typedef s4 (*vmHelper)(const s4*, s4, u2, s4);
-    vmHelper funcPtr = dvmJitHandlePackedSwitch;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmJitHandlePackedSwitch");
-        callFuncPtr((int)funcPtr, "dvmJitHandlePackedSwitch");
-        afterCall("dvmJitHandlePackedSwitch");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmJitHandlePackedSwitch");
-    }
-    return 0;
-}
-
-//!generate native code to call dvmNcgHandleSparseSwitch
-
-//!
-int call_dvmNcgHandleSparseSwitch() {
-    typedef s4 (*vmHelper)(const s4*, u2, s4);
-    vmHelper funcPtr = dvmNcgHandleSparseSwitch;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmNcgHandleSparseSwitch");
-        callFuncPtr((int)funcPtr, "dvmNcgHandleSparseSwitch");
-        afterCall("dvmNcgHandleSparseSwitch");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmNcgHandleSparseSwitch");
-    }
-    return 0;
-}
-
-int call_dvmJitHandleSparseSwitch() {
-    typedef s4 (*vmHelper)(const s4*, u2, s4);
-    vmHelper funcPtr = dvmJitHandleSparseSwitch;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmJitHandleSparseSwitch");
-        callFuncPtr((int)funcPtr, "dvmJitHandleSparseSwitch");
-        afterCall("dvmJitHandleSparseSwitch");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmJitHandleSparseSwitch");
-    }
-    return 0;
-}
-
-//!generate native code to call dvmCanPutArrayElement
-
-//!
-int call_dvmCanPutArrayElement() {
-    typedef bool (*vmHelper)(const ClassObject*, const ClassObject*);
-    vmHelper funcPtr = dvmCanPutArrayElement;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmCanPutArrayElement");
-        callFuncPtr((int)funcPtr, "dvmCanPutArrayElement");
-        afterCall("dvmCanPutArrayElement");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmCanPutArrayElement");
-    }
-    return 0;
-}
-
-//!generate native code to call dvmFindInterfaceMethodInCache
-
-//!
-int call_dvmFindInterfaceMethodInCache() {
-    typedef Method* (*vmHelper)(ClassObject*, u4, const Method*, DvmDex*);
-    vmHelper funcPtr = dvmFindInterfaceMethodInCache;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmFindInterfaceMethodInCache");
-        callFuncPtr((int)funcPtr, "dvmFindInterfaceMethodInCache");
-        afterCall("dvmFindInterfaceMethodInCache");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmFindInterfaceMethodInCache");
-    }
-    return 0;
-}
-
-//!generate native code to call dvmHandleStackOverflow
-
-//!
-int call_dvmHandleStackOverflow() {
-    typedef void (*vmHelper)(Thread*, const Method*);
-    vmHelper funcPtr = dvmHandleStackOverflow;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmHandleStackOverflow");
-        callFuncPtr((int)funcPtr, "dvmHandleStackOverflow");
-        afterCall("dvmHandleStackOverflow");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmHandleStackOverflow");
-    }
-    return 0;
-}
-//!generate native code to call dvmResolveString
-
-//!
-int call_dvmResolveString() {
-    //StringObject* dvmResolveString(const ClassObject* referrer, u4 stringIdx)
-    typedef StringObject* (*vmHelper)(const ClassObject*, u4);
-    vmHelper funcPtr = dvmResolveString;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmResolveString");
-        callFuncPtr((int)funcPtr, "dvmResolveString");
-        afterCall("dvmResolveString");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmResolveString");
-    }
-    return 0;
-}
-//!generate native code to call dvmResolveInstField
-
-//!
-int call_dvmResolveInstField() {
-    //InstField* dvmResolveInstField(const ClassObject* referrer, u4 ifieldIdx)
-    typedef InstField* (*vmHelper)(const ClassObject*, u4);
-    vmHelper funcPtr = dvmResolveInstField;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmResolveInstField");
-        callFuncPtr((int)funcPtr, "dvmResolveInstField");
-        afterCall("dvmResolveInstField");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmResolveInstField");
-    }
-    return 0;
-}
-//!generate native code to call dvmResolveStaticField
-
-//!
-int call_dvmResolveStaticField() {
-    //StaticField* dvmResolveStaticField(const ClassObject* referrer, u4 sfieldIdx)
-    typedef StaticField* (*vmHelper)(const ClassObject*, u4);
-    vmHelper funcPtr = dvmResolveStaticField;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("dvmResolveStaticField");
-        callFuncPtr((int)funcPtr, "dvmResolveStaticField");
-        afterCall("dvmResolveStaticField");
-    } else {
-        callFuncPtr((int)funcPtr, "dvmResolveStaticField");
-    }
-    return 0;
-}
-
-#define P_GPR_2 PhysicalReg_ECX
-/*!
-\brief This function is used to resolve a string reference
-
-INPUT: const pool index in %eax
-
-OUTPUT: resolved string in %eax
-
-The registers are hard-coded, 2 physical registers %esi and %edx are used as scratch registers;
-It calls a C function dvmResolveString;
-The only register that is still live after this function is ebx
-*/
-int const_string_resolve() {
-    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    if (insertLabel(".const_string_resolve", false) == -1)
-        return -1;
-    //method stored in glue structure as well as on the interpreted stack
-    get_glue_method_class(P_GPR_2, true);
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 4, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, P_GPR_2, true, 0, PhysicalReg_ESP, true);
-    call_dvmResolveString();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    compare_imm_reg( OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_E, "common_exceptionThrown", false);
-    x86_return();
-    return 0;
-}
-#undef P_GPR_2
-/*!
-\brief This function is used to resolve a class
-
-INPUT: const pool index in argument "indexReg" (%eax)
-
-OUTPUT: resolved class in %eax
-
-The registers are hard-coded, 3 physical registers (%esi, %edx, startLR:%eax) are used as scratch registers.
-It calls a C function dvmResolveClass;
-The only register that is still live after this function is ebx
-*/
-int resolve_class2(
-           int startLR/*scratch register*/, bool isPhysical, int indexReg/*const pool index*/,
-           bool indexPhysical, int thirdArg) {
-    if (insertLabel(".class_resolve", false) == -1)
-        return -1;
-
-    //Get call back
-    void (*backEndSymbolCreationCallback) (const char *, void *) =
-        gDvmJit.jitFramework.backEndSymbolCreationCallback;
-
-    //Call it if we have one
-    if (backEndSymbolCreationCallback != 0)
-    {
-        backEndSymbolCreationCallback (".class_resolve", (void*) stream);
-    }
-
-    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-
-    //push index to stack first, to free indexReg
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, indexReg, indexPhysical, 4, PhysicalReg_ESP, true);
-    get_glue_method_class(startLR, isPhysical);
-    move_imm_to_mem(OpndSize_32, thirdArg, 8, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, startLR, isPhysical, 0, PhysicalReg_ESP, true);
-    call_dvmResolveClass();
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_E, "common_exceptionThrown", false);
-
-    x86_return();
-    return 0;
-}
-/*!
-\brief This function is used to resolve a method, and it is called once with %eax for both indexReg and startLR
-
-INPUT: const pool index in argument "indexReg" (%eax)
-
-OUTPUT: resolved method in %eax
-
-The registers are hard-coded, 3 physical registers (%esi, %edx, startLR:%eax) are used as scratch registers.
-It calls a C function dvmResolveMethod;
-The only register that is still live after this function is ebx
-*/
-int resolve_method2(
-            int startLR/*logical register index*/, bool isPhysical, int indexReg/*const pool index*/,
-            bool indexPhysical,
-            int thirdArg/*VIRTUAL*/) {
-    if(thirdArg == METHOD_VIRTUAL) {
-        if (insertLabel(".virtual_method_resolve", false) == -1)
-            return -1;
-    }
-    else if(thirdArg == METHOD_DIRECT) {
-        if (insertLabel(".direct_method_resolve", false) == -1)
-            return -1;
-    }
-    else if(thirdArg == METHOD_STATIC) {
-        if (insertLabel(".static_method_resolve", false) == -1)
-            return -1;
-    }
-
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, indexReg, indexPhysical, 4, PhysicalReg_ESP, true);
-
-    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    get_glue_method_class(startLR, isPhysical);
-
-    move_imm_to_mem(OpndSize_32, thirdArg, 8, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, startLR, isPhysical, 0, PhysicalReg_ESP, true);
-    call_dvmResolveMethod();
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_E, "common_exceptionThrown", false);
-
-    x86_return();
-    return 0;
-}
-/*!
-\brief This function is used to resolve an instance field
-
-INPUT: const pool index in argument "indexReg" (%eax)
-
-OUTPUT: resolved field in %eax
-
-The registers are hard-coded, 3 physical registers (%esi, %edx, startLR:%eax) are used as scratch registers.
-It calls a C function dvmResolveInstField;
-The only register that is still live after this function is ebx
-*/
-int resolve_inst_field2(
-            int startLR/*logical register index*/, bool isPhysical,
-            int indexReg/*const pool index*/, bool indexPhysical) {
-    if (insertLabel(".inst_field_resolve", false) == -1)
-        return -1;
-    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, indexReg, indexPhysical, 4, PhysicalReg_ESP, true);
-    //method stored in glue structure as well as interpreted stack
-    get_glue_method_class(startLR, isPhysical);
-    move_reg_to_mem(OpndSize_32, startLR, isPhysical, 0, PhysicalReg_ESP, true);
-    call_dvmResolveInstField();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_E, "common_exceptionThrown", false);
-
-    x86_return();
-    return 0;
-}
-/*!
-\brief This function is used to resolve a static field
-
-INPUT: const pool index in argument "indexReg" (%eax)
-
-OUTPUT: resolved field in %eax
-
-The registers are hard-coded, 3 physical registers (%esi, %edx, startLR:%eax) are used as scratch registers.
-It calls a C function dvmResolveStaticField;
-The only register that is still live after this function is ebx
-*/
-int resolve_static_field2(
-              int startLR/*logical register index*/, bool isPhysical, int indexReg/*const pool index*/,
-              bool indexPhysical) {
-    if (insertLabel(".static_field_resolve", false) == -1)
-        return -1;
-    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, indexReg, indexPhysical, 4, PhysicalReg_ESP, true);
-    get_glue_method_class(startLR, isPhysical);
-    move_reg_to_mem(OpndSize_32, startLR, isPhysical, 0, PhysicalReg_ESP, true);
-    call_dvmResolveStaticField();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_E, "common_exceptionThrown", false);
-
-    x86_return();
-    return 0;
-}
-
-int pushAllRegs() {
-    load_effective_addr(-28, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EAX, true, 24, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
-    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EBX, true, 20, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
-    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_ECX, true, 16, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
-    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EDX, true, 12, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
-    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_ESI, true, 8, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
-    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EDI, true, 4, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
-    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EBP, true, 0, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
-    return 0;
-}
-int popAllRegs() {
-    move_mem_to_reg_noalloc(OpndSize_32, 24, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EAX, true);
-    move_mem_to_reg_noalloc(OpndSize_32, 20, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EBX, true);
-    move_mem_to_reg_noalloc(OpndSize_32, 16, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_ECX, true);
-    move_mem_to_reg_noalloc(OpndSize_32, 12, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EDX, true);
-    move_mem_to_reg_noalloc(OpndSize_32, 8, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_ESI, true);
-    move_mem_to_reg_noalloc(OpndSize_32, 4, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EDI, true);
-    move_mem_to_reg_noalloc(OpndSize_32, 0, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EBP, true);
-    load_effective_addr(28, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    return 0;
-}
-
-/* align the relative offset of jmp/jcc and movl within 16B */
-void alignOffset(int offset) {
-    int rem, nop_size;
-
-    if ((uint)(stream + offset) % 16 > 12) {
-        rem = (uint)(stream + offset) % 16;
-        nop_size = (16 - rem) % 16;
-        stream = encoder_nops(nop_size, stream);
-    }
-}
-
-/**
-  * @brief align a pointer to n-bytes aligned
-  * @param addr the pointer need to be aligned
-  * @param n n-bytes aligned
-  * @return aligned address
-  */
-char* align(char* addr, int n) {
-    char* alignedAddr = reinterpret_cast<char*>((reinterpret_cast<unsigned int>(addr) + (n-1)) & ~(n-1));
-    return alignedAddr;
-}
-/**
- * @brief Returns whether the jump to BB needs alignment
- * because it might be patched later on.
- * @param bb Basic Block to look at
- * @return Returns true for all chaining cells and also for
- * the prebackward block.
- */
-bool doesJumpToBBNeedAlignment(BasicBlock * bb) {
-    // Get type for this BB
-    int type = static_cast<int>(bb->blockType);
-
-    if ((type >= static_cast<int> (kChainingCellNormal)
-            && type < static_cast<int> (kChainingCellLast))
-            && type != static_cast<int> (kChainingCellBackwardBranch))
-    {
-        //We always return true if BB is a chaining cell except if it is
-        //backward branch chaining cell. The reason we make exception for
-        //BBCC is because we always patch the jump to preBackwardBlock and
-        //not the jump to the chaining cell
-        return true;
-    }
-    else if (type == static_cast<int> (kPreBackwardBlock))
-    {
-        //Since the prebackward block is always used in front of
-        //backward branch chaining cell and the jump to it is
-        //the one being patched, we also return true.
-        return true;
-    }
-    else
-    {
-        return false;
-    }
-}
-
-#ifdef WITH_SELF_VERIFICATION
-int selfVerificationLoad(int addr, int opndSize) {
-    assert (opndSize != OpndSize_64);
-    assert(addr != 0);
-
-    Thread *self = dvmThreadSelf();
-    ShadowSpace *shadowSpace = self->shadowSpace;
-    ShadowHeap *heapSpacePtr;
-
-    assert(shadowSpace != 0);
-    assert(shadowSpace->heapSpace != 0);
-    int data = 0;
-
-    for (heapSpacePtr = shadowSpace->heapSpace;
-        heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
-        if (heapSpacePtr->addr == addr) {
-            addr = (unsigned int)(&(heapSpacePtr->data));
-            break;
-        }
-    }
-
-    /* load addr from the shadow heap, native addr-> shadow heap addr
-     * if not found load the data from the native heap
-     */
-    switch (opndSize) {
-        case OpndSize_8:
-            data = *(reinterpret_cast<u1*> (addr));
-            break;
-        case OpndSize_16:
-            data = *(reinterpret_cast<u2*> (addr));
-            break;
-        //signed versions
-        case 0x11:  //signed OpndSize_8
-            data = *(reinterpret_cast<s1*> (addr));
-            break;
-        case 0x22:  //signed OpndSize_16
-            data = *(reinterpret_cast<s2*> (addr));
-            break;
-        case OpndSize_32:
-            data = *(reinterpret_cast<u4*> (addr));
-            break;
-        default:
-            ALOGE("*** ERROR: BAD SIZE IN selfVerificationLoad: %d", opndSize);
-            data = 0;
-            dvmAbort();
-            break;
-    }
-
-#if defined(SELF_VERIFICATION_LOG)
-    ALOGD("*** HEAP LOAD: Addr: %#x Data: %d Size: %d", addr, data, opndSize);
-#endif
-    return data;
-}
-
-void selfVerificationStore(int addr, int data, int opndSize)
-{
-    assert(addr != 0);
-    Thread *self = dvmThreadSelf();
-    ShadowSpace *shadowSpace = self->shadowSpace;
-    ShadowHeap *heapSpacePtr;
-
-    assert(shadowSpace != 0);
-    assert(shadowSpace->heapSpace != 0);
-#if defined(SELF_VERIFICATION_LOG)
-    ALOGD("*** HEAP STORE: Addr: %#x Data: %d Size: %d", addr, data, opndSize);
-#endif
-    for (heapSpacePtr = shadowSpace->heapSpace;
-         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
-        if (heapSpacePtr->addr == addr) {
-            break;
-        }
-    }
-
-    //If the store addr is requested for the first time, its not present in the
-    //heap so add it to the shadow heap.
-    if (heapSpacePtr == shadowSpace->heapSpaceTail) {
-        heapSpacePtr->addr = addr;
-        shadowSpace->heapSpaceTail++;
-        // shadow heap can contain HEAP_SPACE(JIT_MAX_TRACE_LEN) number of entries
-        if(shadowSpace->heapSpaceTail > &(shadowSpace->heapSpace[HEAP_SPACE])) {
-            ALOGD("*** Shadow HEAP store ran out of space, aborting VM");
-            dvmAbort();
-        }
-    }
-
-    addr = ((unsigned int) &(heapSpacePtr->data));
-    switch (opndSize) {
-        case OpndSize_8:
-            *(reinterpret_cast<u1*>(addr)) = data;
-            break;
-        case OpndSize_16:
-            *(reinterpret_cast<u2*>(addr)) = data;
-            break;
-        case OpndSize_32:
-            *(reinterpret_cast<u4*>(addr)) = data;
-            break;
-        default:
-            ALOGE("*** ERROR: BAD SIZE IN selfVerificationSave: %d", opndSize);
-            dvmAbort();
-            break;
-    }
-}
-
-void selfVerificationLoadDoubleword(int addr)
-{
-    assert(addr != 0);
-    Thread *self = dvmThreadSelf();
-    ShadowSpace* shadowSpace = self->shadowSpace;
-    ShadowHeap* heapSpacePtr;
-    int byte_count = 0;
-
-    assert(shadowSpace != 0);
-    assert(shadowSpace->heapSpace != 0);
-    //TODO: do a volatile GET_WIDE implementation
-
-    int addr2 = addr+4;
-    /* load data and data2 from the native heap
-     * so in case this address is not stored in the shadow heap
-     * the value loaded from the native heap is used, else
-     * it is overwritten with the value from the shadow stack
-     */
-    unsigned int data = *(reinterpret_cast<unsigned int*> (addr));
-    unsigned int data2 = *(reinterpret_cast<unsigned int*> (addr2));
-
-    for (heapSpacePtr = shadowSpace->heapSpace;
-         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
-        if (heapSpacePtr->addr == addr) {
-            data = heapSpacePtr->data;
-            byte_count++;
-        } else if (heapSpacePtr->addr == addr2) {
-            data2 = heapSpacePtr->data;
-            byte_count++;
-        }
-        if(byte_count == 2) break;
-    }
-
-#if defined(SELF_VERIFICATION_LOG)
-    ALOGD("*** HEAP LOAD DOUBLEWORD: Addr: %#x Data: %#x Data2: %#x",
-        addr, data, data2);
-#endif
-
-    // xmm6 is scratch; passing value back to aget_common_nohelper in xmm7
-    asm volatile (
-            "movd %0, %%xmm6\n\t"
-            "movd %1, %%xmm7\n\t"
-            "psllq $32, %%xmm6\n\t"
-            "paddq %%xmm6, %%xmm7"
-            :
-            : "rm" (data2), "rm" (data)
-            : "xmm6", "xmm7");
-}
-
-void selfVerificationStoreDoubleword(int addr, s8 double_data)
-{
-    assert(addr != 0);
-
-    Thread *self = dvmThreadSelf();
-    ShadowSpace *shadowSpace = self->shadowSpace;
-    ShadowHeap *heapSpacePtr;
-
-    assert(shadowSpace != 0);
-    assert(shadowSpace->heapSpace != 0);
-
-    int addr2 = addr+4;
-    int data = double_data;
-    int data2 = double_data >> 32;
-    bool store1 = false, store2 = false;
-
-#if defined(SELF_VERIFICATION_LOG)
-    ALOGD("*** HEAP STORE DOUBLEWORD: Addr: %#x Data: %#x, Data2: %#x",
-        addr, data, data2);
-#endif
-
-    //data++; data2++;  // test case for SV detection
-
-    for (heapSpacePtr = shadowSpace->heapSpace;
-         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
-        if (heapSpacePtr->addr == addr) {
-            heapSpacePtr->data = data;
-            store1 = true;
-        } else if (heapSpacePtr->addr == addr2) {
-            heapSpacePtr->data = data2;
-            store2 = true;
-        }
-        if(store1 && store2) {
-            break;
-        }
-    }
-
-    // shadow heap can contain HEAP_SPACE(JIT_MAX_TRACE_LEN) number of entries
-    int additions = store1 ? 1 : 0;
-    additions += store2 ? 1 : 0;
-    if((shadowSpace->heapSpaceTail + additions) >= &(shadowSpace->heapSpace[HEAP_SPACE])) {
-        ALOGD("*** Shadow HEAP store ran out of space, aborting VM");
-        dvmAbort();
-    }
-
-    if (store1 == false) {
-        shadowSpace->heapSpaceTail->addr = addr;
-        shadowSpace->heapSpaceTail->data = data;
-        shadowSpace->heapSpaceTail++;
-    }
-    if (store2 == false) {
-        shadowSpace->heapSpaceTail->addr = addr2;
-        shadowSpace->heapSpaceTail->data = data2;
-        shadowSpace->heapSpaceTail++;
-    }
-}
-
-int call_selfVerificationLoad(void) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("selfVerificationLoad");
-    }
-    typedef int (*vmHelper)(int, int);
-    vmHelper funcPtr = selfVerificationLoad;
-    callFuncPtr((int)funcPtr, "selfVerificationLoad");
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        afterCall("selfVerificationLoad");
-    }
-    return 0;
-}
-
-int call_selfVerificationLoadDoubleword(void) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("selfVerificationLoadDoubleword");
-    }
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = selfVerificationLoadDoubleword;
-    callFuncPtr((int)funcPtr, "selfVerificationLoadDoubleword");
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        afterCall("selfVerificationLoadDoubleword");
-    }
-    return 0;
-}
-
-int call_selfVerificationStore(void) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("selfVerificationStore");
-    }
-    typedef void (*vmHelper)(int, int, int);
-    vmHelper funcPtr = selfVerificationStore;
-    callFuncPtr((int)funcPtr, "selfVerificationStore");
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        afterCall("selfVerificationStore");
-    }
-    return 0;
-}
-
-int call_selfVerificationStoreDoubleword(void) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall("selfVerificationStoreDoubleword");
-    }
-    typedef void (*vmHelper)(int, s8);
-    vmHelper funcPtr = selfVerificationStoreDoubleword;
-    callFuncPtr((int)funcPtr, "selfVerificationStoreDoubleword");
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        afterCall("selfVerificationStoreDoubleword");
-    }
-    return 0;
-}
-#endif
-
-void pushCallerSavedRegs(void) {
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 8, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 4, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, 0, PhysicalReg_ESP, true);
-}
-
-void popCallerSavedRegs(void) {
-    move_mem_to_reg(OpndSize_32, 8, PhysicalReg_ESP, true,  PhysicalReg_EAX, true);
-    move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true,  PhysicalReg_ECX, true);
-    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true,  PhysicalReg_EDX, true);
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-}
-
-//! \brief compareAndExchange with one reg operand and one mem operand
-//! used for implementing monitor-enter
-//! \param size operand size
-//! \param reg src register
-//! \param isPhysical if reg is a physical register
-//! \param disp displacement offset
-//! \param base_reg physical register (PhysicalReg type) or a logical register
-//! \param isBasePhysical if base_reg is a physical register
-void compareAndExchange(OpndSize size,
-             int reg, bool isPhysical,
-             int disp, int base_reg, bool isBasePhysical) {
-    dump_reg_mem(Mnemonic_CMPXCHG, ATOM_NORMAL, size, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, getTypeFromIntSize(size));
-}
-
diff --git a/vm/compiler/codegen/x86/LowerInvoke.cpp b/vm/compiler/codegen/x86/LowerInvoke.cpp
deleted file mode 100644
index d2143ba..0000000
--- a/vm/compiler/codegen/x86/LowerInvoke.cpp
+++ /dev/null
@@ -1,2090 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file LowerInvoke.cpp
-    \brief This file lowers the following bytecodes: INVOKE_XXX
-*/
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "mterp/Mterp.h"
-#include "Lower.h"
-#include "NcgAot.h"
-#include "enc_wrapper.h"
-
-#if defined VTUNE_DALVIK
-#include "compiler/JitProfiling.h"
-#endif
-
-char* streamMisPred = NULL;
-
-/* according to callee, decide the ArgsDoneType*/
-ArgsDoneType convertCalleeToType(const Method* calleeMethod) {
-    if(calleeMethod == NULL)
-        return ArgsDone_Full;
-    if(dvmIsNativeMethod(calleeMethod))
-        return ArgsDone_Native;
-    return ArgsDone_Normal;
-}
-int common_invokeMethodRange(ArgsDoneType,
-        const DecodedInstruction &decodedInst);
-int common_invokeMethodNoRange(ArgsDoneType,
-        const DecodedInstruction &decodedInst);
-void gen_predicted_chain(bool isRange, u2 tmp, int IMMC, bool isInterface,
-        int inputReg, const DecodedInstruction &decodedInst);
-
-//inputs to common_invokeMethodRange: %ecx
-//          common_errNoSuchMethod: %edx
-#define P_GPR_1 PhysicalReg_ESI
-#define P_GPR_2 PhysicalReg_EBX
-#define P_GPR_3 PhysicalReg_ECX
-#define P_SCRATCH_1 PhysicalReg_EDX
-#define PP_GPR_1 PhysicalReg_EBX
-#define PP_GPR_2 PhysicalReg_ESI
-#define PP_GPR_3 PhysicalReg_EAX
-#define PP_GPR_4 PhysicalReg_EDX
-
-#ifdef WITH_JIT_INLINING_PHASE2
-/*
- * The function here takes care the
- * branch over if prediction is correct and the misprediction target for misPredBranchOver.
- */
-static void genLandingPadForMispredictedCallee(MIR* mir) {
-    BasicBlock *fallThrough = traceCurrentBB->fallThrough;
-    /* Bypass the move-result block if there is one */
-    if (fallThrough->firstMIRInsn) {
-        assert(fallThrough->firstMIRInsn->OptimizationFlags & MIR_INLINED_PRED);
-        fallThrough = fallThrough->fallThrough;
-    }
-    /* Generate a branch over if the predicted inlining is correct */
-    jumpToBasicBlock(stream, fallThrough->id);
-    /* Hook up the target to the verification branch */
-    int relativeNCG = stream - streamMisPred;
-    unsigned instSize = encoder_get_inst_size(streamMisPred);
-    relativeNCG -= instSize; //size of the instruction
-    updateJumpInst(streamMisPred, OpndSize_8, relativeNCG);
-}
-#endif
-
-//! LOWER bytecode INVOKE_VIRTUAL without usage of helper function
-
-//!
-int common_invoke_virtual_nohelper(bool isRange, u2 tmp, int vD, const MIR *mir)
-{
-    const DecodedInstruction &decodedInst = mir->dalvikInsn;
-
-#ifdef WITH_JIT_INLINING_PHASE2
-    /*
-     * If the invoke has non-null misPredBranchOver, we need to generate
-     * the non-inlined version of the invoke here to handle the
-     * mispredicted case.
-     */
-    if (mir->meta.callsiteInfo->misPredBranchOver) {
-        genLandingPadForMispredictedCallee (mir);
-    }
-#endif
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    export_pc();
-    beforeCall("exception"); //dump GG, GL VRs
-
-    get_virtual_reg(vD, OpndSize_32, 5, false);
-
-    if ( (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        simpleNullCheck(5, false, vD);
-    }
-#ifndef PREDICTED_CHAINING
-    move_mem_to_reg(OpndSize_32, offObject_clazz, 5, false, 6, false); //clazz of "this"
-    move_mem_to_reg(OpndSize_32, offClassObject_vtable, 6, false, 7, false); //vtable
-    /* method is already resolved in trace-based JIT */
-    int methodIndex =
-                currentMethod->clazz->pDvmDex->pResMethods[tmp]->methodIndex;
-    move_mem_to_reg(OpndSize_32, methodIndex*4, 7, false, PhysicalReg_ECX, true);
-    if(isRange) {
-        common_invokeMethodRange(ArgsDone_Full);
-    }
-    else {
-        common_invokeMethodNoRange(ArgsDone_Full);
-    }
-#else
-    int methodIndex =
-                currentMethod->clazz->pDvmDex->pResMethods[tmp]->methodIndex;
-    gen_predicted_chain(isRange, tmp, methodIndex * 4, false, 5/*tmp5*/,
-            decodedInst);
-#endif
-    ///////////////////////////////////
-    return 0;
-}
-
-#if 0 /* Code is deprecated. If reenabling, must add parameter for decoded instruction */
-//! wrapper to call either common_invoke_virtual_helper or common_invoke_virtual_nohelper
-
-//!
-int common_invoke_virtual(bool isRange, u2 tmp, int vD) {
-    return common_invoke_virtual_nohelper(isRange, tmp, vD);
-}
-#endif
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef P_SCRATCH_1
-#undef PP_GPR_1
-#undef PP_GPR_2
-#undef PP_GPR_3
-#undef PP_GPR_4
-
-#define P_GPR_1 PhysicalReg_ESI
-#define P_GPR_2 PhysicalReg_EBX
-#define P_GPR_3 PhysicalReg_EDX
-#define PP_GPR_1 PhysicalReg_EBX
-#define PP_GPR_2 PhysicalReg_ESI
-#define PP_GPR_3 PhysicalReg_EAX
-#define PP_GPR_4 PhysicalReg_EDX
-//! common section to lower INVOKE_SUPER
-
-//! It will use helper function if the switch is on
-int common_invoke_super(bool isRange, u2 tmp,
-        const DecodedInstruction &decodedInst) {
-    export_pc();
-    beforeCall("exception"); //dump GG, GL VRs
-        ///////////////////////
-        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-#if !defined(WITH_JIT)
-        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-        get_res_methods(3, false);
-        //LR[4] = vB*4(LR[3])
-        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
-        //cmp $0, LR[4]
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-
-        conditional_jump(Condition_NE, ".LinvokeSuper_resolved", true);
-        rememberState(1);
-        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
-        call(".virtual_method_resolve"); //in %eax
-        transferToState(1);
-        if (insertLabel(".LinvokeSuper_resolved", true) == -1)
-            return -1;
-        scratchRegs[0] = PhysicalReg_SCRATCH_3; scratchRegs[1] = PhysicalReg_SCRATCH_4;
-        get_glue_method_class(6, false);
-        move_mem_to_reg(OpndSize_32, offClassObject_super, 6, false, 7, false);
-        movez_mem_to_reg(OpndSize_16, offMethod_methodIndex, PhysicalReg_EAX, true, 8, false);
-        compare_mem_reg(OpndSize_32, offClassObject_vtableCount, 7, false, 8, false);
-
-        conditional_jump_global_API(Condition_NC, ".invoke_super_nsm", false);
-        move_mem_to_reg(OpndSize_32, offClassObject_vtable, 7, false, 9, false);
-        move_mem_scale_to_reg(OpndSize_32, 9, false, 8, false, 4, PhysicalReg_ECX, true);
-        const Method *calleeMethod = NULL;
-#else
-        /* method is already resolved in trace-based JIT */
-        int mIndex = currentMethod->clazz->pDvmDex->
-                pResMethods[tmp]->methodIndex;
-        const Method *calleeMethod =
-                currentMethod->clazz->super->vtable[mIndex];
-        move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
-#endif
-        if(isRange) {
-            common_invokeMethodRange(convertCalleeToType(calleeMethod),
-                    decodedInst);
-        }
-        else {
-            common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
-                    decodedInst);
-        }
- return 0;
-}
-#undef PP_GPR_1
-#undef PP_GPR_2
-#undef PP_GPR_3
-#undef PP_GPR_4
-
-//! \brief helper function to handle no such method error
-//! \return -1 if error, 0 otherwise
-int invoke_super_nsm(void) {
-    if (insertLabel(".invoke_super_nsm", false) == -1)
-        return -1;
-    //NOTE: it seems that the name in %edx is not used in common_errNoSuchMethod
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Method, name), PhysicalReg_EAX, true, PhysicalReg_EDX, true); //method name
-    unconditional_jump("common_errNoSuchMethod", false);
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ESI
-#define P_GPR_3 PhysicalReg_ECX
-#define PP_GPR_1 PhysicalReg_EBX
-#define PP_GPR_2 PhysicalReg_ESI
-#define PP_GPR_3 PhysicalReg_EAX
-#define PP_GPR_4 PhysicalReg_EDX
-//! common section to lower INVOKE_DIRECT
-
-//! It will use helper function if the switch is on
-int common_invoke_direct(bool isRange, u2 tmp, int vD, const MIR *mir)
-{
-    const DecodedInstruction &decodedInst = mir->dalvikInsn;
-    //%ecx can be used as scratch when calling export_pc, get_res_methods and resolve_method
-    export_pc();
-    beforeCall("exception"); //dump GG, GL VRs
-#if !defined(WITH_JIT)
-        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-        get_res_methods(3, false);
-        //LR[4] = vB*4(LR[3])
-        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_ECX, true);
-#endif
-        get_virtual_reg(vD, OpndSize_32, 5, false);
-        if ( (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-        {
-            simpleNullCheck(5, false, vD);
-        }
-#if !defined(WITH_JIT)
-        //cmp $0, LR[4]
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_ECX, true);
-        conditional_jump(Condition_NE, ".LinvokeDirect_resolved", true);
-        rememberState(1);
-        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
-        call_helper_API(".direct_method_resolve"); //in %eax
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
-        transferToState(1);
-        if (insertLabel(".LinvokeDirect_resolved", true) == -1)
-            return -1;
-        const Method *calleeMethod = NULL;
-#else
-        /* method is already resolved in trace-based JIT */
-        const Method *calleeMethod =
-                currentMethod->clazz->pDvmDex->pResMethods[tmp];
-        move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
-#endif
-        //%ecx passed to common_invokeMethod...
-        if(isRange) {
-            common_invokeMethodRange(convertCalleeToType(calleeMethod),
-                    decodedInst);
-        }
-        else {
-            common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
-                    decodedInst);
-        }
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef PP_GPR_1
-#undef PP_GPR_2
-#undef PP_GPR_3
-#undef PP_GPR_4
-
-#define P_GPR_1  PhysicalReg_EBX
-#define P_GPR_3  PhysicalReg_ECX
-#define PP_GPR_1 PhysicalReg_EBX
-#define PP_GPR_2 PhysicalReg_ESI
-#define PP_GPR_3 PhysicalReg_EAX
-#define PP_GPR_4 PhysicalReg_EDX
-//! common section to lower INVOKE_STATIC
-
-//! It will use helper function if the switch is on
-int common_invoke_static(bool isRange, u2 tmp,
-        const DecodedInstruction &decodedInst) {
-    //%ecx can be used as scratch when calling export_pc, get_res_methods and resolve_method
-    export_pc();
-    beforeCall("exception"); //dump GG, GL VRs
-#if !defined(WITH_JIT)
-        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-        scratchRegs[2] = PhysicalReg_Null;      scratchRegs[3] = PhysicalReg_Null;
-        get_res_methods(3, false);
-        //LR[4] = vB*4(LR[3])
-        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_ECX, true);
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_ECX, true);
-
-        conditional_jump(Condition_NE, ".LinvokeStatic_resolved", true);
-        rememberState(1);
-        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
-        call(".static_method_resolve"); //in %eax
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
-        transferToState(1);
-        if (insertLabel(".LinvokeStatic_resolved", true) == -1)
-            return -1;
-        const Method *calleeMethod = NULL;
-#else
-        /* method is already resolved in trace-based JIT */
-        const Method *calleeMethod =
-                currentMethod->clazz->pDvmDex->pResMethods[tmp];
-        move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
-#endif
-        //%ecx passed to common_invokeMethod...
-        if(isRange) {
-            common_invokeMethodRange(convertCalleeToType(calleeMethod),
-                    decodedInst);
-        }
-        else {
-            common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
-                    decodedInst);
-        }
-    return 0;
-}
-#undef P_GPR_1
-#undef PP_GPR_1
-#undef PP_GPR_2
-#undef PP_GPR_3
-#undef PP_GPR_4
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_EAX //scratch
-#define P_GPR_3 PhysicalReg_ECX
-#define P_SCRATCH_1 PhysicalReg_ESI //clazz of object
-#define PP_GPR_1 PhysicalReg_EBX
-#define PP_GPR_2 PhysicalReg_ESI
-#define PP_GPR_3 PhysicalReg_EAX
-#define PP_GPR_4 PhysicalReg_EDX
-//! common section to lower INVOKE_INTERFACE
-
-//! It will use helper function if the switch is on
-int common_invoke_interface(bool isRange, u2 tmp, int vD, const MIR *mir) {
-
-    const DecodedInstruction &decodedInst = mir->dalvikInsn;
-
-#ifdef WITH_JIT_INLINING_PHASE2
-    /*
-     * If the invoke has non-null misPredBranchOver, we need to generate
-     * the non-inlined version of the invoke here to handle the
-     * mispredicted case.
-     */
-    if (mir->meta.callsiteInfo->misPredBranchOver) {
-        genLandingPadForMispredictedCallee (mir);
-    }
-#endif
-    export_pc(); //use %edx
-    beforeCall("exception"); //dump GG, GL VRs
-    ///////////////////////
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    get_virtual_reg(vD, OpndSize_32, 1, false);
-
-    if ( (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        simpleNullCheck(1, false, vD);
-    }
-
-#ifndef PREDICTED_CHAINING
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, tmp, 4, PhysicalReg_ESP, true);
-    /* for trace-based JIT, pDvmDex is a constant at JIT time
-       4th argument to dvmFindInterfaceMethodInCache at -4(%esp) */
-    move_imm_to_mem(OpndSize_32, (int) currentMethod->clazz->pDvmDex, 12, PhysicalReg_ESP, true);
-    move_mem_to_reg(OpndSize_32, offObject_clazz, 1, false, 5, false);
-    /* for trace-based JIT, method is a constant at JIT time
-       3rd argument to dvmFindInterfaceMethodInCache at 8(%esp) */
-    move_imm_to_mem(OpndSize_32, (int) currentMethod, 8, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_3; scratchRegs[1] = PhysicalReg_Null;
-    call_dvmFindInterfaceMethodInCache();
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-
-    conditional_jump_global_API(Condition_E, "common_exceptionThrown", false);
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
-    if(isRange) {
-        common_invokeMethodRange(ArgsDone_Full);
-    }
-    else {
-        common_invokeMethodNoRange(ArgsDone_Full);
-    }
-#else
-        gen_predicted_chain(isRange, tmp, -1, true /*interface*/, 1/*tmp1*/,
-                decodedInst);
-#endif
-    ///////////////////////
-    return 0;
-}
-#undef PP_GPR_1
-#undef PP_GPR_2
-#undef PP_GPR_3
-#undef PP_GPR_4
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef P_SCRATCH_1
-//! lower bytecode INVOKE_VIRTUAL by calling common_invoke_virtual
-
-//!
-int op_invoke_virtual(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    // A|G|op BBBB F|E|D|C
-    // C: the first argument, which is the "this" pointer
-    // A: argument count
-    // C, D, E, F, G: arguments
-    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
-                                               of historical reasons. In reality, first
-                                               argument is in vC */
-    u2 tmp = mir->dalvikInsn.vB;
-    int retval = common_invoke_virtual_nohelper(false/*not range*/, tmp, vD, mir);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_SUPER by calling common_invoke_super
-
-//!
-int op_invoke_super(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    // A|G|op BBBB F|E|D|C
-    // C: the first argument, which is the "this" pointer
-    // A: argument count
-    // C, D, E, F, G: arguments
-    u2 tmp = mir->dalvikInsn.vB;
-    int retval = common_invoke_super(false/*not range*/, tmp, mir->dalvikInsn);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_DIRECT by calling common_invoke_direct
-
-//!
-int op_invoke_direct(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_DIRECT);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    // A|G|op BBBB F|E|D|C
-    // C: the first argument, which is the "this" pointer
-    // A: argument count
-    // C, D, E, F, G: arguments
-    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
-                                               of historical reasons. In reality, first
-                                               argument is in vC */
-    u2 tmp = mir->dalvikInsn.vB;
-    int retval = common_invoke_direct(false/*not range*/, tmp, vD, mir);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_STATIC by calling common_invoke_static
-
-//!
-int op_invoke_static(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_STATIC);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    // A|G|op BBBB F|E|D|C
-    // C: the first argument, which is the "this" pointer
-    // A: argument count
-    // C, D, E, F, G: arguments
-    u2 tmp = mir->dalvikInsn.vB;
-    int retval = common_invoke_static(false/*not range*/, tmp, mir->dalvikInsn);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_INTERFACE by calling common_invoke_interface
-
-//!
-int op_invoke_interface(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_INTERFACE);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    // A|G|op BBBB F|E|D|C
-    // C: the first argument, which is the "this" pointer
-    // A: argument count
-    // C, D, E, F, G: arguments
-    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
-                                               of historical reasons. In reality, first
-                                               argument is in vC */
-    u2 tmp = mir->dalvikInsn.vB;
-    int retval = common_invoke_interface(false/*not range*/, tmp, vD, mir);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_VIRTUAL_RANGE by calling common_invoke_virtual
-
-//!
-int op_invoke_virtual_range(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL_RANGE);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    //AA|op BBBB CCCC
-    //CCCC: the first argument, which is the "this" pointer
-    //AA: argument count
-    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
-                                               of historical reasons. In reality, first
-                                               argument is in vCCCC */
-    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
-    int retval = common_invoke_virtual_nohelper(true/*range*/, tmp, vD, mir);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_SUPER_RANGE by calling common_invoke_super
-
-//!
-int op_invoke_super_range(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER_RANGE);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
-    int retval = common_invoke_super(true/*range*/, tmp, mir->dalvikInsn);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_DIRECT_RANGE by calling common_invoke_direct
-
-//!
-int op_invoke_direct_range(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_DIRECT_RANGE);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
-                                               of historical reasons. In reality, first
-                                               argument is in vCCCC */
-    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
-    int retval = common_invoke_direct(true/*range*/, tmp, vD, mir);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_STATIC_RANGE by calling common_invoke_static
-
-//!
-int op_invoke_static_range(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_STATIC_RANGE);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
-    int retval = common_invoke_static(true/*range*/, tmp, mir->dalvikInsn);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_INTERFACE_RANGE by calling common_invoke_interface
-
-//!
-int op_invoke_interface_range(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_INTERFACE_RANGE);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
-                                               of historical reasons. In reality, first
-                                               argument is in vCCCC */
-    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
-    int retval = common_invoke_interface(true/*range*/, tmp, vD, mir);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart1); //check when helper switch is on
-#endif
-    return retval;
-}
-
-//used %ecx, %edi, %esp %ebp
-#define P_GPR_1 PhysicalReg_EBX
-#define P_SCRATCH_1 PhysicalReg_ESI
-#define P_SCRATCH_2 PhysicalReg_EAX
-#define P_SCRATCH_3 PhysicalReg_EDX
-#define P_SCRATCH_4 PhysicalReg_ESI
-#define P_SCRATCH_5 PhysicalReg_EAX
-
-/**
- * @brief Pass the arguments for invoking method without range
- * @details Use both XMM and gp registers for INVOKE_(VIRTUAL, DIRECT, STATIC, INTERFACE, SUPER)
- * @param decodedInst the decoded Insturction
- * @return 0 always
- */
-int common_invokeMethodNoRange_noJmp(const DecodedInstruction &decodedInst) {
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-    u2 count = decodedInst.vA;
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
-    int offsetFromSaveArea = -4 * count;
-    int numQuad = 0; // keeping track of xmm moves
-    int numMov = 0; // keeping track of gp moves
-    for (int vrNum = 0; vrNum < count; vrNum++) {
-        if (vrNum != 0 && (vrNum + 1 < count) && decodedInst.arg[vrNum] + 1 == decodedInst.arg[vrNum + 1]) {
-            // move 64 bit values from VR to memory if consecutive VRs are to be copied to memory
-            get_virtual_reg(decodedInst.arg[vrNum], OpndSize_64, 22, false);
-            move_reg_to_mem(OpndSize_64, 22, false, offsetFromSaveArea - sizeofStackSaveArea,
-                            PhysicalReg_FP, true);
-            vrNum++;
-            numQuad++;               // keep track of number of 64 bit moves
-            offsetFromSaveArea += 8; // double the offset for 64 bits
-        } else {
-            // move 32 bit values from VR to memory.
-            // We need to use separate temp reg for each case.
-            if (numMov == 4){
-                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 27, false);
-                move_reg_to_mem(OpndSize_32, 27, false, offsetFromSaveArea - sizeofStackSaveArea,
-                                PhysicalReg_FP, true);
-                offsetFromSaveArea += 4;
-                numMov++;
-            }
-            if (numMov == 3){
-                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 26, false);
-                move_reg_to_mem(OpndSize_32, 26, false, offsetFromSaveArea - sizeofStackSaveArea,
-                                PhysicalReg_FP, true);
-                offsetFromSaveArea += 4;
-                numMov++;
-            }
-            if (numMov == 2){
-                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 25, false);
-                move_reg_to_mem(OpndSize_32, 25, false, offsetFromSaveArea - sizeofStackSaveArea,
-                                PhysicalReg_FP, true);
-                offsetFromSaveArea += 4;
-                numMov++;
-            }
-            if (numMov == 1){
-                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 24, false);
-                move_reg_to_mem(OpndSize_32, 24, false, offsetFromSaveArea - sizeofStackSaveArea,
-                                PhysicalReg_FP, true);
-                offsetFromSaveArea += 4;
-                numMov++;
-            }
-            if (numMov == 0){
-                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 23, false);
-                move_reg_to_mem(OpndSize_32, 23, false, offsetFromSaveArea - sizeofStackSaveArea,
-                                PhysicalReg_FP, true);
-                offsetFromSaveArea += 4;
-                numMov++;
-            }
-        }
-    }
-    while(numQuad > 0 && numMov < count){ // refcount update for gp registers not used due to xmm moves
-        updateRefCount2(23+numMov, LowOpndRegType_gp, false);
-        updateRefCount2(23+numMov, LowOpndRegType_gp, false);
-        numMov++;
-    }
-    while(numQuad < 2) { //Max number of arguments is 5. Reading max of 5 VRs needed i.e. upto 2 64 bit moves.
-        updateRefCount2(22, LowOpndRegType_xmm, false);
-        updateRefCount2(22, LowOpndRegType_xmm, false);
-        numQuad++;
-    }
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_invokeMethodNoRange_noJmp");
-    }
-#endif
-    return 0;
-}
-
-int common_invokeMethod_Jmp(ArgsDoneType form) {
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-    nextVersionOfHardReg(PhysicalReg_EDX, 1);
-    move_imm_to_reg(OpndSize_32, (int)rPC, PhysicalReg_EDX, true);
-    //arguments needed in ArgsDone:
-    //    start of HotChainingCell for next bytecode: -4(%esp)
-    //    start of InvokeSingletonChainingCell for callee: -8(%esp)
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    if(!gDvmJit.scheduling) {
-        alignOffset(4); // 4 is (the instruction size of "mov imm32, 4(esp)" - sizeof(imm32))
-        insertChainingWorklist(traceCurrentBB->fallThrough->id, stream);
-    }
-    move_chain_to_mem(OpndSize_32, traceCurrentBB->fallThrough->id, 4, PhysicalReg_ESP, true);
-    // for honeycomb: JNI call doesn't need a chaining cell, so the taken branch is null
-    if(!gDvmJit.scheduling && traceCurrentBB->taken) {
-        alignOffset(3); // 3 is (the instruction size of "mov imm32, 0(esp)" - sizeof(imm32))
-        insertChainingWorklist(traceCurrentBB->taken->id, stream);
-    }
-    int takenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
-    move_chain_to_mem(OpndSize_32, takenId, 0, PhysicalReg_ESP, true);
-    if(form == ArgsDone_Full)
-        unconditional_jump_global_API(".invokeArgsDone_jit", false);
-    else if(form == ArgsDone_Native)
-        unconditional_jump_global_API(".invokeArgsDone_native", false);
-    else
-        unconditional_jump_global_API(".invokeArgsDone_normal", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_invokeMethod_Jmp");
-    }
-#endif
-    return 0;
-}
-
-int common_invokeMethodNoRange(ArgsDoneType form, const DecodedInstruction &decodedInst) {
-    common_invokeMethodNoRange_noJmp(decodedInst);
-    common_invokeMethod_Jmp(form);
-    return 0;
-}
-
-#undef P_GPR_1
-#undef P_SCRATCH_1
-#undef P_SCRATCH_2
-#undef P_SCRATCH_3
-#undef P_SCRATCH_4
-#undef P_SCRATCH_5
-
-//input: %ecx (method to call)
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ESI
-#define P_GPR_3 PhysicalReg_EDX //not used with P_SCRATCH_2
-#define P_SCRATCH_1 PhysicalReg_EAX
-#define P_SCRATCH_2 PhysicalReg_EDX
-#define P_SCRATCH_3 PhysicalReg_EAX
-#define P_SCRATCH_4 PhysicalReg_EDX
-#define P_SCRATCH_5 PhysicalReg_EAX
-#define P_SCRATCH_6 PhysicalReg_EDX
-#define P_SCRATCH_7 PhysicalReg_EAX
-#define P_SCRATCH_8 PhysicalReg_EDX
-#define P_SCRATCH_9 PhysicalReg_EAX
-#define P_SCRATCH_10 PhysicalReg_EDX
-//! pass the arguments for invoking method with range
-
-//! loop is unrolled when count <= 10
-int common_invokeMethodRange_noJmp(const DecodedInstruction &decodedInst) {
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-    u2 count = decodedInst.vA;
-    int vD = decodedInst.vC; //the first argument
-
-    savearea_from_fp(21, false);
-    //vD to rFP-4*count-20
-    //vD+1 to rFP-4*count-20+4 = rFP-20-4*(count-1)
-    if(count >= 1 && count <= 10) {
-        get_virtual_reg(vD, OpndSize_32, 22, false);
-        move_reg_to_mem(OpndSize_32, 22, false, -4*count, 21, false);
-    }
-    if(count >= 2 && count <= 10) {
-        get_virtual_reg(vD+1, OpndSize_32, 23, false);
-        move_reg_to_mem(OpndSize_32, 23, false, -4*(count-1), 21, false);
-    }
-    if(count >= 3 && count <= 10) {
-        get_virtual_reg(vD+2, OpndSize_32, 24, false);
-        move_reg_to_mem(OpndSize_32, 24, false, -4*(count-2), 21, false);
-    }
-    if(count >= 4 && count <= 10) {
-        get_virtual_reg(vD+3, OpndSize_32, 25, false);
-        move_reg_to_mem(OpndSize_32, 25, false, -4*(count-3), 21, false);
-    }
-    if(count >= 5 && count <= 10) {
-        get_virtual_reg(vD+4, OpndSize_32, 26, false);
-        move_reg_to_mem(OpndSize_32, 26, false, -4*(count-4), 21, false);
-    }
-    if(count >= 6 && count <= 10) {
-        get_virtual_reg(vD+5, OpndSize_32, 27, false);
-        move_reg_to_mem(OpndSize_32, 27, false, -4*(count-5), 21, false);
-    }
-    if(count >= 7 && count <= 10) {
-        get_virtual_reg(vD+6, OpndSize_32, 28, false);
-        move_reg_to_mem(OpndSize_32, 28, false, -4*(count-6), 21, false);
-    }
-    if(count >= 8 && count <= 10) {
-        get_virtual_reg(vD+7, OpndSize_32, 29, false);
-        move_reg_to_mem(OpndSize_32, 29, false, -4*(count-7), 21, false);
-    }
-    if(count >= 9 && count <= 10) {
-        get_virtual_reg(vD+8, OpndSize_32, 30, false);
-        move_reg_to_mem(OpndSize_32, 30, false, -4*(count-8), 21, false);
-    }
-    if(count == 10) {
-        get_virtual_reg(vD+9, OpndSize_32, 31, false);
-        move_reg_to_mem(OpndSize_32, 31, false, -4*(count-9), 21, false);
-    }
-    if(count > 10) {
-        //dump to memory first, should we set physicalReg to Null?
-        //this bytecodes uses a set of virtual registers (update getVirtualInfo)
-        //this is necessary to correctly insert transfer points
-        int k;
-        for(k = 0; k < count; k++) {
-            spillVirtualReg(vD+k, LowOpndRegType_gp, true); //will update refCount
-        }
-        load_effective_addr(4*vD, PhysicalReg_FP, true, 12, false);
-        alu_binary_imm_reg(OpndSize_32, sub_opc, 4*count, 21, false);
-        move_imm_to_reg(OpndSize_32, count, 13, false);
-        if (insertLabel(".invokeMethod_1", true) == -1) //if checkDup: will perform work from ShortWorklist
-            return -1;
-        rememberState(1);
-        move_mem_to_reg(OpndSize_32, 0, 12, false, 14, false);
-        move_reg_to_mem(OpndSize_32, 14, false, 0, 21, false);
-        load_effective_addr(4, 12, false, 12, false);
-        alu_binary_imm_reg(OpndSize_32, sub_opc, 1, 13, false);
-        load_effective_addr(4, 21, false, 21, false);
-        transferToState(1);
-        conditional_jump(Condition_NE, ".invokeMethod_1", true); //backward branch
-    }
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_invokeMethodRange_noJmp");
-    }
-#endif
-    return 0;
-}
-
-int common_invokeMethodRange(ArgsDoneType form, const DecodedInstruction &decodedInst) {
-    common_invokeMethodRange_noJmp(decodedInst);
-    common_invokeMethod_Jmp(form);
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef P_SCRATCH_1
-#undef P_SCRATCH_2
-#undef P_SCRATCH_3
-#undef P_SCRATCH_4
-#undef P_SCRATCH_5
-#undef P_SCRATCH_6
-#undef P_SCRATCH_7
-#undef P_SCRATCH_8
-#undef P_SCRATCH_9
-#undef P_SCRATCH_10
-
-//! spill a register to native stack
-
-//! decrease %esp by 4, then store a register at 0(%esp)
-int spill_reg(int reg, bool isPhysical) {
-    load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, reg, isPhysical, 0, PhysicalReg_ESP, true);
-    return 0;
-}
-//! get a register from native stack
-
-//! load a register from 0(%esp), then increase %esp by 4
-int unspill_reg(int reg, bool isPhysical) {
-    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, reg, isPhysical);
-    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    return 0;
-}
-
-int generate_invokeNative(void); //forward declaration
-int generate_stackOverflow(void); //forward declaration
-
-const char *dvmCompilerHandleInvokeArgsHeader (int value)
-{
-    ArgsDoneType form = static_cast<ArgsDoneType> (value);
-
-    // Insert different labels for the various forms
-    const char * sectionLabel = 0;
-
-    //Look at the form
-    switch (form)
-    {
-        case ArgsDone_Full:
-            sectionLabel = ".invokeArgsDone_jit";
-            break;
-        case ArgsDone_Normal:
-            sectionLabel = ".invokeArgsDone_normal";
-            break;
-        default:
-            sectionLabel = ".invokeArgsDone_native";
-            break;
-    }
-
-    return sectionLabel;
-}
-
-/**
- * @brief Common code to invoke a method after all of the arguments
- * are handled.
- * @details Requires that ECX holds the method to be called.
- * @param form Used to decide which variant to generate which may contain
- * fewer instructions than a full implementation. For invokeNativeSingle
- * use ArgsDone_Native. For invokeNonNativeSingle use ArgsDone_Normal.
- * To dynamically determine which one to choose (the full implementation)
- * use ArgsDone_Full.
- * @return value >= 0 on success
- * @todo Since this section is static code that is dynamically generated,
- * it can be written directly in assembly and built at compile time.
- */
-int common_invokeArgsDone(ArgsDoneType form) {
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
-
-    // Define scratch registers
-    scratchRegs[0] = PhysicalReg_EBX;
-    scratchRegs[1] = PhysicalReg_ESI;
-    scratchRegs[2] = PhysicalReg_EDX;
-    scratchRegs[3] = PhysicalReg_Null;
-
-    //Get the callback
-    const char* (*backEndInvokeArgsDone) (int) = gDvmJit.jitFramework.backEndInvokeArgsDone;
-
-    //We actually need this call back for this backend
-    assert (backEndInvokeArgsDone != 0);
-
-    const char *sectionLabel = 0;
-
-    //If we don't have a backend handler, bail
-    if (backEndInvokeArgsDone == 0)
-    {
-        SET_JIT_ERROR (kJitErrorPlugin);
-        return -1;
-    }
-
-    //Get the section label
-    sectionLabel = backEndInvokeArgsDone (form);
-
-    //If we don't have a section label, bail
-    if (sectionLabel == 0)
-    {
-        SET_JIT_ERROR (kJitErrorTraceFormation);
-        return -1;
-    }
-
-    //If we can't insert a label, bail
-    if (insertLabel(sectionLabel, false) == -1)
-    {
-        return -1;
-    }
-
-    // Determine how many ins+locals we have
-    movez_mem_to_reg(OpndSize_16, OFFSETOF_MEMBER(Method,registersSize),
-            PhysicalReg_ECX, true, PhysicalReg_EAX, true);
-
-    // Determine the offset by multiplying size of 4 with how many ins+locals we have
-    alu_binary_imm_reg(OpndSize_32, shl_opc, 2, PhysicalReg_EAX, true);
-
-    // Load save area
-    savearea_from_fp(PhysicalReg_ESI, true);
-
-    // Computer the new FP (old save area - regsSize)
-    alu_binary_reg_reg(OpndSize_32, sub_opc, PhysicalReg_EAX, true,
-            PhysicalReg_ESI, true);
-
-    // Get pointer to self Thread
-    get_self_pointer(PhysicalReg_EAX, true);
-
-    // Make a copy of the new FP
-    move_reg_to_reg(OpndSize_32, PhysicalReg_ESI, true, PhysicalReg_EBX, true);
-
-    // Set newSaveArea->savedPc
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true,
-            OFFSETOF_MEMBER(StackSaveArea,savedPc) - sizeofStackSaveArea,
-            PhysicalReg_ESI, true);
-
-    // Load the size of stack save area into register
-    alu_binary_imm_reg(OpndSize_32, sub_opc, sizeofStackSaveArea,
-            PhysicalReg_ESI, true);
-
-    // Determine how many outs we have
-    movez_mem_to_reg(OpndSize_16, OFFSETOF_MEMBER(Method,outsSize),
-            PhysicalReg_ECX, true, PhysicalReg_EDX, true);
-
-    // Determine the offset by multiplying size of 4 with how many outs we have
-    alu_binary_imm_reg(OpndSize_32, shl_opc, 2, PhysicalReg_EDX, true);
-
-    // Calculate the bottom, namely newSaveArea - outsSize
-    alu_binary_reg_reg(OpndSize_32, sub_opc, PhysicalReg_EDX, true,
-            PhysicalReg_ESI, true);
-
-    // Set newSaveArea->prevFrame
-    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true,
-            OFFSETOF_MEMBER(StackSaveArea,prevFrame) - sizeofStackSaveArea,
-            PhysicalReg_EBX, true);
-
-    // Compare self->interpStackEnd and bottom
-    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(Thread, interpStackEnd),
-            PhysicalReg_EAX, true, PhysicalReg_ESI, true);
-
-    // Handle frame overflow
-    conditional_jump(Condition_B, ".stackOverflow", true);
-
-    if (form == ArgsDone_Full) {
-        // Check for a native call
-        test_imm_mem(OpndSize_32, ACC_NATIVE,
-                OFFSETOF_MEMBER(Method,accessFlags), PhysicalReg_ECX, true);
-    }
-
-    // Set newSaveArea->method
-    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true,
-            OFFSETOF_MEMBER(StackSaveArea,method) - sizeofStackSaveArea,
-            PhysicalReg_EBX, true);
-
-    if (form == ArgsDone_Native || form == ArgsDone_Full) {
-        // to correctly handle code cache reset:
-        //  update returnAddr and check returnAddr after done with the native method
-        //  if returnAddr is set to NULL during code cache reset,
-        //  the execution will correctly continue with interpreter
-        // get returnAddr from 4(%esp) and update the save area with it
-        move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true, PhysicalReg_EDX,
-                true);
-        move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true,
-                OFFSETOF_MEMBER(StackSaveArea,returnAddr) - sizeofStackSaveArea,
-                PhysicalReg_EBX, true);
-    }
-
-    if (form == ArgsDone_Native) {
-        // Since we know we are invoking native method, generate code for the
-        // native invoke and the invoke implementation is done.
-        if (generate_invokeNative() == -1)
-            return -1;
-
-#if defined VTUNE_DALVIK
-        if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-            int endStreamPtr = (int)stream;
-            sendLabelInfoToVTune(startStreamPtr, endStreamPtr, sectionLabel);
-        }
-#endif
-        return 0;
-    }
-
-    if (form == ArgsDone_Full) {
-        // Since we are generating the full implementation, we just did the
-        // check for native method and can now go do the native invoke
-        conditional_jump(Condition_NE, ".invokeNative", true);
-    }
-
-    // Get method->clazz
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Method,clazz), PhysicalReg_ECX,
-            true, PhysicalReg_EDX, true);
-
-    // Update frame pointer with the new FP
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EBX, true, PhysicalReg_FP, true);
-
-    // Get pointer to self Thread
-    get_self_pointer(PhysicalReg_EBX, true);
-
-    // Get method->clazz->pDvmDex
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ClassObject,pDvmDex),
-            PhysicalReg_EDX, true, PhysicalReg_EDX, true);
-
-    // Set self->methodClassDex with method->clazz->pDvmDex
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true,
-            OFFSETOF_MEMBER(Thread, interpSave.methodClassDex), PhysicalReg_EBX,
-            true);
-
-    // Set self->curFrame to the new FP
-    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true,
-            OFFSETOF_MEMBER(Thread,interpSave.curFrame), PhysicalReg_EBX, true);
-
-    // returnAddr updated already for Full. Get returnAddr from 4(%esp)
-    if (form == ArgsDone_Normal) {
-        move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true, PhysicalReg_EDX,
-                true);
-    }
-
-    // Set self->method with method to call
-    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true,
-            OFFSETOF_MEMBER(Thread, interpSave.method), PhysicalReg_EBX, true);
-
-    // Place starting bytecode in EBX for dvmJitToInterp
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Method,insns), PhysicalReg_ECX,
-            true, PhysicalReg_EBX, true);
-
-    if (form == ArgsDone_Normal) {
-        // We have obtained the return address and now we can actually update it
-        move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true,
-                OFFSETOF_MEMBER(StackSaveArea,returnAddr) - sizeofStackSaveArea,
-                PhysicalReg_FP, true);
-    }
-
-    if (insertLabel(".invokeInterp", true) == -1)
-        return -1;
-
-    bool callNoChain = false;
-#ifdef PREDICTED_CHAINING
-    if (form == ArgsDone_Full)
-        callNoChain = true;
-#endif
-
-    if (callNoChain) {
-        scratchRegs[0] = PhysicalReg_EAX;
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-#if defined(WITH_JIT_TUNING)
-        // Predicted chaining failed. Fall back to interpreter and indicated
-        // inline cache miss.
-        move_imm_to_reg(OpndSize_32, kInlineCacheMiss, PhysicalReg_EDX, true);
-#endif
-        call_dvmJitToInterpTraceSelectNoChain(); //input: rPC in %ebx
-    } else {
-        // Jump to the stub at (%esp)
-        move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EDX,
-                true);
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        unconditional_jump_reg(PhysicalReg_EDX, true);
-    }
-
-    if (form == ArgsDone_Full) {
-        // Generate code for handling native invoke
-        if (generate_invokeNative() == -1)
-            return -1;
-    }
-
-    // Generate code for handling stack overflow
-    if (generate_stackOverflow() == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, sectionLabel);
-    }
-#endif
-    return 0;
-}
-
-/* when WITH_JIT is true,
-     JIT'ed code invokes native method, after invoke, execution will continue
-     with the interpreter or with JIT'ed code if chained
-*/
-int generate_invokeNative() {
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
-
-    if (insertLabel(".invokeNative", true) == -1)
-        return -1;
-
-    //if(!generateForNcg)
-    //    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    load_effective_addr(-28, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EBX, true, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EBX, true, 20, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_EDX;
-    get_self_pointer(PhysicalReg_EAX, true); //glue->self
-    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 8, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 12, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 24, PhysicalReg_ESP, true);
-    move_mem_to_reg(OpndSize_32, offThread_jniLocal_nextEntry, PhysicalReg_EAX, true, PhysicalReg_EDX, true); //get self->local_next
-    scratchRegs[1] = PhysicalReg_EAX;
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, OFFSETOF_MEMBER(StackSaveArea, xtra.currentPc) - sizeofStackSaveArea, PhysicalReg_EBX, true); //update jniLocalRef of stack
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EBX, true, OFFSETOF_MEMBER(Thread, interpSave.curFrame), PhysicalReg_EAX, true); //set self->curFrame
-    move_imm_to_mem(OpndSize_32, 0, OFFSETOF_MEMBER(Thread, inJitCodeCache), PhysicalReg_EAX, true); //clear self->inJitCodeCache
-    load_effective_addr(OFFSETOF_MEMBER(Thread, interpSave.retval), PhysicalReg_EAX, true, PhysicalReg_EAX, true); //self->retval
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 4, PhysicalReg_ESP, true);
-    //NOTE: native method checks the interpreted stack for arguments
-    //      The immediate arguments on native stack: address of return value, new FP, self
-    call_mem(40, PhysicalReg_ECX, true);//*40(%ecx)
-    //we can't assume the argument stack is unmodified after the function call
-    //duplicate newFP & glue->self on stack: newFP (-28 & -8) glue->self (-16 & -4)
-    move_mem_to_reg(OpndSize_32, 20, PhysicalReg_ESP, true, PhysicalReg_ESI, true); //new FP
-    move_mem_to_reg(OpndSize_32, 24, PhysicalReg_ESP, true, PhysicalReg_EBX, true); //glue->self
-    load_effective_addr(28, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(StackSaveArea, xtra.currentPc) - sizeofStackSaveArea, PhysicalReg_ESI, true, PhysicalReg_EAX, true); //newSaveArea->jniLocal
-    compare_imm_mem(OpndSize_32, 0, OFFSETOF_MEMBER(Thread, exception), PhysicalReg_EBX, true); //self->exception
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    //NOTE: PhysicalReg_FP should be callee-saved register
-    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true, OFFSETOF_MEMBER(Thread, interpSave.curFrame), PhysicalReg_EBX, true); //set self->curFrame
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, offThread_jniLocal_nextEntry, PhysicalReg_EBX, true); //set self->jniLocal
-    conditional_jump(Condition_NE, "common_exceptionThrown", false);
-
-    //get returnAddr, if it is not NULL,
-    //    return to JIT'ed returnAddr after executing the native method
-    /* to correctly handle code cache reset:
-       update returnAddr and check returnAddr after done with the native method
-       if returnAddr is set to NULL during code cache reset,
-       the execution will correctly continue with interpreter */
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(StackSaveArea, returnAddr)-sizeofStackSaveArea, PhysicalReg_ESI, true, PhysicalReg_EDX, true);
-    //set self->inJitCodeCache to returnAddr (PhysicalReg_EBX is in %ebx)
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, OFFSETOF_MEMBER(Thread, inJitCodeCache), PhysicalReg_EBX, true);
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(StackSaveArea, savedPc) - sizeofStackSaveArea, PhysicalReg_ESI, true, PhysicalReg_EBX, true); //savedPc
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EDX, true);
-    conditional_jump(Condition_E, ".nativeToInterp", true);
-    unconditional_jump_reg(PhysicalReg_EDX, true);
-    //if returnAddr is NULL, return to interpreter after executing the native method
-    if (insertLabel(".nativeToInterp", true) == -1) {
-        return -1;
-    }
-    //move rPC by 6 (3 bytecode units for INVOKE)
-    alu_binary_imm_reg(OpndSize_32, add_opc, 6, PhysicalReg_EBX, true);
-    scratchRegs[0] = PhysicalReg_EAX;
-#if defined(WITH_JIT_TUNING)
-        /* Return address not in code cache. Indicate that continuing with interpreter
-         */
-        move_imm_to_reg(OpndSize_32, kCallsiteInterpreted, PhysicalReg_EDX, true);
-#endif
-    call_dvmJitToInterpTraceSelectNoChain(); //rPC in %ebx
-    return 0;
-}
-
-int generate_stackOverflow() {
-    if (insertLabel(".stackOverflow", true) == -1)
-        return -1;
-    //load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 4, PhysicalReg_ESP, true);
-    get_self_pointer(PhysicalReg_EBX, true); //glue->self
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EBX, true, 0, PhysicalReg_ESP, true);
-    call_dvmHandleStackOverflow();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    unconditional_jump("common_exceptionThrown", false);
-    return 0;
-}
-
-/////////////////////////////////////////////
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-#define P_SCRATCH_1 PhysicalReg_ESI
-#define P_SCRATCH_2 PhysicalReg_EDX
-#define P_SCRATCH_3 PhysicalReg_ESI
-#define P_SCRATCH_4 PhysicalReg_EDX
-//! lower bytecode EXECUTE_INLINE
-
-//!
-int op_execute_inline(const MIR * mir, bool isRange) {
-    assert(mir->dalvikInsn.opcode == OP_EXECUTE_INLINE
-            || mir->dalvikInsn.opcode == OP_EXECUTE_INLINE_RANGE);
-    int num = mir->dalvikInsn.vA;
-    u2 tmp = mir->dalvikInsn.vB;
-    int vC, vD, vE, vF;
-    if(!isRange) {
-        // Note that vC, vD, vE, and vF might have bad values
-        // depending on count. The variable "num" should be
-        // checked before using any of these.
-        vC = mir->dalvikInsn.arg[0];
-        vD = mir->dalvikInsn.arg[1];
-        vE = mir->dalvikInsn.arg[2];
-        vF = mir->dalvikInsn.arg[3];
-    } else {
-        vC = mir->dalvikInsn.vC;
-        vD = vC + 1;
-        vE = vC + 2;
-        vF = vC + 3;
-    }
-    export_pc();
-    switch (tmp) {
-        case INLINE_EMPTYINLINEMETHOD:
-            return 0;  /* Nop */
-        case INLINE_STRING_LENGTH:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            compare_imm_reg(OpndSize_32, 0, 1, false);
-            conditional_jump(Condition_NE, ".do_inlined_string_length", true);
-            scratchRegs[0] = PhysicalReg_SCRATCH_1;
-            rememberState(1);
-            beforeCall("exception"); //dump GG, GL VRs
-            unconditional_jump("common_errNullObject", false);
-            goToState(1);
-            if (insertLabel(".do_inlined_string_length", true) == -1)
-                return -1;
-            move_mem_to_reg(OpndSize_32, 0x14, 1, false, 2, false);
-            get_self_pointer(3, false);
-            move_reg_to_mem(OpndSize_32, 2, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
-            return 0;
-        case INLINE_STRING_IS_EMPTY:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            compare_imm_reg(OpndSize_32, 0, 1, false);
-            conditional_jump(Condition_NE, ".do_inlined_string_length", true);
-            scratchRegs[0] = PhysicalReg_SCRATCH_1;
-            rememberState(1);
-            beforeCall("exception"); //dump GG, GL VRs
-            unconditional_jump("common_errNullObject", false);
-            goToState(1);
-            if (insertLabel(".do_inlined_string_length", true) == -1)
-                return -1;
-            compare_imm_mem(OpndSize_32, 0, 0x14, 1, false);
-            conditional_jump(Condition_E, ".inlined_string_length_return_true",
-                             true);
-            get_self_pointer(2, false);
-            move_imm_to_mem(OpndSize_32, 0, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
-            unconditional_jump(".inlined_string_length_done", true);
-            if (insertLabel(".inlined_string_length_return_true", true) == -1)
-                return -1;
-            get_self_pointer(2, false);
-            move_imm_to_mem(OpndSize_32, 1, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
-            if (insertLabel(".inlined_string_length_done", true) == -1)
-                return -1;
-            return 0;
-        case INLINE_MATH_ABS_INT:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            move_reg_to_reg(OpndSize_32, 1, false, 2, false);
-            alu_binary_imm_reg(OpndSize_32, sar_opc, 0x1f, 2, false);
-            alu_binary_reg_reg(OpndSize_32, xor_opc, 2, false, 1, false);
-            alu_binary_reg_reg(OpndSize_32, sub_opc, 2, false, 1, false);
-            get_self_pointer(3, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
-            return 0;
-        case INLINE_MATH_ABS_LONG:
-            get_virtual_reg(vD, OpndSize_32, 1, false);
-            move_reg_to_reg(OpndSize_32, 1, false, 2, false);
-            alu_binary_imm_reg(OpndSize_32, sar_opc, 0x1f, 1, false);
-            move_reg_to_reg(OpndSize_32, 1, false, 3, false);
-            move_reg_to_reg(OpndSize_32, 1, false, 4, false);
-            get_virtual_reg(vC, OpndSize_32, 5, false);
-            alu_binary_reg_reg(OpndSize_32, xor_opc, 5, false, 1, false);
-            get_self_pointer(6, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 6, false);
-            alu_binary_reg_reg(OpndSize_32, xor_opc, 2, false, 3, false);
-            move_reg_to_mem(OpndSize_32, 3, false, 4 + OFFSETOF_MEMBER(Thread, interpSave.retval), 6, false);
-            alu_binary_reg_mem(OpndSize_32, sub_opc, 4, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 6, false);
-            alu_binary_reg_mem(OpndSize_32, sbb_opc, 4, false, 4 + OFFSETOF_MEMBER(Thread, interpSave.retval), 6, false);
-            return 0;
-        case INLINE_MATH_MAX_INT:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            get_virtual_reg(vD, OpndSize_32, 2, false);
-            compare_reg_reg(1, false, 2, false);
-            conditional_move_reg_to_reg(OpndSize_32, Condition_GE, 2,
-                                        false/*src*/, 1, false/*dst*/);
-            get_self_pointer(3, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
-            return 0;
-        case INLINE_MATH_MIN_INT:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            get_virtual_reg(vD, OpndSize_32, 2, false);
-            compare_reg_reg(1, false, 2, false);
-            conditional_move_reg_to_reg(OpndSize_32, Condition_LE, 2,
-                                        false/*src*/, 1, false/*dst*/);
-            get_self_pointer(3, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
-            return 0;
-        case INLINE_MATH_ABS_FLOAT:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            alu_binary_imm_reg(OpndSize_32, and_opc, 0x7fffffff, 1, false);
-            get_self_pointer(2, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
-            return 0;
-        case INLINE_MATH_ABS_DOUBLE:
-            get_virtual_reg(vC, OpndSize_64, 1, false);
-            alu_binary_mem_reg(OpndSize_64, and_opc, LvaluePosInfLong, PhysicalReg_Null, true, 1, false);
-            get_self_pointer(2, false);
-            move_reg_to_mem(OpndSize_64, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
-            return 0;
-        case INLINE_STRING_CHARAT:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            compare_imm_reg(OpndSize_32, 0, 1, false);
-            conditional_jump(Condition_NE, ".inlined_string_CharAt_arg_validate_1", true);
-            rememberState(1);
-            beforeCall("exception");
-            unconditional_jump("common_errNullObject", false);
-            goToState(1);
-            if (insertLabel(".inlined_string_CharAt_arg_validate_1", true) == -1)
-                return -1;
-            get_virtual_reg(vD, OpndSize_32, 2, false);
-            compare_mem_reg(OpndSize_32, 0x14, 1, false, 2, false);
-            conditional_jump(Condition_L, ".inlined_string_CharAt_arg_validate_2", true);
-            rememberState(2);
-            beforeCall("exception");
-            unconditional_jump("common_errStringIndexOutOfBounds", false);
-            goToState(2);
-            if (insertLabel(".inlined_string_CharAt_arg_validate_2", true) == -1)
-                return -1;
-            compare_imm_reg(OpndSize_32, 0, 2, false);
-            conditional_jump(Condition_NS, ".do_inlined_string_CharAt", true);
-            rememberState(3);
-            beforeCall("exception");
-            unconditional_jump("common_errStringIndexOutOfBounds", false);
-            goToState(3);
-            if (insertLabel(".do_inlined_string_CharAt", true) == -1)
-                return -1;
-            alu_binary_mem_reg(OpndSize_32, add_opc, 0x10, 1, false, 2, false);
-            move_mem_to_reg(OpndSize_32, 0x8, 1, false, 1, false);
-            movez_mem_disp_scale_to_reg(OpndSize_16, 1, false, OFFSETOF_MEMBER(ArrayObject, contents)/*disp*/, 2, false, 2, 2, false);
-            get_self_pointer(3, false);
-            move_reg_to_mem(OpndSize_32, 2, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
-            return 0;
-        case INLINE_STRING_FASTINDEXOF_II:
-#if defined(USE_GLOBAL_STRING_DEFS)
-            break;
-#else
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            compare_imm_reg(OpndSize_32, 0, 1, false);
-            get_virtual_reg(vD, OpndSize_32, 2, false);
-            get_virtual_reg(vE, OpndSize_32, 3, false);
-            conditional_jump(Condition_NE, ".do_inlined_string_fastIndexof",
-                             true);
-            scratchRegs[0] = PhysicalReg_SCRATCH_1;
-            rememberState(1);
-            beforeCall("exception"); //dump GG, GL VRs
-            unconditional_jump("common_errNullObject", false);
-            goToState(1);
-            if (insertLabel(".do_inlined_string_fastIndexof", true) == -1)
-                return -1;
-            move_mem_to_reg(OpndSize_32, 0x14, 1, false, 4, false);
-            move_mem_to_reg(OpndSize_32, 0x8, 1, false, 5, false);
-            move_mem_to_reg(OpndSize_32, 0x10, 1, false, 6, false);
-            alu_binary_reg_reg(OpndSize_32, xor_opc, 1, false, 1, false);
-            compare_imm_reg(OpndSize_32, 0, 3, false);
-            conditional_move_reg_to_reg(OpndSize_32, Condition_NS, 3, false, 1,
-                                        false);
-            compare_reg_reg(4, false, 1, false);
-            conditional_jump(Condition_GE,
-                             ".do_inlined_string_fastIndexof_exitfalse", true);
-            dump_mem_scale_reg(Mnemonic_LEA, OpndSize_32, 5, false, OFFSETOF_MEMBER(ArrayObject, contents)/*disp*/,
-                               6, false, 2, 5, false, LowOpndRegType_gp);
-            movez_mem_disp_scale_to_reg(OpndSize_16, 5, false, 0, 1, false, 2,
-                                        3, false);
-            compare_reg_reg(3, false, 2, false);
-            conditional_jump(Condition_E, ".do_inlined_string_fastIndexof_exit",
-                             true);
-            load_effective_addr(0x1, 1, false, 3, false);
-            load_effective_addr_scale(5, false, 3, false, 2, 5, false);
-            unconditional_jump(".do_inlined_string_fastIndexof_iter", true);
-            if (insertLabel(".do_inlined_string_fastIndexof_ch_cmp", true) == -1)
-                return -1;
-            if(gDvm.executionMode == kExecutionModeNcgO1) {
-                rememberState(1);
-            }
-            movez_mem_to_reg(OpndSize_16, 0, 5, false, 6, false);
-            load_effective_addr(0x2, 5, false, 5, false);
-            compare_reg_reg(6, false, 2, false);
-            conditional_jump(Condition_E, ".do_inlined_string_fastIndexof_exit",
-                             true);
-            load_effective_addr(0x1, 3, false, 3, false);
-            if (insertLabel(".do_inlined_string_fastIndexof_iter", true) == -1)
-                return -1;
-            compare_reg_reg(4, false, 3, false);
-            move_reg_to_reg(OpndSize_32, 3, false, 1, false);
-            if(gDvm.executionMode == kExecutionModeNcgO1) {
-                transferToState(1);
-            }
-            conditional_jump(Condition_NE,
-                             ".do_inlined_string_fastIndexof_ch_cmp", true);
-            if (insertLabel(".do_inlined_string_fastIndexof_exitfalse", true) == -1)
-                return -1;
-            move_imm_to_reg(OpndSize_32, 0xffffffff, 1,  false);
-            if (insertLabel(".do_inlined_string_fastIndexof_exit", true) == -1)
-                return -1;
-            get_self_pointer(7, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 7, false);
-            return 0;
-        case INLINE_FLOAT_TO_RAW_INT_BITS:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            get_self_pointer(2, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
-            return 0;
-        case INLINE_INT_BITS_TO_FLOAT:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            get_self_pointer(2, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
-            return 0;
-        case INLINE_DOUBLE_TO_RAW_LONG_BITS:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            get_self_pointer(3, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
-            get_virtual_reg(vD, OpndSize_32, 2, false);
-            move_reg_to_mem(OpndSize_32, 2, false, 4 + OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
-            return 0;
-        case INLINE_LONG_BITS_TO_DOUBLE:
-            get_virtual_reg(vC, OpndSize_32, 1, false);
-            get_virtual_reg(vD, OpndSize_32, 2, false);
-            get_self_pointer(3, false);
-            move_reg_to_mem(OpndSize_32, 2, false, 4 + OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
-            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
-            return 0;
-        default:
-                break;
-    }
-#endif
-    get_self_pointer(PhysicalReg_SCRATCH_1, false);
-    load_effective_addr(OFFSETOF_MEMBER(Thread, interpSave.retval), PhysicalReg_SCRATCH_1, false, 1, false);
-    load_effective_addr(-24, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 1, false, 16, PhysicalReg_ESP, true);
-    if(num >= 1) {
-        get_virtual_reg(vC, OpndSize_32, 2, false);
-        move_reg_to_mem(OpndSize_32, 2, false, 0, PhysicalReg_ESP, true);
-    }
-    if(num >= 2) {
-        get_virtual_reg(vD, OpndSize_32, 3, false);
-        move_reg_to_mem(OpndSize_32, 3, false, 4, PhysicalReg_ESP, true);
-    }
-    if(num >= 3) {
-        get_virtual_reg(vE, OpndSize_32, 4, false);
-        move_reg_to_mem(OpndSize_32, 4, false, 8, PhysicalReg_ESP, true);
-    }
-    if(num >= 4) {
-        get_virtual_reg(vF, OpndSize_32, 5, false);
-        move_reg_to_mem(OpndSize_32, 5, false, 12, PhysicalReg_ESP, true);
-    }
-    beforeCall("execute_inline");
-    load_imm_global_data_API("gDvmInlineOpsTable", OpndSize_32, 6, false);
-    call_mem(16*tmp, 6, false);//
-    afterCall("execute_inline");
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-
-    load_effective_addr(24, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    conditional_jump(Condition_NE, ".execute_inline_done", true);
-    //jump to dvmJitToExceptionThrown
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    jumpToExceptionThrown(1/*exception number*/);
-    if (insertLabel(".execute_inline_done", true) == -1)
-        return -1;
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_SCRATCH_1
-#undef P_SCRATCH_2
-#undef P_SCRATCH_3
-#undef P_SCRATCH_4
-
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_SCRATCH_1 PhysicalReg_ESI
-#define P_SCRATCH_2 PhysicalReg_EDX
-#define PP_GPR_1 PhysicalReg_EBX
-#define PP_GPR_2 PhysicalReg_ESI
-#define PP_GPR_3 PhysicalReg_EAX
-#define PP_GPR_4 PhysicalReg_EDX
-//! common code for INVOKE_VIRTUAL_QUICK
-
-//! It uses helper function if the switch is on
-int common_invoke_virtual_quick(bool hasRange, int vD, u2 IMMC, const MIR *mir) {
-
-    const DecodedInstruction &decodedInst = mir->dalvikInsn;
-
-#ifdef WITH_JIT_INLINING_PHASE2
-    /*
-     * If the invoke has non-null misPredBranchOver, we need to generate
-     * the non-inlined version of the invoke here to handle the
-     * mispredicted case.
-     */
-    if (mir->meta.callsiteInfo->misPredBranchOver) {
-        genLandingPadForMispredictedCallee (mir);
-    }
-#endif
-    export_pc();
-    beforeCall("exception"); //dump GG, GL VRs
-    /////////////////////////////////////////////////
-    get_virtual_reg(vD, OpndSize_32, 1, false);
-    if ( (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        simpleNullCheck(1, false, vD);
-    }
-#ifndef PREDICTED_CHAINING
-    move_mem_to_reg(OpndSize_32, 0, 1, false, 2, false);
-    move_mem_to_reg(OpndSize_32, offClassObject_vtable, 2, false, 3, false);
-    move_mem_to_reg(OpndSize_32, IMMC, 3, false, PhysicalReg_ECX, true);
-
-    if(hasRange) {
-        common_invokeMethodRange(ArgsDone_Full);
-    }
-    else {
-        common_invokeMethodNoRange(ArgsDone_Full);
-    }
-#else
-    gen_predicted_chain(hasRange, -1, IMMC, false, 1/*tmp1*/, decodedInst);
-#endif
-    ////////////////////////
-    return 0;
-}
-#undef P_GPR_1
-#undef P_SCRATCH_1
-#undef P_SCRATCH_2
-#undef PP_GPR_1
-#undef PP_GPR_2
-#undef PP_GPR_3
-#undef PP_GPR_4
-//! lower bytecode INVOKE_VIRTUAL_QUICK by calling common_invoke_virtual_quick
-
-//!
-int op_invoke_virtual_quick(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL_QUICK);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    int vD = mir->dalvikInsn.vC;
-    u2 IMMC = 4 * mir->dalvikInsn.vB;
-    int retval = common_invoke_virtual_quick(false, vD, IMMC, mir);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_VIRTUAL_QUICK_RANGE by calling common_invoke_virtual_quick
-
-//!
-int op_invoke_virtual_quick_range(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL_QUICK_RANGE);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    int vD = mir->dalvikInsn.vC;
-    u2 IMMC = 4 * mir->dalvikInsn.vB;
-    int retval = common_invoke_virtual_quick(true, vD, IMMC, mir);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ESI
-#define P_SCRATCH_1 PhysicalReg_EDX
-//! common code to lower INVOKE_SUPER_QUICK
-
-//!
-int common_invoke_super_quick(bool hasRange, int vD, u2 IMMC,
-        const DecodedInstruction &decodedInst) {
-    export_pc();
-    beforeCall("exception"); //dump GG, GL VRs
-    compare_imm_VR(OpndSize_32, 0, vD);
-
-    conditional_jump_global_API(Condition_E, "common_errNullObject", false);
-    /* for trace-based JIT, callee is already resolved */
-    int mIndex = IMMC/4;
-    const Method *calleeMethod = currentMethod->clazz->super->vtable[mIndex];
-    move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
-    if(hasRange) {
-        common_invokeMethodRange(convertCalleeToType(calleeMethod),
-                decodedInst);
-    }
-    else {
-        common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
-                decodedInst);
-    }
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_SCRATCH_1
-//! lower bytecode INVOKE_SUPER_QUICK by calling common_invoke_super_quick
-
-//!
-int op_invoke_super_quick(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER_QUICK);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    int vD = mir->dalvikInsn.vC;
-    u2 IMMC = 4 * mir->dalvikInsn.vB;
-    int retval = common_invoke_super_quick(false, vD, IMMC, mir->dalvikInsn);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-//! lower bytecode INVOKE_SUPER_QUICK_RANGE by calling common_invoke_super_quick
-
-//!
-int op_invoke_super_quick_range(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER_QUICK_RANGE);
-
-    /* An invoke with the MIR_INLINED is effectively a no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    int vD = mir->dalvikInsn.vC;
-    u2 IMMC = 4 * mir->dalvikInsn.vB;
-    int retval = common_invoke_super_quick(true, vD, IMMC, mir->dalvikInsn);
-#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
-    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
-#endif
-    return retval;
-}
-/////// code to predict the callee method for invoke_virtual & invoke_interface
-#define offChainingCell_clazz 8
-#define offChainingCell_method 12
-#define offChainingCell_counter 16
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_EAX
-#define P_GPR_3 PhysicalReg_ESI
-#define P_SCRATCH_2 PhysicalReg_EDX
-/* TODO gingerbread: implemented for O1, but not for O0:
-   valid input to JitToPatch & use icRechainCount */
-/* update predicted method for invoke interface */
-// 2 inputs: ChainingCell in P_GPR_1, current class object in P_GPR_3
-void predicted_chain_interface_O0(u2 tmp) {
-    ALOGI("TODO chain_interface_O0");
-
-    /* set up arguments to dvmFindInterfaceMethodInCache */
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, tmp, 4, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, (int) currentMethod->clazz->pDvmDex, 12, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, (int) currentMethod, 8, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, P_GPR_3, true, 0, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_EDX;
-    call_dvmFindInterfaceMethodInCache();
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-    /* if dvmFindInterfaceMethodInCache returns NULL, throw exception
-       otherwise, jump to .find_interface_done */
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_NE, ".find_interface_done", true);
-    scratchRegs[0] = PhysicalReg_EAX;
-    jumpToExceptionThrown(1/*exception number*/);
-
-    /* the interface method is found */
-    if (insertLabel(".find_interface_done", true) == -1)
-        return;
-    /* reduce counter in chaining cell by 1 */
-    move_mem_to_reg(OpndSize_32, offChainingCell_counter, P_GPR_1, true, P_SCRATCH_2, true); //counter
-    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, P_SCRATCH_2, true);
-    move_reg_to_mem(OpndSize_32, P_SCRATCH_2, true, offChainingCell_counter, P_GPR_1, true);
-
-    /* if counter is still greater than zero, skip prediction
-       if it is zero, update predicted method */
-    compare_imm_reg(OpndSize_32, 0, P_SCRATCH_2, true);
-    conditional_jump(Condition_G, ".skipPrediction", true);
-
-    /* call dvmJitToPatchPredictedChain to update predicted method */
-    //%ecx has callee method for virtual, %eax has callee for interface
-    /* set up arguments for dvmJitToPatchPredictedChain */
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
-    if(!gDvmJit.scheduling)
-        insertChainingWorklist(traceCurrentBB->taken->id, stream);
-    move_chain_to_mem(OpndSize_32, traceCurrentBB->taken->id, 8, PhysicalReg_ESP, true); //predictedChainCell
-    move_reg_to_mem(OpndSize_32, P_GPR_3, true, 12, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_EAX;
-    call_dvmJitToPatchPredictedChain(); //inputs: method, unused, predictedChainCell, clazz
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    if (insertLabel(".skipPrediction", true) == -1)
-        return;
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
-}
-
-// 2 inputs: ChainingCell in temp 41, current class object in temp 40
-void predicted_chain_interface_O1(u2 tmp) {
-
-    /* set up arguments to dvmFindInterfaceMethodInCache */
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, tmp, 4, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, (int) currentMethod->clazz->pDvmDex, 12, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, (int) currentMethod, 8, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 40, false, 0, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_10;
-    call_dvmFindInterfaceMethodInCache();
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-    /* if dvmFindInterfaceMethodInCache returns NULL, throw exception
-       otherwise, jump to .find_interface_done */
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_NE, ".find_interface_done", true);
-    rememberState(3);
-    scratchRegs[0] = PhysicalReg_SCRATCH_9;
-    jumpToExceptionThrown(1/*exception number*/);
-
-    goToState(3);
-    /* the interface method is found */
-    if (insertLabel(".find_interface_done", true) == -1)
-        return;
-#if 1 //
-    /* for gingerbread, counter is stored in glue structure
-       if clazz is not initialized, set icRechainCount to 0, otherwise, reduce it by 1 */
-    /* for gingerbread: t43 = 0 t44 = t33 t33-- cmov_ne t43 = t33 cmov_ne t44 = t33 */
-    move_mem_to_reg(OpndSize_32, offChainingCell_clazz, 41, false, 45, false);
-    move_imm_to_reg(OpndSize_32, 0, 43, false);
-    get_self_pointer(PhysicalReg_SCRATCH_7, isScratchPhysical);
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Thread, icRechainCount), PhysicalReg_SCRATCH_7, isScratchPhysical, 33, false); //counter
-    move_reg_to_reg(OpndSize_32, 33, false, 44, false);
-    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, 33, false);
-    /* sub_opc will update control flags, so compare_imm_reg must happen after */
-    compare_imm_reg(OpndSize_32, 0, 45, false);
-    conditional_move_reg_to_reg(OpndSize_32, Condition_NZ, 33, false/*src*/, 43, false/*dst*/);
-    conditional_move_reg_to_reg(OpndSize_32, Condition_NZ, 33, false/*src*/, 44, false/*dst*/);
-    move_reg_to_mem(OpndSize_32, 44, false, OFFSETOF_MEMBER(Thread, icRechainCount), PhysicalReg_SCRATCH_7, isScratchPhysical);
-#else
-    /* reduce counter in chaining cell by 1 */
-    move_mem_to_reg(OpndSize_32, offChainingCell_counter, 41, false, 33, false); //counter
-    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, 33, false);
-    move_reg_to_mem(OpndSize_32, 33, false, offChainingCell_counter, 41, false);
-#endif
-
-    /* if counter is still greater than zero, skip prediction
-       if it is zero, update predicted method */
-    compare_imm_reg(OpndSize_32, 0, 43, false);
-    conditional_jump(Condition_G, ".skipPrediction", true);
-
-    rememberState(4);
-    /* call dvmJitToPatchPredictedChain to update predicted method */
-    //%ecx has callee method for virtual, %eax has callee for interface
-    /* set up arguments for dvmJitToPatchPredictedChain */
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_SCRATCH_7, isScratchPhysical, 4, PhysicalReg_ESP, true);
-   if(!gDvmJit.scheduling)
-        insertChainingWorklist(traceCurrentBB->taken->id, stream);
-    move_chain_to_mem(OpndSize_32, traceCurrentBB->taken->id, 8, PhysicalReg_ESP, true); //predictedChainCell
-    move_reg_to_mem(OpndSize_32, 40, false, 12, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_8;
-    call_dvmJitToPatchPredictedChain(); //inputs: method, unused, predictedChainCell, clazz
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    transferToState(4);
-
-    if (insertLabel(".skipPrediction", true) == -1)
-        return;
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
-}
-
-/* update predicted method for invoke virtual */
-// 2 inputs: ChainingCell in P_GPR_1, current class object in P_GPR_3
-void predicted_chain_virtual_O0(u2 IMMC) {
-    ALOGI("TODO chain_virtual_O0");
-
-    /* reduce counter in chaining cell by 1 */
-    move_mem_to_reg(OpndSize_32, offChainingCell_counter, P_GPR_1, true, P_GPR_2, true); //counter
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ClassObject, vtable), P_GPR_3, true, P_SCRATCH_2, true);
-    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, P_GPR_2, true);
-    move_mem_to_reg(OpndSize_32, IMMC, P_SCRATCH_2, true, PhysicalReg_ECX, true);
-    move_reg_to_mem(OpndSize_32, P_GPR_2, true, offChainingCell_counter, P_GPR_1, true);
-
-    /* if counter is still greater than zero, skip prediction
-       if it is zero, update predicted method */
-    compare_imm_reg(OpndSize_32, 0, P_GPR_2, true);
-    conditional_jump(Condition_G, ".skipPrediction", true);
-
-    /* call dvmJitToPatchPredictedChain to update predicted method */
-    //%ecx has callee method for virtual, %eax has callee for interface
-    /* set up arguments for dvmJitToPatchPredictedChain */
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 0,  PhysicalReg_ESP, true);
-    if(!gDvmJit.scheduling)
-        insertChainingWorklist(traceCurrentBB->taken->id, stream);
-    move_chain_to_mem(OpndSize_32, traceCurrentBB->taken->id, 8, PhysicalReg_ESP, true); //predictedChainCell
-    move_reg_to_mem(OpndSize_32, P_GPR_3, true, 12, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_EAX;
-    call_dvmJitToPatchPredictedChain(); //inputs: method, unused, predictedChainCell, clazz
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-    //callee method in %ecx for invoke virtual
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
-    if (insertLabel(".skipPrediction", true) == -1)
-        return;
-}
-
-// 2 inputs: ChainingCell in temp 41, current class object in temp 40
-// extra input: predicted clazz in temp 32
-void predicted_chain_virtual_O1(u2 IMMC) {
-
-    /* reduce counter in chaining cell by 1 */
-    /* for gingerbread: t43 = 0 t44 = t33 t33-- cmov_ne t43 = t33 cmov_ne t44 = t33 */
-    get_self_pointer(PhysicalReg_SCRATCH_7, isScratchPhysical);
-    move_imm_to_reg(OpndSize_32, 0, 43, false);
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Thread, icRechainCount), PhysicalReg_SCRATCH_7, isScratchPhysical, 33, false); //counter
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ClassObject, vtable), 40, false, 34, false);
-    move_reg_to_reg(OpndSize_32, 33, false, 44, false);
-    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, 33, false);
-    compare_imm_reg(OpndSize_32, 0, 32, false); // after sub_opc
-    move_mem_to_reg(OpndSize_32, IMMC, 34, false, PhysicalReg_ECX, true);
-    conditional_move_reg_to_reg(OpndSize_32, Condition_NZ, 33, false/*src*/, 43, false/*dst*/);
-    conditional_move_reg_to_reg(OpndSize_32, Condition_NZ, 33, false/*src*/, 44, false/*dst*/);
-    move_reg_to_mem(OpndSize_32, 44, false, OFFSETOF_MEMBER(Thread, icRechainCount), PhysicalReg_SCRATCH_7, isScratchPhysical);
-
-    /* if counter is still greater than zero, skip prediction
-       if it is zero, update predicted method */
-    compare_imm_reg(OpndSize_32, 0, 43, false);
-    conditional_jump(Condition_G, ".skipPrediction", true);
-
-    rememberState(2);
-    /* call dvmJitToPatchPredictedChain to update predicted method */
-    //%ecx has callee method for virtual, %eax has callee for interface
-    /* set up arguments for dvmJitToPatchPredictedChain */
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_SCRATCH_7, isScratchPhysical, 4, PhysicalReg_ESP, true);
-    if(!gDvmJit.scheduling && traceCurrentBB->taken)
-        insertChainingWorklist(traceCurrentBB->taken->id, stream);
-    int traceTakenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
-    move_chain_to_mem(OpndSize_32, traceTakenId, 8, PhysicalReg_ESP, true); //predictedChainCell
-    move_reg_to_mem(OpndSize_32, 40, false, 12, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_10;
-    call_dvmJitToPatchPredictedChain(); //inputs: method, unused, predictedChainCell, clazz
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-    //callee method in %ecx for invoke virtual
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
-    transferToState(2);
-
-    if (insertLabel(".skipPrediction", true) == -1)
-        return;
-}
-
-static int invokeChain_inst = 0;
-/* object "this" is in %ebx */
-void gen_predicted_chain_O0(bool isRange, u2 tmp, int IMMC, bool isInterface,
-        int inputReg, const DecodedInstruction &decodedInst) {
-    ALOGI("TODO predicted_chain_O0");
-
-    /* get current class object */
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), PhysicalReg_EBX, true,
-             P_GPR_3, true);
-#ifdef DEBUG_CALL_STACK3
-    scratchRegs[0] = PhysicalReg_EAX;
-    call_debug_dumpSwitch(); //%ebx, %eax, %edx
-    move_imm_to_reg(OpndSize_32, 0xdd11, PhysicalReg_EBX, true);
-    call_debug_dumpSwitch();
-#endif
-
-    /* get predicted clazz
-       get predicted method
-    */
-    if(!gDvmJit.scheduling)
-        insertChainingWorklist(traceCurrentBB->taken->id, stream);
-    move_chain_to_reg(OpndSize_32, traceCurrentBB->taken->id, P_GPR_1, true); //predictedChainCell
-    move_mem_to_reg(OpndSize_32, offChainingCell_clazz, P_GPR_1, true, P_SCRATCH_2, true);//predicted clazz
-    move_mem_to_reg(OpndSize_32, offChainingCell_method, P_GPR_1, true, PhysicalReg_ECX, true);//predicted method
-
-#ifdef DEBUG_CALL_STACK3
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, P_GPR_1, true, 8, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, P_SCRATCH_2, true, 4, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, P_GPR_3, true, 0, PhysicalReg_ESP, true);
-
-    move_reg_to_reg(OpndSize_32, P_SCRATCH_2, true, PhysicalReg_EBX, true);
-    call_debug_dumpSwitch();
-    move_imm_to_reg(OpndSize_32, 0xdd22, PhysicalReg_EBX, true);
-    scratchRegs[0] = PhysicalReg_EAX;
-    call_debug_dumpSwitch(); //%ebx, %eax, %edx
-    move_reg_to_reg(OpndSize_32, P_GPR_3, true, PhysicalReg_EBX, true);
-    call_debug_dumpSwitch();
-    move_reg_to_reg(OpndSize_32, PhysicalReg_ECX, true, PhysicalReg_EBX, true);
-    call_debug_dumpSwitch();
-
-    move_mem_to_reg(OpndSize_32, 8, PhysicalReg_ESP, true, P_GPR_1, true);
-    move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true, P_SCRATCH_2, true);
-    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, P_GPR_3, true);
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-#endif
-
-    /* compare current class object against predicted clazz
-       if equal, prediction is still valid, jump to .invokeChain */
-    //live registers: P_GPR_1, P_GPR_3, P_SCRATCH_2
-    compare_reg_reg(P_GPR_3, true, P_SCRATCH_2, true);
-    conditional_jump(Condition_E, ".invokeChain", true);
-    invokeChain_inst++;
-
-    //get callee method and update predicted method if necessary
-    if(isInterface) {
-        predicted_chain_interface_O0(tmp);
-    } else {
-        predicted_chain_virtual_O0(IMMC);
-    }
-
-#ifdef DEBUG_CALL_STACK3
-    move_imm_to_reg(OpndSize_32, 0xeeee, PhysicalReg_EBX, true);
-    scratchRegs[0] = PhysicalReg_EAX;
-    call_debug_dumpSwitch(); //%ebx, %eax, %edx
-    if(!gDvmJit.scheduling)
-        insertChainingWorklist(traceCurrentBB->taken->id, stream);
-    move_chain_to_reg(OpndSize_32, traceCurrentBB->taken->id, PhysicalReg_EBX, true);
-    call_debug_dumpSwitch();
-#endif
-
-    if(isRange) {
-        common_invokeMethodRange(ArgsDone_Full, decodedInst);
-    }
-    else {
-        common_invokeMethodNoRange(ArgsDone_Full, decodedInst);
-    }
-
-    if (insertLabel(".invokeChain", true) == -1)
-        return;
-#ifdef DEBUG_CALL_STACK3
-    move_imm_to_reg(OpndSize_32, 0xdddd, PhysicalReg_EBX, true);
-    scratchRegs[0] = PhysicalReg_EAX;
-    call_debug_dumpSwitch(); //%ebx, %eax, %edx
-    if(!gDvmJit.scheduling)
-        insertChainingWorklist(traceCurrentBB->taken->id, stream);
-    move_chain_to_reg(OpndSize_32, traceCurrentBB->taken->id, PhysicalReg_EBX, true);
-    call_debug_dumpSwitch();
-    move_reg_to_reg(OpndSize_32, PhysicalReg_ECX, true, PhysicalReg_EBX, true);
-    call_debug_dumpSwitch();
-#endif
-
-    if(isRange) {
-        common_invokeMethodRange(ArgsDone_Normal, decodedInst);
-    }
-    else {
-        common_invokeMethodNoRange(ArgsDone_Normal, decodedInst);
-    }
-}
-
-/* object "this" is in inputReg: 5 for virtual, 1 for interface, 1 for virtual_quick */
-void gen_predicted_chain_O1(bool isRange, u2 tmp, int IMMC, bool isInterface,
-        int inputReg, const DecodedInstruction &decodedInst) {
-
-    /* get current class object */
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), inputReg, false,
-             40, false);
-
-    /* get predicted clazz
-       get predicted method
-    */
-    if(!gDvmJit.scheduling && traceCurrentBB->taken)
-        insertChainingWorklist(traceCurrentBB->taken->id, stream);
-    int traceTakenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
-    move_chain_to_reg(OpndSize_32, traceTakenId, 41, false); //predictedChainCell
-    move_mem_to_reg(OpndSize_32, offChainingCell_clazz, 41, false, 32, false);//predicted clazz
-    move_mem_to_reg(OpndSize_32, offChainingCell_method, 41, false, PhysicalReg_ECX, true);//predicted method
-
-    /* update stack with parameters first, then decide the callee */
-    if(isRange) common_invokeMethodRange_noJmp(decodedInst);
-    else common_invokeMethodNoRange_noJmp(decodedInst);
-
-    /* compare current class object against predicted clazz
-       if equal, prediction is still valid, jump to .invokeChain */
-    compare_reg_reg(40, false, 32, false);
-    conditional_jump(Condition_E, ".invokeChain", true);
-    rememberState(1);
-    invokeChain_inst++;
-
-    //get callee method and update predicted method if necessary
-    if(isInterface) {
-        predicted_chain_interface_O1(tmp);
-    } else {
-        predicted_chain_virtual_O1(IMMC);
-    }
-
-    common_invokeMethod_Jmp(ArgsDone_Full); //will touch %ecx
-
-    if (insertLabel(".invokeChain", true) == -1)
-        return;
-    goToState(1);
-    common_invokeMethod_Jmp(ArgsDone_Normal);
-}
-
-void gen_predicted_chain(bool isRange, u2 tmp, int IMMC, bool isInterface,
-        int inputReg, const DecodedInstruction &decodedInst) {
-    return gen_predicted_chain_O1(isRange, tmp, IMMC, isInterface, inputReg,
-            decodedInst);
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef P_SCRATCH_2
diff --git a/vm/compiler/codegen/x86/LowerJump.cpp b/vm/compiler/codegen/x86/LowerJump.cpp
deleted file mode 100644
index b932a4b..0000000
--- a/vm/compiler/codegen/x86/LowerJump.cpp
+++ /dev/null
@@ -1,2208 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/**
- * @file vm/compiler/codegen/x86/LowerJump.cpp
- * @brief This file lowers the following bytecodes: IF_XXX, GOTO
- */
-#include <math.h>
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "NcgAot.h"
-#include "enc_wrapper.h"
-#include "interp/InterpDefs.h"
-#include "NcgHelper.h"
-#include "RegisterizationBE.h"
-#include "Scheduler.h"
-#include "Singleton.h"
-
-#if defined VTUNE_DALVIK
-#include "compiler/JitProfiling.h"
-#endif
-
-LabelMap* globalMap;
-LabelMap* globalShortMap;//make sure for each bytecode, there is no duplicated label
-LabelMap* globalWorklist = NULL;
-LabelMap* globalShortWorklist;
-
-int globalMapNum;
-int globalWorklistNum;
-int globalDataWorklistNum;
-int VMAPIWorklistNum;
-int globalPCWorklistNum;
-int chainingWorklistNum;
-
-LabelMap* globalDataWorklist = NULL;
-LabelMap* globalPCWorklist = NULL;
-LabelMap* chainingWorklist = NULL;
-LabelMap* VMAPIWorklist = NULL;
-
-char* ncgClassData;
-char* ncgClassDataPtr;
-char* ncgMethodData;
-char* ncgMethodDataPtr;
-int   ncgClassNum;
-int   ncgMethodNum;
-
-NCGWorklist* globalNCGWorklist;
-DataWorklist* methodDataWorklist;
-#ifdef ENABLE_TRACING
-MapWorklist* methodMapWorklist;
-#endif
-/*!
-\brief search globalShortMap to find the entry for the given label
-
-*/
-LabelMap* findItemForShortLabel(const char* label) {
-    LabelMap* ptr = globalShortMap;
-    while(ptr != NULL) {
-        if(!strcmp(label, ptr->label)) {
-            return ptr;
-        }
-        ptr = ptr->nextItem;
-    }
-    return NULL;
-}
-//assume size of "jump reg" is 2
-#define JUMP_REG_SIZE 2
-#define ADD_REG_REG_SIZE 3
-/*!
-\brief update value of the immediate in the given jump instruction
-
-check whether the immediate is out of range for the pre-set size
-*/
-int updateJumpInst(char* jumpInst, OpndSize immSize, int relativeNCG) {
-#ifdef DEBUG_NCG_JUMP
-    ALOGI("update jump inst @ %p with %d", jumpInst, relativeNCG);
-#endif
-    if(immSize == OpndSize_8) { //-128 to 127
-        if(relativeNCG >= 128 || relativeNCG < -128) {
-            ALOGI("JIT_INFO: Pre-allocated space for a forward jump is not big enough\n");
-            SET_JIT_ERROR(kJitErrorShortJumpOffset);
-            return -1;
-        }
-    }
-    if(immSize == OpndSize_16) { //-2^16 to 2^16-1
-        if(relativeNCG >= 32768 || relativeNCG < -32768) {
-            ALOGI("JIT_INFO: Pre-allocated space (16-bit) for a forward jump is not big enough\n");
-            SET_JIT_ERROR(kJitErrorShortJumpOffset);
-            return -1;
-        }
-    }
-    dump_imm_update(relativeNCG, jumpInst, false);
-    return 0;
-}
-
-/*!
-\brief insert a label
-
-It takes argument checkDup, if checkDup is true, an entry is created in globalShortMap, entries in globalShortWorklist are checked, if there exists a match, the immediate in the jump instruction is updated and the entry is removed from globalShortWorklist;
-otherwise, an entry is created in globalMap.
-*/
-int insertLabel(const char* label, bool checkDup) {
-    LabelMap* item = NULL;
-
-    // We are inserting a label. Someone might want to jump to it
-    // so flush scheduler's queue
-    if (gDvmJit.scheduling)
-        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
-
-    if(!checkDup) {
-        item = (LabelMap*)malloc(sizeof(LabelMap));
-        if(item == NULL) {
-            ALOGI("JIT_INFO: Memory allocation failed at insertLabel with checkDup false");
-            SET_JIT_ERROR(kJitErrorMallocFailed);
-            return -1;
-        }
-        snprintf(item->label, LABEL_SIZE, "%s", label);
-        item->codePtr = stream;
-        item->nextItem = globalMap;
-        globalMap = item;
-#ifdef DEBUG_NCG_CODE_SIZE
-        ALOGI("insert global label %s %p", label, stream);
-#endif
-        globalMapNum++;
-        return 0;
-    }
-
-    item = (LabelMap*)malloc(sizeof(LabelMap));
-    if(item == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertLabel with checkDup true");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    snprintf(item->label, LABEL_SIZE, "%s", label);
-    item->codePtr = stream;
-    item->nextItem = globalShortMap;
-    globalShortMap = item;
-#ifdef DEBUG_NCG
-    ALOGI("Insert short-term label %s %p", label, stream);
-#endif
-    LabelMap* ptr = globalShortWorklist;
-    LabelMap* ptr_prevItem = NULL;
-    while(ptr != NULL) {
-        if(!strcmp(ptr->label, label)) {
-            //perform work
-            int relativeNCG = stream - ptr->codePtr;
-            unsigned instSize = encoder_get_inst_size(ptr->codePtr);
-            relativeNCG -= instSize; //size of the instruction
-#ifdef DEBUG_NCG
-            ALOGI("Perform work short-term %p for label %s relative %d\n", ptr->codePtr, label, relativeNCG);
-#endif
-            int retval = updateJumpInst(ptr->codePtr, ptr->size, relativeNCG);
-            //If this fails, the jump offset was not big enough. Raise the corresponding error flag
-            //We may decide to re-compiler the trace with a large jump offset later
-            if (retval == -1){
-                ALOGI("JIT_INFO: Label \"%s\" too far away from jump location", label);
-                SET_JIT_ERROR(kJitErrorShortJumpOffset);
-                return retval;
-            }
-
-            //remove work
-            if(ptr_prevItem == NULL) {
-                globalShortWorklist = ptr->nextItem;
-                free(ptr);
-                ptr = globalShortWorklist; //ptr_prevItem is still NULL
-            }
-            else {
-                ptr_prevItem->nextItem = ptr->nextItem;
-                free(ptr);
-                ptr = ptr_prevItem->nextItem;
-            }
-        }
-        else {
-            ptr_prevItem = ptr;
-            ptr = ptr->nextItem;
-        }
-    } //while
-    return 0;
-}
-/*!
-\brief search globalMap to find the entry for the given label
-
-*/
-char* findCodeForLabel(const char* label) {
-    LabelMap* ptr = globalMap;
-    while(ptr != NULL) {
-        if(!strcmp(label, ptr->label)) {
-            return ptr->codePtr;
-        }
-        ptr = ptr->nextItem;
-    }
-    return NULL;
-}
-/*!
-\brief search globalShortMap to find the entry for the given label
-
-*/
-char* findCodeForShortLabel(const char* label) {
-    LabelMap* ptr = globalShortMap;
-    while(ptr != NULL) {
-        if(!strcmp(label, ptr->label)) {
-            return ptr->codePtr;
-        }
-        ptr = ptr->nextItem;
-    }
-    return NULL;
-}
-int insertLabelWorklist(const char* label, OpndSize immSize) {
-    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
-    if(item == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertLabelWorklist");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    snprintf(item->label, LABEL_SIZE, "%s", label);
-    item->codePtr = stream;
-    item->size = immSize;
-    item->nextItem = globalWorklist;
-    globalWorklist = item;
-#ifdef DEBUG_NCG
-    ALOGI("Insert globalWorklist: %s %p", label, stream);
-#endif
-    return 0;
-}
-
-int insertShortWorklist(const char* label, OpndSize immSize) {
-    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
-    if(item == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertShortWorklist");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    snprintf(item->label, LABEL_SIZE, "%s", label);
-    item->codePtr = stream;
-    item->size = immSize;
-    item->nextItem = globalShortWorklist;
-    globalShortWorklist = item;
-#ifdef DEBUG_NCG
-    ALOGI("Insert globalShortWorklist: %s %p", label, stream);
-#endif
-    return 0;
-}
-/*!
-\brief free memory allocated for globalMap
-
-*/
-void freeLabelMap() {
-    LabelMap* ptr = globalMap;
-    while(ptr != NULL) {
-        globalMap = ptr->nextItem;
-        free(ptr);
-        ptr = globalMap;
-    }
-}
-/*!
-\brief free memory allocated for globalShortMap
-
-*/
-void freeShortMap() {
-    LabelMap* ptr = globalShortMap;
-    while(ptr != NULL) {
-        globalShortMap = ptr->nextItem;
-        free(ptr);
-        ptr = globalShortMap;
-    }
-    globalShortMap = NULL;
-}
-
-int insertGlobalPCWorklist(char * offset, char * codeStart)
-{
-    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
-    if(item == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertGlobalPCWorklist");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    snprintf(item->label, LABEL_SIZE, "%s", "export_pc");
-    item->size = OpndSize_32;
-    item->codePtr = offset; //points to the immediate operand
-    item->addend = codeStart - streamMethodStart; //relative code pointer
-    item->nextItem = globalPCWorklist;
-    globalPCWorklist = item;
-    globalPCWorklistNum ++;
-
-#ifdef DEBUG_NCG
-    ALOGI("Insert globalPCWorklist: %p %p %p %x %p", globalDvmNcg->streamCode,  codeStart, streamCode, item->addend, item->codePtr);
-#endif
-    return 0;
-}
-
-/*
- * search chainingWorklist to return instruction offset address in move instruction
- */
-char* searchChainingWorklist(unsigned int blockId) {
-    LabelMap* ptr = chainingWorklist;
-    unsigned instSize;
-
-    while (ptr != NULL) {
-       if (blockId == ptr->addend) {
-           instSize = encoder_get_inst_size(ptr->codePtr);
-           assert((uint)(ptr->codePtr + instSize - 4) % 16 <= 12);
-           return (ptr->codePtr + instSize - 4); // 32bit relative offset
-       }
-       ptr = ptr->nextItem;
-    }
-#ifdef DEBUG_NCG
-    ALOGI("can't find item for blockId %d in searchChainingWorklist\n", blockId);
-#endif
-    return NULL;
-}
-
-int insertChainingWorklist(int bbId, char * codeStart)
-{
-    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
-    if(item == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertChainingWorklist");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    item->size = OpndSize_32;
-    item->codePtr = codeStart; //points to the move instruction
-    item->addend = bbId; //relative code pointer
-    item->nextItem = chainingWorklist;
-    chainingWorklist = item;
-
-#ifdef DEBUG_NCG
-    ALOGI("InsertChainingWorklist: %p basic block %d", codeStart, bbId);
-#endif
-    return 0;
-}
-
-int insertGlobalDataWorklist(char * offset, const char* label)
-{
-    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
-    if(item == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertGlobalDataWorklist");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    snprintf(item->label, LABEL_SIZE, "%s", label);
-    item->codePtr = offset;
-    item->size = OpndSize_32;
-    item->nextItem = globalDataWorklist;
-    globalDataWorklist = item;
-    globalDataWorklistNum ++;
-
-#ifdef DEBUG_NCG
-    ALOGI("Insert globalDataWorklist: %s %p", label, offset);
-#endif
-
-    return 0;
-}
-
-int insertVMAPIWorklist(char * offset, const char* label)
-{
-    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
-    if(item == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertVMAPIWorklist");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    snprintf(item->label, LABEL_SIZE, "%s", label);
-    item->codePtr = offset;
-    item->size = OpndSize_32;
-
-    item->nextItem = VMAPIWorklist;
-    VMAPIWorklist = item;
-
-    VMAPIWorklistNum ++;
-
-#ifdef DEBUG_NCG
-    ALOGI("Insert VMAPIWorklist: %s %p", label, offset);
-#endif
-    return 0;
-}
-////////////////////////////////////////////////
-
-
-int updateImmRMInst(char* moveInst, const char* label, int relativeNCG); //forward declaration
-//////////////////// performLabelWorklist is defined differently for code cache
-void performChainingWorklist() {
-    LabelMap* ptr = chainingWorklist;
-    while(ptr != NULL) {
-        int tmpNCG = getLabelOffset (ptr->addend);
-        char* NCGaddr = streamMethodStart + tmpNCG;
-        updateImmRMInst(ptr->codePtr, "", (int)NCGaddr);
-        chainingWorklist = ptr->nextItem;
-        free(ptr);
-        ptr = chainingWorklist;
-    }
-}
-void freeChainingWorklist() {
-    LabelMap* ptr = chainingWorklist;
-    while(ptr != NULL) {
-        chainingWorklist = ptr->nextItem;
-        free(ptr);
-        ptr = chainingWorklist;
-    }
-}
-
-/*
- *search globalWorklist to find the jmp/jcc offset address
- */
-char* searchLabelWorklist(char* label) {
-    LabelMap* ptr = globalWorklist;
-    unsigned instSize;
-
-    while(ptr != NULL) {
-        if(!strcmp(label, ptr->label)) {
-            instSize = encoder_get_inst_size(ptr->codePtr);
-            assert((uint)(ptr->codePtr + instSize - 4) % 16 <= 12);
-            return (ptr->codePtr + instSize - 4); // 32bit relative offset
-        }
-        ptr = ptr->nextItem;
-   }
-#ifdef DEBUG_NCG
-    ALOGI("can't find item for label %s in searchLabelWorklist\n", label);
-#endif
-    return NULL;
-}
-
-// delete the node with label "vr_store_at_loop_back" from globalMap
-static void deleteVRStoreLabelGlobalMap()
-{
-    LabelMap * ptr = globalMap;
-    LabelMap * prePtr = NULL;
-
-    while(ptr != NULL) {
-        if (strstr(ptr->label, ".vr_store_at_loop_back") != 0) {
-            if (prePtr == NULL)
-                globalMap = ptr->nextItem;
-            else
-                prePtr->nextItem = ptr->nextItem;
-            free(ptr);
-            return;
-        }
-        prePtr = ptr;
-        ptr = ptr->nextItem;
-    }
-}
-
-//Work only for initNCG
-void performLabelWorklist() {
-    LabelMap* ptr = globalWorklist;
-    while(ptr != NULL) {
-#ifdef DEBUG_NCG
-        ALOGI("Perform work global %p for label %s", ptr->codePtr, ptr->label);
-#endif
-        char* targetCode = findCodeForLabel(ptr->label);
-        assert(targetCode != NULL);
-        int relativeNCG = targetCode - ptr->codePtr;
-        unsigned instSize = encoder_get_inst_size(ptr->codePtr);
-        relativeNCG -= instSize; //size of the instruction
-        updateJumpInst(ptr->codePtr, ptr->size, relativeNCG);
-        globalWorklist = ptr->nextItem;
-        free(ptr);
-        ptr = globalWorklist;
-    }
-    deleteVRStoreLabelGlobalMap();
-}
-
-void freeLabelWorklist() {
-    LabelMap* ptr = globalWorklist;
-    while(ptr != NULL) {
-        globalWorklist = ptr->nextItem;
-        free(ptr);
-        ptr = globalWorklist;
-    }
-}
-
-///////////////////////////////////////////////////
-/*!
-\brief update value of the immediate in the given move instruction
-
-*/
-int updateImmRMInst(char* moveInst, const char* label, int relativeNCG) {
-#ifdef DEBUG_NCG
-    ALOGI("Perform work ImmRM inst @ %p for label %s with %d", moveInst, label, relativeNCG);
-#endif
-    dump_imm_update(relativeNCG, moveInst, true);
-    return 0;
-}
-//! maximum instruction size for jump,jcc,call: 6 for jcc rel32
-#define MAX_JCC_SIZE 6
-//! minimum instruction size for jump,jcc,call: 2
-#define MIN_JCC_SIZE 2
-/*!
-\brief estimate size of the immediate
-
-Somehow, 16 bit jump does not work. This function will return either 8 bit or 32 bit
-EXAMPLE:
-  native code at A: ...
-  native code at B: jump relOffset (target is A)
-  native code at B':
-  --> relOffset = A - B' = A - B - size of the jump instruction
-  Argument "target" is equal to A - B. To determine size of the immediate, we check tha value of "target - size of the jump instructoin"
-*/
-OpndSize estOpndSizeFromImm(int target) {
-    if(target-MIN_JCC_SIZE < 128 && target-MAX_JCC_SIZE >= -128) return OpndSize_8;
-#ifdef SUPPORT_IMM_16
-    if(target-MIN_JCC_SIZE < 32768 && target-MAX_JCC_SIZE >= -32768) return OpndSize_16;
-#endif
-    return OpndSize_32;
-}
-
-/*!
-\brief return size of a jump or call instruction
-*/
-unsigned getJmpCallInstSize(OpndSize size, JmpCall_type type) {
-    if(type == JmpCall_uncond) {
-        if(size == OpndSize_8) return 2;
-        if(size == OpndSize_16) return 4;
-        return 5;
-    }
-    if(type == JmpCall_cond) {
-        if(size == OpndSize_8) return 2;
-        if(size == OpndSize_16) return 5;
-        return 6;
-    }
-    if(type == JmpCall_reg) {
-        assert(size == OpndSize_32);
-        return JUMP_REG_SIZE;
-    }
-    if(type == JmpCall_call) {
-        assert(size != OpndSize_8);
-        if(size == OpndSize_16) return 4;
-        return 5;
-    }
-    return 0;
-}
-
-//! \brief Get the offset given a jump target
-//!
-//! \details check whether a branch target is already handled if yes, return the
-//! size of the immediate; otherwise, call insertShortWorklist or insertLabelWorklist.
-//!
-//! If the branch target is not handled, call insertShortWorklist or insertLabelWorklist
-//! depending on isShortTerm, unknown is set to true, immSize is set to 32 if isShortTerm
-//! is false, set to 32 if isShortTerm is true and target is check_cast_null, set to 8 otherwise.
-//!
-//! If the branch target is handled, call estOpndSizeFromImm to set immSize for jump
-//! instruction, returns the value of the immediate
-//!
-//! \param target the target of the jump
-//! \param isShortTerm whether this is a short term jump
-//! \param type Call or Jmp
-//! \param unknown target known or not
-//! \param immSize size of the jump offset
-//!
-//! \return jump offset (can also return error value, but caller cannot distinguish)
-int getRelativeOffset(const char* target, bool isShortTerm, JmpCall_type type, bool* unknown, OpndSize* immSize) {
-    char* targetPtrInStream = NULL;
-    if(isShortTerm) targetPtrInStream = findCodeForShortLabel(target);
-    else targetPtrInStream = findCodeForLabel(target);
-
-    int relOffset;
-    int retCode = 0;
-    *unknown = false;
-    if(targetPtrInStream == NULL) {
-        //branch target is not handled yet
-        relOffset = 0;
-        *unknown = true;
-        if(isShortTerm) {
-            /* for backward jump, at this point, we don't know how far the target is from this jump
-               since the lable is only used within a single bytecode, we assume OpndSize_8 is big enough
-               but there are special cases where we should use 32 bit offset
-            */
-            //Check if we have failed with 8-bit offset previously. Use 32-bit offsets if so.
-            if (gDvmJit.disableOpt & (1 << kShortJumpOffset)){
-                *immSize = OpndSize_32;
-            }
-            //Check if it is a special case:
-            //These labels are known to be far off from the jump location
-            //Safe to set them to large offset by default
-            else if(!strcmp(target, ".stackOverflow") ||
-                    !strcmp(target, ".invokeChain") ||
-                    !strcmp(target, "after_exception_1") ||
-                    !strncmp(target, "exception_restore_state_", 24)) {
-#ifdef SUPPORT_IMM_16
-                *immSize = OpndSize_16;
-#else
-                *immSize = OpndSize_32;
-#endif
-            } else {
-                *immSize = OpndSize_8;
-            }
-#ifdef WITH_SELF_VERIFICATION
-            if(!strcmp(target, ".aput_object_skip_check") ||
-               !strcmp(target, ".aput_object_after_check") ) {
-                *immSize = OpndSize_32;
-            }
-#endif
-#ifdef DEBUG_NCG_JUMP
-            ALOGI("Insert to short worklist %s %d", target, *immSize);
-#endif
-            retCode = insertShortWorklist(target, *immSize);
-            //NOTE: Returning negative value here cannot indicate an error
-            //The caller accepts any value as correct. Only the premature
-            //return matters here.
-            if (retCode < 0)
-                return retCode;
-        }
-        else {
-#ifdef SUPPORT_IMM_16
-            *immSize = OpndSize_16;
-#else
-            *immSize = OpndSize_32;
-#endif
-            retCode = insertLabelWorklist(target, *immSize);
-            //NOTE: Returning negative value here cannot indicate an error
-            //The caller accepts any value as correct. Only the premature
-            //return matters here.
-            if (retCode < 0) {
-                return retCode;
-            }
-        }
-        if(type == JmpCall_call) { //call sz16 does not work in gdb
-            *immSize = OpndSize_32;
-        }
-        return 0;
-    }
-    else if (!isShortTerm) {
-#ifdef SUPPORT_IMM_16
-        *immSize = OpndSize_16;
-#else
-        *immSize = OpndSize_32;
-#endif
-        retCode = insertLabelWorklist(target, *immSize);
-        if (retCode < 0) {
-            return retCode;
-        }
-    }
-
-#ifdef DEBUG_NCG
-    ALOGI("Backward branch @ %p for label %s", stream, target);
-#endif
-    relOffset = targetPtrInStream - stream;
-    if (type == JmpCall_call) {
-        *immSize = OpndSize_32;
-    }
-    else {
-        *immSize = estOpndSizeFromImm(relOffset);
-    }
-    relOffset -= getJmpCallInstSize(*immSize, type);
-    return relOffset;
-}
-
-/*!
-\brief generate a single native instruction "jcc imm" to jump to a label
-
-*/
-void conditional_jump(ConditionCode cc, const char* target, bool isShortTerm) {
-    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
-        condJumpToBasicBlock(stream, cc, currentExceptionBlockIdx);
-        return;
-    }
-    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
-    bool unknown;
-    OpndSize size = OpndSize_Null;
-    int imm = 0;
-    if(!gDvmJit.scheduling)
-        imm = getRelativeOffset(target, isShortTerm, JmpCall_cond, &unknown, &size);
-    dump_label(m, size, imm, target, isShortTerm);
-}
-
-/*!
-\brief generate a single native instruction "jmp imm" to jump to a label
-
-If the target is ".invokeArgsDone" and mode is NCG O1, extra work is performed to dump content of virtual registers to memory.
-*/
-void unconditional_jump(const char* target, bool isShortTerm) {
-    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
-        jumpToBasicBlock(stream, currentExceptionBlockIdx);
-        return;
-    }
-    Mnemonic m = Mnemonic_JMP;
-    bool unknown;
-    OpndSize size = OpndSize_Null;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        //for other three labels used by JIT: invokeArgsDone_formal, _native, _jit
-        if(!strncmp(target, ".invokeArgsDone", 15)) {
-            touchEcx(); //keep ecx live, if ecx was spilled, it is loaded here
-            beforeCall(target); //
-        }
-        if(!strcmp(target, ".invokeArgsDone")) {
-            nextVersionOfHardReg(PhysicalReg_EDX, 1); //edx will be used in a function
-            call("ncgGetEIP"); //must be immediately before JMP
-        }
-    }
-    int imm = 0;
-    if(!gDvmJit.scheduling)
-        imm = getRelativeOffset(target, isShortTerm, JmpCall_uncond, &unknown, &size);
-    dump_label(m, size, imm, target, isShortTerm);
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        if(!strncmp(target, ".invokeArgsDone", 15)) {
-            afterCall(target); //un-spill before executing the next bytecode
-        }
-    }
-}
-/*!
-\brief generate a single native instruction "jcc imm"
-
-*/
-void conditional_jump_int(ConditionCode cc, int target, OpndSize size) {
-    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
-    dump_imm(m, size, target);
-}
-/*!
-\brief generate a single native instruction "jmp imm"
-
-*/
-void unconditional_jump_int(int target, OpndSize size) {
-    Mnemonic m = Mnemonic_JMP;
-    dump_imm(m, size, target);
-}
-
-//! Used to generate a single native instruction for conditionally
-//! jumping to a block when the immediate is not yet known.
-//! This should only be used when instruction scheduling is enabled.
-//! \param cc type of conditional jump
-//! \param targetBlockId id of MIR basic block
-//! \param immediateNeedsAligned Whether the immediate needs to be aligned
-//! within 16-bytes
-void conditional_jump_block(ConditionCode cc, int targetBlockId,
-        bool immediateNeedsAligned) {
-    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
-    dump_blockid_imm(m, targetBlockId, immediateNeedsAligned);
-}
-
-//! Used to generate a single native instruction for unconditionally
-//! jumping to a block when the immediate is not yet known.
-//! This should only be used when instruction scheduling is enabled.
-//! \param targetBlockId id of MIR basic block
-//! \param immediateNeedsAligned Whether the immediate needs to be aligned
-//! within 16-bytes
-void unconditional_jump_block(int targetBlockId, bool immediateNeedsAligned) {
-    Mnemonic m = Mnemonic_JMP;
-    dump_blockid_imm(m, targetBlockId, immediateNeedsAligned);
-}
-
-/*!
-\brief generate a single native instruction "jmp reg"
-
-*/
-void unconditional_jump_reg(int reg, bool isPhysical) {
-    dump_reg(Mnemonic_JMP, ATOM_NORMAL, OpndSize_32, reg, isPhysical, LowOpndRegType_gp);
-}
-
-/*!
-\brief generate a single native instruction to call a function
-
-If mode is NCG O1, extra work is performed to dump content of virtual registers to memory.
-*/
-void call(const char* target) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        beforeCall(target);
-    }
-    Mnemonic m = Mnemonic_CALL;
-    bool dummy;
-    OpndSize size = OpndSize_Null;
-    int relOffset = 0;
-    if(!gDvmJit.scheduling)
-        relOffset = getRelativeOffset(target, false, JmpCall_call, &dummy, &size);
-    dump_label(m, size, relOffset, target, false);
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        afterCall(target);
-    }
-}
-/*!
-\brief generate a single native instruction to call a function
-
-*/
-void call_reg(int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_CALL;
-    dump_reg(m, ATOM_NORMAL, OpndSize_32, reg, isPhysical, LowOpndRegType_gp);
-}
-void call_reg_noalloc(int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_CALL;
-    dump_reg_noalloc(m, OpndSize_32, reg, isPhysical, LowOpndRegType_gp);
-}
-
-/*!
-\brief generate a single native instruction to call a function
-
-*/
-void call_mem(int disp, int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_CALL;
-    dump_mem(m, ATOM_NORMAL, OpndSize_32, disp, reg, isPhysical);
-}
-
-/*!
-\brief insert an entry to globalNCGWorklist
-
-*/
-int insertNCGWorklist(s4 relativePC, OpndSize immSize) {
-    int offsetNCG2 = stream - streamMethodStart;
-#ifdef DEBUG_NCG
-    ALOGI("Insert NCGWorklist (goto forward) @ %p offsetPC %x relativePC %x offsetNCG %x", stream, offsetPC, relativePC, offsetNCG2);
-#endif
-    NCGWorklist* item = (NCGWorklist*)malloc(sizeof(NCGWorklist));
-    if(item == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertNCGWorklist");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    item->relativePC = relativePC;
-    item->offsetPC = offsetPC;
-    item->offsetNCG = offsetNCG2;
-    item->codePtr = stream;
-    item->size = immSize;
-    item->nextItem = globalNCGWorklist;
-    globalNCGWorklist = item;
-    return 0;
-}
-
-
-/*
- *search globalNCGWorklist to find the jmp/jcc offset address
- */
-char* searchNCGWorklist(int blockId) {
-    NCGWorklist* ptr = globalNCGWorklist;
-    unsigned instSize;
-
-    while (ptr != NULL) {
-      if (blockId == ptr->relativePC) {
-           instSize = encoder_get_inst_size(ptr->codePtr);
-           assert((uint)(ptr->codePtr + instSize - 4) % 16 <= 12);
-           return (ptr->codePtr + instSize - 4); // 32bit relative offset
-       }
-       ptr = ptr->nextItem;
-    }
-#ifdef DEBUG_NCG
-    ALOGI("can't find item for blockId %d in searchNCGWorklist\n", blockId);
-#endif
-    return NULL;
-}
-
-#ifdef ENABLE_TRACING
-int insertMapWorklist(s4 BCOffset, s4 NCGOffset, int isStartOfPC) {
-    return 0;
-}
-#endif
-/*!
-\brief insert an entry to methodDataWorklist
-
-This function is used by bytecode FILL_ARRAY_DATA, PACKED_SWITCH, SPARSE_SWITCH
-*/
-int insertDataWorklist(s4 relativePC, char* codePtr1) {
-    //insert according to offsetPC+relativePC, smallest at the head
-    DataWorklist* item = (DataWorklist*)malloc(sizeof(DataWorklist));
-    if(item == NULL) {
-        ALOGI("JIT_INFO: Memory allocation failed at insertDataWorklist");
-        SET_JIT_ERROR(kJitErrorMallocFailed);
-        return -1;
-    }
-    item->relativePC = relativePC;
-    item->offsetPC = offsetPC;
-    item->codePtr = codePtr1;
-    item->codePtr2 = stream; //jump_reg for switch
-    DataWorklist* ptr = methodDataWorklist;
-    DataWorklist* prev_ptr = NULL;
-    while(ptr != NULL) {
-        int tmpPC = ptr->offsetPC + ptr->relativePC;
-        int tmpPC2 = relativePC + offsetPC;
-        if(tmpPC2 < tmpPC) {
-            break;
-        }
-        prev_ptr = ptr;
-        ptr = ptr->nextItem;
-    }
-    //insert item before ptr
-    if(prev_ptr != NULL) {
-        prev_ptr->nextItem = item;
-    }
-    else methodDataWorklist = item;
-    item->nextItem = ptr;
-    return 0;
-}
-
-/*!
-\brief work on globalNCGWorklist
-
-*/
-int performNCGWorklist() {
-    NCGWorklist* ptr = globalNCGWorklist;
-    while(ptr != NULL) {
-        int tmpNCG = getLabelOffset (ptr->relativePC);
-        ALOGV("Perform NCG worklist: @ %p target block %d target NCG %x",
-             ptr->codePtr, ptr->relativePC, tmpNCG);
-        assert(tmpNCG >= 0);
-        int relativeNCG = tmpNCG - ptr->offsetNCG;
-        unsigned instSize = encoder_get_inst_size(ptr->codePtr);
-        relativeNCG -= instSize;
-        updateJumpInst(ptr->codePtr, ptr->size, relativeNCG);
-        globalNCGWorklist = ptr->nextItem;
-        free(ptr);
-        ptr = globalNCGWorklist;
-    }
-    return 0;
-}
-void freeNCGWorklist() {
-    NCGWorklist* ptr = globalNCGWorklist;
-    while(ptr != NULL) {
-        globalNCGWorklist = ptr->nextItem;
-        free(ptr);
-        ptr = globalNCGWorklist;
-    }
-}
-
-/*!
-\brief used by bytecode SWITCH
-\param targetPC points to start of the data section
-\param codeInst the code instruction pointer
-\return the offset in native code between add_reg_reg and the data section
-
-Code sequence for SWITCH
-  call ncgGetEIP
-  codeInst: add_reg_reg %eax, %edx
-  jump_reg %edx
-This function returns the offset in native code between add_reg_reg and the data section
-*/
-int getRelativeNCGForSwitch(int targetPC, char* codeInst) {
-    int tmpNCG = mapFromBCtoNCG[targetPC];
-    int offsetNCG2 = codeInst - streamMethodStart;
-    int relativeOff = tmpNCG - offsetNCG2;
-    return relativeOff;
-}
-
-/*!
-\brief work on methodDataWorklist
-*/
-int performDataWorklist(void) {
-    DataWorklist* ptr = methodDataWorklist;
-    if(ptr == NULL) return 0;
-
-    char* codeCacheEnd = ((char *) gDvmJit.codeCache) + gDvmJit.codeCacheSize - CODE_CACHE_PADDING;
-    u2 insnsSize = dvmGetMethodInsnsSize(currentMethod); //bytecode
-    //align stream to multiple of 4
-    int alignBytes = (int)stream & 3;
-    if(alignBytes != 0) alignBytes = 4-alignBytes;
-    stream += alignBytes;
-
-    while(ptr != NULL) {
-        int tmpPC = ptr->offsetPC + ptr->relativePC;
-        int endPC = insnsSize;
-        if(ptr->nextItem != NULL) endPC = ptr->nextItem->offsetPC + ptr->nextItem->relativePC;
-        mapFromBCtoNCG[tmpPC] = stream - streamMethodStart; //offsetNCG in byte
-
-        //handle fill_array_data, packed switch & sparse switch
-        u2 tmpInst = *(currentMethod->insns + ptr->offsetPC);
-        u2* sizePtr;
-        s4* entryPtr_bytecode;
-        u2 tSize, iVer;
-        u4 sz;
-
-        if (gDvmJit.codeCacheFull == true) {
-            // We are out of code cache space. Skip writing data/code to
-            //   code cache. Simply free the item.
-            methodDataWorklist = ptr->nextItem;
-            free(ptr);
-            ptr = methodDataWorklist;
-        }
-
-        switch (INST_INST(tmpInst)) {
-        case OP_FILL_ARRAY_DATA:
-            sz = (endPC-tmpPC)*sizeof(u2);
-            if ((stream + sz) < codeCacheEnd) {
-                memcpy(stream, (u2*)currentMethod->insns+tmpPC, sz);
-#ifdef DEBUG_NCG_CODE_SIZE
-                ALOGI("Copy data section to stream %p: start at %d, %d bytes", stream, tmpPC, sz);
-#endif
-#ifdef DEBUG_NCG
-                ALOGI("Update data section at %p with %d", ptr->codePtr, stream-ptr->codePtr);
-#endif
-                updateImmRMInst(ptr->codePtr, "", stream - ptr->codePtr);
-                stream += sz;
-            } else {
-                gDvmJit.codeCacheFull = true;
-            }
-            break;
-        case OP_PACKED_SWITCH:
-            updateImmRMInst(ptr->codePtr, "", stream-ptr->codePtr);
-            sizePtr = (u2*)currentMethod->insns+tmpPC + 1 /*signature*/;
-            entryPtr_bytecode = (s4*)(sizePtr + 1 /*size*/ + 2 /*firstKey*/);
-            tSize = *(sizePtr);
-            sz = tSize * 4;     /* expected size needed in stream */
-            if ((stream + sz) < codeCacheEnd) {
-                for(iVer = 0; iVer < tSize; iVer++) {
-                    //update entries
-                    s4 relativePC = *entryPtr_bytecode; //relative to ptr->offsetPC
-                    //need stream, offsetPC,
-                    int relativeNCG = getRelativeNCGForSwitch(relativePC+ptr->offsetPC, ptr->codePtr2);
-#ifdef DEBUG_NCG_CODE_SIZE
-                    ALOGI("Convert target from %d to %d", relativePC+ptr->offsetPC, relativeNCG);
-#endif
-                    *((s4*)stream) = relativeNCG;
-                    stream += 4;
-                    entryPtr_bytecode++;
-                }
-            } else {
-                gDvmJit.codeCacheFull = true;
-            }
-            break;
-        case OP_SPARSE_SWITCH:
-            updateImmRMInst(ptr->codePtr, "", stream-ptr->codePtr);
-            sizePtr = (u2*)currentMethod->insns+tmpPC + 1 /*signature*/;
-            s4* keyPtr_bytecode = (s4*)(sizePtr + 1 /*size*/);
-            tSize = *(sizePtr);
-            entryPtr_bytecode = (s4*)(keyPtr_bytecode + tSize);
-            sz = tSize * (sizeof(s4) + 4); /* expected size needed in stream */
-            if ((stream + sz) < codeCacheEnd) {
-                memcpy(stream, keyPtr_bytecode, tSize*sizeof(s4));
-                stream += tSize*sizeof(s4);
-                for(iVer = 0; iVer < tSize; iVer++) {
-                    //update entries
-                    s4 relativePC = *entryPtr_bytecode; //relative to ptr->offsetPC
-                    //need stream, offsetPC,
-                    int relativeNCG = getRelativeNCGForSwitch(relativePC+ptr->offsetPC, ptr->codePtr2);
-                    *((s4*)stream) = relativeNCG;
-                    stream += 4;
-                    entryPtr_bytecode++;
-                }
-            } else {
-                gDvmJit.codeCacheFull = true;
-            }
-            break;
-        }
-
-        //remove the item
-        methodDataWorklist = ptr->nextItem;
-        free(ptr);
-        ptr = methodDataWorklist;
-    }
-    return 0;
-}
-void freeDataWorklist() {
-    DataWorklist* ptr = methodDataWorklist;
-    while(ptr != NULL) {
-        methodDataWorklist = ptr->nextItem;
-        free(ptr);
-        ptr = methodDataWorklist;
-    }
-}
-
-//////////////////////////
-/*!
-\brief check whether a branch target (specified by relative offset in bytecode) is already handled, if yes, return the size of the immediate; otherwise, call insertNCGWorklist.
-
-If the branch target is not handled, call insertNCGWorklist, unknown is set to true, immSize is set to 32.
-
-If the branch target is handled, call estOpndSizeFromImm to set immSize for jump instruction, returns the value of the immediate
-*/
-int getRelativeNCG(s4 tmp, JmpCall_type type, bool* unknown, OpndSize* size) {//tmp: relativePC
-    int tmpNCG = getLabelOffset (tmp);
-
-    *unknown = false;
-    if(tmpNCG <0) {
-        *unknown = true;
-#ifdef SUPPORT_IMM_16
-        *size = OpndSize_16;
-#else
-        *size = OpndSize_32;
-#endif
-        insertNCGWorklist(tmp, *size);
-        return 0;
-    }
-    int offsetNCG2 = stream - streamMethodStart;
-#ifdef DEBUG_NCG
-    ALOGI("Goto backward @ %p offsetPC %d relativePC %d offsetNCG %d relativeNCG %d", stream, offsetPC, tmp, offsetNCG2, tmpNCG-offsetNCG2);
-#endif
-    int relativeOff = tmpNCG - offsetNCG2;
-    *size = estOpndSizeFromImm(relativeOff);
-    return relativeOff - getJmpCallInstSize(*size, type);
-}
-/*!
-\brief a helper function to handle backward branch
-
-input: jump target in %eax; at end of the function, jump to %eax
-*/
-int common_backwardBranch() {
-    if (insertLabel("common_backwardBranch", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-     int startStreamPtr = (int)stream;
-#endif
-
-    spill_reg(PhysicalReg_EAX, true);
-    call("common_periodicChecks_entry");
-    unspill_reg(PhysicalReg_EAX, true);
-    unconditional_jump_reg(PhysicalReg_EAX, true);
-
-#if defined(VTUNE_DALVIK)
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_backwardBranch");
-    }
-#endif
-    return 0;
-}
-#if !defined(WITH_JIT)
-/*!
-\brief common code to handle GOTO
-
-If it is a backward branch, call common_periodicChecks4 to handle GC request.
-Since this is the end of a basic block, globalVREndOfBB are called right before the jump instruction.
-*/
-int common_goto(s4 tmp) { //tmp: relativePC
-    int retCode = 0;
-    if(tmp < 0) {
-#ifdef ENABLE_TRACING
-#if !defined(TRACING_OPTION2)
-        insertMapWorklist(offsetPC + tmp, mapFromBCtoNCG[offsetPC+tmp], 1);
-#endif
-        //(target offsetPC * 2)
-        move_imm_to_reg(OpndSize_32, 2*(offsetPC+tmp), PhysicalReg_EDX, true);
-#endif
-        //call( ... ) will dump VRs to memory first
-        //potential garbage collection will work as designed
-        call_helper_API("common_periodicChecks4");
-    }
-    retCode = handleRegistersEndOfBB(true);
-    if (retCode < 0)
-        return retCode;
-    bool unknown;
-    OpndSize size;
-    int relativeNCG = tmp;
-    if(!gDvmJit.scheduling)
-        relativeNCG = getRelativeNCG(tmp, JmpCall_uncond, &unknown, &size);
-    unconditional_jump_int(relativeNCG, size);
-    return 0;
-}
-//the common code to lower a if bytecode
-int common_if(s4 tmp, ConditionCode cc_next, ConditionCode cc_taken) {
-    if(tmp < 0) { //backward
-        conditional_jump(cc_next, ".if_next", true);
-        common_goto(tmp);
-        if (insertLabel(".if_next", true) == -1)
-            return -1;
-    }
-    else {
-        //if(tmp < 0) ALOGI("skip periodicCheck for if");
-        bool unknown;
-        OpndSize size;
-        int relativeNCG = tmp;
-        if(!gDvmJit.scheduling)
-            relativeNCG = getRelativeNCG(tmp, JmpCall_cond, &unknown, &size); //must be known
-        conditional_jump_int(cc_taken, relativeNCG, size); //CHECK
-    }
-    return 0;
-}
-#else
-
-//! \brief common code to handle GOTO
-//!
-//! \details If it is a backward branch, call common_periodicChecks4
-//! to handle GC request.
-//! Since this is the end of a basic block,
-//! globalVREndOfBB is called right before the jump instruction.
-//! when this is called from JIT, there is no need to check GC
-//!
-//! \param targetBlockId
-//!
-//! \return -1 if error
-int common_goto(s4 targetBlockId) {
-    bool unknown;
-    int retCode = 0;
-    OpndSize size;
-    bool needAlignment = doesJumpToBBNeedAlignment(traceCurrentBB->taken);
-
-    // We call it with true because we want to actually want to update
-    // association tables of children and handle ME spill requests
-    retCode = handleRegistersEndOfBB (true);
-    if (retCode < 0)
-    {
-        //Just pass along error information
-        return retCode;
-    }
-
-    if(gDvmJit.scheduling) {
-        // Assuming that gotos never go to chaining cells because they are not
-        // part of the bytecode and are just for trace transitions
-        unconditional_jump_block((int)targetBlockId, needAlignment);
-    } else {
-        if (needAlignment == true) {
-            alignOffset(1);
-        }
-        int relativeNCG = getRelativeNCG(targetBlockId, JmpCall_uncond, &unknown, &size);
-        unconditional_jump_int(relativeNCG, size);
-    }
-    return 1;
-}
-
-int generateConditionalJumpToTakenBlock (ConditionCode takenCondition)
-{
-    // A basic block whose last bytecode is "if" must have two children
-    assert (traceCurrentBB->taken != NULL);
-    assert (traceCurrentBB->fallThrough != NULL);
-
-    BasicBlock_O1 * takenBB =
-            reinterpret_cast<BasicBlock_O1 *>(traceCurrentBB->taken);
-    BasicBlock_O1 * fallThroughBB =
-            reinterpret_cast<BasicBlock_O1 *>(traceCurrentBB->fallThrough);
-
-    // When assert version is disabled, fallthroughBB is not used
-    (void) fallThroughBB;
-
-    // We should always have a pre backward block before backward chaining cell
-    // so we can assert that here.
-    if (takenBB->blockType == kChainingCellBackwardBranch)
-    {
-        ALOGI("JIT_INFO: No pre-backward on taken branch");
-        SET_JIT_ERROR(kJitErrorTraceFormation);
-        return -1;
-    }
-
-    if (fallThroughBB->blockType == kChainingCellBackwardBranch)
-    {
-        ALOGI("JIT_INFO: No pre-backward on fallThrough branch");
-        SET_JIT_ERROR(kJitErrorTraceFormation);
-        return -1;
-    }
-
-    // The prebackward block should always be the taken branch
-    if (fallThroughBB->blockType == kPreBackwardBlock)
-    {
-        ALOGI("JIT_INFO: Pre-backward branch is fallThrough");
-        SET_JIT_ERROR(kJitErrorTraceFormation);
-        return -1;
-    }
-
-    // Since we have reached the end of basic block, let's handle registers at
-    // end of BB without actually syncing the state. We sync the state below
-    // when we handle each child
-    handleRegistersEndOfBB (false);
-
-    // So if we have a Prebackward block, we need to satisfy associations
-    // of loop entry
-    if (takenBB->blockType == kPreBackwardBlock)
-    {
-        // The child of the prebackward block should always be backward
-        // chaining cell so it should never be null.
-        assert (takenBB->fallThrough != 0);
-
-        BasicBlock_O1 * backward =
-                reinterpret_cast<BasicBlock_O1 *> (takenBB->fallThrough);
-
-        //This must be a backward branch chaining cell
-        assert (backward->blockType == kChainingCellBackwardBranch);
-
-        //Backward CC must always have as child the loop entry
-        assert (backward->fallThrough != 0);
-
-        //Get the child
-        BasicBlock_O1 * loopEntry =
-                reinterpret_cast<BasicBlock_O1 *> (backward->fallThrough);
-
-        //Paranoid. We want to make sure that the loop entry has been
-        //already handled.
-        if (loopEntry->associationTable.hasBeenFinalized() == false)
-        {
-            ALOGI("JIT_INFO: Loop entry still not finalized at common_if");
-            SET_JIT_ERROR(kJitErrorTraceFormation);
-            return -1;
-        }
-
-        //Just in case the current BB has any spill requests, let's handle them
-        //before we satisfy BB associations
-        if (AssociationTable::handleSpillRequestsFromME (currentBB) == false)
-        {
-            return -1;
-        }
-
-        //Now we want to satisfy the associations of the loop entry.
-        //We also inform satisfyBBAssociations that this is a backward branch.
-        if (AssociationTable::satisfyBBAssociations (backward, loopEntry,
-                true) == false)
-        {
-            return -1;
-        }
-    }
-
-    // First sync with the taken child
-    if (AssociationTable::createOrSyncTable (currentBB, false) == false)
-    {
-        return -1;
-    }
-
-    if (gDvmJit.scheduling)
-    {
-        conditional_jump_block (takenCondition, takenBB->id, doesJumpToBBNeedAlignment (takenBB));
-    }
-    else
-    {
-        //Conditional jumps in x86 are 2 bytes
-        alignOffset (2);
-
-        bool unknown;
-        OpndSize size = OpndSize_Null;
-        int relativeNCG = getRelativeNCG (takenBB->id, JmpCall_cond, &unknown, &size);
-        conditional_jump_int (takenCondition, relativeNCG, size);
-    }
-
-    // Now sync with the fallthrough child
-    if (AssociationTable::createOrSyncTable (currentBB, true) == false)
-    {
-        return -1;
-    }
-
-    // Return success
-    return 1;
-}
-#endif
-
-/*!
-\brief helper function to handle null object error
-
-*/
-int common_errNullObject() {
-    if (insertLabel("common_errNullObject", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    move_imm_to_reg(OpndSize_32, (int) gDvm.exNullPointerException, PhysicalReg_ECX, true);
-    unconditional_jump("common_throw", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errNullObject");
-    }
-#endif
-
-    return 0;
-}
-/*!
-\brief helper function to handle string index error
-
-*/
-int common_errStringIndexOutOfBounds() {
-    if (insertLabel("common_errStringIndexOutOfBounds", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    move_imm_to_reg(OpndSize_32, (int)gDvm.exStringIndexOutOfBoundsException, PhysicalReg_ECX, true);
-    unconditional_jump("common_throw", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errStringIndexOutOfBounds");
-    }
-#endif
-    return 0;
-}
-
-/*!
-\brief helper function to handle array index error
-
-*/
-int common_errArrayIndex() {
-    if (insertLabel("common_errArrayIndex", false) == -1)
-        return -1;
-
-    //Get call back
-    void (*backEndSymbolCreationCallback) (const char *, void *) =
-        gDvmJit.jitFramework.backEndSymbolCreationCallback;
-
-    if (backEndSymbolCreationCallback != 0)
-    {
-        backEndSymbolCreationCallback ("common_errArrayIndex", (void*) stream);
-    }
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    move_imm_to_reg(OpndSize_32, LstrArrayIndexException, PhysicalReg_ECX, true);
-    unconditional_jump("common_throw", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errArrayIndex");
-    }
-#endif
-    return 0;
-}
-/*!
-\brief helper function to handle array store error
-
-*/
-int common_errArrayStore() {
-    if (insertLabel("common_errArrayStore", false) == -1)
-        return -1;
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    move_imm_to_reg(OpndSize_32, LstrArrayStoreException, PhysicalReg_ECX, true);
-    unconditional_jump("common_throw", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errArrayStore");
-    }
-#endif
-    return 0;
-}
-/*!
-\brief helper function to handle negative array size error
-
-*/
-int common_errNegArraySize() {
-    if (insertLabel("common_errNegArraySize", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    move_imm_to_reg(OpndSize_32, LstrNegativeArraySizeException, PhysicalReg_ECX, true);
-    unconditional_jump("common_throw", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errNegArraySize");
-    }
-#endif
-    return 0;
-}
-/*!
-\brief helper function to handle divide-by-zero error
-
-*/
-int common_errDivideByZero() {
-    if (insertLabel("common_errDivideByZero", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-    move_imm_to_reg(OpndSize_32, LstrDivideByZero, PhysicalReg_EAX, true);
-    move_imm_to_reg(OpndSize_32, LstrArithmeticException, PhysicalReg_ECX, true);
-    unconditional_jump("common_throw", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errDivideByZero");
-    }
-#endif
-    return 0;
-}
-/*!
-\brief helper function to handle no such method error
-
-*/
-int common_errNoSuchMethod() {
-    if (insertLabel("common_errNoSuchMethod", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    move_imm_to_reg(OpndSize_32, LstrNoSuchMethodError, PhysicalReg_ECX, true);
-    unconditional_jump("common_throw", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errNoSuchMethod");
-    }
-#endif
-    return 0;
-}
-int call_dvmFindCatchBlock();
-
-#define P_GPR_1 PhysicalReg_ESI //self callee-saved
-#define P_GPR_2 PhysicalReg_EBX //exception callee-saved
-#define P_GPR_3 PhysicalReg_EAX //method that caused exception
-/*!
-\brief helper function common_exceptionThrown
-
-*/
-int common_exceptionThrown() {
-    if (insertLabel("common_exceptionThrown", false) == -1)
-        return -1;
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmJitToExceptionThrown;
-    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
-    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_exceptionThrown");
-    }
-#endif
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-
-/*!
-\brief helper function to throw an exception with message
-
-INPUT: obj_reg(%eax), exceptionPtrReg(%ecx)
-SCRATCH: C_SCRATCH_1(%esi) & C_SCRATCH_2(%edx)
-OUTPUT: no
-*/
-int throw_exception_message(int exceptionPtrReg, int obj_reg, bool isPhysical,
-                            int startLR/*logical register index*/, bool startPhysical) {
-    if (insertLabel("common_throw_message", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
- #endif
-    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), obj_reg, isPhysical, C_SCRATCH_1, isScratchPhysical);
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ClassObject, descriptor), C_SCRATCH_1, isScratchPhysical, C_SCRATCH_2, isScratchPhysical);
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, C_SCRATCH_2, isScratchPhysical, 4, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, exceptionPtrReg, true, 0, PhysicalReg_ESP, true);
-    call_dvmThrowWithMessage();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    unconditional_jump("common_exceptionThrown", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_throw_message");
-    }
-#endif
-    return 0;
-}
-/*!
-\brief helper function to throw an exception
-
-scratch: C_SCRATCH_1(%edx)
-*/
-int throw_exception(int exceptionPtrReg, int immReg,
-                    int startLR/*logical register index*/, bool startPhysical) {
-    if (insertLabel("common_throw", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-    scratchRegs[0] = PhysicalReg_EDX; scratchRegs[1] = PhysicalReg_Null;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, immReg, true, 4, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, exceptionPtrReg, true, 0, PhysicalReg_ESP, true);
-    call_dvmThrow();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    unconditional_jump("common_exceptionThrown", false);
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_throw");
-    }
-#endif
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode goto
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_goto(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_GOTO);
-#if !defined(WITH_JIT)
-    u2 tt = INST_AA(inst);
-    s2 tmp = (s2)((s2)tt << 8) >> 8; //00AA --> AA00 --> xxAA
-#else
-    s2 tmp = traceCurrentBB->taken->id;
-#endif
-    int retval = common_goto(tmp);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode goto/16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_goto_16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_GOTO_16);
-#if !defined(WITH_JIT)
-    s2 tmp = (s2)FETCH(1);
-#else
-    s2 tmp = traceCurrentBB->taken->id;
-#endif
-    int retval = common_goto(tmp);
-    return retval;
-}
-
-/**
- * @brief Generate native code for bytecode goto/32
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_goto_32(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_GOTO_32);
-#if !defined(WITH_JIT)
-    u4 tmp = (u4)FETCH(1);
-    tmp |= (u4)FETCH(2) << 16;
-#else
-    s2 tmp = traceCurrentBB->taken->id;
-#endif
-    int retval = common_goto((s4)tmp);
-    return retval;
-}
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecode packed-switch
- * @param mir bytecode representation
- * @param dalvikPC program counter for Dalvik bytecode
- * @return value >= 0 when handled
- */
-int op_packed_switch(const MIR * mir, const u2 * dalvikPC) {
-    int retCode = 0;
-    assert(mir->dalvikInsn.opcode == OP_PACKED_SWITCH);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-
-#ifdef DEBUG_EACH_BYTECODE
-    u2 tSize = 0;
-    s4 firstKey = 0;
-    s4* entries = NULL;
-#else
-    u2* switchData = const_cast<u2 *>(dalvikPC) + (s4)tmp;
-    if (*switchData++ != kPackedSwitchSignature) {
-        /* should have been caught by verifier */
-        dvmThrowInternalError(
-                          "bad packed switch magic");
-        return 0; //no_op
-    }
-    u2 tSize = *switchData++;
-    assert(tSize > 0);
-    s4 firstKey = *switchData++;
-    firstKey |= (*switchData++) << 16;
-    s4* entries = (s4*) switchData;
-    assert(((u4)entries & 0x3) == 0);
-#endif
-
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    //dvmNcgHandlePackedSwitch: testVal, size, first_key, targets
-    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, tSize, 8, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, firstKey, 4, PhysicalReg_ESP, true);
-
-    /* "entries" is constant for JIT
-       it is the 1st argument to dvmJitHandlePackedSwitch */
-    move_imm_to_mem(OpndSize_32, (int)entries, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 1, false, 12, PhysicalReg_ESP, true);
-
-    //We are done using the VRs and it is end of BB, so we handle it right now
-    retCode = handleRegistersEndOfBB (true);
-    if (retCode < 0)
-    {
-        //Just pass along error information
-        return retCode;
-    }
-
-    //if value out of range, fall through (no_op)
-    //return targets[testVal - first_key]
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    call_dvmJitHandlePackedSwitch();
-    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-    //get rPC, %eax has the relative PC offset
-    alu_binary_imm_reg(OpndSize_32, add_opc, (int)dalvikPC, PhysicalReg_EAX, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_2;
-#if defined(WITH_JIT_TUNING)
-    /* Fall back to interpreter after resolving address of switch target.
-     * Indicate a kSwitchOverflow. Note: This is not an "overflow". But it helps
-     * count the times we return from a Switch
-     */
-    move_imm_to_mem(OpndSize_32, kSwitchOverflow, 0, PhysicalReg_ESP, true);
-#endif
-    jumpToInterpNoChain();
-    return 0;
-}
-#undef P_GPR_1
-
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecode sparse-switch
- * @param mir bytecode representation
- * @param dalvikPC program counter for Dalvik bytecode
- * @return value >= 0 when handled
- */
-int op_sparse_switch(const MIR * mir, const u2 * dalvikPC) {
-    int retCode = 0;
-    assert(mir->dalvikInsn.opcode == OP_SPARSE_SWITCH);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-#ifdef DEBUG_EACH_BYTECODE
-    u2 tSize = 0;
-    const s4* keys = NULL;
-    s4* entries = NULL;
-#else
-    u2* switchData = const_cast<u2 *>(dalvikPC) + (s4)tmp;
-
-    if (*switchData++ != kSparseSwitchSignature) {
-        /* should have been caught by verifier */
-        dvmThrowInternalError(
-                          "bad sparse switch magic");
-        return 0; //no_op
-    }
-    u2 tSize = *switchData++;
-    assert(tSize > 0);
-    const s4* keys = (const s4*) switchData;
-    assert(((u4)keys & 0x3) == 0);
-    assert((((u4) ((s4*) switchData + tSize)) & 0x3) == 0);
-#endif
-
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    //dvmNcgHandleSparseSwitch: keys, size, testVal
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, tSize, 4, PhysicalReg_ESP, true);
-
-    /* "keys" is constant for JIT
-       it is the 1st argument to dvmJitHandleSparseSwitch */
-    move_imm_to_mem(OpndSize_32, (int)keys, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 1, false, 8, PhysicalReg_ESP, true);
-
-    //We are done using the VRs and it is end of BB, so we handle it right now
-    retCode = handleRegistersEndOfBB (true);
-    if (retCode < 0)
-    {
-        //Just pass along error information
-        return retCode;
-    }
-
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    //if testVal is in keys, return the corresponding target
-    //otherwise, fall through (no_op)
-    call_dvmJitHandleSparseSwitch();
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-    //get rPC, %eax has the relative PC offset
-    alu_binary_imm_reg(OpndSize_32, add_opc, (int)dalvikPC, PhysicalReg_EAX, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_2;
-#if defined(WITH_JIT_TUNING)
-    /* Fall back to interpreter after resolving address of switch target.
-     * Indicate a kSwitchOverflow. Note: This is not an "overflow". But it helps
-     * count the times we return from a Switch
-     */
-    move_imm_to_mem(OpndSize_32, kSwitchOverflow, 0, PhysicalReg_ESP, true);
-#endif
-    jumpToInterpNoChain();
-    return 0;
-}
-
-#undef P_GPR_1
-
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecode if-eq
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_eq(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_EQ);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    compare_VR_reg(OpndSize_32, vB, 1, false);
-
-    return generateConditionalJumpToTakenBlock (Condition_E);
-}
-
-/**
- * @brief Generate native code for bytecode if-ne
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_ne(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_NE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    compare_VR_reg(OpndSize_32, vB, 1, false);
-
-    return generateConditionalJumpToTakenBlock (Condition_NE);
-}
-
-/**
- * @brief Generate native code for bytecode if-lt
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_lt(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_LT);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    compare_VR_reg(OpndSize_32, vB, 1, false);
-
-    return generateConditionalJumpToTakenBlock (Condition_L);
-}
-
-/**
- * @brief Generate native code for bytecode if-ge
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_ge(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_GE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    compare_VR_reg(OpndSize_32, vB, 1, false);
-
-    return generateConditionalJumpToTakenBlock (Condition_GE);
-}
-
-/**
- * @brief Generate native code for bytecode if-gt
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_gt(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_GT);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    compare_VR_reg(OpndSize_32, vB, 1, false);
-
-    return generateConditionalJumpToTakenBlock (Condition_G);
-}
-
-/**
- * @brief Generate native code for bytecode if-le
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_le(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_LE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    compare_VR_reg(OpndSize_32, vB, 1, false);
-
-    return generateConditionalJumpToTakenBlock (Condition_LE);
-}
-#undef P_GPR_1
-
-/**
- * @brief Generate native code for bytecode if-eqz
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_eqz(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_EQZ);
-    int vA = mir->dalvikInsn.vA;
-
-    compare_imm_VR(OpndSize_32, 0, vA);
-
-    return generateConditionalJumpToTakenBlock (Condition_E);
-}
-
-/**
- * @brief Generate native code for bytecode if-nez
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_nez(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_NEZ);
-    int vA = mir->dalvikInsn.vA;
-
-    compare_imm_VR(OpndSize_32, 0, vA);
-
-    return generateConditionalJumpToTakenBlock (Condition_NE);
-}
-
-/**
- * @brief Generate native code for bytecode if-ltz
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_ltz(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_LTZ);
-    int vA = mir->dalvikInsn.vA;
-
-    compare_imm_VR(OpndSize_32, 0, vA);
-
-    return generateConditionalJumpToTakenBlock (Condition_L);
-}
-
-/**
- * @brief Generate native code for bytecode if-gez
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_gez(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_GEZ);
-    int vA = mir->dalvikInsn.vA;
-
-    compare_imm_VR(OpndSize_32, 0, vA);
-
-    return generateConditionalJumpToTakenBlock (Condition_GE);
-}
-
-/**
- * @brief Generate native code for bytecode if-gtz
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_gtz(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_GTZ);
-    int vA = mir->dalvikInsn.vA;
-
-    compare_imm_VR(OpndSize_32, 0, vA);
-
-    return generateConditionalJumpToTakenBlock (Condition_G);
-}
-
-/**
- * @brief Generate native code for bytecode if-lez
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_if_lez(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_IF_LEZ);
-    int vA = mir->dalvikInsn.vA;
-
-    compare_imm_VR(OpndSize_32, 0, vA);
-
-    return generateConditionalJumpToTakenBlock (Condition_LE);
-}
-
-#define P_GPR_1 PhysicalReg_ECX
-#define P_GPR_2 PhysicalReg_EBX
-/*!
-\brief helper function common_periodicChecks4 to check GC request
-BCOffset in %edx
-*/
-int common_periodicChecks4() {
-    if (insertLabel("common_periodicChecks4", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-#if (!defined(ENABLE_TRACING))
-    get_self_pointer(PhysicalReg_ECX, true);
-    move_mem_to_reg(OpndSize_32, offsetof(Thread, suspendCount), PhysicalReg_ECX, true, PhysicalReg_EAX, true);
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //suspendCount
-    conditional_jump(Condition_NE, "common_handleSuspend4", true); //called once
-    x86_return();
-
-    if (insertLabel("common_handleSuspend4", true) == -1)
-        return -1;
-    push_reg_to_stack(OpndSize_32, PhysicalReg_ECX, true);
-    call_dvmCheckSuspendPending();
-    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    x86_return();
-
-#else
-    ///////////////////
-    //get debuggerActive: 3 memory accesses, and $7
-    move_mem_to_reg(OpndSize_32, offGlue_pSelfSuspendCount, PhysicalReg_Glue, true, P_GPR_1, true);
-    move_mem_to_reg(OpndSize_32, offGlue_pIntoDebugger, PhysicalReg_Glue, true, P_GPR_2, true);
-
-    compare_imm_mem(OpndSize_32, 0, 0, P_GPR_1, true); //suspendCount
-    conditional_jump(Condition_NE, "common_handleSuspend4_1", true); //called once
-
-    compare_imm_mem(OpndSize_32, 0, 0, P_GPR_2, true); //debugger active
-
-    conditional_jump(Condition_NE, "common_debuggerActive4", true);
-
-    //recover registers and return
-    x86_return();
-
-    if (insertLabel("common_handleSuspend4_1", true) == -1)
-        return -1;
-    push_mem_to_stack(OpndSize_32, offGlue_self, PhysicalReg_Glue, true);
-    call_dvmCheckSuspendPending();
-    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    x86_return();
-
-    if (insertLabel("common_debuggerActive4", true) == -1)
-        return -1;
-    //%edx: offsetBC (at run time, get method->insns_bytecode, then calculate BCPointer)
-    move_mem_to_reg(OpndSize_32, offGlue_method, PhysicalReg_Glue, true, P_GPR_1, true);
-    move_mem_to_reg(OpndSize_32, offMethod_insns_bytecode, P_GPR_1, true, P_GPR_2, true);
-    alu_binary_reg_reg(OpndSize_32, add_opc, P_GPR_2, true, PhysicalReg_EDX, true);
-    move_imm_to_mem(OpndSize_32, 0, offGlue_entryPoint, PhysicalReg_Glue, true);
-    unconditional_jump("common_gotoBail", false); //update glue->rPC with edx
-#endif
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_periodicChecks4");
-    }
-#endif
-    return 0;
-}
-//input: %edx PC adjustment
-//CHECK: should %edx be saved before calling dvmCheckSuspendPending?
-/*!
-\brief helper function common_periodicChecks_entry to check GC request
-
-*/
-int common_periodicChecks_entry() {
-    if (insertLabel("common_periodicChecks_entry", false) == -1)
-        return -1;
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EAX;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    get_suspendCount(P_GPR_1, true);
-
-    //get debuggerActive: 3 memory accesses, and $7
-#if 0 //defined(WITH_DEBUGGER)
-    get_debuggerActive(P_GPR_2, true);
-#endif
-
-    compare_imm_reg(OpndSize_32, 0, P_GPR_1, true); //suspendCount
-    conditional_jump(Condition_NE, "common_handleSuspend", true); //called once
-
-#if 0 //defined(WITH_DEBUGGER)
-#ifdef NCG_DEBUG
-    compare_imm_reg(OpndSize_32, 0, P_GPR_2, true); //debugger active
-    conditional_jump(Condition_NE, "common_debuggerActive", true);
-#endif
-#endif
-
-    //recover registers and return
-    x86_return();
-    if (insertLabel("common_handleSuspend", true) == -1)
-        return -1;
-    get_self_pointer(P_GPR_1, true);
-    load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, P_GPR_1, true, 0, PhysicalReg_ESP, true);
-    call_dvmCheckSuspendPending();
-    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    x86_return();
-#ifdef NCG_DEBUG
-    if (insertLabel("common_debuggerActive", true) == -1)
-        return -1;
-    //adjust PC!!! use 0(%esp) TODO
-    set_glue_entryPoint_imm(0); //kInterpEntryInstr);
-    unconditional_jump("common_gotoBail", false);
-#endif
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_periodicChecks_entry");
-    }
-#endif
-
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-/*!
-\brief helper function common_gotoBail
-  input: %edx: BCPointer %esi: Glue
-  set %eax to 1 (switch interpreter = true), recover the callee-saved registers and return
-*/
-int common_gotoBail(void) {
-    if (insertLabel("common_gotoBail", false) == -1)
-        return -1;
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-    //scratchRegs[0] = PhysicalReg_EDX; scratchRegs[1] = PhysicalReg_ESI;
-    //scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    get_self_pointer(PhysicalReg_EAX, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true, offsetof(Thread, interpSave.curFrame), PhysicalReg_EAX, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, offsetof(Thread, interpSave.pc), PhysicalReg_EAX, true);
-
-    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.bailPtr), PhysicalReg_EAX, true, PhysicalReg_ESP, true);
-    move_reg_to_reg(OpndSize_32, PhysicalReg_ESP, true, PhysicalReg_EBP, true);
-    load_effective_addr(FRAME_SIZE-4, PhysicalReg_EBP, true, PhysicalReg_EBP, true);
-    move_imm_to_reg(OpndSize_32, 1, PhysicalReg_EAX, true); //return value
-    move_mem_to_reg(OpndSize_32, -4, PhysicalReg_EBP, true, PhysicalReg_EDI, true);
-    move_mem_to_reg(OpndSize_32, -8, PhysicalReg_EBP, true, PhysicalReg_ESI, true);
-    move_mem_to_reg(OpndSize_32, -12, PhysicalReg_EBP, true, PhysicalReg_EBX, true);
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EBP, true, PhysicalReg_ESP, true);
-    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EBP, true);
-    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    x86_return();
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_gotoBail");
-    }
-#endif
-    return 0;
-}
-/*!
-\brief helper function common_gotoBail_0
-
-  set %eax to 0, recover the callee-saved registers and return
-*/
-int common_gotoBail_0(void) {
-    if (insertLabel("common_gotoBail_0", false) == -1)
-        return -1;
-
-    //Get call back
-    void (*backEndSymbolCreationCallback) (const char *, void *) =
-        gDvmJit.jitFramework.backEndSymbolCreationCallback;
-
-    if (backEndSymbolCreationCallback != 0)
-    {
-        backEndSymbolCreationCallback ("common_gotoBail_0", (void*) stream);
-    }
-
-#if defined VTUNE_DALVIK
-    int startStreamPtr = (int)stream;
-#endif
-
-    get_self_pointer(PhysicalReg_EAX, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true, offsetof(Thread, interpSave.curFrame), PhysicalReg_EAX, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, offsetof(Thread, interpSave.pc), PhysicalReg_EAX, true);
-
-    /*
-    movl    offThread_bailPtr(%ecx),%esp # Restore "setjmp" esp
-    movl    %esp,%ebp
-    addl    $(FRAME_SIZE-4), %ebp       # Restore %ebp at point of setjmp
-    movl    EDI_SPILL(%ebp),%edi
-    movl    ESI_SPILL(%ebp),%esi
-    movl    EBX_SPILL(%ebp),%ebx
-    movl    %ebp, %esp                   # strip frame
-    pop     %ebp                         # restore caller's ebp
-    ret                                  # return to dvmMterpStdRun's caller
-    */
-    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.bailPtr), PhysicalReg_EAX, true, PhysicalReg_ESP, true);
-    move_reg_to_reg(OpndSize_32, PhysicalReg_ESP, true, PhysicalReg_EBP, true);
-    load_effective_addr(FRAME_SIZE-4, PhysicalReg_EBP, true, PhysicalReg_EBP, true);
-    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //return value
-    move_mem_to_reg(OpndSize_32, -4, PhysicalReg_EBP, true, PhysicalReg_EDI, true);
-    move_mem_to_reg(OpndSize_32, -8, PhysicalReg_EBP, true, PhysicalReg_ESI, true);
-    move_mem_to_reg(OpndSize_32, -12, PhysicalReg_EBP, true, PhysicalReg_EBX, true);
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EBP, true, PhysicalReg_ESP, true);
-    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EBP, true);
-    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    x86_return();
-
-#if defined VTUNE_DALVIK
-    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
-        int endStreamPtr = (int)stream;
-        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_gotoBail_0");
-    }
-#endif
-    return 0;
-}
diff --git a/vm/compiler/codegen/x86/LowerMove.cpp b/vm/compiler/codegen/x86/LowerMove.cpp
deleted file mode 100644
index df3d07c..0000000
--- a/vm/compiler/codegen/x86/LowerMove.cpp
+++ /dev/null
@@ -1,182 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file LowerMove.cpp
-    \brief This file lowers the following bytecodes: MOVE_XXX
-*/
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "enc_wrapper.h"
-
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecodes move and
- * move-object
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_move(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MOVE
-            || mir->dalvikInsn.opcode == OP_MOVE_OBJECT);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false/*isPhysical*/);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 2;
-}
-
-/**
- * @brief Generate native code for bytecodes move/from16
- * and move-object/from16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_move_from16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MOVE_FROM16
-            || mir->dalvikInsn.opcode == OP_MOVE_OBJECT_FROM16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 2;
-}
-
-/**
- * @brief Generate native code for bytecodes move/16 and
- * move-object/16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_move_16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MOVE_16
-            || mir->dalvikInsn.opcode == OP_MOVE_OBJECT_16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_32, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 2;
-}
-#undef P_GPR_1
-
-/**
- * @brief Generate native code for bytecode move-wide
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_move_wide(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MOVE_WIDE);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_64, 1, false);
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 2;
-}
-
-/**
- * @brief Generate native code for bytecode move-wide/from16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_move_wide_from16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MOVE_WIDE_FROM16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_64, 1, false);
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 2;
-}
-
-/**
- * @brief Generate native code for bytecode move-wide/16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_move_wide_16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MOVE_WIDE_16);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    get_virtual_reg(vB, OpndSize_64, 1, false);
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 2;
-}
-
-/**
- * @brief Generate native code for bytecodes move-result
- * and move-result-object
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_move_result(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MOVE_RESULT
-            || mir->dalvikInsn.opcode == OP_MOVE_RESULT_OBJECT);
-
-    /* An inlined move result is effectively no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    int vA = mir->dalvikInsn.vA;
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    get_return_value(OpndSize_32, 1, false);
-    set_virtual_reg(vA, OpndSize_32, 1, false);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode move-result-wide
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_move_result_wide(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MOVE_RESULT_WIDE);
-
-    /* An inlined move result is effectively no-op */
-    if (mir->OptimizationFlags & MIR_INLINED)
-        return 0;
-
-    int vA = mir->dalvikInsn.vA;
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    get_return_value(OpndSize_64, 1, false);
-    set_virtual_reg(vA, OpndSize_64, 1, false);
-    return 0;
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-
-/**
- * @brief Generate native code for bytecode move-exception
- * @details Updates virtual register with exception from Thread and then
- * clear the exception from Thread.
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_move_exception(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MOVE_EXCEPTION);
-    int vA = mir->dalvikInsn.vA;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_Null;
-    get_self_pointer(2, false);
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Thread, exception), 2, false, 3, false);
-    move_imm_to_mem(OpndSize_32, 0, OFFSETOF_MEMBER(Thread, exception), 2, false);
-    set_virtual_reg(vA, OpndSize_32, 3, false);
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-
diff --git a/vm/compiler/codegen/x86/LowerObject.cpp b/vm/compiler/codegen/x86/LowerObject.cpp
deleted file mode 100644
index 81f0402..0000000
--- a/vm/compiler/codegen/x86/LowerObject.cpp
+++ /dev/null
@@ -1,1035 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*! \file LowerObject.cpp
-    \brief This file lowers the following bytecodes: CHECK_CAST,
-*/
-#include "libdex/DexOpcodes.h"
-#include "libdex/DexFile.h"
-#include "Lower.h"
-#include "NcgAot.h"
-#include "enc_wrapper.h"
-
-extern void markCard_filled(int tgtAddrReg, bool isTgtPhysical, int scratchReg, bool isScratchPhysical);
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_ESI
-//! LOWER bytecode CHECK_CAST and INSTANCE_OF
-//!   CALL class_resolve (%ebx is live across the call)
-//!        dvmInstanceofNonTrivial
-//!   NO register is live through function check_cast_helper
-int check_cast_nohelper(int vA, u4 tmp, bool instance, int vDest) {
-    get_virtual_reg(vA, OpndSize_32, 1, false); //object
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    /* for trace-based JIT, it is likely that the class is already resolved */
-    bool needToResolve = true;
-    ClassObject *classPtr =
-                (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
-    ALOGV("In check_cast, class is resolved to %p", classPtr);
-    if(classPtr != NULL) {
-        needToResolve = false;
-        ALOGV("check_cast class %s", classPtr->descriptor);
-    }
-    if(needToResolve) {
-        //get_res_classes is moved here for NCG O1 to improve performance of GLUE optimization
-        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-        get_res_classes(4, false);
-    }
-    compare_imm_reg(OpndSize_32, 0, 1, false);
-
-    rememberState(1);
-    //for private code cache, previously it jumped to .instance_of_okay_1
-    //if object reference is null, jump to the handler for this special case
-    if(instance) {
-        conditional_jump(Condition_E, ".instance_of_null", true);
-    }
-    else {
-        conditional_jump(Condition_E, ".check_cast_null", true);
-    }
-    //check whether the class is already resolved
-    //if yes, jump to check_cast_resolved
-    //if not, call class_resolve
-    if(needToResolve) {
-        move_mem_to_reg(OpndSize_32, tmp*4, 4, false, PhysicalReg_EAX, true);
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-        if(instance)
-            conditional_jump(Condition_NE, ".instance_of_resolved", true);
-        else
-            conditional_jump(Condition_NE, ".check_cast_resolved", true);
-        //try to resolve the class
-        rememberState(2);
-        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
-        export_pc(); //trying to resolve the class
-        call_helper_API(".class_resolve");
-        transferToState(2);
-    } //needToResolve
-    else {
-        /* the class is already resolved and is constant */
-        move_imm_to_reg(OpndSize_32, (int)classPtr, PhysicalReg_EAX, true);
-    }
-    //class is resolved, and it is in %eax
-    if(!instance) {
-        if (insertLabel(".check_cast_resolved", true) == -1){
-            return -1;
-        }
-    }
-    else {
-        if (insertLabel(".instance_of_resolved", true) == -1) {
-            return -1;
-        }
-    }
-
-    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), 1, false, 6, false); //object->clazz
-
-    //%eax: resolved class
-    //compare resolved class and object->clazz
-    //if the same, jump to the handler for this special case
-    compare_reg_reg(PhysicalReg_EAX, true, 6, false);
-    rememberState(3);
-    if(instance) {
-        conditional_jump(Condition_E, ".instance_of_equal", true);
-    } else {
-        conditional_jump(Condition_E, ".check_cast_equal", true);
-    }
-
-    //prepare to call dvmInstanceofNonTrivial
-    //INPUT: the resolved class & object reference
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 6, false, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 4, PhysicalReg_ESP, true); //resolved class
-    scratchRegs[0] = PhysicalReg_SCRATCH_3;
-    nextVersionOfHardReg(PhysicalReg_EAX, 2); //next version has 2 refs
-    call_dvmInstanceofNonTrivial();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    //
-    if(instance) {
-        //move return value to P_GPR_2
-        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 3, false);
-        rememberState(4);
-        unconditional_jump(".instance_of_okay", true);
-    } else {
-        //if return value of dvmInstanceofNonTrivial is zero, throw exception
-        compare_imm_reg(OpndSize_32, 0,  PhysicalReg_EAX, true);
-        rememberState(4);
-        conditional_jump(Condition_NE, ".check_cast_okay", true);
-        //two inputs for common_throw_message: object reference in eax, exception pointer in ecx
-        nextVersionOfHardReg(PhysicalReg_EAX, 1); //next version has 1 ref
-        move_reg_to_reg(OpndSize_32, 1, false, PhysicalReg_EAX, true);
-
-        load_imm_global_data_API("strClassCastExceptionPtr", OpndSize_32, PhysicalReg_ECX, true);
-
-        nextVersionOfHardReg(PhysicalReg_EDX, 2); //next version has 2 ref count
-        export_pc();
-
-        unconditional_jump_global_API("common_throw_message", false);
-    }
-    //handler for speical case where object reference is null
-    if(instance) {
-        if (insertLabel(".instance_of_null", true) == -1) {
-            return -1;
-        }
-    }
-    else {
-        if (insertLabel(".check_cast_null", true) == -1) {
-            return -1;
-        }
-    }
-    goToState(1);
-    if(instance) {
-        move_imm_to_reg(OpndSize_32, 0, 3, false);
-    }
-    transferToState(4);
-    if(instance)
-        unconditional_jump(".instance_of_okay", true);
-    else
-        unconditional_jump(".check_cast_okay", true);
-
-    //handler for special case where class of object is the same as the resolved class
-    if(instance) {
-        if (insertLabel(".instance_of_equal", true) == -1){
-            return -1;
-        }
-    }
-    else {
-        if (insertLabel(".check_cast_equal", true) == -1)
-            return -1;
-    }
-    goToState(3);
-    if(instance) {
-        move_imm_to_reg(OpndSize_32, 1, 3, false);
-    }
-    transferToState(4);
-    if(instance) {
-        if (insertLabel(".instance_of_okay", true) == -1) {
-            return -1;
-        }
-    }
-    else {
-        if (insertLabel(".check_cast_okay", true) == -1) {
-            return -1;
-        }
-    }
-    //all cases merge here and the value is put to virtual register
-    if(instance) {
-        set_virtual_reg(vDest, OpndSize_32, 3, false);
-    }
-    return 0;
-}
-//! common code to lower CHECK_CAST & INSTANCE_OF
-
-//!
-int common_check_cast_instance_of(int vA, u4 tmp, bool instance, int vDest) {
-    return check_cast_nohelper(vA, tmp, instance, vDest);
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-
-/**
- * @brief Generate native code for bytecode check-cast
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_check_cast(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_CHECK_CAST);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-    return common_check_cast_instance_of(vA, tmp, false, 0);
-}
-
-/**
- * @brief Generate native code for bytecode instance-of
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_instance_of(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_INSTANCE_OF);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-    u4 tmp = mir->dalvikInsn.vC;
-    return common_check_cast_instance_of(vB, tmp, true, vA);
-}
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-//! LOWER bytecode MONITOR_ENTER without usage of helper function
-
-//!   CALL dvmLockObject
-int monitor_enter_nohelper(int vA, const MIR *mir) {
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        requestVRFreeDelay(vA,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
-    }
-
-    //get_self_pointer is separated
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    //to optimize redundant null check, NCG O1 wraps up null check in a function: nullCheck
-    get_self_pointer(3, false);
-    //If we can't ignore the NULL check
-    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-    {
-        nullCheck(1, false, 1, vA); //maybe optimized away
-        cancelVRFreeDelayRequest(vA,VRDELAY_NULLCHECK);
-    }
-
-    /////////////////////////////
-    //inline the simple case with JIT
-    //simple case is thin lock, held by no-one.
-
-    //backup the self pointer and Oject for native implementation
-    //which will be passed to dvmLockObject() as parameter
-    move_reg_to_reg(OpndSize_32, 1, false, 4, false);
-    move_reg_to_reg(OpndSize_32, 3, false, 5, false);
-
-    //get the Obj->lock
-    move_mem_to_reg(OpndSize_32, offsetof(Object, lock), 1, false, 2, false);
-
-    //if it is the simple case, the object lock should contain all 0s except the hash_state bits
-    //save the value to EAX, which will be used for CMPXCHG
-    alu_binary_imm_reg(OpndSize_32, and_opc, (LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT), 2, false);
-    move_reg_to_reg(OpndSize_32, 2, false, PhysicalReg_EAX, true);
-
-    //get self->threadId
-    move_mem_to_reg(OpndSize_32, offsetof(Thread, threadId), 3, false, 3, false);
-
-    //generate the new lock
-    alu_binary_imm_reg(OpndSize_32, shl_opc, LW_LOCK_OWNER_SHIFT, 3, false);
-    alu_binary_reg_reg(OpndSize_32, or_opc, 2, false, 3, false);
-
-    //add the lock to Object using cmpxchg, if it is simple case, EAX value should be same as Object->lock
-    compareAndExchange(OpndSize_32, 3, false, offsetof(Object, lock), 1, false);
-
-    //remember the state of register before comditional_jump
-    rememberState(1);
-
-    //if successful added lock, jump to the end of this function
-    conditional_jump(Condition_Z, ".call_monitor_native_done", true);
-
-    /////////////////////////////
-    //prepare to call dvmLockObject, inputs: object reference and self
-    // TODO: Should reset inJitCodeCache before calling dvmLockObject
-    //       so that code cache can be reset if needed when locking object
-    //       taking a long time. Not resetting inJitCodeCache may delay
-    //       code cache reset when code cache is full, preventing traces from
-    //       JIT compilation. This has performance implication.
-    //       However, after resetting inJitCodeCache, the code should be
-    //       wrapped in a helper instead of directly inlined in code cache.
-    //       If the code after dvmLockObject call is in code cache and the code
-    //       cache is reset during dvmLockObject call, execution after
-    //       dvmLockObject will return to a cleared code cache region,
-    //       resulting in seg fault.
-    if (insertLabel(".call_monitor_native_implementation", true) == -1) {
-       return -1;
-    }
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_2;
-    call_dvmLockObject();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    //now we restore the register state for later use of VR
-    transferToState(1);
-    if (insertLabel(".call_monitor_native_done", true) == -1) {
-       return -1;
-    }
-    /////////////////////////////
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode monitor-enter
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_monitor_enter(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MONITOR_ENTER);
-    int vA = mir->dalvikInsn.vA;
-#ifdef INC_NCG_O0
-    if(gDvm.helper_switch[11]) {
-        // .monitor_enter_helper
-        //   INPUT: P_GPR_1 (virtual register for object)
-        //   OUTPUT: none
-        //   %esi is live through function monitor_enter_helper
-        export_pc(); //use %edx
-        move_imm_to_reg(OpndSize_32, vA, P_GPR_1, true);
-        spillVirtualReg(vA, LowOpndRegType_gp, true);
-        call_helper_API(".monitor_enter_helper");
-    }
-    else
-#endif
-    {
-        export_pc();
-        monitor_enter_nohelper(vA, mir);
-    }
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-
-/**
- * @brief Generate native code for bytecode monitor-exit
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_monitor_exit(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_MONITOR_EXIT);
-    int vA = mir->dalvikInsn.vA;
-#ifdef INC_NCG_O0
-    if(gDvm.helper_switch[11]) {
-        export_pc();
-        // .moniter_exit_helper
-        //   INPUT: in P_GPR_1 (virtual register for object)
-        //   OUTPUT: none
-        //   %esi is live through function moniter_exit_helper
-        move_imm_to_reg(OpndSize_32, vA, P_GPR_1, true);
-        spillVirtualReg(vA, LowOpndRegType_gp, true);
-        call_helper_API(".monitor_exit_helper");
-    }
-    else
-#endif
-    {
-        ////////////////////
-        //LOWER bytecode MONITOR_EXIT without helper function
-        //inline simple case with JIT,
-        //simple case is thin lock held by unlocking thread with recursive count 0
-        //other case will CALL dvmUnlockObject
-        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-
-        if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-        {
-            requestVRFreeDelay(vA,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
-        }
-
-        get_virtual_reg(vA, OpndSize_32, 1, false);
-
-        if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-        {
-            nullCheck(1, false, 1, vA); //maybe optimized away
-            cancelVRFreeDelayRequest(vA,VRDELAY_NULLCHECK);
-        }
-
-        //get the self pointer
-        get_self_pointer(3,false);
-
-        //get self->threadid
-        move_mem_to_reg(OpndSize_32, offsetof(Thread, threadId), 3, false, 4, false);
-
-        //threadid << 3, for comparison with obj->lock
-        alu_binary_imm_reg(OpndSize_32, shl_opc, 3, 4,false);
-
-        //get obj->lock
-        move_reg_to_reg(OpndSize_32, 1, false, 7, false);
-
-        //get Obj->lock
-        move_mem_to_reg(OpndSize_32, offsetof(Object, lock), 7, false, 5, false);
-        move_reg_to_reg(OpndSize_32, 5, false, 6, false);
-
-        //test whether obj->lock is thin lock and object is locked by current thread
-        alu_binary_imm_reg(OpndSize_32, and_opc,  ~(LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT), 5, false);
-        compare_reg_reg( 4, false, 5, false);
-
-        //In native implementation, dvmUnlockObject() invokes beforeCall() that spill VRs which maybe used later
-        //However, after inlining the JIT code,
-        //there will be a chance that dvmUnlockObject() will not be called,
-        //but the spill in beforecall() marked the VR as in memory,
-        //so later use of VR may incorrectly unspill the value from memory.
-        //To avoid this issue, we remember the state.
-        rememberState(1);
-
-        //locked by other thread or fat lock or recursive lock, jump to call the native functions
-        conditional_jump(Condition_NE, "j_call_dvmUnlockObject", true);
-
-        //create the new words(32bit) for obj->lock, it only contains the hash bits of original obj->lock
-        alu_binary_imm_reg(OpndSize_32, and_opc, (LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT), 6, false);
-
-        //release the lock
-        move_reg_to_mem(OpndSize_32, 6, false, offsetof(Object, lock),7, false);
-
-        //jump to the end of the function
-        unconditional_jump(".unlock_object_done", true);
-        if (insertLabel("j_call_dvmUnlockObject", true) == -1) {
-           return -1;
-        }
-
-        /////////////////////////////
-        //prepare to call dvmUnlockObject, inputs: object reference and self
-        push_reg_to_stack(OpndSize_32, 1, false);
-        push_mem_to_stack(OpndSize_32, offEBP_self, PhysicalReg_EBP, true);
-        scratchRegs[0] = PhysicalReg_SCRATCH_2;
-        call_dvmUnlockObject();
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        //transfor the register state for later use of VR
-        transferToState(1);
-
-#if defined(WITH_JIT)
-        conditional_jump(Condition_NE, ".unlock_object_done", true);
-        //jump to dvmJitToExceptionThrown
-        scratchRegs[0] = PhysicalReg_SCRATCH_3;
-        jumpToExceptionThrown(2/*exception number*/);
-#else
-        //throw exception if dvmUnlockObject returns 0
-        char errName[256];
-        sprintf(errName, "common_exceptionThrown");
-        handlePotentialException(
-                                           Condition_E, Condition_NE,
-                                           2, errName);
-#endif
-        if (insertLabel(".unlock_object_done", true) == -1) {
-            return -1;
-        }
-        ///////////////////////////
-    }
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_EDX /*vA*/
-
-/**
- * @brief Generate native code for bytecode array-length
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_array_length(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_ARRAY_LENGTH);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-#ifdef INC_NCG_O0
-    if(gDvm.helper_switch[14]) {
-        // .array_length_helper
-        //   INPUT: P_GPR_1 (virtual register for array object)
-        //          P_GPR_3 (virtual register for length)
-        //   OUTPUT: none
-        //   %eax, %esi, %ebx: live through function array_length_helper
-        export_pc(); //use %edx
-        move_imm_to_reg(OpndSize_32, vA, P_GPR_3, true);
-        move_imm_to_reg(OpndSize_32, vB, P_GPR_1, true);
-        call_helper_API(".array_length_helper");
-    }
-    else
-#endif
-    {
-        ////////////////////
-        //no usage of helper function
-        if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-        {
-            requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
-        }
-
-        get_virtual_reg(vB, OpndSize_32, 1, false);
-
-        if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
-        {
-            nullCheck(1, false, 1, vB); //maybe optimized away
-            cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
-        }
-
-        move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), 1, false, 2, false);
-        set_virtual_reg(vA, OpndSize_32, 2, false);
-        ///////////////////////
-    }
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_ESI
-
-/**
- * @brief Generate native code for bytecode new-instance
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_new_instance(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_NEW_INSTANCE);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-#ifdef INC_NCG_O0
-    if(gDvm.helper_switch[4]) {
-        // .new_instance_helper
-        //   INPUT: P_GPR_3 (const pool index)
-        //   OUTPUT: %eax
-        //   no register is live through function array_length_helper
-        export_pc();
-        move_imm_to_reg(OpndSize_32, tmp, P_GPR_3, true);
-        call_helper_API(".new_instance_helper");
-    }
-    else
-#endif
-    {
-        export_pc();
-#if defined(WITH_JIT)
-        /* for trace-based JIT, class is already resolved */
-        ClassObject *classPtr =
-              (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
-        assert(classPtr != NULL);
-        assert(classPtr->status & CLASS_INITIALIZED);
-        /*
-         * If it is going to throw, it should not make to the trace to begin
-         * with.  However, Alloc might throw, so we need to genExportPC()
-        */
-        assert((classPtr->accessFlags & (ACC_INTERFACE|ACC_ABSTRACT)) == 0);
-#else
-        //////////////////////////////////////////
-        //resolve class, check whether it has been resolved
-        //if yes, jump to resolved
-        //if no, call class_resolve
-        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-        get_res_classes(3, false);
-        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //resolved class
-        conditional_jump(Condition_NE, ".new_instance_resolved", true);
-        rememberState(1);
-        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
-        call_helper_API(".class_resolve");
-        transferToState(1);
-
-        //here, class is resolved
-        if (insertLabel(".new_instance_resolved", true) == -1)
-            return -1;
-        //check whether the class is initialized
-        //if yes, jump to initialized
-        //if no, call new_instance_needinit
-        movez_mem_to_reg(OpndSize_8, offClassObject_status, PhysicalReg_EAX, true, 5, false);
-        compare_imm_reg(OpndSize_32, CLASS_INITIALIZED, 5, false);
-        conditional_jump(Condition_E, ".new_instance_initialized", true);
-        rememberState(2);
-        call_helper_API(".new_instance_needinit");
-        transferToState(2);
-        //here, class is already initialized
-        if (insertLabel(".new_instance_initialized", true) == -1)
-            return -1;
-        //check whether the class is an interface or abstract, if yes, throw exception
-        move_mem_to_reg(OpndSize_32, offClassObject_accessFlags, PhysicalReg_EAX, true, 6, false);
-        test_imm_reg(OpndSize_32, ACC_INTERFACE|ACC_ABSTRACT, 6, false); //access flags
-
-        //two inputs for common_throw_message: object reference in eax, exception pointer in ecx
-        handlePotentialException(
-                                           Condition_NE, Condition_E,
-                                           2, "common_throw_message");
-#endif
-        //prepare to call dvmAllocObject, inputs: resolved class & flag ALLOC_DONT_TRACK
-        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-#if defined(WITH_JIT)
-        /* 1st argument to dvmAllocObject at -8(%esp) */
-        move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
-#else
-        move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true); //resolved class
-#endif
-        move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 4, PhysicalReg_ESP, true);
-        scratchRegs[0] = PhysicalReg_SCRATCH_3;
-        nextVersionOfHardReg(PhysicalReg_EAX, 3); //next version has 3 refs
-        call_dvmAllocObject();
-        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-        //return value of dvmAllocObject is in %eax
-        //if return value is null, throw exception
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-#if defined(WITH_JIT)
-        conditional_jump(Condition_NE, ".new_instance_done", true);
-        //jump to dvmJitToExceptionThrown
-        scratchRegs[0] = PhysicalReg_SCRATCH_4;
-        jumpToExceptionThrown(3/*exception number*/);
-#else
-        handlePotentialException(
-                                           Condition_E, Condition_NE,
-                                           3, "common_exceptionThrown");
-#endif
-    }
-    if (insertLabel(".new_instance_done", true) == -1)
-        return -1;
-    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-    return 0;
-}
-
-//! function to initialize a class
-
-//!INPUT: %eax (class object) %eax is recovered before return
-//!OUTPUT: none
-//!CALL: dvmInitClass
-//!%eax, %esi, %ebx are live through function new_instance_needinit
-int new_instance_needinit() {
-    if (insertLabel(".new_instance_needinit", false) == -1)
-        return -1;
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 4, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_ECX;
-    call_dvmInitClass();
-    //if return value of dvmInitClass is zero, throw exception
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    //recover EAX with the class object
-    move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true, PhysicalReg_EAX, true);
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    conditional_jump(Condition_E, "common_exceptionThrown", false);
-    x86_return();
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-
-#define P_GPR_1 PhysicalReg_EBX //live through C function, must in callee-saved reg
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_EDX
-
-/**
- * @brief Generate native code for bytecode new-array
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_new_array(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_NEW_ARRAY);
-    int vA = mir->dalvikInsn.vA; //destination
-    int vB = mir->dalvikInsn.vB; //length
-    u4 tmp = mir->dalvikInsn.vC;
-#ifdef INC_NCG_O0
-    if(gDvm.helper_switch[17]) {
-        // .new_array_helper
-        //   INPUT: P_GPR_3 (const pool index)
-        //          P_GPR_1 (virtual register with size of the array)
-        //   OUTPUT: %eax
-        //   no reg is live through function new_array_helper
-        export_pc(); //use %edx
-        move_imm_to_reg(OpndSize_32, tmp, P_GPR_3, true);
-        move_imm_to_reg(OpndSize_32, vB, P_GPR_1, true);
-        spillVirtualReg(vB, LowOpndRegType_gp, true);
-        call_helper_API(".new_array_helper");
-        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-    }
-    else
-#endif
-    {
-        /////////////////////////
-        //   REGS used: %esi, %eax, P_GPR_1, P_GPR_2
-        //   CALL class_resolve, dvmAllocArrayByClass
-        export_pc(); //use %edx
-        //check size of the array, if negative, throw exception
-        get_virtual_reg(vB, OpndSize_32, 5, false);
-        compare_imm_reg(OpndSize_32, 0, 5, false);
-        handlePotentialException(
-                                           Condition_S, Condition_NS,
-                                           1, "common_errNegArraySize");
-#if defined(WITH_JIT)
-       void *classPtr = (void*)
-            (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
-       assert(classPtr != NULL);
-#else
-        //try to resolve class, if already resolved, jump to resolved
-        //if not, call class_resolve
-        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-        get_res_classes(3, false);
-        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-        conditional_jump(Condition_NE, ".new_array_resolved", true);
-        rememberState(1);
-        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
-        call_helper_API(".class_resolve");
-        transferToState(1);
-#endif
-        //here, class is already resolved, the class object is in %eax
-        //prepare to call dvmAllocArrayByClass with inputs: resolved class, array length, flag ALLOC_DONT_TRACK
-        if (insertLabel(".new_array_resolved", true) == -1)
-                return -1;
-        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-#if defined(WITH_JIT)
-        /* 1st argument to dvmAllocArrayByClass at 0(%esp) */
-        move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
-#else
-        move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
-#endif
-        move_reg_to_mem(OpndSize_32, 5, false, 4, PhysicalReg_ESP, true);
-        move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 8, PhysicalReg_ESP, true);
-        scratchRegs[0] = PhysicalReg_SCRATCH_3;
-        nextVersionOfHardReg(PhysicalReg_EAX, 3); //next version has 3 refs
-        call_dvmAllocArrayByClass();
-        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-        //the allocated object is in %eax
-        //check whether it is null, throw exception if null
-        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-#if defined(WITH_JIT)
-        conditional_jump(Condition_NE, ".new_array_done", true);
-        //jump to dvmJitToExceptionThrown
-        scratchRegs[0] = PhysicalReg_SCRATCH_4;
-        jumpToExceptionThrown(2/*exception number*/);
-#else
-        handlePotentialException(
-                                           Condition_E, Condition_NE,
-                                           2, "common_exceptionThrown");
-#endif
-        if (insertLabel(".new_array_done", true) == -1)
-            return -1;
-        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
-        //////////////////////////////////////
-    }
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-
-#define P_GPR_1 PhysicalReg_EBX
-#define P_GPR_2 PhysicalReg_ECX
-#define P_GPR_3 PhysicalReg_ESI
-//! common code to lower FILLED_NEW_ARRAY
-
-//! call: class_resolve call_dvmAllocPrimitiveArray
-//! exception: filled_new_array_notimpl common_exceptionThrown
-int common_filled_new_array(int length, u4 tmp, bool hasRange) {
-    ClassObject *classPtr =
-              (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
-    if(classPtr != NULL) ALOGI("FILLED_NEW_ARRAY class %s", classPtr->descriptor);
-    //check whether class is resolved, if yes, jump to resolved
-    //if not, call class_resolve
-    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    get_res_classes(3, false);
-    move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
-    export_pc();
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //resolved class
-    conditional_jump(Condition_NE, ".filled_new_array_resolved", true);
-    rememberState(1);
-    move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
-    call_helper_API(".class_resolve");
-    transferToState(1);
-    //here, class is already resolved
-    if (insertLabel(".filled_new_array_resolved", true) == -1)
-        return -1;
-    //check descriptor of the class object, if not implemented, throws exception
-    move_mem_to_reg(OpndSize_32, 24, PhysicalReg_EAX, true, 5, false);
-    //load a single byte of the descriptor
-    movez_mem_to_reg(OpndSize_8, 1, 5, false, 6, false);
-    compare_imm_reg(OpndSize_32, 'I', 6, false);
-    conditional_jump(Condition_E, ".filled_new_array_impl", true);
-    compare_imm_reg(OpndSize_32, 'L', 6, false);
-    conditional_jump(Condition_E, ".filled_new_array_impl", true);
-    compare_imm_reg(OpndSize_32, '[', 6, false);
-    conditional_jump(Condition_NE, ".filled_new_array_notimpl", false);
-
-    if (insertLabel(".filled_new_array_impl", true) == -1)
-        return -1;
-    //prepare to call dvmAllocArrayByClass with inputs: classObject, length, flag ALLOC_DONT_TRACK
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, length, 4, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 8, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_3; scratchRegs[1] = PhysicalReg_Null;
-    if(hasRange) {
-        nextVersionOfHardReg(PhysicalReg_EAX, 5+(length >= 1 ? LOOP_COUNT : 0)); //next version
-    }
-    else {
-        nextVersionOfHardReg(PhysicalReg_EAX, 5+length); //next version
-    }
-    call_dvmAllocArrayByClass();
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    //return value of dvmAllocPrimitiveArray is in %eax
-    //if the return value is null, throw exception
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    handlePotentialException(
-                                       Condition_E, Condition_NE,
-                                       3, "common_exceptionThrown");
-
-    /* we need to mark the card of the new array, if it's not an int */
-    compare_imm_reg(OpndSize_32, 'I', 6, false);
-    conditional_jump(Condition_E, ".dont_mark_filled_new_array", true);
-
-    // Need to make copy of EAX, because it's used later in op_filled_new_array()
-    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 6, false);
-
-    markCard_filled(6, false, PhysicalReg_SCRATCH_4, false);
-
-    if (insertLabel(".dont_mark_filled_new_array", true) == -1)
-        return -1;
-
-    //return value of bytecode FILLED_NEW_ARRAY is in GLUE structure
-    scratchRegs[0] = PhysicalReg_SCRATCH_4; scratchRegs[1] = PhysicalReg_Null;
-    set_return_value(OpndSize_32, PhysicalReg_EAX, true);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecode filled-new-array
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_filled_new_array(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_FILLED_NEW_ARRAY);
-    u4 length = mir->dalvikInsn.vA;
-    u4 classIdx = mir->dalvikInsn.vB;
-    int v1, v2, v3, v4, v5;
-
-    // Note that v1, v2, v3, v4, and/or v5 may not be valid.
-    // Always check "length" before using any of them.
-    v5 = mir->dalvikInsn.arg[4];
-    v4 = mir->dalvikInsn.arg[3];
-    v3 = mir->dalvikInsn.arg[2];
-    v2 = mir->dalvikInsn.arg[1];
-    v1 = mir->dalvikInsn.arg[0];
-
-    if (common_filled_new_array(length, classIdx, false) == -1)
-        return -1;
-    if(length >= 1) {
-        //move from virtual register to contents of array object
-        get_virtual_reg(v1, OpndSize_32, 7, false);
-        move_reg_to_mem(OpndSize_32, 7, false, OFFSETOF_MEMBER(ArrayObject, contents), PhysicalReg_EAX, true);
-    }
-    if(length >= 2) {
-        //move from virtual register to contents of array object
-        get_virtual_reg(v2, OpndSize_32, 8, false);
-        move_reg_to_mem(OpndSize_32, 8, false, OFFSETOF_MEMBER(ArrayObject, contents)+4, PhysicalReg_EAX, true);
-    }
-    if(length >= 3) {
-        //move from virtual register to contents of array object
-        get_virtual_reg(v3, OpndSize_32, 9, false);
-        move_reg_to_mem(OpndSize_32, 9, false, OFFSETOF_MEMBER(ArrayObject, contents)+8, PhysicalReg_EAX, true);
-    }
-    if(length >= 4) {
-        //move from virtual register to contents of array object
-        get_virtual_reg(v4, OpndSize_32, 10, false);
-        move_reg_to_mem(OpndSize_32, 10, false, OFFSETOF_MEMBER(ArrayObject, contents)+12, PhysicalReg_EAX, true);
-    }
-    if(length >= 5) {
-        //move from virtual register to contents of array object
-        get_virtual_reg(v5, OpndSize_32, 11, false);
-        move_reg_to_mem(OpndSize_32, 11, false, OFFSETOF_MEMBER(ArrayObject, contents)+16, PhysicalReg_EAX, true);
-    }
-    return 0;
-}
-//! function to handle the error of array not implemented
-
-//!
-int filled_new_array_notimpl() {
-    //two inputs for common_throw:
-    if (insertLabel(".filled_new_array_notimpl", false) == -1)
-        return -1;
-    move_imm_to_reg(OpndSize_32, LstrFilledNewArrayNotImpl, PhysicalReg_EAX, true);
-    move_imm_to_reg(OpndSize_32, (int) gDvm.exInternalError, PhysicalReg_ECX, true);
-    unconditional_jump("common_throw", false);
-    return 0;
-}
-
-#define P_SCRATCH_1 PhysicalReg_EDX
-
-/**
- * @brief Generate native code for bytecode filled-new-array/range
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_filled_new_array_range(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_FILLED_NEW_ARRAY_RANGE);
-    int length = mir->dalvikInsn.vA;
-    u4 classIdx = mir->dalvikInsn.vB;
-    int vC = mir->dalvikInsn.vC;
-    if (common_filled_new_array(length, classIdx, true/*hasRange*/) == -1)
-        return -1;
-    //here, %eax points to the array object
-    if(length >= 1) {
-        //dump all virtual registers used by this bytecode to stack, for NCG O1
-        int k;
-        for(k = 0; k < length; k++) {
-            spillVirtualReg(vC+k, LowOpndRegType_gp, true); //will update refCount
-        }
-        //address of the first virtual register that will be moved to the array object
-        load_effective_addr(vC*4, PhysicalReg_FP, true, 7, false); //addr
-        //start address for contents of the array object
-        load_effective_addr(OFFSETOF_MEMBER(ArrayObject, contents), PhysicalReg_EAX, true, 8, false); //addr
-        //loop counter
-        move_imm_to_reg(OpndSize_32, length-1, 9, false); //counter
-        //start of the loop
-        if (insertLabel(".filled_new_array_range_loop1", true) == -1)
-            return -1;
-        rememberState(1);
-        move_mem_to_reg(OpndSize_32, 0, 7, false, 10, false);
-        load_effective_addr(4, 7, false, 7, false);
-        move_reg_to_mem(OpndSize_32, 10, false, 0, 8, false);
-        load_effective_addr(4, 8, false, 8, false);
-        alu_binary_imm_reg(OpndSize_32, sub_opc, 1, 9, false);
-        transferToState(1);
-        //jump back to the loop start
-        conditional_jump(Condition_NS, ".filled_new_array_range_loop1", true);
-    }
-    return 0;
-}
-#undef P_GPR_1
-#undef P_GPR_2
-#undef P_GPR_3
-#undef P_SCRATCH_1
-
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecode fill-array-data
- * @details Calls dvmInterpHandleFillArrayData
- * @param mir bytecode representation
- * @param dalvikPC program counter for Dalvik bytecode
- * @return value >= 0 when handled
- */
-int op_fill_array_data(const MIR * mir, const u2 * dalvikPC) {
-    assert(mir->dalvikInsn.opcode == OP_FILL_ARRAY_DATA);
-    int vA = mir->dalvikInsn.vA;
-    u4 tmp = mir->dalvikInsn.vB;
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    scratchRegs[1] = PhysicalReg_Null;
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    //prepare to call dvmInterpHandleFillArrayData, input: array object, address of the data
-    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 1, false, 0, PhysicalReg_ESP, true);
-    /* 2nd argument to dvmInterpHandleFillArrayData at 4(%esp) */
-    move_imm_to_mem(OpndSize_32, (int)(dalvikPC+tmp), 4, PhysicalReg_ESP, true);
-    call_dvmInterpHandleFillArrayData();
-    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-    //check return value of dvmInterpHandleFillArrayData, if zero, throw exception
-    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump(Condition_NE, ".fill_array_data_done", true);
-    //jump to dvmJitToExceptionThrown
-    scratchRegs[0] = PhysicalReg_SCRATCH_2;
-    jumpToExceptionThrown(2/*exception number*/);
-    if (insertLabel(".fill_array_data_done", true) == -1)
-        return -1;
-    return 0;
-}
-#undef P_GPR_1
-
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecode throw
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_throw(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_THROW);
-    int vA = mir->dalvikInsn.vA;
-    export_pc();
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-    //null check
-    compare_imm_reg(OpndSize_32, 0, 1, false);
-    conditional_jump(Condition_E, "common_errNullObject", false);
-    //set glue->exception & throw exception
-    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
-    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
-    set_exception(1, false);
-    unconditional_jump("common_exceptionThrown", false);
-    return 0;
-}
-#undef P_GPR_1
-#define P_GPR_1 PhysicalReg_EBX
-
-/**
- * @brief Generate native code for bytecode throw-verification-error
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_throw_verification_error(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_THROW_VERIFICATION_ERROR);
-    int vA = mir->dalvikInsn.vA;
-    int vB = mir->dalvikInsn.vB;
-
-    export_pc();
-    scratchRegs[0] = PhysicalReg_SCRATCH_1;
-    get_glue_method(1, false);
-
-    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, vB, 8, PhysicalReg_ESP, true);
-    move_imm_to_mem(OpndSize_32, vA, 4, PhysicalReg_ESP, true);
-    move_reg_to_mem(OpndSize_32, 1, false, 0, PhysicalReg_ESP, true);
-    scratchRegs[0] = PhysicalReg_SCRATCH_2;
-    call_dvmThrowVerificationError();
-    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-    unconditional_jump("common_exceptionThrown", false);
-    return 0;
-}
-#undef P_GPR_1
diff --git a/vm/compiler/codegen/x86/LowerReturn.cpp b/vm/compiler/codegen/x86/LowerReturn.cpp
deleted file mode 100644
index e82772b..0000000
--- a/vm/compiler/codegen/x86/LowerReturn.cpp
+++ /dev/null
@@ -1,98 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-/*! \file LowerReturn.cpp
-    \brief This file lowers the following bytecodes: RETURN
-
-*/
-
-#include "Lower.h"
-#include "NcgHelper.h"
-
-/**
- * @brief Generates jump to dvmJitHelper_returnFromMethod.
- * @details Uses one scratch register to make the jump
- * @return value 0 when successful
- */
-inline int jumpTocommon_returnFromMethod() {
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
-
-    void * funcPtr = reinterpret_cast<void *>(dvmJitHelper_returnFromMethod);
-
-    // Load save area into EDX
-    load_effective_addr(-sizeofStackSaveArea, PhysicalReg_FP, true, PhysicalReg_EDX, true);
-
-    // We may suffer from agen stall here due if edx is not ready
-    // So instead of doing:
-    //   movl offStackSaveArea_prevFrame(%edx), rFP
-    // We can just compute directly
-    //   movl (offStackSaveArea_prevFrame - sizeofStackSaveArea)(rFP), rFP
-    move_mem_to_reg(OpndSize_32,
-            OFFSETOF_MEMBER(StackSaveArea, prevFrame) - sizeofStackSaveArea,
-            PhysicalReg_FP, true, PhysicalReg_FP, true);
-
-    unconditional_jump_rel32(funcPtr);
-    return 0;
-}
-
-/**
- * @brief Generate native code for bytecodes return-void
- * and return-void-barrier
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_return_void(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_RETURN_VOID
-            || mir->dalvikInsn.opcode == OP_RETURN_VOID_BARRIER);
-
-    // Put self pointer in ecx
-    get_self_pointer(PhysicalReg_ECX, true);
-
-    return jumpTocommon_returnFromMethod();
-}
-
-/**
- * @brief Generate native code for bytecodes return
- * and return-object
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_return(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_RETURN
-            || mir->dalvikInsn.opcode == OP_RETURN_OBJECT);
-    int vA = mir->dalvikInsn.vA;
-    get_virtual_reg(vA, OpndSize_32, 1, false);
-
-    set_return_value(OpndSize_32, 1, false, PhysicalReg_ECX, true);
-
-    return jumpTocommon_returnFromMethod();
-}
-
-/**
- * @brief Generate native code for bytecode return-wide
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_return_wide(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_RETURN_WIDE);
-    int vA = mir->dalvikInsn.vA;
-    get_virtual_reg(vA, OpndSize_64, 1, false);
-
-    set_return_value(OpndSize_64, 1, false, PhysicalReg_ECX, true);
-
-    return jumpTocommon_returnFromMethod();
-}
diff --git a/vm/compiler/codegen/x86/NcgAot.cpp b/vm/compiler/codegen/x86/NcgAot.cpp
deleted file mode 100644
index 75cd822..0000000
--- a/vm/compiler/codegen/x86/NcgAot.cpp
+++ /dev/null
@@ -1,226 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-#include "Lower.h"
-#include "NcgAot.h"
-#include "NcgHelper.h"
-#include "Scheduler.h"
-#include "Singleton.h"
-
-//returns # of ops generated by this function
-//entries relocatable: eip + relativePC
-int get_eip_API() {
-    call("ncgGetEIP");//%edx //will push eip to stack
-    return 1;
-}
-#define NEW_EXPORT_PC
-//!update current PC in the stack frame with %eip
-
-//!
-int export_pc() {
-    /* for trace-based JIT, pc points to bytecode
-       for NCG, pc points to native code */
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
-
-    move_imm_to_mem(OpndSize_32, (int)rPC,
-                    -sizeofStackSaveArea+OFFSETOF_MEMBER(StackSaveArea, xtra.currentPc), PhysicalReg_FP, true);
-    return 1; //return number of ops
-}
-
-/* jump from JIT'ed code to interpreter without chaining */
-int jumpToInterpNoChain() {
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmJitToInterpNoChain;
-    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
-
-    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
-    if(gDvm.executionMode == kExecutionModeNcgO1) touchEax();
-    return 0;
-}
-
-/* jump from JIT'ed code to interpreter becaues of exception */
-int jumpToInterpPunt() {
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmJitToInterpPunt;
-    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
-
-    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
-    //if(gDvm.executionMode == kExecutionModeNcgO1) touchEax();
-    return 0;
-}
-
-/* jump to common_exceptionThrown from JIT'ed code */
-int jumpToExceptionThrown(int exceptionNum) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        rememberState(exceptionNum);
-        export_pc();
-        beforeCall("exception"); //dump GG, GL VRs
-    }
-
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmJitToExceptionThrown;
-    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
-    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
-
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        goToState(exceptionNum);
-    }
-    return 0;
-}
-
-//! generate native code to call dvmNcgInvokeInterpreter
-
-//!the interpreter will start execution from %eax
-int invokeInterpreter(bool fromApp)
-{
-    typedef void (*vmHelper)(int);
-    vmHelper funcPtr = dvmNcgInvokeInterpreter;
-
-    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
-
-    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
-    if(gDvm.executionMode == kExecutionModeNcgO1) touchEax();
-    return 0;
-}
-
-//!work to do before calling a function pointer with code cache enabled
-
-//!
-void callFuncPtr(int funcPtr, const char* funcName) {
-
-    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
-    call_reg(C_SCRATCH_1, isScratchPhysical);
-}
-
-/* generate a "call imm32" */
-void callFuncPtrImm(int funcPtr) {
-    Mnemonic m = Mnemonic_CALL;
-    if(gDvmJit.scheduling)
-        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
-    int relOffset = funcPtr - (int)stream - 5; // 5: Bytes of "call imm32"
-    dump_imm(m, OpndSize_32, relOffset);
-}
-
-//.const_string_resolve: input in %eax, output in %eax
-//.const_string_helper:
-//.class_resolve: input in %eax, output in %eax
-int call_helper_API(const char* helperName) {
-    call(helperName);
-    return 1;
-}
-
-/* check whether we are throwing an exception */
-bool jumpToException(const char* target) {
-    bool isException = false;
-    if(!strncmp(target, "common_err", 10)) isException = true;
-    if(!strncmp(target, "common_throw", 12)) isException = true;
-    if(!strncmp(target, "common_exception", 16)) isException = true;
-    return isException;
-}
-
-int conditional_jump_global_API(
-                                ConditionCode cc, const char* target,
-                                bool isShortTerm) {
-    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
-        condJumpToBasicBlock(stream, cc, currentExceptionBlockIdx);
-        return 1; //return number of ops
-    }
-    conditional_jump(cc, target, isShortTerm);
-    return 1;
-}
-int unconditional_jump_global_API(
-                                  const char* target, bool isShortTerm) {
-    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
-        jumpToBasicBlock(stream, currentExceptionBlockIdx);
-        return 1; //return number of ops
-    }
-    unconditional_jump(target, isShortTerm);
-    return 1;
-}
-
-/* @brief Provides address to global constants
- * @details Essentially provides an associative
- * array of global data values indexed by name
- * @param dataName the string for the data
- * @return pointer to the global data
- */
-int getGlobalDataAddr(const char* dataName) {
-    int dataAddr = 0;
-    if(!strcmp(dataName, "doubNeg")) dataAddr = LdoubNeg;
-    else if(!strcmp(dataName, "intMax")) dataAddr = LintMax;
-    else if(!strcmp(dataName, "intMin")) dataAddr = LintMin;
-    else if(!strcmp(dataName, "valueNanLong")) dataAddr = LvalueNanLong;
-    else if(!strcmp(dataName, "valuePosInfLong")) dataAddr = LvaluePosInfLong;
-    else if(!strcmp(dataName, "valueNegInfLong")) dataAddr = LvalueNegInfLong;
-    else if(!strcmp(dataName, "shiftMask")) dataAddr = LshiftMask;
-    else if(!strcmp(dataName, "value64")) dataAddr = Lvalue64;
-    else if(!strcmp(dataName, "64bits")) dataAddr = L64bits;
-    else if(!strcmp(dataName, "strClassCastExceptionPtr")) dataAddr = LstrClassCastExceptionPtr;
-    else if(!strcmp(dataName, "strInstantiationError")) dataAddr = LstrInstantiationErrorPtr;
-    else if(!strcmp(dataName, "gDvmInlineOpsTable")) dataAddr = (int)gDvmInlineOpsTable;
-    else {
-        ALOGI("JIT_INFO: global data %s not supported\n", dataName);
-        SET_JIT_ERROR(kJitErrorGlobalData);
-    }
-    return dataAddr;
-}
-
-//for shared code cache, we use scratchRegs[0] & [1]
-int load_imm_global_data_API(const char* dataName,
-                         OpndSize size,
-                         int reg, bool isPhysical) {
-
-    //find the address from name
-    int dataAddr = getGlobalDataAddr(dataName);
-    if (dataAddr == 0)
-        return -1;
-    move_imm_to_reg(size, dataAddr, reg, isPhysical);
-    return 0;
-}
-//for shared code cache, we use scratchRegs[0] & [1] & [2]
-//FIXME: [2] is assumed to be hard-coded register
-int load_global_data_API(const char* dataName,
-                         OpndSize size,
-                         int reg, bool isPhysical) {
-
-    //find the address from name
-    int dataAddr = getGlobalDataAddr(dataName);
-    if (dataAddr == 0)
-        return -1;
-    move_mem_to_reg(size, dataAddr, PhysicalReg_Null, true, reg, isPhysical);
-    return 0;
-}
-int load_sd_global_data_API(const char* dataName,
-                            int reg, bool isPhysical) {
-
-    //find the address from name
-    int dataAddr = getGlobalDataAddr(dataName);
-    if (dataAddr == 0)
-        return -1;
-    move_sd_mem_to_reg(dataAddr, PhysicalReg_Null, true, reg, isPhysical);
-    return 0;
-}
-
-int load_fp_stack_global_data_API(const char* dataName,
-                                  OpndSize size) {
-
-    int dataAddr = getGlobalDataAddr(dataName);
-    if (dataAddr == 0)
-        return -1;
-    load_int_fp_stack_imm(size, dataAddr); //fildl
-    return 0;
-}
diff --git a/vm/compiler/codegen/x86/NcgAot.h b/vm/compiler/codegen/x86/NcgAot.h
deleted file mode 100644
index 4d3d978..0000000
--- a/vm/compiler/codegen/x86/NcgAot.h
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-
-#ifndef _DALVIK_NCG_AOT
-#define _DALVIK_NCG_AOT
-int ncgAppGetEIP();
-int get_eip_API();
-int invokeInterpreter(bool fromApp);
-int invokeNcg(bool fromApp);
-int jumpToInterpNoChain();
-int jumpToInterpPunt();
-int jumpToExceptionThrown(int exceptionNum);
-void callFuncPtr(int funcPtr, const char* funcName);
-/** @brief generate a "call imm32"*/
-void callFuncPtrImm(int funcPtr);
-int call_helper_API(const char* helperName);
-int conditional_jump_global_API(
-                                ConditionCode cc, const char* target,
-                                bool isShortTerm);
-int unconditional_jump_global_API(
-                                  const char* target, bool isShortTerm);
-int load_imm_global_data_API(const char* dataName,
-                             OpndSize size,
-                             int reg, bool isPhysical);
-int load_global_data_API(const char* dataName,
-                         OpndSize size,
-                         int reg, bool isPhysical);
-int load_sd_global_data_API(const char* dataName,
-                            int reg, bool isPhysical);
-int load_fp_stack_global_data_API(const char* dataName,
-                                  OpndSize size);
-#endif
-
diff --git a/vm/compiler/codegen/x86/NcgHelper.cpp b/vm/compiler/codegen/x86/NcgHelper.cpp
deleted file mode 100644
index 739dc2a..0000000
--- a/vm/compiler/codegen/x86/NcgHelper.cpp
+++ /dev/null
@@ -1,104 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-#include "Dalvik.h"
-#include "NcgHelper.h"
-#include "interp/InterpDefs.h"
-
-
-/*
- * Find the matching case.  Returns the offset to the handler instructions.
- *
- * Returns 3 if we don't find a match (it's the size of the packed-switch
- * instruction).
- */
-s4 dvmNcgHandlePackedSwitch(const s4* entries, s4 firstKey, u2 size, s4 testVal)
-{
-    //skip add_reg_reg (ADD_REG_REG_SIZE) and jump_reg (JUMP_REG_SIZE)
-    const int kInstrLen = 4; //default to next bytecode
-    if (testVal < firstKey || testVal >= firstKey + size) {
-        LOGVV("Value %d not found in switch (%d-%d)",
-            testVal, firstKey, firstKey+size-1);
-        return kInstrLen;
-    }
-
-    assert(testVal - firstKey >= 0 && testVal - firstKey < size);
-    LOGVV("Value %d found in slot %d (goto 0x%02x)",
-        testVal, testVal - firstKey,
-        s4FromSwitchData(&entries[testVal - firstKey]));
-    return s4FromSwitchData(&entries[testVal - firstKey]);
-
-}
-/* return the number of bytes to increase the bytecode pointer by */
-s4 dvmJitHandlePackedSwitch(const s4* entries, s4 firstKey, u2 size, s4 testVal)
-{
-    if (testVal < firstKey || testVal >= firstKey + size) {
-        LOGVV("Value %d not found in switch (%d-%d)",
-            testVal, firstKey, firstKey+size-1);
-        return 2*3;//bytecode packed_switch is 6(2*3) bytes long
-    }
-
-    LOGVV("Value %d found in slot %d (goto 0x%02x)",
-        testVal, testVal - firstKey,
-        s4FromSwitchData(&entries[testVal - firstKey]));
-    return 2*s4FromSwitchData(&entries[testVal - firstKey]); //convert from u2 to byte
-
-}
-/*
- * Find the matching case.  Returns the offset to the handler instructions.
- *
- * Returns 3 if we don't find a match (it's the size of the sparse-switch
- * instruction).
- */
-s4 dvmNcgHandleSparseSwitch(const s4* keys, u2 size, s4 testVal)
-{
-    const int kInstrLen = 4; //CHECK
-    const s4* entries = keys + size;
-    int i;
-    for (i = 0; i < size; i++) {
-        s4 k = s4FromSwitchData(&keys[i]);
-        if (k == testVal) {
-            LOGVV("Value %d found in entry %d (goto 0x%02x)",
-                testVal, i, s4FromSwitchData(&entries[i]));
-            return s4FromSwitchData(&entries[i]);
-        } else if (k > testVal) {
-            break;
-        }
-    }
-
-    LOGVV("Value %d not found in switch", testVal);
-    return kInstrLen;
-}
-/* return the number of bytes to increase the bytecode pointer by */
-s4 dvmJitHandleSparseSwitch(const s4* keys, u2 size, s4 testVal)
-{
-    const s4* entries = keys + size;
-    int i;
-    for (i = 0; i < size; i++) {
-        s4 k = s4FromSwitchData(&keys[i]);
-        if (k == testVal) {
-            LOGVV("Value %d found in entry %d (goto 0x%02x)",
-                testVal, i, s4FromSwitchData(&entries[i]));
-            return 2*s4FromSwitchData(&entries[i]); //convert from u2 to byte
-        } else if (k > testVal) {
-            break;
-        }
-    }
-
-    LOGVV("Value %d not found in switch", testVal);
-    return 2*3; //bytecode sparse_switch is 6(2*3) bytes long
-}
diff --git a/vm/compiler/codegen/x86/NcgHelper.h b/vm/compiler/codegen/x86/NcgHelper.h
deleted file mode 100644
index ec56c85..0000000
--- a/vm/compiler/codegen/x86/NcgHelper.h
+++ /dev/null
@@ -1,45 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-#ifndef _DALVIK_NCG_HELPER
-#define _DALVIK_NCG_HELPER
-#include "mterp/Mterp.h"
-s4 dvmNcgHandlePackedSwitch(const s4*, s4, u2, s4);
-s4 dvmNcgHandleSparseSwitch(const s4*, u2, s4);
-extern "C" s4 dvmJitHandlePackedSwitch(const s4*, s4, u2, s4);
-s4 dvmJitHandleSparseSwitch(const s4*, u2, s4);
-extern "C" void dvmNcgInvokeInterpreter(int pc); //interpreter to execute at pc
-extern "C" void dvmNcgInvokeNcg(int pc);
-
-#if defined(WITH_JIT)
-extern "C" void dvmJitHelper_returnFromMethod();
-extern "C" void dvmJitToInterpNormal(int targetpc); //in %ebx
-/** @brief interface function between FI and JIT for backward chaining cell */
-extern "C" void dvmJitToInterpBackwardBranch(int targetpc);
-extern "C" void dvmJitToInterpTraceSelect(int targetpc); //in %ebx
-extern "C" void dvmJitToInterpTraceSelectNoChain(int targetpc); //in %ebx
-extern "C" void dvmJitToInterpNoChain(int targetpc); //in %eax
-extern "C" void dvmJitToInterpNoChainNoProfile(int targetpc); //in %eax
-extern "C" void dvmJitToInterpPunt(int targetpc); //in currentPc
-extern "C" void dvmJitToExceptionThrown(int targetpc); //in currentPc
-#endif
-
-extern "C" const Method *dvmJitToPatchPredictedChain(const Method *method,
-                                          Thread *self,
-                                          PredictedChainingCell *cell,
-                                          const ClassObject *clazz);
-#endif /*_DALVIK_NCG_HELPER*/
diff --git a/vm/compiler/codegen/x86/Profile.cpp b/vm/compiler/codegen/x86/Profile.cpp
deleted file mode 100644
index 076b524..0000000
--- a/vm/compiler/codegen/x86/Profile.cpp
+++ /dev/null
@@ -1,492 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "Dalvik.h"
-#include "libdex/DexOpcodes.h"
-
-#include "compiler/CompilerInternals.h"
-#include <sys/mman.h>           /* for protection change */
-#include "Profile.h"
-#include "Singleton.h"
-#include "Scheduler.h"
-
-#if defined(WITH_JIT_TPROFILE)
-/*
- * Translation layout in the code cache.  Note that the codeAddress pointer
- * in JitTable will point directly to the code body (field codeAddress).  The
- * chain cell offset codeAddress - 4, the address of the trace profile counter
- * is at codeAddress - 8, and the loop counter address is codeAddress - 12.
- *
- *      +----------------------------+
- *      | Trace Loop Counter addr    |  -> 4 bytes (EXTRA_BYTES_FOR_LOOP_COUNT_ADDR)
- *      +----------------------------+
- *      | Trace Profile Counter addr |  -> 4 bytes (EXTRA_BYTES_FOR_PROF_ADDR)
- *      +----------------------------+
- *   +--| Offset to chain cell counts|  -> 2 bytes (CHAIN_CELL_COUNT_OFFSET)
- *   |  +----------------------------+
- *   |  | Offset to chain cell       |  -> 2 bytes (CHAIN_CELL_OFFSET)
- *   |  +----------------------------+
- *   |  | Trace profile code         |  <- entry point when profiling (16 bytes)
- *   |  .  -   -   -   -   -   -   - .
- *   |  | Code body                  |  <- entry point when not profiling
- *   |  .                            .
- *   |  |                            |
- *   |  +----------------------------+
- *   |  | Chaining Cells             |  -> 16/20 bytes, 4 byte aligned
- *   |  .                            .
- *   |  .                            .
- *   |  |                            |
- *   |  +----------------------------+
- *   |  | Gap for large switch stmt  |  -> # cases >= MAX_CHAINED_SWITCH_CASES
- *   |  +----------------------------+
- *   +->| Chaining cell counts       |  -> 8 bytes, chain cell counts by type
- *      +----------------------------+
- *      | Trace description          |  -> variable sized
- *      .                            .
- *      |                            |
- *      +----------------------------+
- *      | # Class pointer pool size  |  -> 4 bytes
- *      +----------------------------+
- *      | Class pointer pool         |  -> 4-byte aligned, variable size
- *      .                            .
- *      .                            .
- *      |                            |
- *      +----------------------------+
- *      | Literal pool               |  -> 4-byte aligned, variable size
- *      .                            .
- *      .                            .
- *      |                            |
- *      +----------------------------+
- *
- */
-
-/**
- * @brief A map between bytecode offset and source code line number
- */
-typedef struct jitProfileAddrToLine {
-    u4 lineNum;              /**< @brief The source code line number */
-    u4 bytecodeOffset;       /**< @brief The bytecode offset */
-} jitProfileAddrToLine;
-
-/**
- * @brief Get the loop counter's address
- * @param p The JitEntry of the trace
- * @return The address of the loop counter
- */
-static inline char *getLoopCounterBase(const JitEntry *p)
-{
-    return (char*)p->codeAddress -
-        (EXTRA_BYTES_FOR_PROF_ADDR + EXTRA_BYTES_FOR_CHAINING + EXTRA_BYTES_FOR_LOOP_COUNT_ADDR);
-}
-
-/**
- * @brief Get the trace counter's address
- * @param p The JitEntry of the trace
- * @return The address of the trace counter
- */
-static inline char *getTraceCounterBase(const JitEntry *p)
-{
-    return (char*)p->codeAddress -
-        (EXTRA_BYTES_FOR_PROF_ADDR + EXTRA_BYTES_FOR_CHAINING);
-}
-
-/**
- * @brief Check the trace's loop info
- * @param entry The JitEntry of the trace
- * @return 0 for non-loop, -1 for nested loop, otherwise non-nested loop
- */
-static inline int checkLoopInfo(const JitEntry *entry)
-{
-    if (entry->dPC == 0 || entry->codeAddress == 0) {
-        return 0;
-    }
-
-    JitTraceCounter_t **addr = (JitTraceCounter_t **) getLoopCounterBase(entry);
-    return (int) *addr;
-}
-
-/**
- * @brief Retrieve the profile loop count for a loop trace
- * @param entry The JitEntry of the trace
- * @return The loop count value
- */
-static inline JitTraceCounter_t getProfileLoopCount(const JitEntry *entry)
-{
-    if (entry->dPC == 0 || entry->codeAddress == 0) {
-        return 0;
-    }
-
-    JitTraceCounter_t **p = (JitTraceCounter_t **) getLoopCounterBase(entry);
-
-    return **p;
-}
-
-/**
- * @brief Callback function to track the bytecode offset/line number relationiship
- * @param cnxt A point of jitProfileAddrToLine
- * @param bytecodeOffset The offset of the bytecode
- * @param lineNum The line number
- * @return 0 for success
- */
-static int addrToLineCb (void *cnxt, u4 bytecodeOffset, u4 lineNum)
-{
-    jitProfileAddrToLine *addrToLine = (jitProfileAddrToLine *) cnxt;
-
-    /* Best match so far for this offset */
-    if (addrToLine->bytecodeOffset >= bytecodeOffset) {
-        addrToLine->lineNum = lineNum;
-    }
-    return 0;
-}
-
-/**
- * @brief Reset the trace profile count
- * @param entry The JitEntry of the trace
- */
-static inline void resetProfileCount(const JitEntry *entry)
-{
-    if (entry->dPC == 0 || entry->codeAddress == 0) {
-        return;
-    }
-
-    JitTraceCounter_t **p = (JitTraceCounter_t **) getTraceCounterBase(entry);
-
-    **p = 0;
-}
-
-/**
- * @brief Get the pointer of the chain cell count
- * @param base The pointer point to trace counter
- * @return The pointer of Chain Cell Counts
- */
-static inline ChainCellCounts* getChainCellCountsPointer(const char *base)
-{
-    /* 4 is the size of the profile count */
-    u2 *chainCellOffsetP = (u2 *) (base + EXTRA_BYTES_FOR_PROF_ADDR);
-    u2 chainCellOffset = *chainCellOffsetP;
-    return (ChainCellCounts *) ((char *) chainCellOffsetP + chainCellOffset + EXTRA_BYTES_FOR_CHAINING);
-}
-
-/**
- * @brief Get the starting pointer of the trace description section
- * @param base The pointer point to trace counter
- * @return The pointer of Trace Description
- */
-static JitTraceDescription* getTraceDescriptionPointer(const char *base)
-{
-    ChainCellCounts* pCellCounts = getChainCellCountsPointer(base);
-    return (JitTraceDescription*) ((char*)pCellCounts + sizeof(*pCellCounts));
-}
-
-/**
- * @brief Retrieve the trace profile count
- * @param entry The JitEntry of the trace
- * @return The trace profile count
- */
-static inline JitTraceCounter_t getProfileCount(const JitEntry *entry)
-{
-    if (entry->dPC == 0 || entry->codeAddress == 0) {
-        return 0;
-    }
-
-    JitTraceCounter_t **p = (JitTraceCounter_t **) getTraceCounterBase(entry);
-
-    return **p;
-}
-
-/**
- * @brief Qsort callback function
- * @param entry1 The JitEntry compared
- * @param entry2 The JitEntry compared
- * @return 0 if count is equal, -1 for entry1's counter greater than entry2's, otherwise return 1
- */
-static int sortTraceProfileCount(const void *entry1, const void *entry2)
-{
-    const JitEntry *jitEntry1 = (const JitEntry *)entry1;
-    const JitEntry *jitEntry2 = (const JitEntry *)entry2;
-
-    JitTraceCounter_t count1 = getProfileCount(jitEntry1);
-    JitTraceCounter_t count2 = getProfileCount(jitEntry2);
-
-    return (count1 == count2) ? 0 : ((count1 > count2) ? -1 : 1);
-}
-
-/**
- * @brief Dumps profile info for a single trace
- * @param p The JitEntry of the trace
- * @param silent Wheter to dump the trace count info
- * @param reset Whether to reset the counter
- * @param sum The total count of all the trace
- * @return The trace count
- */
-static int dumpTraceProfile(JitEntry *p, bool silent, bool reset,
-                            unsigned long sum)
-{
-    int idx;
-
-    if (p->codeAddress == 0) {
-        if (silent == false) {
-            ALOGD("TRACEPROFILE NULL");
-        }
-        return 0;
-    }
-
-    JitTraceCounter_t count = getProfileCount(p);
-
-    if (reset == true) {
-        resetProfileCount(p);
-    }
-    if (silent == true) {
-        return count;
-    }
-
-    JitTraceDescription *desc = getTraceDescriptionPointer(getTraceCounterBase(p));
-    const Method *method = desc->method;
-    char *methodDesc = dexProtoCopyMethodDescriptor(&method->prototype);
-    jitProfileAddrToLine addrToLine = {0, desc->trace[0].info.frag.startOffset};
-
-    /*
-     * We may end up decoding the debug information for the same method
-     * multiple times, but the tradeoff is we don't need to allocate extra
-     * space to store the addr/line mapping. Since this is a debugging feature
-     * and done infrequently so the slower but simpler mechanism should work
-     * just fine.
-     */
-    dexDecodeDebugInfo(method->clazz->pDvmDex->pDexFile,
-                       dvmGetMethodCode(method),
-                       method->clazz->descriptor,
-                       method->prototype.protoIdx,
-                       method->accessFlags,
-                       addrToLineCb, 0, &addrToLine);
-
-    ALOGD("TRACEPROFILE 0x%08x % 10d %5.2f%% [%#x(+%d), %d] %s%s;%s",
-         (int) getTraceCounterBase(p),
-         count,
-         ((float ) count) / sum * 100.0,
-         desc->trace[0].info.frag.startOffset,
-         desc->trace[0].info.frag.numInsts,
-         addrToLine.lineNum,
-         method->clazz->descriptor, method->name, methodDesc);
-    free(methodDesc);
-    methodDesc = 0;
-
-    if (checkLoopInfo(p) != 0 && checkLoopInfo(p) != -1) {
-        ALOGD("++++++++++ Loop Trace, loop executed: %d ++++++++++", (int) getProfileLoopCount(p));
-    } else if (checkLoopInfo(p) == -1) {
-        ALOGD("++++++++++ Loop Trace with Nested Loop, can't handle the loop counter for this currently ++++++++++");
-    }
-
-    /* Find the last fragment (ie runEnd is set) */
-    for (idx = 0;
-         (desc->trace[idx].isCode == true) && (desc->trace[idx].info.frag.runEnd == false);
-         idx++) {
-    }
-
-    /*
-     * runEnd must comes with a JitCodeDesc frag. If isCode is false it must
-     * be a meta info field (only used by callsite info for now).
-     */
-    if (desc->trace[idx].isCode == false) {
-        const Method *method = (const Method *)
-            desc->trace[idx+JIT_TRACE_CUR_METHOD-1].info.meta;
-        char *methodDesc = dexProtoCopyMethodDescriptor(&method->prototype);
-        /* Print the callee info in the trace */
-        ALOGD("    -> %s%s;%s", method->clazz->descriptor, method->name,
-             methodDesc);
-        free(methodDesc);
-        methodDesc = 0;
-    }
-    return count;
-}
-
-/**
- * @brief Get the size of a jit trace description
- * @param desc the point of jit trace description we want check
- * @return The size of the jit trace description
- */
-int getTraceDescriptionSize(const JitTraceDescription *desc)
-{
-    int runCount;
-    /* Trace end is always of non-meta type (ie isCode == true) */
-    for (runCount = 0; ; runCount++) {
-        if (desc->trace[runCount].isCode &&
-            desc->trace[runCount].info.frag.runEnd)
-           break;
-    }
-    return sizeof(JitTraceDescription) + ((runCount+1) * sizeof(JitTraceRun));
-}
-
-/**
- * @brief Generate the loop counter profile code for loop trace
- *   Currently only handle the loop trace without nested loops, so just add code to bump up the loop counter before the loop entry basic block
- *   For loop trace with nested loops, set the loop counter's addr to -1
- * @param cUnit The compilation unit of the trace
- * @param bb The basic block is processing
- * @param bbO1 The processing basic block's BasicBlock_O1 version
- * @return the size (in bytes) of the generated code
- */
-int genLoopCounterProfileCode(CompilationUnit *cUnit, BasicBlock_O1 *bbO1)
-{
-    //If the trace is loop trace without nested loop, and the bb processing is the loop entry basic block,
-    //      add loop counter before the trace stream and profile code before the bb
-    //else if the trace is loop trace with nested loop, and the bb processing is the loop entry basic block,
-    //      set the loop counter to -1, so that we can dump the infomation later
-    LoopInformation *info = cUnit->loopInformation;
-    if (info != 0 && bbO1->lastMIRInsn != 0 && info->getLoopInformationByEntry(bbO1) != 0) {
-        if (info->getLoopInformationByEntry(bbO1) != 0) {
-            int nesting = info->getNestedNbr();
-            if (nesting == 0) {
-                if ((gDvmJit.profileMode == kTraceProfilingContinuous) ||
-                    (gDvmJit.profileMode == kTraceProfilingDisabled)) {
-                        //Set the loop counter address
-                        intptr_t addr = (intptr_t)dvmJitNextTraceCounter();
-                        unsigned int *intaddr = reinterpret_cast<unsigned int *>(streamMethodStart
-                                    - EXTRA_BYTES_FOR_LOOP_COUNT_ADDR - EXTRA_BYTES_FOR_PROF_ADDR - EXTRA_BYTES_FOR_CHAINING);
-                        *intaddr = addr;
-
-                        //Add the code before loop entry basic block to bump up the loop counter, the generated code may looks like (19 bytes):
-                        //  LEA -4(ESP), ESP
-                        //  MOV EAX, 0(ESP)
-                        //  MOV #80049734, EAX
-                        //  ADD #1, 0(EAX)
-                        //  MOV 0(ESP), EAX
-                        //  LEA 4(ESP), ESP
-                        load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-                        move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
-                        move_imm_to_reg(OpndSize_32, (int)addr, PhysicalReg_EAX, true);
-                        alu_binary_imm_mem(OpndSize_32, add_opc, 1, 0, PhysicalReg_EAX, true);
-                        move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EAX, true);
-                        load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-
-                        return 19;
-                }
-            } else {
-                // TODO: we should refine the nested loop handle when nested loop enabled
-                // For nested loop, currently we just set the loop counter's addr to -1
-                ALOGD("This trace contains nested loops, cann't handle this currently");
-                unsigned int *intaddr = reinterpret_cast<unsigned int *>(streamMethodStart
-                                    - EXTRA_BYTES_FOR_LOOP_COUNT_ADDR - EXTRA_BYTES_FOR_PROF_ADDR - EXTRA_BYTES_FOR_CHAINING);
-                *intaddr = -1;
-
-                return 0;
-            }
-        }
-    }
-    return 0;
-}
-#endif /* WITH_JIT_TPROFILE */
-
-/**
- * @brief Sort the trace profile counts and dump them
- */
-void dvmCompilerSortAndPrintTraceProfiles(void)
-{
-#if defined(WITH_JIT_TPROFILE)
-    JitEntry *sortedEntries;
-    int numTraces = 0;
-    unsigned long sum = 0;
-    unsigned int i;
-
-    /* Make sure that the table is not changing */
-    dvmLockMutex(&gDvmJit.tableLock);
-
-    /* Sort the entries by descending order */
-    sortedEntries = (JitEntry *)alloca(sizeof(JitEntry) * gDvmJit.jitTableSize);
-    memcpy(sortedEntries, gDvmJit.pJitEntryTable,
-           sizeof(JitEntry) * gDvmJit.jitTableSize);
-    qsort(sortedEntries, gDvmJit.jitTableSize, sizeof(JitEntry),
-          sortTraceProfileCount);
-
-    /* Dump the sorted entries */
-    for (i=0; i < gDvmJit.jitTableSize; i++) {
-        if (sortedEntries[i].dPC != 0) {
-            sum += dumpTraceProfile(&sortedEntries[i],
-                                    true,
-                                    false,
-                                    0);
-            numTraces++;
-        }
-    }
-
-    if (numTraces == 0) {
-        numTraces = 1;
-    }
-    if (sum == 0) {
-        sum = 1;
-    }
-
-    ALOGI("JIT: Average execution count -> %d",(int)(sum / numTraces));
-
-    /* Dump the sorted entries. The count of each trace will be reset to 0. */
-    for (i=0; i < gDvmJit.jitTableSize; i++) {
-        if (sortedEntries[i].dPC != 0) {
-                dumpTraceProfile(&sortedEntries[i],
-                             false /* silent */,
-                             true /* reset */,
-                             sum);
-        }
-    }
-
-done:
-    dvmUnlockMutex(&gDvmJit.tableLock);
-#endif
-    return;
-}
-
-/**
- *@brief Generate the trace count profile code before the begin of trace code
- *@details Reserve 12 bytes at the beginning of the trace
- *        +----------------------------+
- *        | loop counter addr (4 bytes)|
- *        +----------------------------+
- *        | prof counter addr (4 bytes)|
- *        +----------------------------+
- *        | chain cell offset (4 bytes)|
- *        +----------------------------+
- *
- * ...and then code to increment the execution
- *
- * For continuous profiling (16 bytes)
- *       MOV   EAX, addr     @ get prof count addr    [5 bytes]
- *       ADD   #1, 0(EAX)    @ increment counter      [6 bytes]
- *       NOPS                                         [5 bytes]
- *
- *@param cUnit the compilation unit
- *@return the size (in bytes) of the generated code.
- */
-int genTraceProfileEntry(CompilationUnit *cUnit)
-{
-#if defined(WITH_JIT_TPROFILE)
-    intptr_t addr = (intptr_t)dvmJitNextTraceCounter();
-    assert(__BYTE_ORDER == __LITTLE_ENDIAN);
-    unsigned int *intaddr = reinterpret_cast<unsigned int *>(streamMethodStart - EXTRA_BYTES_FOR_PROF_ADDR - EXTRA_BYTES_FOR_CHAINING);
-    *intaddr = addr;
-
-    cUnit->headerSize = EXTRA_BYTES_FOR_PROF_ADDR + EXTRA_BYTES_FOR_CHAINING + EXTRA_BYTES_FOR_LOOP_COUNT_ADDR;
-    if ((gDvmJit.profileMode == kTraceProfilingContinuous) ||
-        (gDvmJit.profileMode == kTraceProfilingDisabled)) {
-        move_imm_to_reg(OpndSize_32, (int)addr, PhysicalReg_EAX, true);
-        alu_binary_imm_mem(OpndSize_32, add_opc, 1, 0, PhysicalReg_EAX, true);
-        if(gDvmJit.scheduling == true) {
-            singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
-        }
-        /*Add 5 nops to the end to make sure trace can align with 16B*/
-        stream = encoder_nops(5, stream);
-        return 16;
-    }
-#endif
-    return 0;
-}
diff --git a/vm/compiler/codegen/x86/Profile.h b/vm/compiler/codegen/x86/Profile.h
deleted file mode 100644
index 9d58a43..0000000
--- a/vm/compiler/codegen/x86/Profile.h
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef H_ASSEMBLE
-#define H_ASSEMBLE
-
-struct BasicBlock_O1;
-struct JitTraceDescription;
-struct CompilationUnit;
-
-/* 4 is the number  f additional bytes needed for chaining information for trace:
- * 2 bytes for chaining cell count offset and 2 bytes for chaining cell offset */
-#define EXTRA_BYTES_FOR_CHAINING 4
-
-#ifdef WITH_JIT_TPROFILE
-
-/* 4 is the number  f additional bytes needed for loop count addr */
-#define EXTRA_BYTES_FOR_LOOP_COUNT_ADDR 4
-/* 4 is the number  f additional bytes needed for execution count addr */
-#define EXTRA_BYTES_FOR_PROF_ADDR 4
-
-/**
- * @brief Get the size of a jit trace description
- * @param desc the point of jit trace description we want check
- * @return The size of the jit trace description
- */
-int getTraceDescriptionSize(const JitTraceDescription *desc);
-
-/**
- * @brief Generate the loop counter profile code for loop trace
- *   Currently only handle the loop trace without nested loops, so just add code to bump up the loop counter before the loop entry basic block
- *   For loop trace with nested loops, set the loop counter's addr to -1
- * @param cUnit The compilation unit of the trace
- * @param bbO1 The current basic block being processed
- * @return the size (in bytes) of the generated code
- */
-int genLoopCounterProfileCode(CompilationUnit *cUnit, BasicBlock_O1 *bbO1);
-
-#endif /* WITH_JIT_TPROFILE */
-
-/**
- * @brief Generate the trace counter profile code for each trace
- * @param cUnit The compilation unit of the trace
- * @return the size (in bytes) of the generated code
- */
-int genTraceProfileEntry(CompilationUnit *cUnit);
-
-#endif
diff --git a/vm/compiler/codegen/x86/RegisterizationBE.cpp b/vm/compiler/codegen/x86/RegisterizationBE.cpp
deleted file mode 100644
index 77f86e1..0000000
--- a/vm/compiler/codegen/x86/RegisterizationBE.cpp
+++ /dev/null
@@ -1,1569 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include <map>
-#include <set>
-#include <algorithm>
-#include "Dalvik.h"
-#include "Lower.h"
-#include "AnalysisO1.h"
-#include "CodegenErrors.h"
-#include "RegisterizationBE.h"
-
-//#define DEBUG_REGISTERIZATION
-
-#ifdef DEBUG_REGISTERIZATION
-#define DEBUG_ASSOCIATION(X) X
-#define DEBUG_SPILLING(X) X
-#define DEBUG_ASSOCIATION_MERGE(X) X
-#define DEBUG_COMPILETABLE_UPDATE(X) X
-#else
-#define DEBUG_ASSOCIATION(X)
-#define DEBUG_SPILLING(X)
-#define DEBUG_ASSOCIATION_MERGE(X)
-#define DEBUG_COMPILETABLE_UPDATE(X)
-#endif
-
-AssociationTable::AssociationTable(void) {
-    //Call clear function, it will reset everything
-    clear();
-}
-
-AssociationTable::~AssociationTable(void) {
-    //Call clear function, it will reset everything
-    clear();
-}
-
-void AssociationTable::clear(void) {
-    DEBUG_ASSOCIATION (ALOGD ("Clearing association table\n"));
-
-    //Clear maps
-    associations.clear();
-    inMemoryTracker.clear();
-    constTracker.clear();
-
-    //Have we finalized the table
-    isFinal = false;
-}
-
-bool AssociationTable::copy(AssociationTable &source) {
-    //We cannot copy anything if we are finalized
-    assert (hasBeenFinalized() == false);
-
-    //Insert all associations from source
-    associations.insert(source.associations.begin(), source.associations.end());
-
-    //Insert all memory trackers
-    inMemoryTracker.insert(source.inMemoryTracker.begin(),
-            source.inMemoryTracker.end());
-
-    //Insert all constants
-    constTracker.insert(source.constTracker.begin(), source.constTracker.end());
-
-    //Finalize the current table and return success
-    finalize();
-    return true;
-}
-
-bool AssociationTable::associate(const CompileTableEntry &compileEntry)
-{
-    // We cannot update once the association table has been finalized
-    assert (hasBeenFinalized() == false);
-
-    // Paranoid: this must be a virtual register
-    assert (compileEntry.isVirtualReg () == true);
-
-    //Get local versions of the compileEntry
-    int VR = compileEntry.regNum;
-    int physicalReg = compileEntry.physicalReg;
-
-    bool safeToUpdate = true;
-
-    // Check if we are overwriting an existing association
-    iterator assocEntry = associations.find(VR);
-    if (assocEntry != associations.end())
-    {
-        int oldPhysicalReg = assocEntry->second.physicalReg;
-
-        //If the new physical register is null, then we don't want to update the
-        //association that we saved already.
-        if (physicalReg == PhysicalReg_Null)
-        {
-            safeToUpdate = false;
-        }
-
-        // We might be saving VRs even when they don't have physical register
-        // associated and thus we don't care for overwriting unless one has
-        // physical register
-        if (oldPhysicalReg != PhysicalReg_Null && physicalReg != PhysicalReg_Null) {
-            // Overwriting an association must mean that we are reading from a source
-            // that has the duplicate entries for the same VR. Most likely this can
-            // happen when a VR is associated with XMM and GP in same trace
-            ALOGI ("JIT_INFO: Overwriting association of v%d:%s with %s\n", VR,
-                    physicalRegToString(static_cast<PhysicalReg>(oldPhysicalReg)),
-                    physicalRegToString(static_cast<PhysicalReg>(physicalReg)));
-            SET_JIT_ERROR(kJitErrorBERegisterization);
-            return false;
-        }
-    }
-
-    //We only do the update if it is safe
-    if (safeToUpdate == true)
-    {
-        if (assocEntry != associations.end())
-        {
-            //If we already have an entry for this VR then simply update its compile entry
-            assocEntry->second = compileEntry;
-        }
-        else
-        {
-            //Otherwise we insert it into our associations
-            associations.insert (std::make_pair (VR, compileEntry));
-        }
-
-        DEBUG_ASSOCIATION (ALOGD ("Associating v%d with %s\n", VR,
-                physicalRegToString(static_cast<PhysicalReg> (physicalReg))));
-    }
-
-    //Report success
-    return true;
-}
-
-bool AssociationTable::associate(const MemoryVRInfo &memVRInfo) {
-    // We cannot update once the association table has been finalized
-    assert (hasBeenFinalized() == false);
-
-    int VR = memVRInfo.regNum;
-
-    // Make a copy of the in memory information
-    inMemoryTracker[VR] = memVRInfo;
-
-    return true;
-}
-
-bool AssociationTable::associate(const ConstVRInfo &constVRInfo) {
-    // We cannot update once the association table has been finalized
-    assert (hasBeenFinalized() == false);
-
-    int VR = constVRInfo.regNum;
-
-    // Make a copy of the in memory information
-    constTracker[VR] = constVRInfo;
-
-    return true;
-}
-
-bool AssociationTable::wasVRInMemory(int VR) const
-{
-    //Find the VR in the inMemoryTracker map
-    inMemoryTrackerConstIterator entry = inMemoryTracker.find(VR);
-
-    //If we cannot find then it must be in memory. Our parent would have kept track of it
-    //if it used it. Since it did not use it, it must be in memory.
-    if (entry == inMemoryTracker.end())
-    {
-        return true;
-    }
-    else
-    {
-        //Return what the entry tells us
-        return entry->second.inMemory;
-    }
-}
-
-bool AssociationTable::wasVRConstant(int VR) const
-{
-    //Find the VR in the constTracker map
-    constantTrackerConstIterator entry = constTracker.find(VR);
-
-    //Return whether we found it
-    return (entry != constTracker.end());
-}
-
-int AssociationTable::getVRConstValue(int VR) const
-{
-    //Find the VR in the constTracker map
-    constantTrackerConstIterator entry = constTracker.find(VR);
-
-    //Paranoid: this function should not be called if wasVRConstant returns false
-    assert(entry != constTracker.end());
-
-    //Return value
-    return entry->second.value;
-}
-
-void AssociationTable::finalize() {
-    //Set to final
-    isFinal = true;
-}
-
-void AssociationTable::findUsedRegisters (std::set<PhysicalReg> & outUsedRegisters) const
-{
-    // Go through all association table entries and find used registers
-    for (const_iterator iter = begin(); iter != end(); iter++)
-    {
-        //Get a local version of the register used
-        PhysicalReg regUsed = iter->second.getPhysicalReg ();
-
-        //If not the Null register, insert it
-        if (regUsed != PhysicalReg_Null)
-        {
-            outUsedRegisters.insert(regUsed);
-        }
-    }
-}
-
-void AssociationTable::printToDot(FILE * file) {
-    DEBUG_ASSOCIATION(ALOGD("Printing association table to dot file"));
-
-    if (associations.size() == 0) {
-        fprintf(file, " {Association table is empty} |\\\n");
-    }
-    else
-    {
-        fprintf(file, " {Association table at entry:}|\\\n");
-
-        //Now go through the iteration
-        for (const_iterator iter = associations.begin(); iter != associations.end(); iter++) {
-            //If it's a constant, print it out using %d
-            if (wasVRConstant(iter->second.regNum)) {
-                fprintf(file, "{v%d : %d} | \\\n", iter->first,
-                        getVRConstValue(iter->second.regNum));
-            } else {
-                //Otherwise, use the physicalRegToString function
-                fprintf(file, "{v%d : %s} | \\\n", iter->first,
-                        physicalRegToString(
-                            static_cast<PhysicalReg>(iter->second.physicalReg)));
-            }
-        }
-    }
-}
-
-static bool shouldSaveAssociation(const CompileTableEntry &compileEntry)
-{
-    int vR = compileEntry.regNum;
-    LowOpndRegType type = compileEntry.getPhysicalType ();
-
-    // We want to save association if the VR is either in a physical register or is a constant
-    bool res = compileEntry.inPhysicalRegister () || isVirtualRegConstant (vR, type, 0, false) != VR_IS_NOT_CONSTANT;
-
-    return res;
-}
-
-bool AssociationTable::syncAssociationsWithCompileTable (AssociationTable & associationsToUpdate)
-{
-    if (associationsToUpdate.hasBeenFinalized ())
-    {
-        ALOGI ("JIT_INFO: Association table has been finalized but we want to update it.");
-        SET_JIT_ERROR(kJitErrorBERegisterization);
-        return false;
-    }
-
-    // Go through each entry of the compile table
-    for (int entry = 0; entry < compileTable.size (); entry++)
-    {
-        // Update associations for every VR entry we find
-        if (compileTable[entry].isVirtualReg () == true && shouldSaveAssociation (compileTable[entry]) == true)
-        {
-            if (associationsToUpdate.associate (compileTable[entry]) == false)
-            {
-                return false;
-            }
-        }
-    }
-
-    // Go through each entry in memVRTable to save whether VR is in memory
-    for (int entry = 0; entry < num_memory_vr; entry++)
-    {
-        if (associationsToUpdate.associate (memVRTable[entry]) == false)
-        {
-            return false;
-        }
-    }
-
-    // Go through each entry in constVRTable
-    for (int entry = 0; entry < num_const_vr; entry++)
-    {
-        // Only save entry if it is actually a constant
-        if (constVRTable[entry].isConst == true)
-        {
-            if (associationsToUpdate.associate (constVRTable[entry]) == false)
-            {
-                return false;
-            }
-        }
-    }
-
-    //Finalize the table and report success
-    associationsToUpdate.finalize();
-
-    return true;
-}
-
-bool AssociationTable::syncCompileTableWithAssociations(AssociationTable & associationsToUse) {
-    DEBUG_COMPILETABLE_UPDATE(ALOGD("There are %d associations to merge",
-            associationsToUse.size()));
-
-    // Go through every association we saved
-    for (AssociationTable::const_iterator assocIter = associationsToUse.begin();
-            assocIter != associationsToUse.end(); assocIter++)
-    {
-        //Suppose we will not find the entry
-        bool foundCompileTableEntry = false;
-
-        DEBUG_COMPILETABLE_UPDATE(ALOGD("Starting to search through compile "
-                "table that has %d entries", compileTable.size ()));
-
-        int vR = assocIter->first;
-        const CompileTableEntry &associationEntry = assocIter->second;
-
-        // Now search the compile table for an appropriate entry
-        for (int entry = 0; entry < compileTable.size (); entry++)
-        {
-            //If it is a virtual register and the right register
-            if (compileTable[entry].isVirtualReg () == true
-                    && compileTable[entry].getPhysicalType () == associationEntry.getPhysicalType ()
-                    && compileTable[entry].getRegisterNumber () == vR)
-            {
-                DEBUG_COMPILETABLE_UPDATE(ALOGD("Found that v%d is in compile "
-                        "table already.", assocIter->first));
-
-                // The only relevant part we care about updating is the physical register
-                compileTable[entry].setPhysicalReg (associationEntry.getPhysicalReg ());
-
-                //Mark that we found it
-                foundCompileTableEntry = true;
-                break;
-            }
-        }
-
-        // If we did not find an entry, we must insert it
-        if (foundCompileTableEntry == false)
-        {
-            DEBUG_COMPILETABLE_UPDATE(ALOGD("We have not found v%d in compile "
-                        "table so we will make a new entry.", assocIter->first));
-
-            CompileTableEntry newEntry = assocIter->second;
-
-            //Since we added it ourselves and it wasn't there before, lets reset it
-            newEntry.reset ();
-
-            //Now set its physical register correctly
-            newEntry.setPhysicalReg (assocIter->second.getPhysicalReg ());
-
-            //Add it to the global compileTable
-            compileTable.insert (newEntry);
-        }
-    }
-
-    // In case we have updated the compile table, we must also update the
-    // state of registers to match what compile table believes
-    if (associationsToUse.size() > 0) {
-        syncAllRegs();
-    }
-
-    DEBUG_COMPILETABLE_UPDATE (ALOGD ("Finished merging associations into compile table"));
-
-    //Report success
-    return true;
-}
-
-/**
- * @brief Used to represent the possibilities of the state of a virtual register.
- */
-enum VirtualRegisterState
-{
-    VRState_InMemory = 0, //!< In memory.
-    VRState_InGP,         //!< In general purpose register.
-    VRState_Constant,     //!< Constant value.
-    VRState_NonWideInXmm, //!< Non-wide VR in xmm register.
-    VRState_WideInXmm,    //!< Wide VR in xmm register.
-    VRState_HighOfWideVR, //!< The high bits when we have wide VR.
-};
-
-#ifdef DEBUG_REGISTERIZATION
-/**
- * @brief Provides a mapping between the virtual register state and representative string.
- * @param state The virtual register state.
- * @return Returns string representation of virtual register state.
- */
-static const char* convertVirtualRegisterStateToString (VirtualRegisterState state)
-{
-    switch (state)
-    {
-        case VRState_InMemory:
-            return "in memory";
-        case VRState_InGP:
-            return "in GP";
-        case VRState_Constant:
-            return "constant";
-        case VRState_NonWideInXmm:
-            return "non-wide in xmm";
-        case VRState_WideInXmm:
-            return "wide in xmm";
-        case VRState_HighOfWideVR:
-            return "high of wide";
-        default:
-            break;
-    }
-
-    return "invalid state";
-}
-#endif
-
-/**
- * @brief Container for keeping track of actions attributed with a VR when state mismatch
- * is found between two basic blocks.
- */
-struct VirtualRegisterStateActions
-{
-    std::set<int> virtualRegistersToStore;        //!< Set of VRs to store on stack.
-    std::set<int> virtualRegistersToLoad;         //!< Set of VRs to load into registers.
-    std::set<int> virtualRegistersRegToReg;       //!< Set of VRs that must be moved to different registers.
-    std::set<int> virtualRegistersCheckConstants; //!< Set of VRs that are constants but must be checked for consistency.
-    std::set<int> virtualRegistersImmToReg;       //!< Set of VRs that are constant but must be moved to register.
-};
-
-/**
- * @brief Fills the set of virtual registers with the union of all VRs used in both parent and child.
- * @param parentAssociations The association table of parent.
- * @param childAssociations The association table of child.
- * @param virtualRegisters Updated by function to contain the set of all VRs used in both parent and child.
- */
-static void filterVirtualRegisters (const AssociationTable &parentAssociations,
-                                    const AssociationTable &childAssociations,
-                                    std::set<int> &virtualRegisters)
-{
-    AssociationTable::const_iterator assocIter;
-
-    //Simply look through all of parent's associations and save all those VRs
-    for (assocIter = parentAssociations.begin (); assocIter != parentAssociations.end (); assocIter++)
-    {
-        virtualRegisters.insert (assocIter->first);
-    }
-
-    //Now look through all of child's associations and save all those VRs
-    for (assocIter = childAssociations.begin (); assocIter != childAssociations.end (); assocIter++)
-    {
-        virtualRegisters.insert (assocIter->first);
-    }
-}
-
-/**
- * @brief Looks through the association table to determine the state of each VR of interest.
- * @param associations The association table to look at.
- * @param virtualRegisters The virtual registers to determine state for.
- * @param vrState Updated by function
- * @return True if we can determine state of all VRs of interest. Otherwise error is set and false is returned.
- */
-static bool determineVirtualRegisterState (const AssociationTable &associations,
-                                           const std::set<int> &virtualRegisters,
-                                           std::map<int, VirtualRegisterState> &vrState)
-{
-    //We iterate through every VR of interest
-    for (std::set<int>::const_iterator iter = virtualRegisters.begin (); iter != virtualRegisters.end (); iter++)
-    {
-        int vR = *iter;
-
-        //We are iterating over a set which is sorted container. So if we are dealing with
-        //a wide VR, then we have already set the mapping for the low bits to contain information
-        //about the wideness.
-        std::map<int, VirtualRegisterState>::const_iterator wideIter = vrState.find (vR - 1);
-
-        //Do we have an entry for the low VR?
-        if (wideIter != vrState.end())
-        {
-            VirtualRegisterState lowState = wideIter->second;
-
-            //If we have a wide VR, then set the high bits correspondingly
-            if (lowState == VRState_WideInXmm)
-            {
-                vrState[vR] = VRState_HighOfWideVR;
-                continue;
-            }
-        }
-
-        //Look for the compile table entry for this VR
-        AssociationTable::const_iterator assocIter = associations.find (vR);
-
-        if (assocIter != associations.end ())
-        {
-            const CompileTableEntry &compileEntry = assocIter->second;
-
-            bool inPhysicalReg = compileEntry.inPhysicalRegister ();
-            bool inGP = compileEntry.inGeneralPurposeRegister ();
-            bool inXMM = compileEntry.inXMMRegister ();
-
-            //In order to have saved it, it must have been in either GP or XMM
-            //It also can be in a contant, which doesn't associated with physical reg
-            assert (inPhysicalReg == false || (inGP || inXMM) == true);
-
-            if (inGP == true)
-            {
-                vrState[vR] = VRState_InGP;
-                continue;
-            }
-            else if (inXMM == true)
-            {
-                //If it is in XMM, let's figure out if the VR is wide or not
-                OpndSize size = compileEntry.getSize ();
-
-                if (size == OpndSize_64)
-                {
-                    vrState[vR] = VRState_WideInXmm;
-                    continue;
-                }
-                else if (size == OpndSize_32)
-                {
-                    vrState[vR] = VRState_NonWideInXmm;
-                    continue;
-                }
-            }
-            else if (inPhysicalReg == true)
-            {
-                ALOGI ("JIT_INFO: We failed to satisfy BB associations because we found a VR that "
-                        "is in physical register but not in GP or XMM.");
-                SET_JIT_ERROR(kJitErrorBERegisterization);
-                return false;
-            }
-        }
-
-        //Let's figure out if it is believed that this VR is constant.
-        //We do this before checking if it was in memory because even if it was in memory,
-        //a child generated code using the assumptions of constant.
-        if (associations.wasVRConstant (vR) == true)
-        {
-            vrState[vR] = VRState_Constant;
-            continue;
-        }
-
-        //When we get here, we have tried our best to determine what physical register was used
-        //for this VR or if it was a constant. Only thing left is to see if this VR was marked as
-        //in memory
-        if (associations.wasVRInMemory (vR) == true)
-        {
-            vrState[vR] = VRState_InMemory;
-            continue;
-        }
-
-        //If we make it here it means we have not figured out the state of the VR
-        ALOGI ("JIT_INFO: We failed to satisfy BB associations because we couldn't figure "
-                "out state of virtual register v%d.", vR);
-        SET_JIT_ERROR(kJitErrorBERegisterization);
-        return false;
-    }
-
-    //If we make it here we are all good
-    return true;
-}
-
-/**
- * @brief For every virtual register, it compares state in parent and child and then makes a decision on action to take.
- * @param parentState Map of virtual register to its state in the parent association table.
- * @param childState Map of virtual register to its state in the child association table.
- * @param virtualRegisters List of virtual registers to make a decision for.
- * @param actions Updated by function to contain the actions to take in order to merge the two states.
- * @return Returns true if all state merging can be handled. Return false if mismatch of state is detected which
- * cannot be handled safely.
- */
-static bool decideOnMismatchAction (const std::map<int, VirtualRegisterState> &parentState,
-                                    const std::map<int, VirtualRegisterState> &childState,
-                                    const std::set<int> &virtualRegisters,
-                                    VirtualRegisterStateActions &actions)
-{
-    //We iterate through every VR of interest
-    for (std::set<int>::const_iterator iter = virtualRegisters.begin (); iter != virtualRegisters.end (); iter++)
-    {
-        //Get the VR
-        int vR = *iter;
-
-        //Create an iterator so we can look through child state and parent state for the VR
-        std::map<int, VirtualRegisterState>::const_iterator vrStateIter;
-
-        //Get the state of this VR in parent
-        vrStateIter = parentState.find (vR);
-
-        //Paranoid because parentState should contain all VRs in set of virtualRegisters
-        assert (vrStateIter != parentState.end());
-
-        //Save state of this VR in parent
-        VirtualRegisterState vrStateInParent = vrStateIter->second;
-
-        //Get the state of this VR in child
-        vrStateIter = childState.find (vR);
-
-        //Paranoid because childState should contain all VRs in set of virtualRegisters
-        assert (vrStateIter != childState.end());
-
-        //Save state of this VR in child
-        VirtualRegisterState vrStateInChild = vrStateIter->second;
-
-        DEBUG_ASSOCIATION_MERGE (ALOGD ("We are looking at v%d that is %s for parent and "
-                "%s for child", vR, convertVirtualRegisterStateToString (vrStateInParent),
-                convertVirtualRegisterStateToString (vrStateInChild)));
-
-        bool mismatched = (vrStateInParent != vrStateInChild);
-
-        if (mismatched == true)
-        {
-            //First let's check to see if child believes VR is constant
-            if (vrStateInChild == VRState_Constant)
-            {
-                //So we have a state mismatch and child believes that VR is a constant
-                ALOGI ("JIT_INFO: Child believes VR is constant but we don't. Without a runtime check "
-                        "we cannot confirm.");
-                SET_JIT_ERROR(kJitErrorBERegisterization);
-                return false;
-            }
-            //Now check if parent has it in memory
-            else if (vrStateInParent == VRState_InMemory)
-            {
-                //The high bits of this VR will be taken care of along with the low bits since
-                //we know we have a wide VR.
-                if (vrStateInChild == VRState_HighOfWideVR)
-                {
-                    continue;
-                }
-
-                //Paranoid because we are expecting to load it into register
-                assert (vrStateInChild == VRState_InGP || vrStateInChild == VRState_NonWideInXmm
-                        || vrStateInChild == VRState_WideInXmm);
-
-                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We need to load v%d into register", vR));
-
-                //If parent has it in memory but child has it in register then we need
-                //to load it.
-                actions.virtualRegistersToLoad.insert (vR);
-            }
-            else if (vrStateInChild == VRState_InMemory)
-            {
-                //The high bits of this VR will be taken care of along with the low bits since
-                //we know we have a wide VR.
-                if (vrStateInParent == VRState_HighOfWideVR)
-                {
-                    continue;
-                }
-
-                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We need to store v%d back on stack", vR));
-
-                //Add it to set to store back
-                actions.virtualRegistersToStore.insert (vR);
-            }
-            else if (vrStateInParent == VRState_Constant && vrStateInChild == VRState_InGP)
-            {
-                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We need to move immediate into GP for v%d", vR));
-
-                //Add it to set to do imm to reg move
-                actions.virtualRegistersImmToReg.insert (vR);
-            }
-            else
-            {
-                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We must store v%d in memory and then reload in "
-                        "proper place due to mismatch", vR));
-
-                //On state mismatch, the easiest solution is to store the VR into memory and then load
-                //it back into proper state
-                actions.virtualRegistersToStore.insert (vR);
-
-                //If child believes that this VR is the high part of the wide VR,
-                //then the load of the low part into xmm will take care of this case
-                if (vrStateInChild != VRState_HighOfWideVR)
-                {
-                    actions.virtualRegistersToLoad.insert (vR);
-                }
-            }
-        }
-        else
-        {
-            if (vrStateInParent == VRState_InGP || vrStateInParent == VRState_NonWideInXmm
-                    || vrStateInParent == VRState_WideInXmm)
-            {
-                DEBUG_ASSOCIATION_MERGE (ALOGD(">> We need to do a reg to reg move for v%d", vR));
-
-                // Insert it into set that needs to be handled via reg to reg moves
-                actions.virtualRegistersRegToReg.insert (vR);
-            }
-            else if (vrStateInParent == VRState_Constant)
-            {
-                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We need to check constants to ensure "
-                        "consistency for v%d", vR));
-
-                //We will need to do a constant check to make sure we have same constant
-                actions.virtualRegistersCheckConstants.insert (vR);
-            }
-            else
-            {
-                //We have nothing to do
-                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We have nothing to do because state matches for v%d", vR));
-            }
-        }
-    }
-
-    //If we make it here everything went okay
-    return true;
-}
-
-/**
- * @brief Compares the constant in each virtual register in order to figure out that they match.
- * @param parentAssociations The association table of parent.
- * @param childAssociations The association table of child.
- * @param virtualRegistersCheckConstants
- * @return Returns false if it finds a case when the constant value for same VR differs between parent and child.
- */
-static bool checkConstants (const AssociationTable &parentAssociations,
-                            const AssociationTable &childAssociations,
-                            const std::set<int> &virtualRegistersCheckConstants)
-{
-    //Iterate through all VRs that are constants in both parent and child to check that
-    //the constant value matches
-    for (std::set<int>::const_iterator iter = virtualRegistersCheckConstants.begin ();
-            iter != virtualRegistersCheckConstants.end (); iter++)
-    {
-        int vR = *iter;
-
-        //Get value parent believes for this VR
-        int parentValue = parentAssociations.getVRConstValue (vR);
-
-        //Get value child believes for this VR
-        int childValue = childAssociations.getVRConstValue (vR);
-
-        if (parentValue != childValue)
-        {
-            //If there is a mismatch, there's nothing we can do about it
-            ALOGI ("JIT_INFO: Both child and parent believe VR is constant but each believes "
-                        "it is a different value");
-            SET_JIT_ERROR(kJitErrorBERegisterization);
-            return false;
-        }
-    }
-
-    //If we make it here, all constants match in value
-    return true;
-}
-
-/**
- * @brief Decides whether merging state of parent to match its child can be done safely.
- * @param childAssociations The association table of child.
- * @param actions Updated by function to contain the actions to take in order to merge the two states.
- * @return Returns true if all state merging can be handled. Return false if mismatch of state is detected which
- * cannot be handled safely.
- */
-static bool canHandleMismatch (const AssociationTable &childAssociations,
-                               VirtualRegisterStateActions &actions)
-{
-    //We want to make it easy to compare state of child and state of parent and thus
-    //we load the compile table into an association table. The parent associations
-    //will no longer be valid once we start actioning on mismatch
-    AssociationTable parentAssociations;
-    if (AssociationTable::syncAssociationsWithCompileTable (parentAssociations) == false)
-    {
-        //When loading from compile table problems were found. It's best to bail early.
-        return false;
-    }
-
-    //Now figure out which virtual registers are used in each state so we can start
-    //figuring out any state mismatch
-    std::set<int> virtualRegisters;
-    filterVirtualRegisters (parentAssociations, childAssociations, virtualRegisters);
-
-    //For each virtual register we want to figure out the state in both parent and child
-    std::map<int, VirtualRegisterState> childState, parentState;
-    if (determineVirtualRegisterState(childAssociations, virtualRegisters, childState) == false)
-    {
-        return false;
-    }
-    if (determineVirtualRegisterState(parentAssociations, virtualRegisters, parentState) == false)
-    {
-        return false;
-    }
-
-    //Now we need to make a decision when we have a mismatch
-
-    bool result = decideOnMismatchAction (parentState, childState, virtualRegisters, actions);
-
-    if (result == false)
-    {
-        //While searching for action on mismatch, we found a state we couldn't deal with
-        //so we now return false.
-        return false;
-    }
-
-    // Now that we figured out mismatch and also actions for each, let's look at constants
-    // for both parent and child. We want to make sure that if child believes a VR is constant,
-    // the parent believes it is the same constant.
-    result = checkConstants (parentAssociations, childAssociations, actions.virtualRegistersCheckConstants);
-
-    if (result == false)
-    {
-        //If we found non matching constants, we must bail out because there's nothing we can do
-        return false;
-    }
-
-    //If we make it here, we can handle the mismatch
-    return true;
-}
-
-/**
- * @brief Sets up mapping between virtual registers and their physical registers.
- * @param associationsToUse the association to compare ourselves with
- * @param otherVRToPhysicalReg what is the associationsToUse having as associations (updated by the function)
- * @param currentVRToPhysicalReg what is the current association state between VRs to Physical (updated by the function)
- */
-static void initAssociationHelperTables (const AssociationTable &associationsToUse,
-                                         std::map<int, PhysicalReg> &otherVRToPhysicalReg,
-                                         std::map<int, PhysicalReg> &currentVRToPhysicalReg)
-{
-    // First we need to go through each of the child's association entries
-    // to figure out each VR's association with physical register
-    for (AssociationTable::const_iterator assocIter = associationsToUse.begin();
-            assocIter != associationsToUse.end(); assocIter++)
-    {
-        if (assocIter->second.physicalReg != PhysicalReg_Null)
-        {
-            // Save the mapping to physical register
-            otherVRToPhysicalReg[assocIter->first] =
-                    static_cast<PhysicalReg>(assocIter->second.physicalReg);
-        }
-    }
-
-    // Now go through current compile table to figure out what VRs are in
-    // physical registers
-    for (int entry = 0; entry < compileTable.size (); entry++)
-    {
-        if (isVirtualReg(compileTable[entry].physicalType)
-                && compileTable[entry].physicalReg != PhysicalReg_Null)
-        {
-            // Save the mapping to physical register
-            currentVRToPhysicalReg[compileTable[entry].regNum] =
-                    static_cast<PhysicalReg>(compileTable[entry].physicalReg);
-        }
-    }
-}
-
-/**
- * @brief Writes back virtual registers to stack.
- * @param virtualRegistersToStore Set of virtual registers to write back.
- * @param trySkipWriteBack This enables an optimization where the write backs are only handled
- * if VR is in the BB's write back requests. This can be useful when spilling to memory for loop
- * entry but loop never reads the written value.
- * @param writeBackRequests Used when trySkipWriteBack is true. This is a vector of VR writeback
- * requests from the basic block.
- * @param registersToFree Used when trySkipWriteBack is true. This is a set of physical registers
- * to ensure that the VR associated with it gets written back.
- * @return Returns true if successfully writes back all VRs to memory.
- */
-static bool writeBackVirtualRegistersToMemory (const std::set<int> &virtualRegistersToStore,
-                                               bool trySkipWriteBack = false,
-                                               const BitVector *writeBackRequests = 0,
-                                               const std::set<PhysicalReg> *registersToFree = 0)
-{
-    //Write back anything that is in the set of VRs to store
-    for (std::set<int>::const_iterator setOpIter = virtualRegistersToStore.begin ();
-            setOpIter != virtualRegistersToStore.end ();
-            setOpIter++)
-    {
-        int vR = *setOpIter;
-
-        //Now look through compile table to find the matching entry
-        for (int entry = 0; entry < compileTable.size (); entry++)
-        {
-            CompileTableEntry &compileEntry = compileTable[entry];
-
-            //Do we have a match in compile table with this VR we want to write back?
-            if (compileEntry.isVirtualReg () == true && compileEntry.getRegisterNumber () == vR)
-            {
-                //We want to skip the write back if the optimization is enabled.
-                bool skipWriteBack = (trySkipWriteBack == true);
-
-                //However, we do NOT want to skip writeback if it is in set of registers to free
-                //because someone wants this VR out of that physical register.
-                if (skipWriteBack == true && registersToFree != 0)
-                {
-                    skipWriteBack = registersToFree->find (compileTable[entry].getPhysicalReg ())
-                            == registersToFree->end ();
-                }
-
-                //Finally we do NOT want to skip writeback if this VR is in vector of writeback requests
-                if (skipWriteBack == true && writeBackRequests != 0)
-                {
-                    skipWriteBack = dvmIsBitSet (writeBackRequests, vR) == false;
-                }
-
-                //If we are not skipping the write back, then we actually need to do it
-                if (skipWriteBack == false)
-                {
-                    DEBUG_ASSOCIATION_MERGE (ALOGD ("Writing v%d back to memory", vR));
-
-                    //Handle the write back when in physical register
-                    if (compileEntry.inPhysicalRegister () == true)
-                    {
-                        //Try to write back the virtual register
-                        if (spillLogicalReg (entry, true) < 0)
-                        {
-                            return false;
-                        }
-                    }
-                    else
-                    {
-                        //We make it here if the VR is not in physical register. Try figuring out
-                        //if this is a constant. If it isn't a constant, we are okay because there's
-                        //nothing we need to write back.
-                        bool res = writeBackVRIfConstant (vR, LowOpndRegType_gp);
-
-                        //If this VR was constant, then since we wrote it back we mark it as non-constant.
-                        if (res == true)
-                        {
-                            setVRToNonConst (vR, OpndSize_32);
-                        }
-                    }
-                }
-            }
-        }
-    }
-
-    //Since we have spilled VRs, lets make sure we properly keep track
-    //of which physical registers are currently being used
-    syncAllRegs();
-
-    //If we make it here, everything is okay.
-    return true;
-}
-
-/**
- * @brief Find scratch registers and fill the scratchReg set
- * @param childUsedReg registers used by the child at entrance
- * @param scratchRegs any scratch register at the end of parent's code generation (updated by the function)
- */
-static void findScratchRegisters (const std::set<PhysicalReg> &childUsedReg,
-                                  std::set<PhysicalReg> &scratchRegs)
-{
-    // All free registers are candidates for use as scratch
-    std::set<PhysicalReg> parentFreeReg;
-    findFreeRegisters (parentFreeReg);
-
-    // Subtract child used registers from parent free registers
-    // so we can figure out what we can use as scratch
-    std::set_difference(parentFreeReg.begin(), parentFreeReg.end(),
-            childUsedReg.begin(), childUsedReg.end(),
-            std::inserter(scratchRegs, scratchRegs.end()));
-
-#ifdef DEBUG_REGISTERIZATION
-    //Debugging purposes
-    std::set<PhysicalReg>::const_iterator scratchRegIter;
-    for (scratchRegIter = scratchRegs.begin();
-            scratchRegIter != scratchRegs.end(); scratchRegIter++) {
-        DEBUG_ASSOCIATION_MERGE(ALOGD("%s is free for use as scratch\n",
-                physicalRegToString(*scratchRegIter)));
-    }
-#endif
-}
-
-/**
- * @brief Find the registers to be moved and fill the regToRegMoves map
- * @param virtualRegistersRegToReg Set of virtual registers that must be moved to new registers
- * @param childVRToPhysicalReg child association between VRs and physical registers at child code generation entrance
- * @param currentVRToPhysicalReg child association between VRs and physical registers at current code generation exit
- * @param regToRegMoves register to register moves to be done (updated by function)
- * @return Returns true if we successfully can determine the moves to be done.
- */
-static bool findRegistersToMove (const std::set<int> &virtualRegistersRegToReg,
-                                 std::map<int, PhysicalReg> &childVRToPhysicalReg,
-                                 std::map<int, PhysicalReg> &currentVRToPhysicalReg,
-                                 std::map<PhysicalReg, PhysicalReg> &regToRegMoves)
-{
-    std::set<int>::const_iterator setOpIter;
-
-    //Now we need to filter the reg to reg moves so walk through all of them
-    for (setOpIter = virtualRegistersRegToReg.begin ();
-            setOpIter != virtualRegistersRegToReg.end ();
-            setOpIter++)
-    {
-        int VR = *setOpIter;
-        PhysicalReg childReg = childVRToPhysicalReg[VR];
-        PhysicalReg currentReg = currentVRToPhysicalReg[VR];
-
-        // Check whether they are in the same physical register
-        if(childReg != currentReg) {
-            DEBUG_ASSOCIATION_MERGE(ALOGD("We are moving %s to %s",
-                    physicalRegToString(currentReg),
-                    physicalRegToString(childReg)));
-
-            if (regToRegMoves.find(currentReg) != regToRegMoves.end()) {
-                ALOGI ("JIT_INFO: We are overwriting the reg to reg move from %s",
-                        physicalRegToString(currentReg));
-                SET_JIT_ERROR(kJitErrorBERegisterization);
-                return false;
-            }
-
-            // We want to generate a move that takes value from current physical
-            // register and puts it in the physical register child expects it
-            regToRegMoves[currentReg] = childReg;
-        }
-    }
-
-    //Report success
-    return true;
-}
-
-/**
- * @brief Move registers following the regToReg map
- * @param regToRegMoves the register to register move order
- * @param scratchRegs the registers that are scratch and can be safely used for moving registers
- * @param currentVRToPhysicalReg current VR to physical registers at end of current BasicBlock generation
- * @return whether or not the move registers succeeds
- */
-static bool moveRegisters (std::map<PhysicalReg, PhysicalReg> &regToRegMoves,
-                           const std::set<PhysicalReg> &scratchRegs,
-                           const std::map<int, PhysicalReg> &currentVRToPhysicalReg)
-{
-    std::map<PhysicalReg, PhysicalReg>::const_iterator regToRegIter;
-
-    //Go through each register to register request
-    for (regToRegIter = regToRegMoves.begin (); regToRegIter != regToRegMoves.end (); regToRegIter++)
-    {
-        std::vector<PhysicalReg> toBeMoved;
-        std::vector<PhysicalReg>::reverse_iterator moveIter;
-        PhysicalReg source = regToRegIter->first;
-        PhysicalReg dest = regToRegIter->second;
-
-        // Paranoid because we cannot move to null register
-        if (dest == PhysicalReg_Null)
-        {
-            continue;
-        }
-
-        if (regToRegMoves.count (source) > 1)
-        {
-            ALOGI ("JIT_INFO: We have the same physical register as source "
-                    "multiple times.");
-            SET_JIT_ERROR(kJitErrorBERegisterization);
-            return false;
-        }
-
-        DEBUG_ASSOCIATION_MERGE (ALOGD ("We want to move from %s to %s", physicalRegToString(source),
-                physicalRegToString(dest)));
-
-        toBeMoved.push_back (source);
-        toBeMoved.push_back (dest);
-
-        // We eagerly assume that we won't find a cycle but we want to do something special if we do
-        bool cycleFound = false;
-
-        // Now look through the rest of the moves to see if anyone
-        // is going to replace source register
-        std::map<PhysicalReg, PhysicalReg>::const_iterator regToRegFinder;
-        for (regToRegFinder = regToRegMoves.find (dest); regToRegFinder != regToRegMoves.end (); regToRegFinder =
-                regToRegMoves.find (regToRegFinder->second))
-        {
-
-            // If we already have this register in the toBeMoved list,
-            // it means we found a cycle. Instead of doing a find,
-            // keeping track of this information in a bit vector would be
-            // better. However, PhysicalReg enum contains invalid
-            // physical registers so it is hard to decide which ones we
-            // need to keep track of.
-            if (std::find (toBeMoved.begin (), toBeMoved.end (), regToRegFinder->second) != toBeMoved.end ())
-            {
-                cycleFound = true;
-                // Save this because we will need to move value into it
-                toBeMoved.push_back (regToRegFinder->second);
-                break;
-            }
-
-            // Save this because we will need to move value into it
-            toBeMoved.push_back (regToRegFinder->second);
-        }
-
-        // If we have a cycle, we might need to use memory for doing the swap
-        bool useMemoryForSwap = false;
-
-        if (cycleFound == true)
-        {
-            // If we find a cycle, the last value in the toBeMoved list
-            // is the register that caused cycle. Lets pop it off
-            // for now so we can use std::replace below but we will
-            // reinsert it
-            PhysicalReg cycleCause = toBeMoved.back ();
-            toBeMoved.pop_back ();
-
-            // Lets hope we have a scratch register to break the cycle
-            PhysicalReg scratch = getScratch (scratchRegs, getTypeOfRegister (source));
-
-            if (scratch != PhysicalReg_Null)
-            {
-                // When we get here we found a scratch register
-
-                // Thus if we had C->A B->C A->B
-                // Now we want to have A->T C->A B->C T->B
-
-                // Which means that toBeMoved contains A B C when we get here and now we
-                // want it to have T B C A T
-
-                // Replace A with T so we have T B C instead of A B C
-                std::replace (toBeMoved.begin (), toBeMoved.end (), cycleCause, scratch);
-
-                // Now add A so we have T B C A
-                toBeMoved.push_back (cycleCause);
-
-                // Now add T so we have T B C A T
-                toBeMoved.push_back (scratch);
-            }
-            else
-            {
-                useMemoryForSwap = true;
-            }
-        }
-
-        if (useMemoryForSwap == true)
-        {
-            ALOGI ("JIT_INFO: We have no scratch registers so we must use memory for swap");
-            SET_JIT_ERROR(kJitErrorBERegisterization);
-            return false;
-        }
-
-        // Now handle the actual reg to reg moves
-        PhysicalReg tmpIter = PhysicalReg_Null;
-        for (moveIter = toBeMoved.rbegin (); moveIter != toBeMoved.rend (); moveIter++)
-        {
-            PhysicalReg dest = tmpIter;
-            PhysicalReg source = *(moveIter);
-
-            //Remember source
-            tmpIter = source;
-
-            //If destination is null, next iteration
-            if (dest == PhysicalReg_Null)
-            {
-                continue;
-            }
-
-            DEBUG_ASSOCIATION_MERGE(ALOGD("Moving %s to %s", physicalRegToString(source), physicalRegToString(dest)));
-
-            OpndSize regSize = OpndSize_32;
-
-            //If we have an xmm to xmm move, then we set the operand size to 64-bits. The reason
-            //for this is because move_reg_to_reg function expects this size so it can use a MOVQ.
-            //We may be able to get away with doing a MOVD if we have a 32-bit FP loaded with a MOVSS,
-            //but we don't have the API for it and we would need additional logic here.
-            if (source >= PhysicalReg_StartOfXmmMarker && source <= PhysicalReg_EndOfXmmMarker)
-            {
-                regSize = OpndSize_64;
-            }
-
-            // Do the actual reg to reg move
-            move_reg_to_reg_noalloc (regSize, source, true, dest, true);
-
-            // We have moved from reg to reg, but we must also update the
-            // entry in the compile table
-            std::map<int, PhysicalReg>::const_iterator currentVRToRegIter;
-            for (currentVRToRegIter = currentVRToPhysicalReg.begin ();
-                    currentVRToRegIter != currentVRToPhysicalReg.end (); currentVRToRegIter++)
-            {
-                int VR = currentVRToRegIter->first;
-                int oldReg = currentVRToRegIter->second;
-
-                if (oldReg == source)
-                {
-                    updatePhysicalRegForVR (VR, source, dest);
-                }
-            }
-
-            // Now remove entry from map
-            regToRegMoves[source] = PhysicalReg_Null;
-        }
-    }
-
-    // Since we update the physical registers for some of the VRs lets sync up register usage with compile table
-    syncAllRegs();
-
-    //Report success
-    return true;
-}
-
-/**
- * @brief Load virtual registers to child's physical registers requests
- * @param virtualRegistersToLoad Set of virtual registers we need to load into physical registers.
- * @param associationsToUse the association table for the child at code generation entrance
- * @param childVRToPhysicalReg the child map of Virtual Register to physical register at start of code generation
- * @return whether or not the function succeeds
- */
-static bool loadVirtualRegistersForChild (const std::set<int> &virtualRegistersToLoad,
-                                          const AssociationTable &associationsToUse,
-                                          const std::map<int, PhysicalReg> &childVRToPhysicalReg)
-{
-    std::set<int>::const_iterator setOpIter;
-
-    //Now walk through the VRs we need to load
-    for (setOpIter = virtualRegistersToLoad.begin();
-            setOpIter != virtualRegistersToLoad.end(); setOpIter++)
-    {
-        int VR = *setOpIter;
-
-        //Look to see if we have a physical register for this VR
-        std::map<int, PhysicalReg>::const_iterator findRegIter;
-        findRegIter = childVRToPhysicalReg.find (VR);
-
-        //This should never happen but could happen in rare cases. For example,
-        //if a VR is wide in child, then it might be possible we get a request
-        //to load the high part of VR into a non-existent register. However if
-        //the VR is wide, we should have already handled case of loading both
-        //low and high parts into an XMM.
-        if (findRegIter == childVRToPhysicalReg.end ())
-        {
-            //If we don't actually have a physical register then there's nowhere
-            //to load this VR. Thus we can safely skip it.
-            continue;
-        }
-
-        PhysicalReg targetReg = findRegIter->second;
-
-        //This should never happen but it will make buffer overflow checkers happy
-        if (targetReg >= PhysicalReg_Null)
-        {
-            continue;
-        }
-
-        const CompileTableEntry *childCompileEntry = 0;
-
-        // Look through child's association entries to find the type of the VR
-        // so we can load it properly into the physical register
-        for (AssociationTable::const_iterator assocIter = associationsToUse.begin ();
-                assocIter != associationsToUse.end (); assocIter++)
-        {
-            const CompileTableEntry &compileEntry = assocIter->second;
-
-            if (compileEntry.getPhysicalReg () == targetReg)
-            {
-                //We found the proper entry so let's get some information from it
-                childCompileEntry = &compileEntry;
-                break;
-            }
-        }
-
-        //Paranoid: this should never happen
-        if (childCompileEntry == 0)
-        {
-            ALOGD ("JIT_INFO: Trying to load virtual register for child but cannot find compile entry");
-            SET_JIT_ERROR (kJitErrorBERegisterization);
-            return false;
-        }
-
-        //Paranoid
-        assert (childCompileEntry->isVirtualReg () == true);
-
-        //Get the physical type
-        LowOpndRegType type = childCompileEntry->getPhysicalType ();
-
-        DEBUG_ASSOCIATION_MERGE (ALOGD ("Loading v%d to %s", VR, physicalRegToString(targetReg)));
-
-        // Load VR into the target physical register
-        if (type == LowOpndRegType_ss)
-        {
-            move_ss_mem_to_reg_noalloc (4 * VR, PhysicalReg_FP, true, MemoryAccess_VR, VR, targetReg, true);
-        }
-        else
-        {
-            OpndSize size = childCompileEntry->getSize ();
-            get_virtual_reg_noalloc (VR, size, targetReg, true);
-        }
-
-        //Look for the entry to update in compile table
-        CompileTable::iterator entryToUpdate = compileTable.findVirtualRegister (VR, type);
-
-        if (entryToUpdate != compileTable.end ())
-        {
-            //We found a matching entry so simply update its physical register
-            entryToUpdate->setPhysicalReg (targetReg);
-        }
-        else
-        {
-            //If we were not able to find an entry, then we can just copy it from child's association table
-
-            //Make a copy of new entry
-            CompileTableEntry newEntry = (*childCompileEntry);
-
-            //Since we copied it over, let's reset it
-            newEntry.reset ();
-
-            //Make sure that the physical register is set
-            newEntry.setPhysicalReg (targetReg);
-
-            //We now copy into the compile table
-            compileTable.insert (newEntry);
-
-            //Since we just loaded it from memory, we keep it marked as being in memory and add it to the
-            //memory table in order to keep track of it.
-            addToMemVRTable (newEntry.regNum, true);
-
-            //If it is a 64-bit wide operand, we also need to add its high part to the memory table.
-            if (newEntry.getSize() == OpndSize_64)
-            {
-                addToMemVRTable (newEntry.regNum + 1, true);
-            }
-        }
-    }
-
-    // We loaded some VRs into physical registers. Lets keep registers synced
-    syncAllRegs();
-
-    //Report success
-    return true;
-}
-
-/**
- * @brief Moves constant virtual registers values into physical registers.
- * @details The parent must believe VR is constant and child must want it in physical register.
- * @param immToRegMoves The virtual registers whose constant value must be moved to physical register.
- * @param childVRToPhysicalReg Mapping between child virtual register and the physical register they are in.
- * @return Returns true if successfully moves all immediates into physical registers. Otherwise it sets
- * error and returns false.
- */
-static bool moveImmediates (const std::set<int> &immToRegMoves,
-                            const std::map<int, PhysicalReg> &childVRToPhysicalReg)
-{
-    std::set<int>::const_iterator setOpIter;
-
-    //Now walk through the constant VRs we need to move to physical registers
-    for (setOpIter = immToRegMoves.begin (); setOpIter != immToRegMoves.end (); setOpIter++)
-    {
-        int vR = *setOpIter;
-
-        //We can only handle immediate to GP moves so we can preset the type
-        LowOpndRegType type = LowOpndRegType_gp;
-        OpndSize size = getRegSize (static_cast<int> (LowOpndRegType_gp));
-
-        //Make space for constant value
-        int constantValue;
-
-        //We want to get the constant value so we check if virtual register is constant.
-        //Since we just care to do immediate to GP register move, we pass only enough space
-        //for non-wide VR.
-        if (isVirtualRegConstant (vR, type, &constantValue) == VR_IS_NOT_CONSTANT)
-        {
-            ALOGI ("JIT_INFO: We decided that we need to do an imm to reg move but now VR is no longer constant.");
-            SET_JIT_ERROR (kJitErrorBERegisterization);
-            return false;
-        }
-
-        //Look to see if we have a physical register for this VR
-        std::map<int, PhysicalReg>::const_iterator findRegIter;
-        findRegIter = childVRToPhysicalReg.find (vR);
-
-        //This should never happen
-        if (findRegIter == childVRToPhysicalReg.end ())
-        {
-            ALOGI ("JIT_INFO: We decided that we need to do an imm to reg move but we cannot find register.");
-            SET_JIT_ERROR (kJitErrorBERegisterization);
-            return false;
-        }
-
-        PhysicalReg targetReg = findRegIter->second;
-
-        //Paranoid because we only support GP moves
-        assert (targetReg >= PhysicalReg_StartOfGPMarker && targetReg <= PhysicalReg_EndOfGPMarker);
-
-        //Do the actual move now
-        move_imm_to_reg_noalloc (size, constantValue, targetReg, true);
-
-        //Since we have it in physical register, lets invalidate its constantness
-        setVRToNonConst (vR, size);
-
-        //Look for the entry to update in compile table
-        CompileTable::iterator entryToUpdate = compileTable.findVirtualRegister (vR, type);
-
-        if (entryToUpdate != compileTable.end ())
-        {
-            //We found a matching entry so simply update its physical register
-            entryToUpdate->setPhysicalReg (targetReg);
-        }
-        else
-        {
-            //Since we don't have an entry already we can make one right now
-            CompileTableEntry newEntry (vR, type, LowOpndRegType_virtual);
-
-            //We now copy into the compile table
-            compileTable.insert (newEntry);
-
-            //If the constant was already marked as being in memory, then our VR is still technically
-            //in memory and thus we don't need to update its in memory state right now
-        }
-    }
-
-    //Report success
-    return true;
-}
-
-bool AssociationTable::satisfyBBAssociations (BasicBlock_O1 * parent,
-        BasicBlock_O1 * child, bool isBackward)
-{
-    // To get here, it must be the case that this child's associations have
-    // already been finalized
-    assert (child->associationTable.hasBeenFinalized() == true);
-
-    /**
-     * This function merges associations, therefore it needs to know:
-     *   - The child's associations
-     *   - The parent's associations
-     *   - How both associations can be synchronized
-     */
-
-    AssociationTable &childAssociations = child->associationTable;
-    VirtualRegisterStateActions actions;
-
-    // 1) Gather information on current associations and the child's and decide
-    // on actions for dealing with state mismatch between VRs
-    if (canHandleMismatch (childAssociations, actions) == false)
-    {
-        return false;
-    }
-
-    //Look at child to see what physical registers it is using
-    std::set<PhysicalReg> childUsedReg;
-    childAssociations.findUsedRegisters (childUsedReg);
-
-    // 2) We write back anything child wants in memory because this will allow us to have scratch
-    // registers in case we need to do reg to reg moves.
-    if (writeBackVirtualRegistersToMemory (actions.virtualRegistersToStore, isBackward == true,
-            parent->requestWriteBack, &childUsedReg) == false)
-    {
-        return false;
-    }
-
-    // 3) Prepare for doing reg to reg moves by finding scratch registers, finding mapping between
-    // VRs and their physical register, and for deciding which registers to move.
-
-    //Figure out the scratch registers we have available.
-    std::set<PhysicalReg> scratchRegs;
-    findScratchRegisters (childUsedReg, scratchRegs);
-
-    //Initialize helper maps in regards to parent and child associations
-    std::map<int, PhysicalReg> childVRToPhysicalReg, currentVRToPhysicalReg;
-    initAssociationHelperTables (childAssociations, childVRToPhysicalReg, currentVRToPhysicalReg);
-
-    //Find the registers that should be moved
-    std::map<PhysicalReg, PhysicalReg> regToRegMoves;
-
-    if (findRegistersToMove (actions.virtualRegistersRegToReg, childVRToPhysicalReg,
-            currentVRToPhysicalReg, regToRegMoves) == false)
-    {
-        //If findRegistersToMove fails, we bail too
-        return false;
-    }
-
-    // 4) Do the actual moving of registers to the correct physical register
-    if (moveRegisters (regToRegMoves, scratchRegs, currentVRToPhysicalReg) == false)
-    {
-        //If moveRegisters fails, we bail too
-        return false;
-    }
-
-    // 5) Load any VRs we believe is in memory because child wants it in physical register
-    if (loadVirtualRegistersForChild (actions.virtualRegistersToLoad, childAssociations, childVRToPhysicalReg) == false)
-    {
-        //If moveToChildPhysical fails, we bail too
-        return false;
-    }
-
-    // 6) Now handle any immediate to GP register moves
-    if (moveImmediates (actions.virtualRegistersImmToReg, childVRToPhysicalReg) == false)
-    {
-        //If move immediates fails it will set error message. We simply propagate it now.
-        return false;
-    }
-
-    //If we make it here, everything went okay so we report success
-    return true;
-}
-
-bool AssociationTable::handleSpillRequestsFromME (BasicBlock_O1 *bb)
-{
-    //Initialize empty set of VRs to write back
-    std::set<int> virtualRegisterToWriteBack;
-
-    //We need to iterate through the writeback requests to add them to our set of VRs
-    BitVectorIterator bvIterator;
-    dvmBitVectorIteratorInit(bb->requestWriteBack, &bvIterator);
-
-    //Go through each VR so we can add it to our set
-    for (int vR = dvmBitVectorIteratorNext (&bvIterator); vR != -1; vR = dvmBitVectorIteratorNext (&bvIterator))
-    {
-        virtualRegisterToWriteBack.insert (vR);
-    }
-
-    //Do the actual write back
-    bool result = writeBackVirtualRegistersToMemory (virtualRegisterToWriteBack);
-
-    return result;
-}
-
-/**
- * @details First we handle any spill requests for the current basic block
- * so we do not pass useless associations to child. Then if child already
- * has an existing association table, we generate instructions to match
- * our state to that. If the child does not, then we tell it what our
- * current associations are. If the child is a chaining cell or exit block,
- * we spill everything because those BBs are handled specially and are exit
- * points.
- */
-bool AssociationTable::createOrSyncTable (BasicBlock_O1 * bb, bool forFallthrough)
-{
-    // Before we pass association tables, lets handle spill requests from ME
-    // so we don't pass anything useless for associations
-    if (handleSpillRequestsFromME (bb) == false)
-    {
-        return false;
-    }
-
-    //Get child depending on the forFallthrough boolean
-    BasicBlock_O1 * child = reinterpret_cast<BasicBlock_O1 *> (forFallthrough ? bb->fallThrough : bb->taken);
-
-    //If there is a child
-    if (child != NULL) {
-
-        //If it is not a dalvik code and it's not prebackward block,
-        //then write back and free all registers because we might
-        //be exiting to interpreter.
-        if (child->blockType != kDalvikByteCode && child->blockType != kPreBackwardBlock)
-        {
-            freeReg (true);
-        }
-        else
-        {
-            if (child->associationTable.hasBeenFinalized () == false)
-            {
-                // If the child's association table has not been finalized then we can
-                // update it now. However, if we don't have any MIRs in this BB,
-                // it means the compile table has not been updated and thus we can
-                // just copy associations
-                if (syncAssociationsWithCompileTable (child->associationTable) == false)
-                {
-                    return false;
-                }
-            }
-            else
-            {
-                //Otherwise, let's satisfy the associations for the child
-                if (satisfyBBAssociations (bb, child) == false)
-                {
-                    return false;
-                }
-            }
-        }
-    }
-
-    //Report success
-    return true;
-}
diff --git a/vm/compiler/codegen/x86/RegisterizationBE.h b/vm/compiler/codegen/x86/RegisterizationBE.h
deleted file mode 100644
index 8bb11d7..0000000
--- a/vm/compiler/codegen/x86/RegisterizationBE.h
+++ /dev/null
@@ -1,271 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef REGISTERIZATIONBE_H_
-#define REGISTERIZATIONBE_H_
-
-#include <map>
-
-// Forward declarations
-struct BasicBlock_O1;
-class CompileTableEntry;
-struct ConstVRInfo;
-struct MemoryVRInfo;
-
-/**
- * @brief Used to keep track of virtual registers and their various associations.
- * @details Keeps track of compile table information associated with VR including
- * the physical register, the in memory state of a VR, and the constantness of VR.
- */
-class AssociationTable {
-public:
-    /**
-     * @brief Looks through all associations and finds used physical registers
-     * @param outUsedRegisters is a set that is updated with the used physical
-     * registers
-     */
-    void findUsedRegisters(std::set<PhysicalReg> & outUsedRegisters) const;
-
-    /**
-     * @brief Once association table is been finalized, this can be called to
-     * find out if the virtual register was in memory.
-     * @param vR virtual register
-     * @return Returns whether or not VR was in memory
-     */
-    bool wasVRInMemory(int vR) const;
-
-    /**
-     * @brief Once association table is been finalized, this can be called to
-     * find out if the virtual register was a constant.
-     * @details For wide VRs, this should be called twice to find out if both
-     * low order bits and high order bits were constant.
-     * @param vR virtual register
-     * @return Returns whether or not virtual register was a constant.
-     */
-    bool wasVRConstant(int vR) const;
-
-    /**
-     * @brief Returns the 32 bit constant value associated with VR
-     * @pre wasVRConstant should return true
-     * @param vR virtual register
-     * @return Returns the constant value of virtual register.
-     */
-    int getVRConstValue(int vR) const;
-
-    /**
-     * @brief Updates association table given a compile entry from the compile table.
-     * @param compileEntry compilation entry
-     * @return Returns whether adding compile entry to associations was successful
-     */
-    bool associate(const CompileTableEntry &compileEntry);
-
-    /**
-     * @brief Updates association table given a memory VR information
-     * @param memVRInfo the memory virtual register information entry
-     * @return Returns whether adding compile entry to associations was successful
-     */
-    bool associate(const MemoryVRInfo &memVRInfo);
-
-    /**
-     * @brief Updates association table given a constant VR information
-     * @param constVRInfo the constant virtual register entry
-     * @return Returns whether adding compile entry to associations was successful
-     */
-    bool associate(const ConstVRInfo &constVRInfo);
-
-    /**
-     * @brief Used to determine whether the association table can be
-     * updated anymore.
-     * @return Returns whether the associations are final and cannot be updated.
-     */
-    bool hasBeenFinalized(void) const {
-        return isFinal;
-    }
-
-    /**
-     * @brief Used to tell association table that it cannot accept anymore
-     * updates.
-     */
-    void finalize();
-
-    /**
-     * @brief Clears the association table.
-     */
-    void clear(void);
-
-    /**
-     * @brief Used to copy another association table into current one
-     * @param source association table to read from
-     * @return whether the copy was successful
-     */
-    bool copy(AssociationTable & source);
-
-    /**
-     * Returns number of entries in association table
-     * @return size of association table
-     */
-    size_t size(void) const {
-        return this->associations.size();
-    }
-
-    /**
-     * @brief Prints the association table to a file separating entries with
-     * vertical bar.
-     * @param file File to print to.
-     */
-    void printToDot(FILE * file);
-
-    /**
-     * @brief Random access const iterator. This does not modify structure it is iterating.
-     */
-    typedef std::map<int, CompileTableEntry>::const_iterator const_iterator;
-
-    /**
-     * @brief Random access iterator. This may modify structure it is iterating.
-     */
-    typedef std::map<int, CompileTableEntry>::iterator iterator;
-
-    /**
-     * @brief Returns an iterator pointing to the first association.
-     * @return iterator to beginning
-     */
-    iterator begin() {
-        return associations.begin();
-    }
-
-    /**
-     * @brief Returns a const iterator pointing to the first association.
-     * @return iterator to beginning
-     */
-    const_iterator begin() const {
-        return associations.begin();
-    }
-
-    /**
-     * @brief Returns an iterator referring to the past-the-end association.
-     * @return iterator past end
-     */
-    iterator end() {
-        return associations.end();
-    }
-
-    /**
-     * @brief Returns a const iterator referring to the past-the-end
-     * association.
-     * @return iterator past end
-     */
-    const_iterator end() const {
-        return associations.end();
-    }
-
-    /**
-     * @brief Returns a const interator to the compile table entry matching the desired VR.
-     * @param vR The virtual register to look for.
-     * @return Returns iterator matching the result found or iterator that equals end() when
-     * no match is found.
-     */
-    const_iterator find (const int &vR) const {
-        return associations.find(vR);
-    }
-
-    AssociationTable(void); /**< @brief Constructor */
-
-    ~AssociationTable(void); /**< @brief Destructor */
-
-    //Static functions of the RegisterizationBE class
-
-    /**
-     * @brief Updates a given association table using the current state of the
-     * compile table.
-     * @param associationsToUpdate Association table to update.
-     * @return Returns whether the association table was updated successfully.
-     */
-    static bool syncAssociationsWithCompileTable(AssociationTable & associationsToUpdate);
-
-    /**
-     * @brief Updates the current state of the compile table to all VR entries
-     * in the association table
-     * @param associationsToUse Association table to use for compile table updated.
-     * @return Returns whether the compile table update was successful.
-     */
-    static bool syncCompileTableWithAssociations(AssociationTable & associationsToUse);
-
-    /**
-     * @brief Creates association table for child or generates instructions to
-     * match it.
-     * @param bb Parent basic block
-     * @param forFallthrough Flag on whether should update fallthrough child. Else
-     * we update taken child.
-     * @return Whether the sync was successful.
-     */
-    static bool createOrSyncTable(BasicBlock_O1 * bb, bool forFallthrough = true);
-
-    /**
-     * @brief Generates instructions to match current state of parent basic block
-     * to the association table state of child.
-     * @param parent Parent basic block.
-     * @param child Child basic block.
-     * @param isBackward Used to denote whether we are satisfying associations
-     * of loop entry. If yes, then we only write back phi nodes.
-     * @return Returns whether the state of parent successfully matches state of
-     * child.
-     */
-    static bool satisfyBBAssociations (BasicBlock_O1 * parent,
-            BasicBlock_O1 * child, bool isBackward = false);
-
-    /**
-     * @brief Spills virtual registers marked for spilling by the middle end.
-     * @param bb Basic block whose spill requests we need to handle.
-     * @return Returns whether we successfully handled spill requests.
-     */
-    static bool handleSpillRequestsFromME(BasicBlock_O1 * bb);
-
-private:
-    /**
-     * @brief Map for every VR to its corresponding compile table entry
-     * when association occurred.
-     */
-    std::map<int, CompileTableEntry> associations;
-
-    /**
-     * @brief Map for every VR to its state in memory when the association
-     * occurred.
-     */
-    std::map<int, MemoryVRInfo> inMemoryTracker;
-
-    /**
-     * @typedef Iterator for use with inMemoryTracker.
-     */
-    typedef std::map<int, MemoryVRInfo>::const_iterator inMemoryTrackerConstIterator;
-
-    /**
-     * @brief Map for every VR to its constant value (if it had any) when the
-     * association occurred.
-     */
-    std::map<int, ConstVRInfo> constTracker;
-
-    /**
-     * @typedef Iterator for use with constTracker
-     */
-    typedef std::map<int, ConstVRInfo>::const_iterator constantTrackerConstIterator;
-
-    /**
-     * @brief Keeps track whether association table has been finalized.
-     */
-    bool isFinal;
-};
-
-#endif /* REGISTERIZATIONBE_H_ */
diff --git a/vm/compiler/codegen/x86/Scheduler.cpp b/vm/compiler/codegen/x86/Scheduler.cpp
deleted file mode 100644
index 0e06436..0000000
--- a/vm/compiler/codegen/x86/Scheduler.cpp
+++ /dev/null
@@ -1,2073 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*! \file Scheduler.cpp
-    \brief This file implements the Atom Instruction Scheduler.
-    \details Scheduling algorithm implemented is basic block scheduling.
-*/
-
-#include "Lower.h"
-#include "interp/InterpDefs.h"
-#include "Scheduler.h"
-
-//! \def DISABLE_ATOM_SCHEDULING_STATISTICS
-//! \brief Disables printing of scheduling statistics.
-//! \details Defining this macro disables printing of scheduling statistics pre
-//! and post scheduling. Undefine macro when statistics are needed.
-#define DISABLE_ATOM_SCHEDULING_STATISTICS
-
-//! \def DISABLE_DEBUG_ATOM_SCHEDULER
-//! \brief Disables debug printing for atom scheduler.
-//! \details Defining macro DISABLE_DEBUG_ATOM_SCHEDULER disables debug printing.
-//! Undefine macro when debugging scheduler implementation.
-#define DISABLE_DEBUG_ATOM_SCHEDULER
-
-//! \def DISABLE_DEPENDENGY_GRAPH_DEBUG
-//! \brief Disables printing of dependency graph
-//! \details Undefine this macro when wanting to debug dependency graph.
-//! The dot files for each basic block will be dumped to folder /data/local/tmp
-//! and the name for each file will be depengraph_<pid>_<stream_start_of_BB>.dot
-#define DISABLE_DEPENDENGY_GRAPH_DEBUG
-
-#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
-#include <fstream>
-#include <sstream>
-#include <string>
-#include <set>
-#endif
-
-//! \enum IssuePort
-//! \brief Defines possible combinations of port-binding information for use
-//! with information about each x86 mnemonic.
-enum IssuePort {
-    //! \brief invalid port, used for table only when some
-    //! operands are not supported for the mnemonic
-    INVALID_PORT = -1,
-    //! \brief the mnemonic can only be issued on port 0
-    PORT0 = 0,
-    //! \brief the mnemonic can only be issued on port 1
-    PORT1 = 1,
-    //! \brief the mnemonic can be issued on either port
-    EITHER_PORT,
-    //! \brief both ports are used for the mnemonic
-    BOTH_PORTS
-};
-
-#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
-//! \brief Transforms from IssuePort enum to a string representation.
-inline const char * getIssuePort(IssuePort port) {
-    switch (port) {
-    case INVALID_PORT:
-        return "invalid";
-    case PORT0:
-        return "0";
-    case PORT1:
-        return "1";
-    case EITHER_PORT:
-        return "either";
-    case BOTH_PORTS:
-        return "both";
-    }
-    return "Invalid";
-}
-#endif
-
-//! \class MachineModelEntry
-//! \brief Information needed to define the machine model for each x86 mnemonic.
-struct MachineModelEntry {
-    //! \brief which port the instruction can execute on
-    IssuePort issuePortType;
-    //! \brief execute to execute time for one instruction
-    int executeToExecuteLatency;
-};
-
-//! \def INVP
-//! \brief This is an abbreviation of INVALID_PORT and is used for readability
-//! reasons.
-#define INVP INVALID_PORT
-
-//! \def INVN
-//! \brief This is an abbreviation of invalid node latency and is used for
-//! readability reasons.
-#define INVN 0
-
-//! \def REG_NOT_USED
-//! \brief This is an abbreviation for register not used and is used for
-//! readability reasons whenever a Scheduler method needs register type
-//! to update some data structure but a register number does not make
-//! sense in the context.
-#define REG_NOT_USED -1
-
-//! \brief This table lists the parameters for each Mnemonic in the Atom Machine Model.
-//! \details This table includes port and latency information for each mnemonic for each possible
-//! configuration of operands. 6 entries of MachineModelEntry are reserved for each Mnemonic:
-//! - If a Mnemonic has zero operand, only the first entry is valid
-//! - If a Mnemonic has a single operand, the first 3 entries are valid, for operand type
-//! imm, reg and mem respectively
-//! - If a Mnemonic has two operands, the last 5 entries are valid, for operand types
-//! imm_to_reg, imm_to_mem, reg_to_reg, mem_to_reg and reg_to_mem
-//!
-//! This table matches content from Intel 64 and IA-32 Architectures Optimization
-//! Reference Manual (April 2012), Section 13.4
-//! \warning This table is not complete and if new mnemonics are used that do not have an
-//! entry, then the schedule selection will not be optimal.
-MachineModelEntry atomMachineModel[Mnemonic_Count*6] = {
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NULL, Null
-
-    {PORT1,1},{PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //JMP
-
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //MOV
-
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_O
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NO
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_B
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NB
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_Z
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NZ
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_BE
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NBE
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_S
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NS
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_P
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NP
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_L
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NL
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_LE
-    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NLE
-
-    {BOTH_PORTS,1},{BOTH_PORTS,1},{EITHER_PORT,2},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CALL
-
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //ADC
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //ADD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //ADDSD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //ADDSS
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //AND
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //BSF
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //BSR
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMC
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{INVP,INVN},{INVP,INVN}, //CWD, CDQ
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_O
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NO
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_B,NAE,C
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NB,AE,NC
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_Z,E
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NZ,NE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_BE,NA
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NBE,A
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_S
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_P,PE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NP,PO
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_L,NGE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NL,GE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_LE,NG
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NLE,G
-
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //CMP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{PORT0,5}, //CMPXCHG Note: info missed in section 13.4
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{PORT0,5}, //CMPXCHG8B Note: info missed in section 13.4
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPSB
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPSW
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPSD
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,7},{BOTH_PORTS,8},{INVP,INVN}, //CVTSD2SS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,8},{BOTH_PORTS,9},{INVP,INVN}, //CVTSD2SI
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,8},{BOTH_PORTS,9},{INVP,INVN}, //CVTTSD2SI
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,7},{BOTH_PORTS,8},{INVP,INVN}, //CVTSS2SD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //CVTSS2SI
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //CVTTSS2SI
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,7},{BOTH_PORTS,8},{INVP,INVN}, //CVTSI2SD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,6},{BOTH_PORTS,7},{INVP,INVN}, //CVTSI2SS
-
-    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //COMISD
-    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //COMISS
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //DEC
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,62},{BOTH_PORTS,62},{INVP,INVN}, //DIVSD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,34},{BOTH_PORTS,34},{INVP,INVN}, //DIVSS
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //ENTER
-    {INVP,INVN},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN}, //FLDCW
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{INVP,INVN}, //FADDP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDZ
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{INVP,INVN}, //FADD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{INVP,INVN}, //FSUBP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{INVP,INVN}, //FSUB
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FISUB
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{INVP,INVN}, //FMUL
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{INVP,INVN}, //FMULP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,65},{INVP,INVN}, //FDIVP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,65},{INVP,INVN}, //FDIV
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,1},{INVP,INVN},{INVP,INVN}, //FUCOM
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FUCOMI
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,1},{INVP,INVN},{INVP,INVN}, //FUCOMP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FUCOMIP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FUCOMPP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FRNDINT
-    {INVP,INVN},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,5}, //FNSTCW
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSTSW
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FNSTSW
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN}, //FILD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{INVP,INVN}, //FLD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDLG2
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDLN2
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLD1
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FCLEX
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FCHS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FNCLEX
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,6}, //FIST
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,6}, //FISTP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FISTTP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPREM
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPREM1
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1}, //FST fp_mem
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1}, //FSTP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,65},{INVP,INVN}, //FSQRT
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN}, //FABS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSIN
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FCOS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPTAN
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FYL2X
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FYL2XP1
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //F2XM1
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPATAN
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FXCH
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSCALE
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //XCHG
-    // There is no way to differentiate operand sizes in this table, so just assume 32-bit
-    {INVP,INVN},{BOTH_PORTS,57},{BOTH_PORTS,57},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //DIV
-    {INVP,INVN},{BOTH_PORTS,57},{BOTH_PORTS,57},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //IDIV
-    {INVP,INVN},{BOTH_PORTS,6},{BOTH_PORTS,7},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MUL
-    // This table does not support IMUL with single reg or mem operand
-    {INVP,INVN},{PORT0,5},{PORT0,5},{PORT0,5},{PORT0,5},{INVP,INVN}, //IMUL
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //INC
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //INT3
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,1},{INVP,INVN}, //LEA
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LEAVE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LOOP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LOOPE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LOOPNE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LAHF
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1}, //MOVD
-    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{PORT0,1},{PORT0,1}, //MOVQ
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS8
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS16
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS32
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS64
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{PORT0,1}, //MOVAPD
-    {INVP,INVN},{PORT0,1},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //MOVSD
-    {INVP,INVN},{PORT0,1},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //MOVSS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{PORT0,1},{INVP,INVN}, //MOVSX
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{PORT0,1},{INVP,INVN}, //MOVZX
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{PORT0,5},{INVP,INVN}, //MULSD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,4},{PORT0,4},{INVP,INVN}, //MULSS
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,10},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NEG
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NOP
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,10},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NOT
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //OR
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //PREFETCH
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,2},{EITHER_PORT,3},{INVP,INVN}, //PADDQ
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PAND
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //POR
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,2},{EITHER_PORT,3},{INVP,INVN}, //PSUBQ
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PANDN
-    {INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{EITHER_PORT,2},{EITHER_PORT,3},{INVP,INVN}, //PSLLQ
-    {INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{EITHER_PORT,2},{EITHER_PORT,3},{INVP,INVN}, //PSRLQ
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PXOR
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //POP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //POPFD
-    {INVP,INVN},{BOTH_PORTS,1},{BOTH_PORTS,2},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //PUSH
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //PUSHFD
-    {BOTH_PORTS,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //RET
-
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_O
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NO
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_B
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NB
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_Z
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NZ
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_BE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NBE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_S
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_P
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NP
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_L
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NL
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_LE
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NLE
-
-    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{INVP,INVN},{INVP,INVN}, //SAL,SHL
-    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{INVP,INVN},{INVP,INVN}, //SAR
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //ROR
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //RCR
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //ROL
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //RCL
-    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{INVP,INVN},{INVP,INVN}, //SHR
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,4},{BOTH_PORTS,4},{BOTH_PORTS,2}, //SHRD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,4},{BOTH_PORTS,4},{BOTH_PORTS,2}, //SHLD
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //SBB
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //SUB
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //SUBSD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //SUBSS
-
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{INVP,INVN},{PORT0,1}, //TEST
-    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //UCOMISD
-    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //UCOMISS
-    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //XOR
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //XORPD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //XORPS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTDQ2PD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTTPD2DQ
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTDQ2PS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTTPS2DQ
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //STD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CLD
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SCAS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //STOS
-    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //WAIT
-};
-
-//! \brief Get issue port for mnemonic with no operands
-inline IssuePort getAtomMnemonicPort(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVALID_PORT;
-    return atomMachineModel[m*6].issuePortType;
-}
-//! \brief Get issue port for mnemonic with one immediate operand
-inline IssuePort getAtomMnemonicPort_imm(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVALID_PORT;
-    return atomMachineModel[m*6].issuePortType;
-}
-//! \brief Get issue port for mnemonic with one register operand
-inline IssuePort getAtomMnemonicPort_reg(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVALID_PORT;
-    return atomMachineModel[m*6+1].issuePortType;
-}
-//! \brief Get issue port for mnemonic with one memory operand
-inline IssuePort getAtomMnemonicPort_mem(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVALID_PORT;
-    return atomMachineModel[m*6+2].issuePortType;
-}
-//! \brief Get issue port for mnemonic with two operands: immediate to register
-inline IssuePort getAtomMnemonicPort_imm_to_reg(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVALID_PORT;
-    return atomMachineModel[m*6+1].issuePortType;
-}
-//! \brief Get issue port for mnemonic with two operands: immediate to memory
-inline IssuePort getAtomMnemonicPort_imm_to_mem(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVALID_PORT;
-    return atomMachineModel[m*6+2].issuePortType;
-}
-//! \brief Get issue port for mnemonic with two operands: register to register
-inline IssuePort getAtomMnemonicPort_reg_to_reg(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVALID_PORT;
-    return atomMachineModel[m*6+3].issuePortType;
-}
-//! \brief Get issue port for mnemonic with two operands: memory to register
-inline IssuePort getAtomMnemonicPort_mem_to_reg(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVALID_PORT;
-    return atomMachineModel[m*6+4].issuePortType;
-}
-//! \brief Get issue port for mnemonic with two operands: register to memory
-inline IssuePort getAtomMnemonicPort_reg_to_mem(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVALID_PORT;
-    return atomMachineModel[m*6+5].issuePortType;
-}
-
-//! \brief Get execute to execute latency for mnemonic with no operands
-inline int getAtomMnemonicLatency(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVN;
-    return atomMachineModel[m*6].executeToExecuteLatency;
-}
-//! \brief Get execute to execute latency for mnemonic with one immediate operand
-inline int getAtomMnemonicLatency_imm(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVN;
-    return atomMachineModel[m*6].executeToExecuteLatency;
-}
-//! \brief Get execute to execute latency for mnemonic with one register operand
-inline int getAtomMnemonicLatency_reg(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVN;
-    return atomMachineModel[m*6+1].executeToExecuteLatency;
-}
-//! \brief Get execute to execute latency for mnemonic with one memory operand
-inline int getAtomMnemonicLatency_mem(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVN;
-    return atomMachineModel[m*6+2].executeToExecuteLatency;
-}
-//! \brief Get execute to execute latency for mnemonic with two operands: immediate to register
-inline int getAtomMnemonicLatency_imm_to_reg(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVN;
-    return atomMachineModel[m*6+1].executeToExecuteLatency;
-}
-//! \brief Get execute to execute latency for mnemonic with two operands: immediate to memory
-inline int getAtomMnemonicLatency_imm_to_mem(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVN;
-    return atomMachineModel[m*6+2].executeToExecuteLatency;
-}
-//! \brief Get execute to execute latency for mnemonic with two operands: register to register
-inline int getAtomMnemonicLatency_reg_to_reg(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVN;
-    return atomMachineModel[m*6+3].executeToExecuteLatency;
-}
-//! \brief Get execute to execute latency for mnemonic with two operands: memory to register
-inline int getAtomMnemonicLatency_mem_to_reg(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVN;
-    return atomMachineModel[m*6+4].executeToExecuteLatency;
-}
-//! \brief Get execute to execute latency for mnemonic with two operands: register to memory
-inline int getAtomMnemonicLatency_reg_to_mem(Mnemonic m) {
-    if (m >= Mnemonic_Count) return INVN;
-    return atomMachineModel[m*6+5].executeToExecuteLatency;
-}
-
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-//! \brief Transforms from LowOpndDefUse enum to string representation of the usedef
-//! \see LowOpndDefUse
-inline const char * getUseDefType(LowOpndDefUse defuse) {
-    switch (defuse) {
-    case LowOpndDefUse_Def:
-        return "Def";
-    case LowOpndDefUse_Use:
-        return "Use";
-    case LowOpndDefUse_UseDef:
-        return "UseDef";
-    }
-    return "-";
-}
-#endif
-
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-//! \brief Transforms from UseDefEntryType enum to a string representation.
-//! \see UseDefEntryType
-inline const char * getUseDefEntryType(UseDefEntryType type) {
-    switch (type) {
-    case UseDefType_Ctrl:
-        return "Ctrl";
-    case UseDefType_Float:
-        return "Float";
-    case UseDefType_MemVR:
-        return "MemVR";
-    case UseDefType_MemSpill:
-        return "MemSpill";
-    case UseDefType_MemUnknown:
-        return "MemUnknown";
-    case UseDefType_Reg:
-        return "Reg";
-    }
-    return "-";
-}
-#endif
-
-//! \brief Returns true if mnemonic is a variant of MOV.
-inline bool isMoveMnemonic(Mnemonic m) {
-    return m == Mnemonic_MOV || m == Mnemonic_MOVQ || m == Mnemonic_MOVSD
-            || m == Mnemonic_MOVSS || m == Mnemonic_MOVZX || m == Mnemonic_MOVSX;
-}
-
-//! \brief Returns true if mnemonic is used for comparisons.
-//! \details Returns false for FPU comparison mnemonics.
-inline bool isCompareMnemonic(Mnemonic m) {
-    return m == Mnemonic_CMP || m == Mnemonic_COMISD || m == Mnemonic_COMISS
-            || m == Mnemonic_TEST;
-}
-
-//! \brief Returns true if mnemonic is SSE conversion routine
-inline bool isConvertMnemonic(Mnemonic m) {
-    return m == Mnemonic_CVTSD2SS || m == Mnemonic_CVTSD2SI
-            || m == Mnemonic_CVTTSD2SI || m == Mnemonic_CVTSS2SD
-            || m == Mnemonic_CVTSS2SI || m == Mnemonic_CVTTSS2SI
-            || m == Mnemonic_CVTSI2SD || m == Mnemonic_CVTSI2SS;
-}
-
-//! \brief Returns true if mnemonic uses and then defines the FLAGS register
-inline bool usesAndDefinesFlags(Mnemonic m) {
-    return m == Mnemonic_ADC || m == Mnemonic_SBB;
-}
-
-//! \brief Returns true if mnemonic is CMPXCHG, which use and define EAX register
-inline bool isCmpxchgMnemonic(Mnemonic m) {
-    return m == Mnemonic_CMPXCHG;
-}
-
-//! \brief Returns true if ALU mnemonic has a variant that has implicit
-//! register usage.
-//! \details Returns true for div, idiv, mul, imul, and cdq. However, note
-//! that implicit register usage is dependent on variant being used. For example,
-//! only idiv with single reg operand has implicit register usage.
-inline bool isAluOpWithImplicitRegisterUsage(Mnemonic m) {
-    return m == Mnemonic_DIV || m == Mnemonic_IDIV
-            || m == Mnemonic_IMUL || m == Mnemonic_MUL
-            || m == Mnemonic_CDQ;
-}
-
-//! \brief Detects whether the mnemonic is a native basic block delimiter.
-//! \details Unconditional jumps, conditional jumps, calls, and returns
-//! always end a native basic block.
-inline bool Scheduler::isBasicBlockDelimiter(Mnemonic m) {
-    return (m == Mnemonic_JMP || m == Mnemonic_CALL
-            || (m >= Mnemonic_Jcc && m <= Mnemonic_JG) || m == Mnemonic_RET);
-}
-
-//! \details Defines a mapping between the reason for edge latencies between
-//! instructions and the actual latency value.
-//! \see LatencyBetweenNativeInstructions
-static int mapLatencyReasonToValue[] = {
-    // Latency_None
-    0,
-    // Latency_Agen_stall
-    3,
-    //Latency_Load_blocked_by_store
-    0,
-    //Latency_Memory_Load
-    0,
-};
-
-//! \brief Atom scheduler destructor
-Scheduler::~Scheduler(void) {
-    // Clear all scheduler data structures
-    this->reset();
-}
-
-//! \brief Resets data structures used by Scheduler
-void Scheduler::reset(void) {
-    queuedLIREntries.clear();
-    scheduledLIREntries.clear();
-
-    for (std::vector<UseDefUserEntry>::iterator it = userEntries.begin();
-            it != userEntries.end(); ++it) {
-        it->useSlotsList.clear();
-    }
-    userEntries.clear();
-
-    for (std::map<LowOp*, Dependencies>::iterator it =
-            dependencyAssociation.begin(); it != dependencyAssociation.end();
-            it++) {
-        Dependencies &d = it->second;
-        d.predecessorDependencies.clear();
-        d.successorDependencies.clear();
-    }
-    dependencyAssociation.clear();
-
-    // Safe to clear
-    producerEntries.clear();
-    ctrlEntries.clear();
-}
-
-//! \brief Returns true if Scheduler has no LIRs in its
-//! queue for scheduling.
-//! \return true when Scheduler queue is empty
-bool Scheduler::isQueueEmpty() const {
-    return queuedLIREntries.empty();
-}
-
-//! \brief Given an access to a resource (Control, register, VR, unknown memory
-//! access), this updates dependency graph, usedef information, and control flags.
-//! \details Algorithm description for dependency update:
-//! - for Use or UseDef:
-//!   -# insert RAW dependency from producer for this op
-//! - for Def or UseDef:
-//!   -# insert WAR dependency from earlier user for this op
-//!   -# insert WAW dependency from earlier producer for this op
-//! - Internal data structure updates for record keeping:
-//!   -# for Def or UseDef: update producerEntries
-//!   -# for Def: clear corresponding use slots for entry in userEntries
-//!   -# for UseDef: clear corresponding use slots for entry in userEntries
-//!   -# for Use: update userEntries
-//!
-//! \param type resource that causes dependency
-//! \param regNum is a number corresponding to a physical register or a Dalvik
-//! virtual register. When physical, this is of enum type PhysicalReg.
-//! \param defuse definition, usage, or both
-//! \param causeOfLatency Weight to use on the edge.
-//! \param op LIR for which to update dependencies
-void Scheduler::updateDependencyGraph(UseDefEntryType type, int regNum,
-        LowOpndDefUse defuse, LatencyBetweenNativeInstructions causeOfLatency,
-        LowOp* op) {
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-    const char * string_defuse = getUseDefType(defuse);
-    const char * string_type = getUseDefEntryType(type);
-    ALOGD("---updateDependencyGraph for resource <%s %d> "
-            "at slot %d with %s---\n", string_type,
-            regNum, op->slotId, string_defuse);
-#endif
-
-    unsigned int k;
-    unsigned int index_for_user = userEntries.size();
-    unsigned int index_for_producer = producerEntries.size();
-
-    // Look for the producer of this resource. If none is found, then
-    // index_for_producer will remain length of producerEntries list.
-    if (type != UseDefType_Ctrl) {
-        for (k = 0; k < producerEntries.size(); ++k) {
-            if (producerEntries[k].type == type
-                    && producerEntries[k].regNum == regNum) {
-                index_for_producer = k;
-                break;
-            }
-        }
-    }
-
-    // Look for the users of this resource. If none are found, then
-    // index_for_user will remain length of userEntries list.
-    for (k = 0; k < userEntries.size(); ++k) {
-        if (userEntries[k].type == type && userEntries[k].regNum == regNum) {
-            index_for_user = k;
-            break;
-        }
-    }
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-    ALOGD("index_for_producer %d %d index_for_user %d %d\n",
-            index_for_producer, producerEntries.size(),
-            index_for_user, userEntries.size());
-#endif
-
-    if (defuse == LowOpndDefUse_Use || defuse == LowOpndDefUse_UseDef) {
-        // If use or usedef, then there is a RAW dependency from producer
-        // of the resource.
-        if (type != UseDefType_Ctrl
-                && index_for_producer != producerEntries.size()) {
-            assert(producerEntries[index_for_producer].producerSlot != op->slotId);
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-            ALOGD("RAW dependency from %d to %d due to resource <%s %d>\n",
-                    producerEntries[index_for_producer].producerSlot, op->slotId, string_type, regNum);
-#endif
-            DependencyInformation ds;
-            ds.dataHazard = Dependency_RAW;
-            ds.lowopSlotId = producerEntries[index_for_producer].producerSlot;
-            ds.causeOfEdgeLatency = causeOfLatency;
-            ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
-            // If producer is a memory load, then also add memory latency
-            if (isMoveMnemonic(queuedLIREntries[ds.lowopSlotId]->opCode) &&
-                    queuedLIREntries[ds.lowopSlotId]->opndSrc.type == LowOpndType_Mem) {
-                // If memory load latency is greater than current latency,
-                // replace it with the memory load
-                if (mapLatencyReasonToValue[Latency_Memory_Load] > ds.edgeLatency) {
-                    ds.causeOfEdgeLatency = Latency_Memory_Load;
-                    ds.edgeLatency += mapLatencyReasonToValue[Latency_Memory_Load];
-                }
-            }
-            dependencyAssociation[op].predecessorDependencies.push_back(ds);
-        }
-        // For Ctrl dependencies, when there is a user of a resource
-        // it depends on the last producer. However, the last producer
-        // depends on all previous producers. This is done as an
-        // optimization because flag writers don't need to depend on
-        // each other unless there is a flag reader.
-        if (type == UseDefType_Ctrl && ctrlEntries.size() > 0) {
-            // insert RAW from the last producer
-            assert(ctrlEntries.back() != op->slotId);
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-            ALOGD("insert RAW from %d to %d due to Ctrl\n",
-                    ctrlEntries.back(), op->slotId);
-#endif
-            DependencyInformation ds;
-            ds.dataHazard = Dependency_RAW;
-            ds.lowopSlotId = ctrlEntries.back();
-            ds.causeOfEdgeLatency = causeOfLatency;
-            ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
-            dependencyAssociation[op].predecessorDependencies.push_back(ds);
-            // insert WAW from earlier producers to the last producer
-            LowOp* opLast = (queuedLIREntries[ctrlEntries.back()]);
-            for (k = 0; k < ctrlEntries.size() - 1; k++) {
-                assert(ctrlEntries[k] != opLast->slotId);
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-                ALOGD("insert WAW from %d to %d due to Ctrl\n", ctrlEntries[k], ctrlEntries.back());
-#endif
-                DependencyInformation ds;
-                ds.dataHazard = Dependency_WAW;
-                ds.lowopSlotId = ctrlEntries[k];
-                ds.causeOfEdgeLatency = causeOfLatency;
-                ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
-                dependencyAssociation[opLast].predecessorDependencies.push_back(ds);
-            }
-        }
-
-        // If this is the first use of this resource, then an
-        // entry should be created in the userEntries list
-        if (index_for_user == userEntries.size()) {
-            UseDefUserEntry entry;
-            entry.type = type;
-            entry.regNum = regNum;
-            userEntries.push_back(entry);
-        } else if (type == UseDefType_Ctrl) {
-            userEntries[index_for_user].useSlotsList.clear();
-        }
-        // Add current op as user of resource
-        userEntries[index_for_user].useSlotsList.push_back(op->slotId);
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-        ALOGD("op with slot %d uses resource <%s %d>\n", op->slotId, string_type, regNum);
-#endif
-
-        if (type == UseDefType_Ctrl)
-            ctrlEntries.clear();
-    }
-
-    if (defuse == LowOpndDefUse_Def || defuse == LowOpndDefUse_UseDef) {
-        // If def or usedef, then there is a WAR dependency from earlier users
-        // of this resource and a WAW dependency due to earlier producer
-        if (index_for_user != userEntries.size()) {
-            // Go through every user of resource and update current op with a WAR
-            // from each user.
-            for (k = 0; k < userEntries[index_for_user].useSlotsList.size();
-                    k++) {
-                if (userEntries[index_for_user].useSlotsList[k] == op->slotId)
-                    continue; // No need to create dependency on self
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-                ALOGD("WAR dependency from %d to %d due to resource <%s %d>\n",
-                        userEntries[index_for_user].useSlotsList[k], op->slotId, string_type, regNum);
-#endif
-                DependencyInformation ds;
-                ds.dataHazard = Dependency_WAR;
-                ds.lowopSlotId = userEntries[index_for_user].useSlotsList[k];
-                ds.causeOfEdgeLatency = causeOfLatency;
-                ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
-                dependencyAssociation[op].predecessorDependencies.push_back(ds);
-            }
-        }
-        if (type != UseDefType_Ctrl
-                && index_for_producer != producerEntries.size()) {
-            // There is WAW dependency from earlier producer to current producer
-            // For Ctrl resource, WAW is not relevant until there is a reader
-            assert(producerEntries[index_for_producer].producerSlot != op->slotId);
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-            ALOGD("WAW dependency from %d to %d due to resource <%s %d>\n",
-                    producerEntries[index_for_producer].producerSlot, op->slotId, string_type, regNum);
-#endif
-            DependencyInformation ds;
-            ds.dataHazard = Dependency_WAW;
-            ds.lowopSlotId = producerEntries[index_for_producer].producerSlot;
-            ds.causeOfEdgeLatency = causeOfLatency;
-            ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
-            dependencyAssociation[op].predecessorDependencies.push_back(ds);
-        }
-
-        if (type != UseDefType_Ctrl
-                && index_for_producer == producerEntries.size()) {
-            // If we get here it means that this the first known producer
-            // of this resource and therefore we should keep track of this
-            UseDefProducerEntry entry;
-            entry.type = type;
-            entry.regNum = regNum;
-            producerEntries.push_back(entry);
-        }
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-        ALOGD("op with slot %d produces/defines resource <%s %d>\n",
-                op->slotId, string_type, regNum);
-#endif
-        if (type != UseDefType_Ctrl)
-            // Add current op as producer of resource
-            producerEntries[index_for_producer].producerSlot = op->slotId;
-        else {
-            // Save the current op as one of the producers of this resource
-            ctrlEntries.push_back(op->slotId);
-        }
-
-        // Since this a new producer of the resource, we can now forget
-        // all past users. This behavior is also correct if current op is
-        // user and then producer because when handling usedef, use is
-        // handled first.
-        if (type != UseDefType_Ctrl && index_for_user != userEntries.size()) {
-            userEntries[index_for_user].useSlotsList.clear();
-        }
-    }
-}
-
-//! \brief Given an access to a memory location this updates dependency graph,
-//! usedef information, and control flags.
-//! \details This method uses updateDependencyGraph internally to update
-//! dependency graph, but knows the type of memory resource that is being used.
-//! \param mOpnd reference to the structure for memory operand
-//! \param defuse definition, usage, or both
-//! \param op LIR for which to update dependencies
-void Scheduler::updateDependencyGraphForMem(LowOpndMem & mOpnd, LowOpndDefUse defuse,
-        LowOp* op) {
-    MemoryAccessType mType = mOpnd.mType;
-    int index = mOpnd.index;
-    bool is64 = false;
-
-    // We know that if we have an access to the memory holding constants
-    // then it is okay to reorder instructions accessing that since it
-    // is not updated at runtime. Thus, we don't add any dependencies for
-    // this operand.
-    if (mType == MemoryAccess_Constants)
-    {
-        // All accesses to the constants section should be done with
-        // null base register
-        assert(mOpnd.m_base.regNum == PhysicalReg_Null);
-
-        return;
-    }
-
-    // Update dependency on registers used
-    updateDependencyGraph(UseDefType_Reg, mOpnd.m_base.regNum,
-            LowOpndDefUse_Use, Latency_Agen_stall, op);
-    if (mOpnd.hasScale)
-        updateDependencyGraph(UseDefType_Reg, mOpnd.m_index.regNum,
-                LowOpndDefUse_Use, Latency_Agen_stall, op);
-
-    // In order to be safe, if one of the operands has size 64, assume it is size 64
-    if (op->numOperands >= 1 && op->opndDest.size == OpndSize_64)
-        is64 = true;
-    if (op->numOperands >= 2 && op->opndSrc.size == OpndSize_64)
-        is64 = true;
-
-    // At this point make a decision on whether or not to do memory disambiguation.
-    // If it is not VR or SPILL access, then it may not be safe to disambiguate.
-    if (mType == MemoryAccess_VR) {
-        // All VR accesses should be made via Java frame pointer
-        assert(mOpnd.m_base.regNum == PhysicalReg_FP);
-
-        updateDependencyGraph(UseDefType_MemVR, index, defuse, Latency_None, op);
-        if (is64)
-            updateDependencyGraph(UseDefType_MemVR, index + 1, defuse, Latency_None, op);
-    } else if (mType == MemoryAccess_SPILL) {
-        // All spill accesses should be made via offset from EBP
-        assert(mOpnd.m_base.regNum == PhysicalReg_EBP);
-
-        updateDependencyGraph(UseDefType_MemSpill, index, defuse, Latency_None, op);
-        if (is64) {
-            updateDependencyGraph(UseDefType_MemSpill, index + 4, defuse, Latency_None, op);
-        }
-    } else // No disambiguation
-        updateDependencyGraph(UseDefType_MemUnknown, REG_NOT_USED, defuse,
-                Latency_None, op);
-}
-
-//! \brief Updates dependency information for PUSH which uses then defines %esp
-//! and also updates native stack.
-void inline Scheduler::handlePushDependencyUpdate(LowOp* op) {
-    if (op->opCode == Mnemonic_PUSH) {
-        updateDependencyGraph(UseDefType_Reg, PhysicalReg_ESP,
-                LowOpndDefUse_UseDef, Latency_Agen_stall, op);
-        updateDependencyGraph(UseDefType_MemUnknown, REG_NOT_USED,
-                LowOpndDefUse_Def, Latency_None, op);
-    }
-}
-
-//! \brief Updates dependency information for operations on floating point stack.
-//! \details This should be called for all x87 instructions. This will ensure
-//! that they are never reordered.
-void inline Scheduler::handleFloatDependencyUpdate(LowOp* op) {
-    // UseDef dependency is used so that x87 instructions won't be reordered
-    // Whenever reordering support is added, this function should be replaced
-    // and new resources defined like FPU flags, control word, status word, etc.
-    updateDependencyGraph(UseDefType_Float, REG_NOT_USED, LowOpndDefUse_UseDef,
-            Latency_None, op);
-}
-
-//! \brief Sets up dependencies on resources that must be live out.
-//! \details Last write to a resource should be ensured to be live
-//! out.
-void Scheduler::setupLiveOutDependencies() {
-    // Handle live out control flags. Namely, make sure that last flag
-    // writer depends on all previous flag writers.
-    if (ctrlEntries.size() != 0) {
-        // If the ctrlEntries list is empty, it means we have no flag producers
-        // that we need to update. This is caused if there really were not flag
-        // producers or a flag reader has already cleared this list which means
-        // the flag read already will be the one live out.
-        LowOp* lastFlagWriter = (queuedLIREntries[ctrlEntries.back()]);
-
-        // Don't include last flag writer in the iteration
-        for (std::vector<unsigned int>::const_iterator iter = ctrlEntries.begin ();
-                                                       (iter + 1) != ctrlEntries.end ();
-                                                       iter++) {
-            // Add a WAW dependency to the last flag writer from all other
-            // flag writers
-            DependencyInformation ds;
-            ds.dataHazard = Dependency_WAW;
-            ds.lowopSlotId = *iter;
-            ds.causeOfEdgeLatency = Latency_None;
-            ds.edgeLatency = mapLatencyReasonToValue[Latency_None];
-            dependencyAssociation[lastFlagWriter].predecessorDependencies.push_back(
-                    ds);
-        }
-    }
-
-    //! @todo Take care of live out dependencies for all types of resources
-    //! including physical registers.
-}
-
-//! \brief Updates dependency graph with the implicit dependencies on eax
-//! and edx for imul, mul, div, idiv, and cdq
-//! \warning Assumes that operand size is 32 bits
-void inline Scheduler::handleImplicitDependenciesEaxEdx(LowOp* op) {
-    if (isAluOpWithImplicitRegisterUsage(op->opCode)) {
-        // mul and imul with a reg operand implicitly usedef eax and def edx
-        // div and idiv with a reg operand implicitly usedef eax and usedef edx
-        // cdq implicitly usedef eax and def edx
-        if (op->opCode == Mnemonic_MUL || op->opCode == Mnemonic_IMUL
-                || op->opCode == Mnemonic_CDQ) {
-            updateDependencyGraph(UseDefType_Reg, PhysicalReg_EAX,
-                    LowOpndDefUse_UseDef, Latency_None, op);
-            updateDependencyGraph(UseDefType_Reg, PhysicalReg_EDX,
-                    LowOpndDefUse_Def, Latency_None, op);
-        } else if (op->opCode == Mnemonic_IDIV || op->opCode == Mnemonic_DIV) {
-            updateDependencyGraph(UseDefType_Reg, PhysicalReg_EAX,
-                    LowOpndDefUse_UseDef, Latency_None, op);
-            updateDependencyGraph(UseDefType_Reg, PhysicalReg_EDX,
-                    LowOpndDefUse_UseDef, Latency_None, op);
-        }
-    }
-}
-
-//! \brief Updates dependency information for LowOps with zero operands.
-//! \param op has mnemonic RET
-void Scheduler::updateUseDefInformation(LowOp * op) {
-    assert(op->opCode == Mnemonic_RET);
-    op->instructionLatency = getAtomMnemonicLatency(op->opCode);
-    op->portType = getAtomMnemonicPort(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
-            Latency_None, op);
-    signalEndOfNativeBasicBlock(); // RET ends native basic block
-}
-
-//! \brief Updates dependency information for LowOps with a single immediate
-//! operand.
-//! \param op has mnemonic JMP, Jcc, or CALL
-void Scheduler::updateUseDefInformation_imm(LowOp * op) {
-    assert((op->opCode >= Mnemonic_Jcc && op->opCode <= Mnemonic_JG)
-            || op->opCode == Mnemonic_JMP || op->opCode == Mnemonic_CALL);
-    op->instructionLatency = getAtomMnemonicLatency_imm(op->opCode);
-    op->portType = getAtomMnemonicPort_imm(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    if (op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_JMP)
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
-                Latency_None, op);
-    else
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
-                Latency_None, op);
-    if (isBasicBlockDelimiter(op->opCode))
-        signalEndOfNativeBasicBlock();
-}
-
-//! \brief Updates dependency information for LowOps with a single register operand.
-//! \param op has mnemonic JMP, CALL, PUSH or it is an ALU instruction
-void Scheduler::updateUseDefInformation_reg(LowOpReg * op) {
-    assert(op->opCode == Mnemonic_JMP || op->opCode == Mnemonic_CALL
-            || op->opCode == Mnemonic_PUSH || op->opCode2 == ATOM_NORMAL_ALU);
-    op->instructionLatency = getAtomMnemonicLatency_reg(op->opCode);
-    op->portType = getAtomMnemonicPort_reg(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    if (op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_JMP
-            || op->opCode == Mnemonic_PUSH
-            || isAluOpWithImplicitRegisterUsage(op->opCode))
-        op->opndSrc.defuse = LowOpndDefUse_Use;
-    else // ALU ops with a single operand and no implicit operands use and then define
-        op->opndSrc.defuse = LowOpndDefUse_UseDef;
-    updateDependencyGraph(UseDefType_Reg, op->regOpnd.regNum,
-            op->opndSrc.defuse, Latency_None, op);
-
-    // PUSH will not update control flag
-    if (op->opCode != Mnemonic_PUSH)
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
-                Latency_None, op);
-
-    handleImplicitDependenciesEaxEdx(op);
-    handlePushDependencyUpdate(op);
-
-    if (isBasicBlockDelimiter(op->opCode))
-        signalEndOfNativeBasicBlock();
-}
-
-//! \brief Updates dependency information for for LowOps with a single
-//! memory operand.
-//! \param op has mnemonic CALL, FLDCW, FNSTCW, PUSH or it is an ALU
-//! instruction
-void Scheduler::updateUseDefInformation_mem(LowOpMem * op) {
-    assert(op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_FLDCW
-            || op->opCode == Mnemonic_FNSTCW || op->opCode == Mnemonic_PUSH
-            || op->opCode2 == ATOM_NORMAL_ALU);
-    op->instructionLatency = getAtomMnemonicLatency_mem(op->opCode);
-    op->portType = getAtomMnemonicPort_mem(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    if (op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_FLDCW
-            || op->opCode == Mnemonic_PUSH
-            || isAluOpWithImplicitRegisterUsage(op->opCode))
-        op->opndSrc.defuse = LowOpndDefUse_Use;
-    else if (op->opCode == Mnemonic_FNSTCW)
-        op->opndSrc.defuse = LowOpndDefUse_Def;
-    else // ALU ops with a single operand and no implicit operands use and then define
-        op->opndSrc.defuse = LowOpndDefUse_UseDef;
-    updateDependencyGraphForMem(op->memOpnd, op->opndSrc.defuse, op);
-
-    // PUSH will not update control flag
-    if (op->opCode != Mnemonic_PUSH && op->opCode != Mnemonic_FLDCW
-            && op->opCode != Mnemonic_FNSTCW)
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
-                Latency_None, op);
-
-    handleImplicitDependenciesEaxEdx(op);
-    handlePushDependencyUpdate(op);
-
-    if (op->opCode == Mnemonic_FLDCW || op->opCode == Mnemonic_FNSTCW)
-        handleFloatDependencyUpdate(op);
-    if (isBasicBlockDelimiter(op->opCode))
-        signalEndOfNativeBasicBlock();
-}
-
-//! \brief Updates dependency information for LowOps with two operands:
-//! immediate to register
-//! \param op must be a MOV variant, a comparison (CMP, TEST, COMISS, COMISD),
-//! or an ALU instruction
-void Scheduler::updateUseDefInformation_imm_to_reg(LowOpImmReg * op) {
-    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
-            || op->opCode2 == ATOM_NORMAL_ALU);
-    bool isMove = isMoveMnemonic(op->opCode);
-    op->instructionLatency = getAtomMnemonicLatency_imm_to_reg(op->opCode);
-    op->portType = getAtomMnemonicPort_imm_to_reg(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    if (usesAndDefinesFlags(op->opCode))
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
-                Latency_None, op);
-    if (!isMove)
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
-                Latency_None, op);
-
-    if (isMove)
-        op->opndDest.defuse = LowOpndDefUse_Def;
-    else if (isCompareMnemonic(op->opCode))
-        op->opndDest.defuse = LowOpndDefUse_Use;
-    else // ALU ops
-        op->opndDest.defuse = LowOpndDefUse_UseDef;
-    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum,
-            op->opndDest.defuse, Latency_None, op);
-}
-
-//! \brief Updates dependency information for LowOps with two operands:
-//! immediate and memory.
-//! \param op must be a MOV variant, a comparison (CMP, TEST, COMISS, COMISD),
-//! or an ALU instruction
-void Scheduler::updateUseDefInformation_imm_to_mem(LowOpImmMem * op) {
-    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
-            || op->opCode2 == ATOM_NORMAL_ALU);
-    bool isMove = isMoveMnemonic(op->opCode);
-    op->instructionLatency = getAtomMnemonicLatency_imm_to_mem(op->opCode);
-    op->portType = getAtomMnemonicPort_imm_to_mem(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    if (usesAndDefinesFlags(op->opCode))
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
-                Latency_None, op);
-    if (!isMove)
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
-                Latency_None, op);
-
-    if (isMove)
-        op->opndDest.defuse = LowOpndDefUse_Def;
-    else if (isCompareMnemonic(op->opCode))
-        op->opndDest.defuse = LowOpndDefUse_Use;
-    else // ALU ops
-        op->opndDest.defuse = LowOpndDefUse_UseDef;
-    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
-}
-
-//! \brief Updates dependency information for LowOps with two operands:
-//! register to register.
-//! \param op must be a MOV variant, a comparison (CMP, TEST, COMISS, COMISD),
-//! an ALU instruction including SSE variants SD and SS, SSE conversion,
-//! or must have mnemonic FUCOM, FUCOMP, CMOVcc, or CDQ.
-void Scheduler::updateUseDefInformation_reg_to_reg(LowOpRegReg * op) {
-    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
-            || isConvertMnemonic(op->opCode) ||op->opCode2 == ATOM_NORMAL_ALU
-            || op->opCode == Mnemonic_FUCOM || op->opCode == Mnemonic_FUCOMP
-            || op->opCode == Mnemonic_CDQ ||
-            (op->opCode >= Mnemonic_CMOVcc && op->opCode < Mnemonic_CMP));
-    bool isMove = isMoveMnemonic(op->opCode);
-    bool isConvert = isConvertMnemonic(op->opCode);
-    op->instructionLatency = getAtomMnemonicLatency_reg_to_reg(op->opCode);
-    op->portType = getAtomMnemonicPort_reg_to_reg(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    if ((op->opCode >= Mnemonic_CMOVcc && op->opCode < Mnemonic_CMP)
-            || usesAndDefinesFlags(op->opCode))
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
-                Latency_None, op);
-    else if (!isMove && !isConvert && op->opCode != Mnemonic_FUCOM
-            && op->opCode != Mnemonic_FUCOMP && op->opCode != Mnemonic_CDQ)
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
-                Latency_None, op);
-
-    if (op->opCode == Mnemonic_CDQ) {
-        // CDQ has no explicit operands but for encoding reasons it is treated like
-        // it does and therefore comes to Scheduler via this interface function.
-        // We can handle it here.
-        assert(op->opndSrc.size == OpndSize_32
-                && op->opndDest.size == OpndSize_32);
-        handleImplicitDependenciesEaxEdx(op);
-        return;
-    }
-
-    op->opndSrc.defuse = LowOpndDefUse_Use;
-    updateDependencyGraph(UseDefType_Reg, op->regSrc.regNum, op->opndSrc.defuse,
-            Latency_None, op);
-
-    if (isMove || isConvert
-            || (op->opCode >= Mnemonic_CMOVcc && op->opCode < Mnemonic_CMP))
-        op->opndDest.defuse = LowOpndDefUse_Def;
-    else if (isCompareMnemonic(op->opCode))
-        op->opndDest.defuse = LowOpndDefUse_Use;
-    else // ALU ops
-        op->opndDest.defuse = LowOpndDefUse_UseDef;
-    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum,
-            op->opndDest.defuse, Latency_None, op);
-
-    if (op->opCode == Mnemonic_FUCOM || op->opCode == Mnemonic_FUCOMP)
-        handleFloatDependencyUpdate(op);
-}
-
-//! \brief Updates dependency information for LowOps with two operands:
-//! memory to register.
-//! \param op must be a MOV variant, a comparison (CMP, COMISS, COMISD),
-//! an ALU instruction including SSE variants SD and SS, SSE conversion,
-//! or must have mnemonic LEA
-void Scheduler::updateUseDefInformation_mem_to_reg(LowOpMemReg * op) {
-    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
-            || isConvertMnemonic(op->opCode) || op->opCode2 == ATOM_NORMAL_ALU
-            || op->opCode == Mnemonic_LEA);
-    bool isMove = isMoveMnemonic(op->opCode);
-    bool isConvert = isConvertMnemonic(op->opCode);
-    op->instructionLatency = getAtomMnemonicLatency_mem_to_reg(op->opCode);
-    op->portType = getAtomMnemonicPort_mem_to_reg(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    if (usesAndDefinesFlags(op->opCode))
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
-                Latency_None, op);
-    if (!isMove && !isConvert && op->opCode != Mnemonic_LEA)
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
-                Latency_None, op);
-
-    // Read from memory
-    // However, LEA does not load from memory, and instead it uses the register
-    op->opndSrc.defuse = LowOpndDefUse_Use;
-    if (op->opCode != Mnemonic_LEA)
-        updateDependencyGraphForMem(op->memSrc, op->opndSrc.defuse, op);
-    else {
-        updateDependencyGraph(UseDefType_Reg, op->memSrc.m_base.regNum,
-                op->opndSrc.defuse, Latency_Agen_stall, op);
-        if(op->memSrc.hasScale)
-            updateDependencyGraph(UseDefType_Reg, op->memSrc.m_index.regNum,
-                    op->opndSrc.defuse, Latency_Agen_stall, op);
-    }
-
-    if (isMove || isConvert || op->opCode == Mnemonic_LEA)
-        op->opndDest.defuse = LowOpndDefUse_Def;
-    else if (isCompareMnemonic(op->opCode))
-        op->opndDest.defuse = LowOpndDefUse_Use;
-    else // ALU ops
-        op->opndDest.defuse = LowOpndDefUse_UseDef;
-    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum,
-            op->opndDest.defuse, Latency_None, op);
-}
-
-//! \brief Updates dependency information for LowOps with two operands:
-//! register to memory.
-//! \param op must be a MOV variant, a comparison (CMP), a cmpxchange (CMPXCHG), or an ALU instruction
-void Scheduler::updateUseDefInformation_reg_to_mem(LowOpRegMem * op) {
-    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
-            || op->opCode2 == ATOM_NORMAL_ALU || isCmpxchgMnemonic(op->opCode));
-    bool isMove = isMoveMnemonic(op->opCode);
-    bool isCmpxchg = isCmpxchgMnemonic(op->opCode);
-    op->instructionLatency = getAtomMnemonicLatency_reg_to_mem(op->opCode);
-    op->portType = getAtomMnemonicPort_reg_to_mem(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    if (usesAndDefinesFlags(op->opCode))
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
-                Latency_None, op);
-    if (!isMove)
-        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
-                Latency_None, op);
-
-    //CMPXCHG uses and defines EAX
-    if (isCmpxchg == true) {
-        updateDependencyGraph(UseDefType_Reg, PhysicalReg_EAX, LowOpndDefUse_UseDef,
-            Latency_None, op);
-    }
-
-    op->opndSrc.defuse = LowOpndDefUse_Use;
-    updateDependencyGraph(UseDefType_Reg, op->regSrc.regNum, op->opndSrc.defuse,
-            Latency_None, op);
-
-    if (isMove)
-        op->opndDest.defuse = LowOpndDefUse_Def;
-    else if (isCompareMnemonic(op->opCode))
-        op->opndDest.defuse = LowOpndDefUse_Use;
-    else // ALU ops
-        op->opndDest.defuse = LowOpndDefUse_UseDef;
-    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
-}
-
-//! \brief Updates dependency information for LowOps with two operands:
-//! floating point stack to memory.
-//! \param op must have mnemonic FSTP, FST, FISTP, or FIST
-void Scheduler::updateUseDefInformation_fp_to_mem(LowOpRegMem * op) {
-    assert(op->opCode == Mnemonic_FSTP || op->opCode == Mnemonic_FST
-            || op->opCode == Mnemonic_FISTP || op->opCode == Mnemonic_FIST);
-    op->instructionLatency = getAtomMnemonicLatency_reg_to_mem(op->opCode);
-    op->portType = getAtomMnemonicPort_reg_to_mem(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    handleFloatDependencyUpdate(op);
-
-    op->opndSrc.defuse = LowOpndDefUse_Use;
-    updateDependencyGraph(UseDefType_Reg, PhysicalReg_ST0, op->opndSrc.defuse,
-            Latency_None, op);
-    op->opndDest.defuse = LowOpndDefUse_Def;
-    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
-}
-
-//! \brief Updates dependency information for LowOps with two operands:
-//! memory to floating point stack.
-//! \param op must have mnemonic FLD or FILD, or must be an x87 ALU op
-void Scheduler::updateUseDefInformation_mem_to_fp(LowOpMemReg * op) {
-    assert(op->opCode == Mnemonic_FLD || op->opCode == Mnemonic_FILD
-            || op->opCode2 == ATOM_NORMAL_ALU);
-    op->instructionLatency = getAtomMnemonicLatency_mem_to_reg(op->opCode);
-    op->portType = getAtomMnemonicPort_mem_to_reg(op->opCode);
-    assert(op->instructionLatency != INVN);
-    assert(op->portType != INVP);
-
-    handleFloatDependencyUpdate(op);
-
-    op->opndSrc.defuse = LowOpndDefUse_Use;
-    updateDependencyGraphForMem(op->memSrc, op->opndSrc.defuse, op);
-    if (op->opCode == Mnemonic_FLD || op->opCode == Mnemonic_FILD)
-        op->opndDest.defuse = LowOpndDefUse_Def;
-    else // x87 ALU ops
-        op->opndDest.defuse = LowOpndDefUse_UseDef;
-    updateDependencyGraph(UseDefType_Reg, PhysicalReg_ST0, op->opndDest.defuse,
-            Latency_None, op);
-}
-
-//! \brief Generates IA native code for given LowOp
-//! \details This method takes a LowOp and generates x86 instructions into the
-//! code stream by making calls to the encoder.
-//! \param op to be encoded and placed into code stream.
-void Scheduler::generateAssembly(LowOp * op) {
-    if(IS_ANY_JIT_ERROR_SET())
-        return;
-    if (op->numOperands == 0) {
-        stream = encoder_return(stream);
-    } else if (op->numOperands == 1) {
-        if (op->opndSrc.type == LowOpndType_Label) {
-            bool unknown;
-            OpndSize size;
-            int imm;
-            if (op->opCode == Mnemonic_JMP)
-                imm = getRelativeOffset(((LowOpLabel*) op)->labelOpnd.label,
-                        ((LowOpLabel*) op)->labelOpnd.isLocal, JmpCall_uncond,
-                        &unknown, &size);
-            else if (op->opCode == Mnemonic_CALL)
-                imm = getRelativeOffset(((LowOpLabel*) op)->labelOpnd.label,
-                        ((LowOpLabel*) op)->labelOpnd.isLocal, JmpCall_call,
-                        &unknown, &size);
-            else
-                imm = getRelativeOffset(((LowOpLabel*) op)->labelOpnd.label,
-                        ((LowOpLabel*) op)->labelOpnd.isLocal, JmpCall_cond,
-                        &unknown, &size);
-            op->opndSrc.size = size;
-            stream = encoder_imm(op->opCode, op->opndSrc.size, imm, stream);
-        } else if (op->opndSrc.type == LowOpndType_BlockId) {
-            LowOpBlock * blockOp = reinterpret_cast<LowOpBlock *>(op);
-
-            // If the immediate needs aligned, then do a test dump to see
-            // if the immediate will cross the 16-byte boundary. If we plan
-            // on aligning immediate, we expect that its size will be 32-bit
-            if (blockOp->blockIdOpnd.immediateNeedsAligned) {
-                // Dump to stream but don't update stream pointer
-                char * newStream = encoder_imm(blockOp->opCode, OpndSize_32, 0,
-                        stream);
-
-                // Immediates are assumed to be at end of instruction, so just check
-                // that the updated stream pointer does not break up the 32-bit immediate
-                unsigned int bytesCrossing =
-                        reinterpret_cast<unsigned int>(newStream) % 16;
-                bool needNops =
-                        (bytesCrossing > OpndSize_Null
-                                && bytesCrossing < OpndSize_32) ? true : false;
-
-                if (needNops)
-                    stream = encoder_nops(OpndSize_32 - bytesCrossing, stream);
-            }
-
-            bool unknown;
-            OpndSize actualSize = OpndSize_Null;
-            int imm;
-            if (blockOp->opCode == Mnemonic_JMP)
-                imm = getRelativeNCG(blockOp->blockIdOpnd.value,
-                        JmpCall_uncond, &unknown, &actualSize);
-            else
-                imm = getRelativeNCG(blockOp->blockIdOpnd.value,
-                        JmpCall_cond, &unknown, &actualSize);
-
-            // When we need to align, we expect that the size of the immediate is
-            // 32-bit so we make sure of that now.
-            blockOp->opndSrc.size =
-                    blockOp->blockIdOpnd.immediateNeedsAligned ?
-                            OpndSize_32 : actualSize;
-
-            stream = encoder_imm(blockOp->opCode, blockOp->opndSrc.size, imm, stream);
-        } else if (op->opndSrc.type == LowOpndType_Imm) {
-            stream = encoder_imm(op->opCode, op->opndSrc.size,
-                    ((LowOpImm*) op)->immOpnd.value, stream);
-        } else if (op->opndSrc.type == LowOpndType_Reg) {
-            stream = encoder_reg(op->opCode, op->opndSrc.size,
-                    ((LowOpReg*) op)->regOpnd.regNum,
-                    ((LowOpReg*) op)->regOpnd.isPhysical,
-                    ((LowOpReg*) op)->regOpnd.regType, stream);
-        } else { // Corresponds to lower_mem
-            stream = encoder_mem(op->opCode, op->opndSrc.size,
-                    ((LowOpMem*) op)->memOpnd.m_disp.value,
-                    ((LowOpMem*) op)->memOpnd.m_base.regNum,
-                    ((LowOpMem*) op)->memOpnd.m_base.isPhysical, stream);
-        }
-    }
-    // Number of operands is 2
-    // Handles LowOps coming from  lower_imm_reg, lower_imm_mem,
-    // lower_reg_mem, lower_mem_reg, lower_mem_scale_reg,
-    // lower_reg_mem_scale, lower_reg_reg, lower_fp_mem, and lower_mem_fp
-    else if (op->opndDest.type == LowOpndType_Reg
-            && op->opndSrc.type == LowOpndType_Imm) {
-        stream = encoder_imm_reg_diff_sizes(op->opCode, op->opndSrc.size,
-                ((LowOpImmReg*) op)->immSrc.value,
-                op->opndDest.size,
-                ((LowOpImmReg*) op)->regDest.regNum,
-                ((LowOpImmReg*) op)->regDest.isPhysical,
-                ((LowOpImmReg*) op)->regDest.regType, stream);
-    } else if (op->opndDest.type == LowOpndType_Reg
-            && op->opndSrc.type == LowOpndType_Chain) {
-        // The immediates used for chaining must be aligned within a 16-byte
-        // region so we need to ensure that now.
-
-        // First, dump to code stream but do not update stream pointer
-        char * newStream = encoder_imm_reg(op->opCode, op->opndDest.size,
-                ((LowOpImmReg*) op)->immSrc.value,
-                ((LowOpImmReg*) op)->regDest.regNum,
-                ((LowOpImmReg*) op)->regDest.isPhysical,
-                ((LowOpImmReg*) op)->regDest.regType, stream);
-
-        // Immediates are assumed to be at end of instruction, so just check
-        // that the updated stream pointer does not break up the immediate
-        unsigned int bytesCrossing =
-                reinterpret_cast<unsigned int>(newStream) % 16;
-        bool needNops =
-                (bytesCrossing > OpndSize_Null
-                        && bytesCrossing < op->opndDest.size) ? true : false;
-
-        if (needNops)
-            stream = encoder_nops(op->opndDest.size - bytesCrossing, stream);
-
-        // Now we are ready to do the actual encoding
-        insertChainingWorklist(((LowOpImmReg*) op)->immSrc.value, stream);
-        stream = encoder_imm_reg(op->opCode, op->opndDest.size,
-                ((LowOpImmReg*) op)->immSrc.value,
-                ((LowOpImmReg*) op)->regDest.regNum,
-                ((LowOpImmReg*) op)->regDest.isPhysical,
-                ((LowOpImmReg*) op)->regDest.regType, stream);
-    } else if (op->opndDest.type == LowOpndType_Mem
-            && op->opndSrc.type == LowOpndType_Imm) {
-        stream = encoder_imm_mem(op->opCode, op->opndDest.size,
-                ((LowOpImmMem*) op)->immSrc.value,
-                ((LowOpImmMem*) op)->memDest.m_disp.value,
-                ((LowOpImmMem*) op)->memDest.m_base.regNum,
-                ((LowOpImmMem*) op)->memDest.m_base.isPhysical, stream);
-    } else if (op->opndDest.type == LowOpndType_Mem
-            && op->opndSrc.type == LowOpndType_Chain) {
-        // The immediates used for chaining must be aligned within a 16-byte
-        // region so we need to ensure that now.
-
-        // First, dump to code stream but do not update stream pointer
-        char * newStream = encoder_imm_mem(op->opCode, op->opndDest.size,
-                ((LowOpImmMem*) op)->immSrc.value,
-                ((LowOpImmMem*) op)->memDest.m_disp.value,
-                ((LowOpImmMem*) op)->memDest.m_base.regNum,
-                ((LowOpImmMem*) op)->memDest.m_base.isPhysical, stream);
-
-        // Immediates are assumed to be at end of instruction, so just check
-        // that the updated stream pointer does not break up the immediate
-        unsigned int bytesCrossing =
-                reinterpret_cast<unsigned int>(newStream) % 16;
-        bool needNops =
-                (bytesCrossing > OpndSize_Null
-                        && bytesCrossing < op->opndDest.size) ? true : false;
-
-        if (needNops)
-            stream = encoder_nops(op->opndDest.size - bytesCrossing, stream);
-
-        // Now we are ready to do the actual encoding
-        insertChainingWorklist(((LowOpImmMem*) op)->immSrc.value, stream);
-        stream = encoder_imm_mem(op->opCode, op->opndDest.size,
-                ((LowOpImmMem*) op)->immSrc.value,
-                ((LowOpImmMem*) op)->memDest.m_disp.value,
-                ((LowOpImmMem*) op)->memDest.m_base.regNum,
-                ((LowOpImmMem*) op)->memDest.m_base.isPhysical, stream);
-    } else if (op->opndDest.type == LowOpndType_Reg
-            && op->opndSrc.type == LowOpndType_Reg) {
-        if (op->opCode == Mnemonic_FUCOMP || op->opCode == Mnemonic_FUCOM) {
-            stream = encoder_compare_fp_stack(op->opCode == Mnemonic_FUCOMP,
-                    ((LowOpRegReg*) op)->regSrc.regNum
-                            - ((LowOpRegReg*) op)->regDest.regNum,
-                    op->opndDest.size == OpndSize_64, stream);
-        } else {
-            stream = encoder_reg_reg_diff_sizes(op->opCode, op->opndSrc.size,
-                    ((LowOpRegReg*) op)->regSrc.regNum,
-                    ((LowOpRegReg*) op)->regSrc.isPhysical,
-                    op->opndDest.size,
-                    ((LowOpRegReg*) op)->regDest.regNum,
-                    ((LowOpRegReg*) op)->regDest.isPhysical,
-                    ((LowOpRegReg*) op)->regDest.regType, stream);
-        }
-    } else if (op->opndDest.type == LowOpndType_Reg
-            && op->opndSrc.type == LowOpndType_Mem) {
-        // Corresponds to lower_mem_reg, lower_mem_fp, or lower_mem_scale_reg
-        LowOpMemReg * regmem_op = (LowOpMemReg*) op;
-
-        // Constant initialization for 64 bit data requires saving stream address location
-        struct ConstInfo* tmpPtr;
-        tmpPtr = regmem_op->constLink;
-        if (tmpPtr != NULL && tmpPtr->constAddr == NULL){
-            // save address of instruction post scheduling
-            tmpPtr->streamAddr = stream;
-        }
-
-        if (regmem_op->regDest.regType == LowOpndRegType_fs)
-            stream = encoder_mem_fp(regmem_op->opCode, regmem_op->opndSrc.size,
-                    regmem_op->memSrc.m_disp.value,
-                    regmem_op->memSrc.m_base.regNum,
-                    regmem_op->memSrc.m_base.isPhysical,
-                    regmem_op->regDest.regNum - PhysicalReg_ST0, stream);
-        else if (regmem_op->memSrc.hasScale)
-            stream = encoder_mem_disp_scale_to_reg_diff_sizes(regmem_op->opCode,
-                    regmem_op->opndSrc.size, regmem_op->memSrc.m_base.regNum,
-                    regmem_op->memSrc.m_base.isPhysical,
-                    regmem_op->memSrc.m_disp.value,
-                    regmem_op->memSrc.m_index.regNum,
-                    regmem_op->memSrc.m_index.isPhysical,
-                    regmem_op->memSrc.m_scale.value, regmem_op->opndDest.size,
-                    regmem_op->regDest.regNum, regmem_op->regDest.isPhysical,
-                    regmem_op->regDest.regType, stream);
-        else
-            stream = encoder_mem_to_reg_diff_sizes(regmem_op->opCode,
-                    regmem_op->opndSrc.size, regmem_op->memSrc.m_disp.value,
-                    regmem_op->memSrc.m_base.regNum,
-                    regmem_op->memSrc.m_base.isPhysical,
-                    regmem_op->opndDest.size, regmem_op->regDest.regNum,
-                    regmem_op->regDest.isPhysical, regmem_op->regDest.regType,
-                    stream);
-    } else if (op->opndDest.type == LowOpndType_Mem
-            && op->opndSrc.type == LowOpndType_Reg) {
-        // Corresponds to lower_reg_mem, lower_fp_mem, or lower_reg_mem_scale
-        LowOpRegMem * memreg_op = (LowOpRegMem*) op;
-        if (memreg_op->regSrc.regType == LowOpndRegType_fs)
-            stream = encoder_fp_mem(memreg_op->opCode, memreg_op->opndDest.size,
-                    memreg_op->regSrc.regNum - PhysicalReg_ST0,
-                    memreg_op->memDest.m_disp.value,
-                    memreg_op->memDest.m_base.regNum,
-                    memreg_op->memDest.m_base.isPhysical, stream);
-        else if (memreg_op->memDest.hasScale)
-            stream = encoder_reg_mem_disp_scale(memreg_op->opCode,
-                    memreg_op->opndDest.size, memreg_op->regSrc.regNum,
-                    memreg_op->regSrc.isPhysical,
-                    memreg_op->memDest.m_base.regNum,
-                    memreg_op->memDest.m_base.isPhysical,
-                    memreg_op->memDest.m_disp.value,
-                    memreg_op->memDest.m_index.regNum,
-                    memreg_op->memDest.m_index.isPhysical,
-                    memreg_op->memDest.m_scale.value,
-                    memreg_op->regSrc.regType, stream);
-        else
-            stream = encoder_reg_mem(op->opCode, op->opndDest.size,
-                    memreg_op->regSrc.regNum, memreg_op->regSrc.isPhysical,
-                    memreg_op->memDest.m_disp.value,
-                    memreg_op->memDest.m_base.regNum,
-                    memreg_op->memDest.m_base.isPhysical,
-                    memreg_op->regSrc.regType, stream);
-    }
-    if(gDvmJit.codeCacheByteUsed + (stream - streamStart) +
-       CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
-        ALOGI("JIT_INFO: Code cache full after Scheduler::generateAssembly (trace uses %uB)", (stream - streamStart));
-        SET_JIT_ERROR(kJitErrorCodeCacheFull);
-        gDvmJit.codeCacheFull = true;
-    }
-}
-
-//! \brief Figures out which LowOps are ready after an instruction at chosenIdx
-//! is scheduled.
-//! \details It also updates the readyTime of every LowOp waiting to be scheduled.
-//! \param chosenIdx is the index of the chosen instruction for scheduling
-//! \param scheduledOps is an input list of scheduled LowOps
-//! \param readyOps is an output list of LowOps that are ready
-void Scheduler::updateReadyOps(int chosenIdx, BitVector * scheduledOps,
-        BitVector * readyOps) {
-    // Go through each successor LIR that depends on selected LIR
-    for (unsigned int k = 0; k < dependencyAssociation[queuedLIREntries[chosenIdx]].successorDependencies.size(); ++k) {
-        int dst = dependencyAssociation[queuedLIREntries[chosenIdx]].successorDependencies[k].lowopSlotId;
-        bool isReady = true;
-        int readyTime = -1;
-        // If all predecessors are scheduled, insert into ready queue
-        for (unsigned int k2 = 0; k2 < dependencyAssociation[queuedLIREntries[dst]].predecessorDependencies.size(); ++k2) {
-            int src = dependencyAssociation[queuedLIREntries[dst]].predecessorDependencies[k2].lowopSlotId;
-            if (dvmIsBitSet(scheduledOps, src) == false) {
-                // If one of parents hasn't been scheduled, then current instruction is not ready
-                isReady = false;
-                break;
-            }
-
-            // If our candidate is a RAW, then we must wait until parent finishes
-            // executing. However, if we have a WAW, WAR, or RAR, then we can issue
-            // next cycle.
-            int readyDelay =
-                    (dependencyAssociation[queuedLIREntries[dst]].predecessorDependencies[k2].dataHazard
-                            == Dependency_RAW) ?
-                            queuedLIREntries[src]->instructionLatency : 1;
-
-            // Candidate ready time is the sum of the scheduled time of parent,
-            // the latency of parent, and the weight of edge between parent
-            // and self
-            int candidateReadyTime = queuedLIREntries[src]->scheduledTime + readyDelay
-                    + dependencyAssociation[queuedLIREntries[dst]].predecessorDependencies[k2].edgeLatency;
-
-            if (readyTime < candidateReadyTime) {
-                // This is ready after ALL predecessors have finished executing
-                readyTime = candidateReadyTime;
-            }
-        }
-        if (isReady) {
-            dvmSetBit(readyOps, dst);
-            queuedLIREntries[dst]->readyTime = readyTime;
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-            ALOGD("update readyTime of slot %d: %d\n", dst, readyTime);
-#endif
-        }
-    }
-}
-
-//! This method constructs the inverse topological sort of the
-//! dependency graph of current basic block (queuedLIREntries)
-//! \param nodeId Index of LowOp in queuedLIREntries
-//! \param visitedList List with same indexing as queuedLIREntries
-//! that keeps track of LowOps that have already been visited
-//! \param inverseTopologicalOrder A list that will eventually
-//! hold the inverse topological order of the dependency graph.
-//! Inverse order means that parent nodes come later in the list
-//! compared to its children.
-void Scheduler::visitNodeTopologicalSort(unsigned int nodeId, int * visitedList,
-        NativeBasicBlock & inverseTopologicalOrder) {
-    // If it has been visited already, there's no need to do anything
-    if (visitedList[nodeId] == 0) {
-        assert(queuedLIREntries[nodeId]->slotId == nodeId);
-        // Mark as visited
-        visitedList[nodeId]++;
-        for (unsigned int child = 0;
-                child < dependencyAssociation[queuedLIREntries[nodeId]].successorDependencies.size();
-                ++child) {
-            // visit children
-            visitNodeTopologicalSort(
-                    dependencyAssociation[queuedLIREntries[nodeId]].successorDependencies[child].lowopSlotId,
-                    visitedList, inverseTopologicalOrder);
-        }
-        // Since all children have been visited, can now add node to list
-        inverseTopologicalOrder.push_back(queuedLIREntries[nodeId]);
-    }
-}
-
-//! \brief Finds longest path latency for every node in every tree in the
-//! dependency graph.
-//! \details This updates the longest path field of every LowOp in the current
-//! native basic block.
-//! \see queuedLIREntries
-void Scheduler::findLongestPath() {
-    NativeBasicBlock inverseTopologicalOrder;
-
-    // Initialize visited list to 0 (false) for all nodes
-    int visitedList[queuedLIREntries.size()];
-    memset(visitedList, 0, queuedLIREntries.size() * sizeof(int));
-
-    // Determine topological order.
-    for (unsigned int node = 0; node < queuedLIREntries.size(); ++node) {
-        visitNodeTopologicalSort(node, visitedList, inverseTopologicalOrder);
-    }
-
-    assert(queuedLIREntries.size() == inverseTopologicalOrder.size());
-
-    // for each node in inverse topological order
-    for(unsigned int vindex = 0; vindex < inverseTopologicalOrder.size(); ++vindex) {
-        int bestLongestPath = 0;
-        // Go through each child find the best longest path.
-        // Since we are doing this in inverse topological order,
-        // we know the longest path for all children has already
-        // been updated.
-        for(unsigned int windex = 0; windex < dependencyAssociation[inverseTopologicalOrder[vindex]].successorDependencies.size();
-                ++windex) {
-            int successorSlotId = dependencyAssociation[inverseTopologicalOrder[vindex]].successorDependencies[windex].lowopSlotId;
-            int edgeLatency = dependencyAssociation[inverseTopologicalOrder[vindex]].successorDependencies[windex].edgeLatency;
-            if (queuedLIREntries[successorSlotId]->longestPath > bestLongestPath) {
-                bestLongestPath = queuedLIREntries[successorSlotId]->longestPath + edgeLatency;
-            }
-        }
-        // Longest path to self is sum of best longest path to children plus
-        // instruction latency of self
-        inverseTopologicalOrder[vindex]->longestPath = inverseTopologicalOrder[vindex]->instructionLatency
-                + bestLongestPath;
-    }
-}
-
-//! \brief Reorders basic block to minimize block latency and make use of both
-//! Atom issue ports.
-//! \details The result of scheduling is stored in scheduledLIREntries. Additionally,
-//! each LowOp individually stores its scheduled time (logical based on index ordering).
-//!
-//! Algorithm details:
-//! - select one LIR from readyQueue with 2 criteria:
-//!   -# smallest readyTime
-//!   -# on critical path
-//! - A pair of LowOps can be issued at the same time slot if they use different issue ports.
-//! - A LowOp can be issued if
-//!   -# all pending ops can commit ahead of this LowOp (restriction reflected in readyTime)
-//!   -# it is ready
-//! - At end, currentTime is advanced to readyTime of the selected LowOps
-//! - If any LIR has jmp, jcc, call, or ret mnemonic, it must be scheduled last
-//!
-//! \see scheduledLIREntries
-//! \post Scheduler::scheduledLIREntries is same size as Scheduler::queuedLIREntries
-//! \post If last LIR in Scheduler::queuedLIREntries is a jump, call, or return, it must
-//! also be the last LIR in Scheduler::scheduledLIREntries
-void Scheduler::schedule() {
-    // Declare data structures for scheduling
-    unsigned int candidateArray[queuedLIREntries.size()]; // ready candidates for scheduling
-    unsigned int num_candidates = 0 /*index for candidateArray*/, numScheduled = 0, lirID;
-    int currentTime = 0;
-
-    // LIRs ready for scheduling
-    BitVector * readyOps = dvmCompilerAllocBitVector(queuedLIREntries.size(), false);
-    dvmClearAllBits(readyOps);
-    // LIRs that have been scheduled
-    BitVector * scheduledOps = dvmCompilerAllocBitVector(queuedLIREntries.size(), false);
-    dvmClearAllBits(scheduledOps);
-
-    // Set up the live out dependencies
-    setupLiveOutDependencies();
-
-    // Predecessor dependencies have already been initialized in the dependency graph building.
-    // Now, initialize successor dependencies to complete dependency graph.
-    for (lirID = 0; lirID < queuedLIREntries.size(); ++lirID) {
-        for (unsigned int k2 = 0; k2 < dependencyAssociation[queuedLIREntries[lirID]].predecessorDependencies.size(); ++k2) {
-            int src = dependencyAssociation[queuedLIREntries[lirID]].predecessorDependencies[k2].lowopSlotId;
-            DependencyInformation ds;
-            ds.lowopSlotId = lirID;
-
-            // Since edges are directional, no need to invert weight.
-            ds.edgeLatency = dependencyAssociation[queuedLIREntries[lirID]].predecessorDependencies[k2].edgeLatency;
-            dependencyAssociation[queuedLIREntries[src]].successorDependencies.push_back(ds);
-        }
-    }
-
-    // Find longest path from each LIR to the leaves of the dependency trees
-    findLongestPath();
-
-    // When a LowOp is ready, it means all its predecessors are scheduled
-    // and the readyTime of this LowOp has been set already.
-    for (lirID = 0; lirID < queuedLIREntries.size(); lirID++) {
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-        ALOGD("-- slot %d: latency %d port type %d\n", lirID, queuedLIREntries[lirID]->instructionLatency,
-                queuedLIREntries[lirID]->portType);
-#endif
-        if (dependencyAssociation[queuedLIREntries[lirID]].predecessorDependencies.size() == 0) {
-            dvmSetBit(readyOps, lirID);
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-            ALOGD("slot %d is ready\n", lirID);
-#endif
-            queuedLIREntries[lirID]->readyTime = 0;
-        }
-    }
-
-    // Schedule each of LIRs in the basic block
-    while (numScheduled < queuedLIREntries.size()) {
-        // Set chosen indices to BB size since no LIR will have
-        // this id.
-        unsigned int chosenIdx1 = queuedLIREntries.size();
-        unsigned int chosenIdx2 = queuedLIREntries.size();
-
-        // Reset number of picked candidates
-        num_candidates = 0;
-
-        // Select candidates that are ready (readyTime <= currentTime)
-        for (lirID = 0; lirID < queuedLIREntries.size(); lirID++) {
-            if (dvmIsBitSet(readyOps, lirID)
-                    && queuedLIREntries[lirID]->readyTime <= currentTime
-                    && (!isBasicBlockDelimiter(queuedLIREntries[lirID]->opCode)))
-                candidateArray[num_candidates++] = lirID;
-        }
-
-        // If no candidate is ready to be issued, just pick the one with
-        // the smallest readyTime
-        if (num_candidates == 0) {
-
-            // First, find the smallest ready time out of instructions that are
-            // ready.
-            for (lirID = 0; lirID < queuedLIREntries.size(); lirID++) {
-                if (dvmIsBitSet(readyOps, lirID)
-                        && (!isBasicBlockDelimiter(queuedLIREntries[lirID]->opCode))) {
-                    if (chosenIdx1 == queuedLIREntries.size()
-                            || queuedLIREntries[lirID]->readyTime
-                                    < queuedLIREntries[chosenIdx1]->readyTime) {
-                        chosenIdx1 = lirID;
-                        // Update current time with smallest ready time
-                        currentTime = queuedLIREntries[lirID]->readyTime;
-                    }
-                }
-            }
-
-            // Select any other candidates that also are ready at the same time.
-            for (lirID = 0; lirID < queuedLIREntries.size(); lirID++) {
-                if (dvmIsBitSet(readyOps, lirID)
-                        && (!isBasicBlockDelimiter(queuedLIREntries[lirID]->opCode))
-                        && queuedLIREntries[lirID]->readyTime <= currentTime) {
-                    candidateArray[num_candidates++] = lirID;
-                }
-            }
-        }
-
-        // This is the last gate for picking a candidate.
-        // By this point if still we don't have a candidate, it means that
-        // only the sync point instruction remains.
-        if (num_candidates == 0)
-            candidateArray[num_candidates++] = queuedLIREntries.size() - 1;
-
-        // Reinitialize chosenIdx1 since it was used earlier for
-        // finding smallest ready time
-        chosenIdx1 = queuedLIREntries.size();
-
-        // Pick candidate that is on the critical path
-        for (unsigned int i = 0; i < num_candidates; i++) {
-            lirID = candidateArray[i];
-            // Always try to pick a candidate. Once we've picked one,
-            // then we can start looking for another with a longer
-            // critical path
-            if (chosenIdx1 == queuedLIREntries.size()
-                    || queuedLIREntries[lirID]->longestPath
-                            > queuedLIREntries[chosenIdx1]->longestPath) {
-                chosenIdx1 = lirID;
-            }
-        }
-
-        // By the time we get to this point, we better have picked
-        // an instruction to schedule OR ELSE ...
-        assert (chosenIdx1 < queuedLIREntries.size());
-
-        // Pick 2 candidates if possible.
-        // If current candidate must issue on both ports, we cannot pick another
-        if (queuedLIREntries[chosenIdx1]->portType == BOTH_PORTS)
-            num_candidates = 0;
-
-        // The only way we will go through this logic is if chosen instruction
-        // doesn't issue on both ports
-        for (unsigned int i = 0; i < num_candidates; i++) {
-            lirID = candidateArray[i];
-            if (lirID == chosenIdx1)
-                continue; // Should skip the one already chosen
-
-            // Check for port conflict
-            if (queuedLIREntries[lirID]->portType == BOTH_PORTS)
-                continue; // Look for another one that doesn't issue on both ports
-            if (queuedLIREntries[chosenIdx1]->portType == EITHER_PORT
-                    || queuedLIREntries[lirID]->portType == EITHER_PORT
-                    || (queuedLIREntries[chosenIdx1]->portType == PORT0
-                            && queuedLIREntries[lirID]->portType == PORT1)
-                    || (queuedLIREntries[chosenIdx1]->portType == PORT1
-                            && queuedLIREntries[lirID]->portType == PORT0)) {
-                // Looks like we found one that doesn't conflict on ports
-                // However, still try to find one on critical path
-                if (chosenIdx2 == queuedLIREntries.size()
-                        || queuedLIREntries[lirID]->longestPath
-                                > queuedLIREntries[chosenIdx2]->longestPath) {
-                    chosenIdx2 = lirID;
-                }
-            }
-        }
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-        ALOGD("pick ready instructions at slots %d %d\n", chosenIdx1, chosenIdx2);
-#endif
-
-        scheduledLIREntries.push_back(queuedLIREntries[chosenIdx1]);
-        dvmSetBit(scheduledOps, chosenIdx1);
-        dvmClearBit(readyOps, chosenIdx1);
-        queuedLIREntries[chosenIdx1]->scheduledTime = currentTime;
-        numScheduled++;
-
-        if (chosenIdx2 < queuedLIREntries.size()) {
-            scheduledLIREntries.push_back(queuedLIREntries[chosenIdx2]);
-            dvmSetBit(scheduledOps, chosenIdx2);
-            dvmClearBit(readyOps, chosenIdx2);
-            queuedLIREntries[chosenIdx2]->scheduledTime = currentTime;
-            numScheduled++;
-        }
-
-        // Since we have scheduled instructions in this cycle, we should
-        // update the ready queue now to find new instructions whose
-        // dependencies have been satisfied
-        updateReadyOps(chosenIdx1, scheduledOps, readyOps);
-        if (chosenIdx2 < queuedLIREntries.size())
-            updateReadyOps(chosenIdx2, scheduledOps, readyOps);
-
-        // Advance time to next cycle
-        currentTime++;
-    }
-
-    // Make sure that original and scheduled basic blocks are same size
-    if (scheduledLIREntries.size() != queuedLIREntries.size()) {
-        ALOGI("JIT_INFO: (Atom Scheduler) Original basic block is not same \
-                size as the scheduled basic block");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return;
-    }
-
-    // Make sure that basic block delimiter mnemonic is always last one in
-    // scheduled basic block
-    if (isBasicBlockDelimiter(queuedLIREntries.back()->opCode)
-            && !isBasicBlockDelimiter(scheduledLIREntries.back()->opCode)) {
-        ALOGI("JIT_INFO: (Atom Scheduler) Sync point should be the last \
-                scheduled instruction.");
-        SET_JIT_ERROR(kJitErrorInsScheduling);
-        return;
-    }
-}
-
-//! \brief Called to signal the scheduler that the native basic block it has
-//! been building is finished.
-//! \details This method should be called from other modules to signal that the
-//! native basic block the Scheduler has been building is finished. This has
-//! side effects because it starts the scheduling process using already created
-//! dependency graphs and then updates the code stream with the scheduled
-//! instructions.
-//! \warning Jumps to immediate must signal end of native basic block for target.
-//! If the target has a label, then this is not a problem. But if jumping to an
-//! address without label, this method must be called before building dependency
-//! graph for target basic block.
-void Scheduler::signalEndOfNativeBasicBlock() {
-    if(queuedLIREntries.empty())
-            return; // No need to do any work
-
-#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
-    std::ostringstream dependGraphFileName;
-    char * streamStartBasicBlock = stream;
-    dependGraphFileName << "depengraph_" << gDvm.threadList[0].systemTid << "_"
-            << std::hex << (int)streamStartBasicBlock;
-#endif
-
-    printStatistics(true /*prescheduling*/);
-    schedule();
-    printStatistics(false /*prescheduling*/);
-
-    for(unsigned int k = 0; k < scheduledLIREntries.size(); ++k) {
-        generateAssembly(scheduledLIREntries[k]);
-    }
-
-#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
-    printDependencyGraph("/data/local/tmp/", dependGraphFileName.str(),
-            streamStartBasicBlock, true, true, true, true, true);
-#endif
-
-    // Clear all scheduler data structures
-    this->reset();
-}
-
-#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
-//! \brief Transforms LowOpndType enum to string
-//! \see LowOpndType
-inline const char * operandTypeToString(LowOpndType type) {
-    switch (type) {
-    case LowOpndType_Imm:
-        return "Imm";
-    case LowOpndType_Reg:
-        return "Reg";
-    case LowOpndType_Mem:
-        return "Mem";
-    case LowOpndType_Label:
-        return "Label";
-    case LowOpndType_BlockId:
-        return "BlockId";
-    case LowOpndType_Chain:
-        return "Chain";
-    }
-    return "-";
-}
-#endif
-
-//! \brief Returns a scaled distance between two basic blocks.
-//! \details Computes the Hamming distance between two basic blocks and then scales
-//! result by block size and turns it into percentage. block1 and block2 must
-//! have same size.
-//! \retval scaled Hamming distance
-inline double Scheduler::basicBlockEditDistance(const NativeBasicBlock & block1,
-        const NativeBasicBlock & block2) {
-#ifndef DISABLE_ATOM_SCHEDULING_STATISTICS
-    int distance = 0;
-    assert(block1.size() == block2.size());
-    for(unsigned int i = 0; i < block1.size(); ++i) {
-        if(block1[i] != block2[i]) {
-            distance += 1;
-        }
-    }
-    return (distance * 100.0) / block1.size();
-#else
-    return 0.0;
-#endif
-}
-
-//! \brief Prints Atom Instruction Scheduling statistics.
-//! Details prints block size and basic block difference.
-//! \todo Comparing basic block latencies pre and post scheduling is a useful
-//! statistic.
-//! \param prescheduling is used to indicate whether the statistics are requested
-//! before the scheduling
-void Scheduler::printStatistics(bool prescheduling) {
-#ifndef DISABLE_ATOM_SCHEDULING_STATISTICS
-    const char * message_tag =
-            prescheduling ?
-                    "Atom Sched Stats: Pre-schedule:" :
-                    "Atom Sched Stats: Post-schedule:";
-    NativeBasicBlock * lowOpList;
-    if (prescheduling)
-        lowOpList = &queuedLIREntries;
-    else
-        lowOpList = &scheduledLIREntries;
-
-    ALOGD("%s The block size is %d\n", message_tag, lowOpList->size());
-    if (!prescheduling) {
-        ALOGD("%s Difference in basic blocks after scheduling is %5.2f%%\n",
-                message_tag, basicBlockEditDistance(queuedLIREntries, scheduledLIREntries));
-    }
-#endif
-}
-
-//! \brief Prints dependency graph in dot format
-//! \details Creates dot files in /data/local/tmp with every basic block
-//! that has been scheduled.
-//! \param directoryPath the path to the directory
-//! \param dgfilename Name to use for dot file created
-//! \param startStream The pointer to the start of code cache stream where
-//! the basic block has been encoded
-//! \param printScheduledTime Allow printing of scheduled time of each LIR
-//! \param printIssuePort Allow printing of issue port of each LIR
-//! \param printInstructionLatency Allow printing of latency of each LIR
-//! \param printCriticalPath Allows printing of longest path latency for each
-//! LIR.
-//! \param printOriginalOrder Appends to front of instruction the original
-//! instruction order before scheduling.
-void Scheduler::printDependencyGraph(const char * directoryPath,
-        const std::string &dgfilename, const char * startStream,
-        bool printScheduledTime, bool printIssuePort,
-        bool printInstructionLatency, bool printCriticalPath,
-        bool printOriginalOrder) {
-#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
-    std::ofstream depengraphfile;
-    const unsigned int maxInstSize = 30;
-    char decodedInst[maxInstSize];
-
-    // Create dot file
-    std::string completeFSPath = directoryPath;
-    completeFSPath.append(dgfilename);
-    completeFSPath.append(".dot");
-    ALOGD("Dumping dependency graph to %s", completeFSPath.c_str());
-    depengraphfile.open(completeFSPath.c_str(), std::ios::out);
-
-    // A little error handling
-    if (depengraphfile.fail()) {
-        ALOGD("Encountered error when trying to open the file %s",
-                completeFSPath.c_str());
-        depengraphfile.close();
-        return;
-    }
-
-    // Print header
-    depengraphfile << "digraph BB" << dgfilename.c_str() << " {" << std::endl;
-    depengraphfile << "forcelabels = true" << std::endl;
-
-    // Print nodes
-    for (unsigned int i = 0; i < scheduledLIREntries.size(); ++i) {
-        startStream = decoder_disassemble_instr(const_cast<char *>(startStream),
-                decodedInst, maxInstSize);
-        // Add node with the x86 instruction as label
-        depengraphfile << "LIR" << scheduledLIREntries[i]->slotId
-                << " [shape=record, label=\"{";
-        if (printOriginalOrder)
-            depengraphfile << scheduledLIREntries[i]->slotId << ": ";
-        depengraphfile << decodedInst;
-        if (printScheduledTime) // Conditional print of time instruction was scheduled
-            depengraphfile << " | ScheduledTime:"
-                    << scheduledLIREntries[i]->scheduledTime;
-        if (printIssuePort) // Conditional print of issue port of instruction
-            // I promise the cast is safe
-            depengraphfile << " | IssuePort:"
-                    << getIssuePort(
-                            static_cast<IssuePort>(scheduledLIREntries[i]->portType));
-        if (printInstructionLatency) // Conditional print of instruction latency
-            depengraphfile << " | Latency:"
-                    << scheduledLIREntries[i]->instructionLatency;
-        if (printCriticalPath) // Conditional print of critical path
-            depengraphfile << " | LongestPath:"
-                    << scheduledLIREntries[i]->longestPath;
-        // Close label
-        depengraphfile << "}\"";
-        // Close node attributes
-        depengraphfile << "]" << std::endl;
-    }
-
-    // Print edge between each node and its successors
-    for (unsigned int i = 0; i < scheduledLIREntries.size(); ++i) {
-        // It is possible that successorDependencies contains duplicates
-        // So we set up a set here to avoid creating multiple edges
-        std::set<int> successors;
-        for (unsigned int j = 0;
-                j < dependencyAssociation[scheduledLIREntries[i]].successorDependencies.size(); ++j) {
-            int successorSlotId = dependencyAssociation[scheduledLIREntries[i]].successorDependencies[j].lowopSlotId;
-            if(successors.find(successorSlotId) != successors.end())
-                continue; // If we already generated edge for this successor, don't generate another
-            successors.insert(successorSlotId);
-            depengraphfile << "LIR" << scheduledLIREntries[i]->slotId << "->LIR"
-                    << dependencyAssociation[scheduledLIREntries[i]].successorDependencies[j].lowopSlotId
-                    << std::endl;
-        }
-    }
-    depengraphfile << "}" << std::endl;
-    depengraphfile.close();
-#endif
-}
diff --git a/vm/compiler/codegen/x86/Scheduler.h b/vm/compiler/codegen/x86/Scheduler.h
deleted file mode 100644
index e19b4f6..0000000
--- a/vm/compiler/codegen/x86/Scheduler.h
+++ /dev/null
@@ -1,171 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/*! \file Scheduler.h
-    \brief This file implements the interface for Atom Scheduler
-*/
-
-#ifndef ATOM_SCHEDULER_H_
-#define ATOM_SCHEDULER_H_
-
-#include "Lower.h"
-#include <map>
-#include <vector>
-#include "BitVector.h"
-
-/**
- * @class Dependencies
- * @brief Provides vectors for the dependencies
- */
-struct Dependencies
-{
-    //! \brief Holds information about LowOps on which current LowOp
-    //! depends on (predecessors).
-    //! \details For example, if a LowOp with slotId of 3 depends on
-    //! LowOp with slotId of 2 because of a RAW, then the LowOp with
-    //! slotId of 3 will have an entry in the predecessorDependencies
-    //! with a Dependency_RAW and slotId of 2. This field is used
-    //! only for scheduling.
-    std::vector<DependencyInformation> predecessorDependencies;
-    //! \brief Holds information about LowOps that depend on current
-    //! LowOp (successors).
-    //! \details For example, if a LowOp with slotId of 3 depends on
-    //! LowOp with slotId of 2 because of a RAW, then the LowOp with
-    //! slotId of 2 will have an entry in the successorDependencies
-    //! with a Dependency_RAW and slotId of 3. This field is used
-    //! only for scheduling.
-    std::vector<DependencyInformation> successorDependencies;
-};
-
-//! \brief Interface for Atom Instruction Scheduler
-class Scheduler {
-private:
-    //! \brief Defines implementation of a native basic block for Atom LIRs.
-    typedef std::vector<LowOp*> NativeBasicBlock;
-
-    //! \brief A map providing a link between LowOp and scheduling dependencies
-    std::map<LowOp *, Dependencies> dependencyAssociation;
-
-    //! \brief Holds a list of all LIRs allocated via allocateNewEmptyLIR
-    //! which are not yet in code stream.
-    //! \details The field LowOp::slotId corresponds to index into this list
-    //! when the LIR is allocated by scheduler via allocateNewEmptyLIR.
-    //! \see allocateNewEmptyLIR
-    NativeBasicBlock queuedLIREntries;
-
-    //! \brief Holds a list of scheduled LIRs in their scheduled order.
-    //! \details It contains the same LIRs are queuedLIREntries, just
-    //! in a possibly different order.
-    //! \see queuedLIREntries
-    NativeBasicBlock scheduledLIREntries;
-
-    //! \brief Used to keep track of writes to a resource.
-    //! \details This is used only during dependency building but corresponding
-    //! LIRs are also updated to keep track of their own dependencies which is
-    //! used during scheduling.
-    //! \see LowOp::predecessorDependencies
-    //! \see UseDefUserEntry
-    std::vector<UseDefProducerEntry> producerEntries;
-
-    //! \brief Used to keep track of reads from a resource.
-    //! \details This is used only during dependency building but corresponding
-    //! LIRs are also updated to keep track of their own dependencies which is
-    //! used during scheduling.
-    //! \see LowOp::predecessorDependencies
-    //! \see UseDefUserEntry
-    std::vector<UseDefUserEntry> userEntries;
-
-    //! \brief Used to keep track of dependencies on control flags. It keeps a
-    //! a list of all flag writers until a flag reader is seen.
-    //! \details This is used only during dependency building but corresponding
-    //! LIRs are also updated to keep track of their own dependencies which is
-    //! used during scheduling. This list holds values from LowOp::slotId.
-    //! \see LowOp::predecessorDependencies
-    //! \see LowOp::slotId
-    std::vector<unsigned int> ctrlEntries;
-
-    // See documentation in Schedule.cpp or in doxygen output for undocumented prototypes.
-    void updateDependencyGraph(UseDefEntryType type, int regNum,
-            LowOpndDefUse defuse,
-            LatencyBetweenNativeInstructions causeOfLatency, LowOp* op);
-    void updateDependencyGraphForMem(LowOpndMem & mOpnd, LowOpndDefUse defuse,
-            LowOp* op);
-    void handlePushDependencyUpdate(LowOp* op);
-    void handleFloatDependencyUpdate(LowOp* op);
-    void handleImplicitDependenciesEaxEdx(LowOp* op);
-    void setupLiveOutDependencies();
-
-    bool isBasicBlockDelimiter(Mnemonic m);
-    void generateAssembly(LowOp * op);
-
-    void visitNodeTopologicalSort(unsigned int nodeId, int * visitedList,
-            NativeBasicBlock & inverseTopologicalOrder);
-    void findLongestPath();
-    void updateReadyOps(int chosenIdx, BitVector * scheduledOps,
-            BitVector * readyOps);
-    void schedule();
-
-    double basicBlockEditDistance(const NativeBasicBlock & block1,
-            const NativeBasicBlock & block2);
-    void printStatistics(bool prescheduling);
-    void printDependencyGraph(const char * directoryPath,
-            const std::string &filename, const char * startStream,
-            bool printScheduledTime, bool printIssuePort,
-            bool printInstructionLatency, bool printCriticalPath,
-            bool printOriginalOrder);
-
-    //! \brief Reset the internal structures
-    void reset(void);
-public:
-    ~Scheduler(void);
-
-    //! \brief Called by users of scheduler to allocate an empty LIR (no mnemonic
-    //! or operands).
-    //! \details The caller of this method takes the LIR, updates the mnemonic
-    //! and operand information, and then calls one of the updateUseDefInformation
-    //! methods in the scheduler with this LIR as parameter. This method should not
-    //! be called when scheduling is not enabled because the LIR will never be freed.
-    //! Internally, the scheduler will add this LIR to the native basic block it
-    //! is building and also assign it an id.
-    //! Because of specialization this method definition must stay in the header in
-    //! order to prevent linker errors.
-    //! \tparam is a LowOp or any of its specialized children.
-    //! \see LowOp
-    template<typename LowOpType> LowOpType * allocateNewEmptyLIR() {
-        LowOpType * op = static_cast<LowOpType *>(dvmCompilerNew(
-                sizeof(LowOpType), true /*zero*/));
-        op->slotId = queuedLIREntries.size();
-        queuedLIREntries.push_back(op);
-        return op;
-    }
-
-    // See documentation in Schedule.cpp or in doxygen output for undocumented prototypes.
-    void updateUseDefInformation(LowOp * op);
-    void updateUseDefInformation_imm(LowOp * op);
-    void updateUseDefInformation_reg(LowOpReg * op);
-    void updateUseDefInformation_mem(LowOpMem * op);
-    void updateUseDefInformation_imm_to_reg(LowOpImmReg * op);
-    void updateUseDefInformation_imm_to_mem(LowOpImmMem * op);
-    void updateUseDefInformation_reg_to_reg(LowOpRegReg * op);
-    void updateUseDefInformation_mem_to_reg(LowOpMemReg * op);
-    void updateUseDefInformation_reg_to_mem(LowOpRegMem * op);
-    void updateUseDefInformation_fp_to_mem(LowOpRegMem * op);
-    void updateUseDefInformation_mem_to_fp(LowOpMemReg * op);
-    void signalEndOfNativeBasicBlock();
-    bool isQueueEmpty() const;
-};
-
-#endif /* ATOM_SCHEDULER_H_ */
diff --git a/vm/compiler/codegen/x86/Singleton.h b/vm/compiler/codegen/x86/Singleton.h
deleted file mode 100644
index 59ce8db..0000000
--- a/vm/compiler/codegen/x86/Singleton.h
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * @file Singleton.h
- * @brief Implements the interface for singleton pattern.
- */
-
-/**
- * @details Calling this function allows any non-singleton class to be treated
- * as a singleton. However, please note that making a copy of the instance
- * is still allowed and your new copy is no longer the singleton instance.
- * @return singleton instance
- */
-template <class T>
-T& singleton() {
-    // Better not to use dynamic allocation so that we can make sure
-    // that it will be destroyed.
-    static T instance;
-    return instance;
-}
-
-/**
- * @details Calling this function allows any non-singleton class to be treated
- * as a singleton.
- * @return pointer to singleton instance
- * @warning This is not thread safe. It also does not call class destructor
- * and leaks on program exit. Calls to this should not be mixed with calls to
- * singleton<T>() because they will not retrieve the same instance.
- */
-template <class T>
-T* singletonPtr() {
-    static T * instance = 0;
-
-    // Create a new instance if one doesn't already exist
-    if (instance == 0)
-        instance = new T();
-
-    return instance;
-}
diff --git a/vm/compiler/codegen/x86/StackExtensionX86.cpp b/vm/compiler/codegen/x86/StackExtensionX86.cpp
deleted file mode 100644
index 94f6dc8..0000000
--- a/vm/compiler/codegen/x86/StackExtensionX86.cpp
+++ /dev/null
@@ -1,58 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "Dalvik.h"
-#include "StackExtensionX86.h"
-
-/**
- * @brief Gives number of available scratch registers for x86.
- * @return Total number of scratch registers
- */
-unsigned int dvmArchSpecGetNumberOfScratch (void)
-{
-    return StackTemporaries::getTotalScratchVRs ();
-}
-
-/**
- * @brief Given a stratch register index, it gives the VR register number.
- * @param method Method that contains the MIR for which we want to
- * use scratch register.
- * @param idx Index of scratch register. Must be in range [0 .. N-1] where
- * N is the maximum number of scratch registers available.
- * @return Return virtual register number when it finds one for the index.
- * Otherwise, it returns -1.
- */
-int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx)
-{
-    unsigned int maxScratch = dvmArchSpecGetNumberOfScratch ();
-
-    //Sanity check to make sure that requested index is in
-    //range [0 .. maxScratch-1]
-    if (idx > (maxScratch - 1))
-    {
-        return -1;
-    }
-
-    //We know the index is okay. Index of 0 corresponds to virtual register
-    //whose number is: 0 + locals + ins
-    int numLocals = method->registersSize - method->insSize;
-    int numIns = method->insSize;
-
-    //Calculate the regnum
-    int regnum = idx + numLocals + numIns;
-
-    return regnum;
-}
diff --git a/vm/compiler/codegen/x86/StackExtensionX86.h b/vm/compiler/codegen/x86/StackExtensionX86.h
deleted file mode 100644
index 2df5657..0000000
--- a/vm/compiler/codegen/x86/StackExtensionX86.h
+++ /dev/null
@@ -1,68 +0,0 @@
-/*
- * Copyright (C) 2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef STACKEXTENSIONX86_H_
-#define STACKEXTENSIONX86_H_
-
-/**
- * @brief Space in frame to use for scratch registers.
- */
-class StackTemporaries
-{
-public:
-    /**
-     * @brief Gives the total number of scratch VRs available for every frame.
-     * @return Maximum number of scratch VRs.
-     */
-    static unsigned int getTotalScratchVRs (void)
-    {
-        return numScratch;
-    }
-
-private:
-    /**
-     * @brief Hardcoded number of scratch registers per frame.
-     */
-#ifdef EXTRA_SCRATCH_VR
-    static const unsigned int numScratch = 4;
-#else
-    static const unsigned int numScratch = 0;
-#endif
-
-    /**
-     * @brief Allocated space for the scratch registers.
-     */
-    u4 scratchVirtualRegisters[numScratch];
-};
-
-/**
- * @brief Stack frame extension for x86.
- */
-struct ArchSpecificStackExtension
-{
-
-#ifdef EXTRA_SCRATCH_VR
-    /**
-     * @brief Allocated space for temporaries.
-     * @warning If this structure gets moved, dvmArchSpecGetScratchRegister
-     * must be updated to provide a new mapping.
-     */
-    StackTemporaries temps;
-#endif
-
-};
-
-#endif /* STACKEXTENSIONX86_H_ */
diff --git a/vm/compiler/codegen/x86/Translator.h b/vm/compiler/codegen/x86/Translator.h
deleted file mode 100644
index 5873ddf..0000000
--- a/vm/compiler/codegen/x86/Translator.h
+++ /dev/null
@@ -1,28 +0,0 @@
-/*
- * Copyright (C) 2010-2013 Intel Corporation
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-
-#ifndef _DALVIK_TRANSLATOR
-#define _DALVIK_TRANSLATOR
-
-#include "Dalvik.h"
-#include "enc_wrapper.h"
-
-/* initialization for trace-based JIT */
-void initJIT(const char* curFileName, DvmDex *pDvmDex);
-
-#endif
diff --git a/vm/compiler/codegen/x86/doxygen-config-x86-jit b/vm/compiler/codegen/x86/doxygen-config-x86-jit
deleted file mode 100644
index ff1d620..0000000
--- a/vm/compiler/codegen/x86/doxygen-config-x86-jit
+++ /dev/null
@@ -1,9 +0,0 @@
-PROJECT_NAME = "X86 JIT Compiler for Dalvik"
-INPUT = AnalysisO1.cpp GlueOpt.cpp LowerHelper.cpp  NcgAot.cpp AnalysisO1.h LowerAlu.cpp LowerInvoke.cpp NcgAot.h Schedule.cpp BytecodeVisitor.cpp LowerConst.cpp LowerJump.cpp NcgCodegenO1.cpp Scheduler.h CodegenInterface.cpp Lower.cpp LowerMove.cpp NcgHelper.cpp Translator.h DataFlow.cpp LowerGetPut.cpp LowerObject.cpp NcgHelper.h Lower.h LowerReturn.cpp NullCheckElim.cpp
-OUTPUT_DIRECTORY = x86-jit-docs
-GENERATE_HTML = YES
-HTML_OUTPUT = html/
-HTML_FILE_EXTENSION = .html
-EXTRACT_PRIVATE = YES
-EXTRACT_STATIC = YES
-EXTRACT_LOCAL_CLASSES = YES
diff --git a/vm/compiler/codegen/x86/libenc/Android.mk b/vm/compiler/codegen/x86/libenc/Android.mk
deleted file mode 100644
index 6fe9cdb..0000000
--- a/vm/compiler/codegen/x86/libenc/Android.mk
+++ /dev/null
@@ -1,65 +0,0 @@
-#
-# Copyright (C) 2012 The Android Open Source Project
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#      http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-# Only include the x86 encoder/decoder for x86 architecture
-ifeq ($(TARGET_ARCH),x86)
-
-LOCAL_PATH:= $(call my-dir)
-
-ifneq ($(LIBENC_INCLUDED),true)
-
-LIBENC_INCLUDED := true
-
-enc_src_files := \
-        enc_base.cpp \
-        dec_base.cpp \
-        enc_wrapper.cpp \
-        enc_tabl.cpp
-
-enc_include_files :=
-
-##
-##
-## Build the device version of libenc
-##
-##
-ifneq ($(SDK_ONLY),true)  # SDK_only doesn't need device version
-
-include $(CLEAR_VARS)
-LOCAL_SRC_FILES := $(enc_src_files)
-LOCAL_C_INCLUDES += $(enc_include_files)
-LOCAL_MODULE_TAGS := optional
-LOCAL_MODULE := libenc
-include $(BUILD_STATIC_LIBRARY)
-
-endif # !SDK_ONLY
-
-
-##
-##
-## Build the host version of libenc
-##
-##
-include $(CLEAR_VARS)
-LOCAL_SRC_FILES := $(enc_src_files)
-LOCAL_C_INCLUDES += $(enc_include_files)
-LOCAL_MODULE_TAGS := optional
-LOCAL_MODULE := libenc
-include $(BUILD_HOST_STATIC_LIBRARY)
-
-endif   # ifneq ($(LIBENC_INCLUDED),true)
-
-endif   # ifeq ($(TARGET_ARCH),x86)
diff --git a/vm/compiler/codegen/x86/libenc/README.txt b/vm/compiler/codegen/x86/libenc/README.txt
deleted file mode 100644
index 30df760..0000000
--- a/vm/compiler/codegen/x86/libenc/README.txt
+++ /dev/null
@@ -1,26 +0,0 @@
-Original source from Apache Harmony 5.0M15 (r991518 from 2010-09-01) at
-http://harmony.apache.org/.
-
-The following files are from drlvm/vm/port/src/encoder/ia32_em64t.
-
-    dec_base.cpp
-    dec_base.h
-    enc_base.cpp
-    enc_base.h
-    enc_defs.h
-    enc_prvt.h
-    enc_tabl.cpp
-    encoder.cpp
-    encoder.h
-    encoder.inl
-
-The following files are derived partially from the original Apache
-Harmony files.
-
-    enc_defs_ext.h -- derived from enc_defs.h
-    enc_wrapper.h  -- derived from encoder.h
-
-
-
-
-
diff --git a/vm/compiler/codegen/x86/libenc/dec_base.cpp b/vm/compiler/codegen/x86/libenc/dec_base.cpp
deleted file mode 100644
index 17471c7..0000000
--- a/vm/compiler/codegen/x86/libenc/dec_base.cpp
+++ /dev/null
@@ -1,542 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-
-/**
- * @file
- * @brief Main decoding (disassembling) routines implementation.
- */
-
-#include "dec_base.h"
-#include "enc_prvt.h"
-#include <stdio.h>
-//#include "open/common.h"
-
-bool DecoderBase::is_prefix(const unsigned char * bytes)
-{
-    unsigned char b0 = *bytes;
-    unsigned char b1 = *(bytes+1);
-    if (b0 == 0xF0) { // LOCK
-        return true;
-    }
-    if (b0==0xF2 || b0==0xF3) { // REPNZ/REPZ prefixes
-        if (b1 == 0x0F) {   // .... but may be a part of SIMD opcode
-            return false;
-        }
-        return true;
-    }
-    if (b0 == 0x2E || b0 == 0x36 || b0==0x3E || b0==0x26 || b0==0x64 || b0==0x3E) {
-        // branch hints, segment prefixes
-        return true;
-    }
-    if (b0==0x66) { // operand-size prefix
-        if (b1 == 0x0F) {   // .... but may be a part of SIMD opcode
-            return false;
-        }
-        return false; //XXX - currently considered as part of opcode//true;
-    }
-    if (b0==0x67) { // address size prefix
-        return true;
-    }
-    return false;
-}
-
-// Returns prefix count from 0 to 4, or ((unsigned int)-1) on error
-unsigned int DecoderBase::fill_prefs(const unsigned char * bytes, Inst * pinst)
-{
-    const unsigned char * my_bytes = bytes;
-
-    while( 1 )
-    {
-        unsigned char by1 = *my_bytes;
-        unsigned char by2 = *(my_bytes + 1);
-        Inst::PrefGroups where;
-
-        switch( by1 )
-        {
-        case InstPrefix_REPNE:
-        case InstPrefix_REP:
-        {
-            if( 0x0F == by2)
-            {
-                return pinst->prefc;
-            }
-        }
-        case InstPrefix_LOCK:
-        {
-            where = Inst::Group1;
-            break;
-        }
-        case InstPrefix_CS:
-        case InstPrefix_SS:
-        case InstPrefix_DS:
-        case InstPrefix_ES:
-        case InstPrefix_FS:
-        case InstPrefix_GS:
-//      case InstPrefix_HintTaken: the same as CS override
-//      case InstPrefix_HintNotTaken: the same as DS override
-        {
-            where = Inst::Group2;
-            break;
-        }
-        case InstPrefix_OpndSize:
-        {
-//NOTE:   prefix does not work for JMP Sz16, the opcode is 0x66 0xe9
-//        here 0x66 will be treated as prefix, try_mn will try to match the code starting at 0xe9
-//        it will match JMP Sz32 ...
-//HACK:   assume it is the last prefix, return any way
-            if( 0x0F == by2)
-            {
-                return pinst->prefc;
-            }
-            return pinst->prefc;
-            where = Inst::Group3;
-            break;
-        }
-        case InstPrefix_AddrSize:
-        {
-            where = Inst::Group4;
-            break;
-        }
-        default:
-        {
-            return pinst->prefc;
-        }
-        }
-        // Assertions are not allowed here.
-        // Error situations should result in returning error status
-        if (InstPrefix_Null != pinst->pref[where]) //only one prefix in each group
-            return (unsigned int)-1;
-
-        pinst->pref[where] = (InstPrefix)by1;
-
-        if (pinst->prefc >= 4) //no more than 4 prefixes
-            return (unsigned int)-1;
-
-        pinst->prefc++;
-        ++my_bytes;
-    }
-}
-
-
-
-unsigned DecoderBase::decode(const void * addr, Inst * pinst)
-{
-    Inst tmp;
-
-    //assert( *(unsigned char*)addr != 0x66);
-
-    const unsigned char * bytes = (unsigned char*)addr;
-
-    // Load up to 4 prefixes
-    // for each Mnemonic
-    unsigned int pref_count = fill_prefs(bytes, &tmp);
-
-    if (pref_count == (unsigned int)-1) // Wrong prefix sequence, or >4 prefixes
-        return 0; // Error
-
-    bytes += pref_count;
-
-    //  for each opcodedesc
-    //      if (raw_len == 0) memcmp(, raw_len)
-    //  else check the mixed state which is one of the following:
-    //      /digit /i /rw /rd /rb
-
-    bool found = false;
-    const unsigned char * saveBytes = bytes;
-    for (unsigned mn=1; mn<Mnemonic_Count; mn++) {
-        bytes = saveBytes;
-        found=try_mn((Mnemonic)mn, &bytes, &tmp);
-        if (found) {
-            tmp.mn = (Mnemonic)mn;
-            break;
-        }
-    }
-    if (!found) {
-        // Unknown opcode
-        return 0;
-    }
-    tmp.size = (unsigned)(bytes-(const unsigned char*)addr);
-    if (pinst) {
-        *pinst = tmp;
-    }
-    return tmp.size;
-}
-
-#ifdef _EM64T_
-#define EXTEND_REG(reg, flag)                        \
-    ((NULL == rex || 0 == rex->flag) ? reg : (reg + 8))
-#else
-#define EXTEND_REG(reg, flag) (reg)
-#endif
-
-//don't know the use of rex, seems not used when _EM64T_ is not enabled
-bool DecoderBase::decode_aux(const EncoderBase::OpcodeDesc& odesc, unsigned aux,
-    const unsigned char ** pbuf, Inst * pinst
-#ifdef _EM64T_
-    , const Rex UNREF *rex
-#endif
-    )
-{
-    OpcodeByteKind kind = (OpcodeByteKind)(aux & OpcodeByteKind_KindMask);
-    unsigned byte = (aux & OpcodeByteKind_OpcodeMask);
-    unsigned data_byte = **pbuf;
-    EncoderBase::Operand& opnd = pinst->operands[pinst->argc];
-    const EncoderBase::OpndDesc& opndDesc = odesc.opnds[pinst->argc];
-
-    switch (kind) {
-    case OpcodeByteKind_SlashR:
-        {
-            RegName reg;
-            OpndKind okind;
-            const ModRM& modrm = *(ModRM*)*pbuf;
-            if (opndDesc.kind & OpndKind_Mem) { // 1st operand is memory
-#ifdef _EM64T_
-                decodeModRM(odesc, pbuf, pinst, rex);
-#else
-                decodeModRM(odesc, pbuf, pinst);
-#endif
-                ++pinst->argc;
-                const EncoderBase::OpndDesc& opndDesc2 = odesc.opnds[pinst->argc];
-                okind = ((opndDesc2.kind & OpndKind_XMMReg) || opndDesc2.size==OpndSize_64) ? OpndKind_XMMReg : OpndKind_GPReg;
-                EncoderBase::Operand& regOpnd = pinst->operands[pinst->argc];
-                reg = getRegName(okind, opndDesc2.size, EXTEND_REG(modrm.reg, r));
-                regOpnd = EncoderBase::Operand(reg);
-            } else {                            // 2nd operand is memory
-                okind = ((opndDesc.kind & OpndKind_XMMReg) || opndDesc.size==OpndSize_64) ? OpndKind_XMMReg : OpndKind_GPReg;
-                EncoderBase::Operand& regOpnd = pinst->operands[pinst->argc];
-                reg = getRegName(okind, opndDesc.size, EXTEND_REG(modrm.reg, r));
-                regOpnd = EncoderBase::Operand(reg);
-                ++pinst->argc;
-#ifdef _EM64T_
-                decodeModRM(odesc, pbuf, pinst, rex);
-#else
-                decodeModRM(odesc, pbuf, pinst);
-#endif
-            }
-            ++pinst->argc;
-        }
-        return true;
-    case OpcodeByteKind_rb:
-    case OpcodeByteKind_rw:
-    case OpcodeByteKind_rd:
-        {
-            // Gregory -
-            // Here we don't parse register because for current needs
-            // disassembler doesn't require to parse all operands
-            unsigned regid = data_byte - byte;
-            if (regid>7) {
-                return false;
-            }
-            OpndSize opnd_size;
-            switch(kind)
-            {
-            case OpcodeByteKind_rb:
-            {
-                opnd_size = OpndSize_8;
-                break;
-            }
-            case OpcodeByteKind_rw:
-            {
-                opnd_size = OpndSize_16;
-                break;
-            }
-            case OpcodeByteKind_rd:
-            {
-                opnd_size = OpndSize_32;
-                break;
-            }
-            default:
-                opnd_size = OpndSize_32;  // so there is no compiler warning
-                assert( false );
-            }
-            opnd = EncoderBase::Operand( getRegName(OpndKind_GPReg, opnd_size, regid) );
-
-            ++pinst->argc;
-            ++*pbuf;
-            return true;
-        }
-    case OpcodeByteKind_cb:
-        {
-        char offset = *(char*)*pbuf;
-        *pbuf += 1;
-        opnd = EncoderBase::Operand(offset);
-        ++pinst->argc;
-        //pinst->direct_addr = (void*)(pinst->offset + *pbuf);
-        }
-        return true;
-    case OpcodeByteKind_cw:
-        // not an error, but not expected in current env
-        // Android x86
-        {
-        short offset = *(short*)*pbuf;
-        *pbuf += 2;
-        opnd = EncoderBase::Operand(offset);
-        ++pinst->argc;
-        }
-        return true;
-        //return false;
-    case OpcodeByteKind_cd:
-        {
-        int offset = *(int*)*pbuf;
-        *pbuf += 4;
-        opnd = EncoderBase::Operand(offset);
-        ++pinst->argc;
-        }
-        return true;
-    case OpcodeByteKind_SlashNum:
-        {
-        const ModRM& modrm = *(ModRM*)*pbuf;
-        if (modrm.reg != byte) {
-            return false;
-        }
-        decodeModRM(odesc, pbuf, pinst
-#ifdef _EM64T_
-                        , rex
-#endif
-                        );
-        ++pinst->argc;
-        }
-        return true;
-    case OpcodeByteKind_ib:
-        {
-        char ival = *(char*)*pbuf;
-        opnd = EncoderBase::Operand(ival);
-        ++pinst->argc;
-        *pbuf += 1;
-        }
-        return true;
-    case OpcodeByteKind_iw:
-        {
-        short ival = *(short*)*pbuf;
-        opnd = EncoderBase::Operand(ival);
-        ++pinst->argc;
-        *pbuf += 2;
-        }
-        return true;
-    case OpcodeByteKind_id:
-        {
-        int ival = *(int*)*pbuf;
-        opnd = EncoderBase::Operand(ival);
-        ++pinst->argc;
-        *pbuf += 4;
-        }
-        return true;
-#ifdef _EM64T_
-    case OpcodeByteKind_io:
-        {
-        long long int ival = *(long long int*)*pbuf;
-        opnd = EncoderBase::Operand(OpndSize_64, ival);
-        ++pinst->argc;
-        *pbuf += 8;
-        }
-        return true;
-#endif
-    case OpcodeByteKind_plus_i:
-        {
-            unsigned regid = data_byte - byte;
-            if (regid>7) {
-                return false;
-            }
-            ++*pbuf;
-            return true;
-        }
-    case OpcodeByteKind_ZeroOpcodeByte: // cant be here
-        return false;
-    default:
-        // unknown kind ? how comes ?
-        break;
-    }
-    return false;
-}
-
-bool DecoderBase::try_mn(Mnemonic mn, const unsigned char ** pbuf, Inst * pinst) {
-    const unsigned char * save_pbuf = *pbuf;
-    EncoderBase::OpcodeDesc * opcodes = EncoderBase::opcodes[mn];
-
-    for (unsigned i=0; !opcodes[i].last; i++) {
-        const EncoderBase::OpcodeDesc& odesc = opcodes[i];
-        char *opcode_ptr = const_cast<char *>(odesc.opcode);
-        int opcode_len = odesc.opcode_len;
-#ifdef _EM64T_
-        Rex *prex = NULL;
-        Rex rex;
-#endif
-
-        *pbuf = save_pbuf;
-#ifdef _EM64T_
-        // Match REX prefixes
-        unsigned char rex_byte = (*pbuf)[0];
-        if ((rex_byte & 0xf0) == 0x40)
-        {
-            if ((rex_byte & 0x08) != 0)
-            {
-                // Have REX.W
-                if (opcode_len > 0 && opcode_ptr[0] == 0x48)
-                {
-                    // Have REX.W in opcode. All mnemonics that allow
-                    // REX.W have to have specified it in opcode,
-                    // otherwise it is not allowed
-                    rex = *(Rex *)*pbuf;
-                    prex = &rex;
-                    (*pbuf)++;
-                    opcode_ptr++;
-                    opcode_len--;
-                }
-            }
-            else
-            {
-                // No REX.W, so it doesn't have to be in opcode. We
-                // have REX.B, REX.X, REX.R or their combination, but
-                // not in opcode, they may extend any part of the
-                // instruction
-                rex = *(Rex *)*pbuf;
-                prex = &rex;
-                (*pbuf)++;
-            }
-        }
-#endif
-        if (opcode_len != 0) {
-            if (memcmp(*pbuf, opcode_ptr, opcode_len)) {
-                continue;
-            }
-            *pbuf += opcode_len;
-        }
-        if (odesc.aux0 != 0) {
-
-            if (!decode_aux(odesc, odesc.aux0, pbuf, pinst
-#ifdef _EM64T_
-                            , prex
-#endif
-                            )) {
-                continue;
-            }
-            if (odesc.aux1 != 0) {
-                if (!decode_aux(odesc, odesc.aux1, pbuf, pinst
-#ifdef _EM64T_
-                            , prex
-#endif
-                            )) {
-                    continue;
-                }
-            }
-            pinst->odesc = &opcodes[i];
-            return true;
-        }
-        else {
-            // Can't have empty opcode
-            assert(opcode_len != 0);
-            pinst->odesc = &opcodes[i];
-            return true;
-        }
-    }
-    return false;
-}
-
-bool DecoderBase::decodeModRM(const EncoderBase::OpcodeDesc& odesc,
-    const unsigned char ** pbuf, Inst * pinst
-#ifdef _EM64T_
-    , const Rex *rex
-#endif
-    )
-{
-    EncoderBase::Operand& opnd = pinst->operands[pinst->argc];
-    const EncoderBase::OpndDesc& opndDesc = odesc.opnds[pinst->argc];
-
-    //XXX debug ///assert(0x66 != *(*pbuf-2));
-    const ModRM& modrm = *(ModRM*)*pbuf;
-    *pbuf += 1;
-
-    RegName base = RegName_Null;
-    RegName index = RegName_Null;
-    int disp = 0;
-    unsigned scale = 0;
-
-    // On x86_64 all mnemonics that allow REX.W have REX.W in opcode.
-    // Therefore REX.W is simply ignored, and opndDesc.size is used
-
-    if (modrm.mod == 3) {
-        // we have only modrm. no sib, no disp.
-        // Android x86: Use XMMReg for 64b operand.
-        OpndKind okind = ((opndDesc.kind & OpndKind_XMMReg) || opndDesc.size == OpndSize_64) ? OpndKind_XMMReg : OpndKind_GPReg;
-        RegName reg = getRegName(okind, opndDesc.size, EXTEND_REG(modrm.rm, b));
-        opnd = EncoderBase::Operand(reg);
-        return true;
-    }
-    //Android x86: m16, m32, m64: mean a byte[word|doubleword] operand in memory
-    //base and index should be 32 bits!!!
-    const SIB& sib = *(SIB*)*pbuf;
-    // check whether we have a sib
-    if (modrm.rm == 4) {
-        // yes, we have SIB
-        *pbuf += 1;
-        if (sib.index != 4) {
-            index = getRegName(OpndKind_GPReg, OpndSize_32, EXTEND_REG(sib.index, x)); //Android x86: OpndDesc.size
-        } else {
-            // (sib.index == 4) => no index
-            //%esp can't be sib.index
-        }
-
-        // scale = sib.scale == 0 ? 0 : (1<<sib.scale);
-        // scale = (1<<sib.scale);
-        scale = (index == RegName_Null) ? 0 : (1<<sib.scale);
-
-        if (sib.base != 5 || modrm.mod != 0) {
-            base = getRegName(OpndKind_GPReg, OpndSize_32, EXTEND_REG(sib.base, b)); //Android x86: OpndDesc.size
-        } else {
-            // (sib.base == 5 && modrm.mod == 0) => no base
-        }
-    }
-    else {
-        if (modrm.mod != 0 || modrm.rm != 5) {
-            base = getRegName(OpndKind_GPReg, OpndSize_32, EXTEND_REG(modrm.rm, b)); //Android x86: OpndDesc.size
-        }
-        else {
-            // mod=0 && rm == 5 => only disp32
-        }
-    }
-
-    //update disp and pbuf
-    if (modrm.mod == 2) {
-        // have disp32
-        disp = *(int*)*pbuf;
-        *pbuf += 4;
-    }
-    else if (modrm.mod == 1) {
-        // have disp8
-        disp = *(char*)*pbuf;
-        *pbuf += 1;
-    }
-    else {
-        assert(modrm.mod == 0);
-        if (modrm.rm == 5) {
-            // have disp32 w/o sib
-            disp = *(int*)*pbuf;
-            *pbuf += 4;
-        }
-        else if (modrm.rm == 4 && sib.base == 5) {
-            // have disp32 with SI in sib
-            disp = *(int*)*pbuf;
-            *pbuf += 4;
-        }
-    }
-    opnd = EncoderBase::Operand(opndDesc.size, base, index, scale, disp);
-    return true;
-}
-
diff --git a/vm/compiler/codegen/x86/libenc/dec_base.h b/vm/compiler/codegen/x86/libenc/dec_base.h
deleted file mode 100644
index 909c743..0000000
--- a/vm/compiler/codegen/x86/libenc/dec_base.h
+++ /dev/null
@@ -1,136 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-
-/**
- * @file
- * @brief Main decoding (disassembling) routines and structures.
- *
- * @note Quick and rough implementation, subject for a change.
- */
-
-#ifndef __DEC_BASE_H_INCLUDED__
-#define __DEC_BASE_H_INCLUDED__
-
-
-#include "enc_base.h"
-#include "enc_prvt.h"
-
-#ifdef ENCODER_ISOLATE
-using namespace enc_ia32;
-#endif
-
-#define IF_CONDITIONAL  (0x00000000)
-#define IF_SYMMETRIC    (0x00000000)
-#define IF_BRANCH       (0x00000000)
-
-struct Inst {
-    Inst() {
-        mn = Mnemonic_Null;
-        prefc = 0;
-        size = 0;
-        flags = 0;
-        //offset = 0;
-        //direct_addr = NULL;
-        argc = 0;
-        for(int i = 0; i < 4; ++i)
-        {
-            pref[i] = InstPrefix_Null;
-        }
-    }
-    /**
-     * Mnemonic of the instruction.s
-     */
-    Mnemonic mn;
-    /**
-     * Enumerating of indexes in the pref array.
-     */
-    enum PrefGroups
-    {
-        Group1 = 0,
-        Group2,
-        Group3,
-        Group4
-    };
-    /**
-     * Number of prefixes (1 byte each).
-     */
-    unsigned int prefc;
-    /**
-     * Instruction prefixes. Prefix should be placed here according to its group.
-     */
-    InstPrefix pref[4];
-    /**
-     * Size, in bytes, of the instruction.
-     */
-    unsigned size;
-    /**
-     * Flags of the instruction.
-     * @see MF_
-     */
-    unsigned flags;
-    /**
-     * An offset of target address, in case of 'CALL offset',
-     * 'JMP/Jcc offset'.
-     */
-    //int      offset;
-    /**
-     * Direct address of the target (on Intel64/IA-32 is 'instruction IP' +
-     * 'instruction length' + offset).
-     */
-    //void *   direct_addr;
-    /**
-     * Number of arguments of the instruction.
-     */
-    unsigned argc;
-    //
-    EncoderBase::Operand operands[3];
-    //
-    const EncoderBase::OpcodeDesc * odesc;
-};
-
-inline bool is_jcc(Mnemonic mn)
-{
-    return Mnemonic_JO <= mn && mn<=Mnemonic_JG;
-}
-
-class DecoderBase {
-public:
-    static unsigned decode(const void * addr, Inst * pinst);
-private:
-    static bool decodeModRM(const EncoderBase::OpcodeDesc& odesc,
-        const unsigned char ** pbuf, Inst * pinst
-#ifdef _EM64T_
-        , const Rex *rex
-#endif
-        );
-    static bool decode_aux(const EncoderBase::OpcodeDesc& odesc,
-        unsigned aux, const unsigned char ** pbuf,
-        Inst * pinst
-#ifdef _EM64T_
-        , const Rex *rex
-#endif
-        );
-    static bool try_mn(Mnemonic mn, const unsigned char ** pbuf, Inst * pinst);
-    static unsigned int fill_prefs( const unsigned char * bytes, Inst * pinst);
-    static bool is_prefix(const unsigned char * bytes);
-};
-
-#endif  // ~ __DEC_BASE_H_INCLUDED__
-
diff --git a/vm/compiler/codegen/x86/libenc/enc_base.cpp b/vm/compiler/codegen/x86/libenc/enc_base.cpp
deleted file mode 100644
index 0562ce8..0000000
--- a/vm/compiler/codegen/x86/libenc/enc_base.cpp
+++ /dev/null
@@ -1,1137 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-#include "enc_base.h"
-//#include <climits>
-#include <string.h>
-#define USE_ENCODER_DEFINES
-#include "enc_prvt.h"
-#include <stdio.h>
-
-//#define JET_PROTO
-
-#ifdef JET_PROTO
-#include "dec_base.h"
-#include "jvmti_dasm.h"
-#endif
-
-ENCODER_NAMESPACE_START
-
-/**
- * @file
- * @brief Main encoding routines and structures.
- */
-
-#ifndef _WIN32
-    #define strcmpi strcasecmp
-#endif
-
-int EncoderBase::dummy = EncoderBase::buildTable();
-
-const unsigned char EncoderBase::size_hash[OpndSize_64+1] = {
-    //
-    0xFF,   // OpndSize_Null        = 0,
-    3,              // OpndSize_8           = 0x1,
-    2,              // OpndSize_16          = 0x2,
-    0xFF,   // 0x3
-    1,              // OpndSize_32          = 0x4,
-    0xFF,   // 0x5
-    0xFF,   // 0x6
-    0xFF,   // 0x7
-    0,              // OpndSize_64          = 0x8,
-    //
-};
-
-const unsigned char EncoderBase::kind_hash[OpndKind_Mem+1] = {
-    //
-    //gp reg                -> 000 = 0
-    //memory                -> 001 = 1
-    //immediate             -> 010 = 2
-    //xmm reg               -> 011 = 3
-    //segment regs  -> 100 = 4
-    //fp reg                -> 101 = 5
-    //mmx reg               -> 110 = 6
-    //
-    0xFF,                          // 0    OpndKind_Null=0,
-    0<<2,                          // 1    OpndKind_GPReg =
-                                   //           OpndKind_MinRegKind=0x1,
-    4<<2,                          // 2    OpndKind_SReg=0x2,
-
-#ifdef _HAVE_MMX_
-    6<<2,                          // 3
-#else
-    0xFF,                          // 3
-#endif
-
-    5<<2,                          // 4    OpndKind_FPReg=0x4,
-    0xFF, 0xFF, 0xFF,              // 5, 6, 7
-    3<<2,                                   //      OpndKind_XMMReg=0x8,
-    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 9, 0xA, 0xB, 0xC, 0xD,
-                                              // 0xE, 0xF
-    0xFF,                          // OpndKind_MaxRegKind =
-                                   // OpndKind_StatusReg =
-                                   // OpndKind_OtherReg=0x10,
-    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 0x11-0x18
-    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,               // 0x19-0x1F
-    2<<2,                                   // OpndKind_Immediate=0x20,
-    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 0x21-0x28
-    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 0x29-0x30
-    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 0x31-0x38
-    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,               // 0x39-0x3F
-    1<<2,                                   // OpndKind_Memory=0x40
-};
-
-char * EncoderBase::curRelOpnd[3];
-
-char* EncoderBase::encode_aux(char* stream, unsigned aux,
-                              const Operands& opnds, const OpcodeDesc * odesc,
-                              unsigned * pargsCount, Rex * prex)
-{
-    const unsigned byte = aux;
-    OpcodeByteKind kind = (OpcodeByteKind)(byte & OpcodeByteKind_KindMask);
-    // The '>>' here is to force the switch to be table-based) instead of
-    // set of CMP+Jcc.
-    if (*pargsCount >= COUNTOF(opnds)) {
-        assert(false);
-        return stream;
-    }
-    switch(kind>>8) {
-    case OpcodeByteKind_SlashR>>8:
-        // /r - Indicates that the ModR/M byte of the instruction contains
-        // both a register operand and an r/m operand.
-        {
-        assert(opnds.count() > 1);
-    // not true anymore for MOVQ xmm<->r
-        //assert((odesc->opnds[0].kind & OpndKind_Mem) ||
-        //       (odesc->opnds[1].kind & OpndKind_Mem));
-        unsigned memidx = odesc->opnds[0].kind & OpndKind_Mem ? 0 : 1;
-        unsigned regidx = memidx == 0 ? 1 : 0;
-        memidx += *pargsCount;
-        regidx += *pargsCount;
-        ModRM& modrm = *(ModRM*)stream;
-        if (memidx >= COUNTOF(opnds) || regidx >= COUNTOF(opnds)) {
-            assert(false);
-            break;
-        }
-        if (opnds[memidx].is_mem()) {
-            stream = encodeModRM(stream, opnds, memidx, odesc, prex);
-        }
-        else {
-            modrm.mod = 3; // 11
-            modrm.rm = getHWRegIndex(opnds[memidx].reg());
-#ifdef _EM64T_
-            if (opnds[memidx].need_rex() && needs_rex_r(opnds[memidx].reg())) {
-                prex->b = 1;
-            }
-#endif
-            ++stream;
-        }
-        modrm.reg = getHWRegIndex(opnds[regidx].reg());
-#ifdef _EM64T_
-        if (opnds[regidx].need_rex() && needs_rex_r(opnds[regidx].reg())) {
-            prex->r = 1;
-        }
-#endif
-        *pargsCount += 2;
-        }
-        break;
-    case OpcodeByteKind_SlashNum>>8:
-        //  /digit - A digit between 0 and 7 indicates that the
-        //  ModR/M byte of the instruction uses only the r/m
-        //  (register or memory) operand. The reg field contains
-        //  the digit that provides an extension to the instruction's
-        //  opcode.
-        {
-        const unsigned lowByte = (byte & OpcodeByteKind_OpcodeMask);
-        assert(lowByte <= 7);
-        ModRM& modrm = *(ModRM*)stream;
-        unsigned idx = *pargsCount;
-        assert(opnds[idx].is_mem() || opnds[idx].is_reg());
-        if (opnds[idx].is_mem()) {
-            stream = encodeModRM(stream, opnds, idx, odesc, prex);
-        }
-        else {
-            modrm.mod = 3; // 11
-            modrm.rm = getHWRegIndex(opnds[idx].reg());
-#ifdef _EM64T_
-            if (opnds[idx].need_rex() && needs_rex_r(opnds[idx].reg())) {
-                prex->b = 1;
-            }
-#endif
-            ++stream;
-        }
-        modrm.reg = (char)lowByte;
-        *pargsCount += 1;
-        }
-        break;
-    case OpcodeByteKind_plus_i>>8:
-        //  +i - A number used in floating-point instructions when one
-        //  of the operands is ST(i) from the FPU register stack. The
-        //  number i (which can range from 0 to 7) is added to the
-        //  hexadecimal byte given at the left of the plus sign to form
-        //  a single opcode byte.
-        {
-            unsigned idx = *pargsCount;
-            const unsigned lowByte = (byte & OpcodeByteKind_OpcodeMask);
-            *stream = (char)lowByte + getHWRegIndex(opnds[idx].reg());
-            ++stream;
-            *pargsCount += 1;
-        }
-        break;
-    case OpcodeByteKind_ib>>8:
-    case OpcodeByteKind_iw>>8:
-    case OpcodeByteKind_id>>8:
-#ifdef _EM64T_
-    case OpcodeByteKind_io>>8:
-#endif //_EM64T_
-        //  ib, iw, id - A 1-byte (ib), 2-byte (iw), or 4-byte (id)
-        //  immediate operand to the instruction that follows the
-        //  opcode, ModR/M bytes or scale-indexing bytes. The opcode
-        //  determines if the operand is a signed value. All words
-        //  and double words are given with the low-order byte first.
-        {
-            unsigned idx = *pargsCount;
-            *pargsCount += 1;
-            assert(opnds[idx].is_imm());
-            if (kind == OpcodeByteKind_ib) {
-                *(unsigned char*)stream = (unsigned char)opnds[idx].imm();
-                curRelOpnd[idx] = stream;
-                stream += 1;
-            }
-            else if (kind == OpcodeByteKind_iw) {
-                *(unsigned short*)stream = (unsigned short)opnds[idx].imm();
-                curRelOpnd[idx] = stream;
-                stream += 2;
-            }
-            else if (kind == OpcodeByteKind_id) {
-                *(unsigned*)stream = (unsigned)opnds[idx].imm();
-                curRelOpnd[idx] = stream;
-                stream += 4;
-            }
-#ifdef _EM64T_
-            else {
-                assert(kind == OpcodeByteKind_io);
-                *(long long*)stream = (long long)opnds[idx].imm();
-                curRelOpnd[idx] = stream;
-                stream += 8;
-            }
-#else
-            else {
-                assert(false);
-            }
-#endif
-        }
-        break;
-    case OpcodeByteKind_cb>>8:
-        assert(opnds[*pargsCount].is_imm());
-        *(unsigned char*)stream = (unsigned char)opnds[*pargsCount].imm();
-        curRelOpnd[*pargsCount]= stream;
-        stream += 1;
-        *pargsCount += 1;
-        break;
-    case OpcodeByteKind_cw>>8:
-        assert(opnds[*pargsCount].is_imm());
-        *(unsigned short*)stream = (unsigned short)opnds[*pargsCount].imm();
-        curRelOpnd[*pargsCount]= stream;
-        stream += 2;
-        *pargsCount += 1;
-        break;
-    case OpcodeByteKind_cd>>8:
-        assert(opnds[*pargsCount].is_imm());
-        *(unsigned*)stream = (unsigned)opnds[*pargsCount].imm();
-        curRelOpnd[*pargsCount]= stream;
-        stream += 4;
-        *pargsCount += 1;
-        break;
-    //OpcodeByteKind_cp                             = 0x0B00,
-    //OpcodeByteKind_co                             = 0x0C00,
-    //OpcodeByteKind_ct                             = 0x0D00,
-    case OpcodeByteKind_rb>>8:
-    case OpcodeByteKind_rw>>8:
-    case OpcodeByteKind_rd>>8:
-        //  +rb, +rw, +rd - A register code, from 0 through 7,
-        //  added to the hexadecimal byte given at the left of
-        //  the plus sign to form a single opcode byte.
-        assert(opnds.count() > 0);
-        assert(opnds[*pargsCount].is_reg());
-        {
-        const unsigned lowByte = (byte & OpcodeByteKind_OpcodeMask);
-        *(unsigned char*)stream = (unsigned char)lowByte +
-                                   getHWRegIndex(opnds[*pargsCount].reg());
-#ifdef _EM64T_
-        if (opnds[*pargsCount].need_rex() && needs_rex_r(opnds[*pargsCount].reg())) {
-        prex->b = 1;
-        }
-#endif
-        ++stream;
-        *pargsCount += 1;
-        }
-        break;
-    default:
-        assert(false);
-        break;
-    }
-    return stream;
-}
-
-char * EncoderBase::encode(char * stream, Mnemonic mn, const Operands& opnds)
-{
-#ifdef _DEBUG
-    if (opnds.count() > 0) {
-        if (opnds[0].is_mem()) {
-            assert(getRegKind(opnds[0].base()) != OpndKind_SReg);
-        }
-        else if (opnds.count() >1 && opnds[1].is_mem()) {
-            assert(getRegKind(opnds[1].base()) != OpndKind_SReg);
-        }
-    }
-#endif
-
-#ifdef JET_PROTO
-    char* saveStream = stream;
-#endif
-
-    const OpcodeDesc * odesc = lookup(mn, opnds);
-#if !defined(_EM64T_)
-    bool copy_opcode = true;
-    Rex *prex = NULL;
-#else
-    // We need rex if
-    //  either of registers used as operand or address form is new extended register
-    //  it's explicitly specified by opcode
-    // So, if we don't have REX in opcode but need_rex, then set rex here
-    // otherwise, wait until opcode is set, and then update REX
-
-    bool copy_opcode = true;
-    unsigned char _1st = odesc->opcode[0];
-
-    Rex *prex = (Rex*)stream;
-    if (opnds.need_rex() &&
-        ((_1st == 0x66) || (_1st == 0xF2 || _1st == 0xF3) && odesc->opcode[1] == 0x0F)) {
-        // Special processing
-        //
-        copy_opcode = false;
-        //
-        *(unsigned char*)stream = _1st;
-        ++stream;
-        //
-        prex = (Rex*)stream;
-        prex->dummy = 4;
-        prex->w = 0;
-        prex->b = 0;
-        prex->x = 0;
-        prex->r = 0;
-        ++stream;
-        //
-        memcpy(stream, &odesc->opcode[1], odesc->opcode_len-1);
-        stream += odesc->opcode_len-1;
-    }
-    else if (_1st != 0x48 && opnds.need_rex()) {
-        prex = (Rex*)stream;
-        prex->dummy = 4;
-        prex->w = 0;
-        prex->b = 0;
-        prex->x = 0;
-        prex->r = 0;
-        ++stream;
-    }
-#endif  // ifndef EM64T
-
-    if (copy_opcode) {
-        if (odesc->opcode_len==1) {
-            unsigned char *dest = (unsigned char *) (stream);
-            unsigned char *src = (unsigned char *) (& (odesc->opcode));
-            *dest = *src;
-        }
-        else if (odesc->opcode_len==2) {
-            short *dest = (short *) (stream);
-            void *ptr = (void *) (& (odesc->opcode));
-            short *src = (short *) (ptr);
-            *dest = *src;
-        }
-        else if (odesc->opcode_len==3) {
-            unsigned short *dest = (unsigned short *) (stream);
-            void *ptr = (void *) (& (odesc->opcode));
-            unsigned short *src = (unsigned short *) (ptr);
-            *dest = *src;
-
-            //Now handle the last part
-            unsigned char *dest2 = (unsigned char *) (stream + 2);
-            *dest2 = odesc->opcode[2];
-        }
-        else if (odesc->opcode_len==4) {
-            unsigned int *dest = (unsigned int *) (stream);
-            void *ptr = (void *) (& (odesc->opcode));
-            unsigned int *src = (unsigned int *) (ptr);
-            *dest = *src;
-        }
-        stream += odesc->opcode_len;
-    }
-
-    unsigned argsCount = odesc->first_opnd;
-
-    if (odesc->aux0) {
-        stream = encode_aux(stream, odesc->aux0, opnds, odesc, &argsCount, prex);
-        if (odesc->aux1) {
-            stream = encode_aux(stream, odesc->aux1, opnds, odesc, &argsCount, prex);
-        }
-    }
-#ifdef JET_PROTO
-    //saveStream
-    Inst inst;
-    unsigned len = DecoderBase::decode(saveStream, &inst);
-    assert(inst.mn == mn);
-    assert(len == (unsigned)(stream-saveStream));
-    if (mn == Mnemonic_CALL || mn == Mnemonic_JMP ||
-        Mnemonic_RET == mn ||
-        (Mnemonic_JO<=mn && mn<=Mnemonic_JG)) {
-        assert(inst.argc == opnds.count());
-
-        InstructionDisassembler idi(saveStream);
-
-        for (unsigned i=0; i<inst.argc; i++) {
-            const EncoderBase::Operand& original = opnds[i];
-            const EncoderBase::Operand& decoded = inst.operands[i];
-            assert(original.kind() == decoded.kind());
-            assert(original.size() == decoded.size());
-            if (original.is_imm()) {
-                assert(original.imm() == decoded.imm());
-                assert(idi.get_opnd(0).kind == InstructionDisassembler::Kind_Imm);
-                if (mn == Mnemonic_CALL) {
-                    assert(idi.get_type() == InstructionDisassembler::RELATIVE_CALL);
-                }
-                else if (mn == Mnemonic_JMP) {
-                    assert(idi.get_type() == InstructionDisassembler::RELATIVE_JUMP);
-                }
-                else if (mn == Mnemonic_RET) {
-                    assert(idi.get_type() == InstructionDisassembler::RET);
-                }
-                else {
-                    assert(idi.get_type() == InstructionDisassembler::RELATIVE_COND_JUMP);
-                }
-            }
-            else if (original.is_mem()) {
-                assert(original.base() == decoded.base());
-                assert(original.index() == decoded.index());
-                assert(original.scale() == decoded.scale());
-                assert(original.disp() == decoded.disp());
-                assert(idi.get_opnd(0).kind == InstructionDisassembler::Kind_Mem);
-                if (mn == Mnemonic_CALL) {
-                    assert(idi.get_type() == InstructionDisassembler::INDIRECT_CALL);
-                }
-                else if (mn == Mnemonic_JMP) {
-                    assert(idi.get_type() == InstructionDisassembler::INDIRECT_JUMP);
-                }
-                else {
-                    assert(false);
-                }
-            }
-            else {
-                assert(original.is_reg());
-                assert(original.reg() == decoded.reg());
-                assert(idi.get_opnd(0).kind == InstructionDisassembler::Kind_Reg);
-                if (mn == Mnemonic_CALL) {
-                    assert(idi.get_type() == InstructionDisassembler::INDIRECT_CALL);
-                }
-                else if (mn == Mnemonic_JMP) {
-                    assert(idi.get_type() == InstructionDisassembler::INDIRECT_JUMP);
-                }
-                else {
-                    assert(false);
-                }
-            }
-        }
-
-        Inst inst2;
-        len = DecoderBase::decode(saveStream, &inst2);
-    }
-
- //   if(idi.get_length_with_prefix() != (int)len) {
-	//__asm { int 3 };
- //   }
-#endif
-
-    return stream;
-}
-
-char* EncoderBase::encodeModRM(char* stream, const Operands& opnds,
-                               unsigned idx, const OpcodeDesc * odesc,
-                               Rex * prex)
-{
-    const Operand& op = opnds[idx];
-    assert(op.is_mem());
-    assert(idx < COUNTOF(curRelOpnd));
-    ModRM& modrm = *(ModRM*)stream;
-    ++stream;
-    SIB& sib = *(SIB*)stream;
-
-    // we need SIB if
-    //      we have index & scale (nb: having index w/o base and w/o scale
-    //      treated as error)
-    //      the base is EBP w/o disp, BUT let's use a fake disp8
-    //      the base is ESP (nb: cant have ESP as index)
-
-    RegName base = op.base();
-    // only disp ?..
-    if (base == RegName_Null && op.index() == RegName_Null) {
-        assert(op.scale() == 0); // 'scale!=0' has no meaning without index
-        // ... yes - only have disp
-        // On EM64T, the simply [disp] addressing means 'RIP-based' one -
-        // must have to use SIB to encode 'DS: based'
-#ifdef _EM64T_
-        modrm.mod = 0;  // 00 - ..
-        modrm.rm = 4;   // 100 - have SIB
-
-        sib.base = 5;   // 101 - none
-        sib.index = 4;  // 100 - none
-        sib.scale = 0;  //
-        ++stream; // bypass SIB
-#else
-        // ignore disp_fits8, always use disp32.
-        modrm.mod = 0;
-        modrm.rm = 5;
-#endif
-        *(unsigned*)stream = (unsigned)op.disp();
-        curRelOpnd[idx]= stream;
-        stream += 4;
-        return stream;
-    }
-
-    //climits: error when targeting compal
-#define CHAR_MIN -127
-#define CHAR_MAX 127
-    const bool disp_fits8 = CHAR_MIN <= op.disp() && op.disp() <= CHAR_MAX;
-    /*&& op.base() != RegName_Null - just checked above*/
-    if (op.index() == RegName_Null && getHWRegIndex(op.base()) != getHWRegIndex(REG_STACK)) {
-        assert(op.scale() == 0); // 'scale!=0' has no meaning without index
-        // ... luckily no SIB, only base and may be a disp
-
-        // EBP base is a special case. Need to use [EBP] + disp8 form
-        if (op.disp() == 0  && getHWRegIndex(op.base()) != getHWRegIndex(RegName_EBP)) {
-            modrm.mod = 0; // mod=00, no disp et all
-        }
-        else if (disp_fits8) {
-            modrm.mod = 1; // mod=01, use disp8
-            *(unsigned char*)stream = (unsigned char)op.disp();
-            curRelOpnd[idx]= stream;
-            ++stream;
-        }
-        else {
-            modrm.mod = 2; // mod=10, use disp32
-            *(unsigned*)stream = (unsigned)op.disp();
-            curRelOpnd[idx]= stream;
-            stream += 4;
-        }
-        modrm.rm = getHWRegIndex(op.base());
-    if (is_em64t_extra_reg(op.base())) {
-        prex->b = 1;
-    }
-        return stream;
-    }
-
-    // cool, we do have SIB.
-    ++stream; // bypass SIB in stream
-
-    // {E|R}SP cannot be scaled index, however, R12 which has the same index in modrm - can
-    assert(op.index() == RegName_Null || !equals(op.index(), REG_STACK));
-
-    // Only GPRegs can be encoded in the SIB
-    assert(op.base() == RegName_Null ||
-            getRegKind(op.base()) == OpndKind_GPReg);
-    assert(op.index() == RegName_Null ||
-            getRegKind(op.index()) == OpndKind_GPReg);
-
-    modrm.rm = 4;   // r/m = 100, means 'we have SIB here'
-    if (op.base() == RegName_Null) {
-        // no base.
-        // already checked above if
-        // the first if() //assert(op.index() != RegName_Null);
-
-        modrm.mod = 0;  // mod=00 - here it means 'no base, but disp32'
-        sib.base = 5;   // 101 with mod=00  ^^^
-
-        // encode at least fake disp32 to avoid having [base=ebp]
-        *(unsigned*)stream = op.disp();
-        curRelOpnd[idx]= stream;
-        stream += 4;
-
-        unsigned sc = op.scale();
-        if (sc == 1 || sc==0)   { sib.scale = 0; }    // SS=00
-        else if (sc == 2)       { sib.scale = 1; }    // SS=01
-        else if (sc == 4)       { sib.scale = 2; }    // SS=10
-        else if (sc == 8)       { sib.scale = 3; }    // SS=11
-        sib.index = getHWRegIndex(op.index());
-    if (is_em64t_extra_reg(op.index())) {
-        prex->x = 1;
-    }
-
-        return stream;
-    }
-
-    if (op.disp() == 0 && getHWRegIndex(op.base()) != getHWRegIndex(RegName_EBP)) {
-        modrm.mod = 0;  // mod=00, no disp
-    }
-    else if (disp_fits8) {
-        modrm.mod = 1;  // mod=01, use disp8
-        *(unsigned char*)stream = (unsigned char)op.disp();
-        curRelOpnd[idx]= stream;
-        stream += 1;
-    }
-    else {
-        modrm.mod = 2;  // mod=10, use disp32
-        *(unsigned*)stream = (unsigned)op.disp();
-        curRelOpnd[idx]= stream;
-        stream += 4;
-    }
-
-    if (op.index() == RegName_Null) {
-        assert(op.scale() == 0); // 'scale!=0' has no meaning without index
-        // the only reason we're here without index, is that we have {E|R}SP
-        // or R12 as a base. Another possible reason - EBP without a disp -
-        // is handled above by adding a fake disp8
-#ifdef _EM64T_
-        assert(op.base() != RegName_Null && (equals(op.base(), REG_STACK) ||
-                                             equals(op.base(), RegName_R12)));
-#else  // _EM64T_
-        assert(op.base() != RegName_Null && equals(op.base(), REG_STACK));
-#endif //_EM64T_
-        sib.scale = 0;  // SS = 00
-        sib.index = 4;  // SS + index=100 means 'no index'
-    }
-    else {
-        unsigned sc = op.scale();
-        if (sc == 1 || sc==0)   { sib.scale = 0; }    // SS=00
-        else if (sc == 2)       { sib.scale = 1; }    // SS=01
-        else if (sc == 4)       { sib.scale = 2; }    // SS=10
-        else if (sc == 8)       { sib.scale = 3; }    // SS=11
-        sib.index = getHWRegIndex(op.index());
-    if (is_em64t_extra_reg(op.index())) {
-        prex->x = 1;
-    }
-        // not an error by itself, but the usage of [index*1] instead
-        // of [base] is discouraged
-        assert(op.base() != RegName_Null || op.scale() != 1);
-    }
-    sib.base = getHWRegIndex(op.base());
-    if (is_em64t_extra_reg(op.base())) {
-    prex->b = 1;
-    }
-    return stream;
-}
-
-char * EncoderBase::nops(char * stream, unsigned howMany)
-{
-    // Recommended multi-byte NOPs from the Intel architecture manual
-    static const unsigned char nops[10][9] = {
-        { 0, },                                                     // 0, this line is dummy and not used in the loop below
-        { 0x90, },                                                  // 1-byte NOP
-        { 0x66, 0x90, },                                            // 2
-        { 0x0F, 0x1F, 0x00, },                                      // 3
-        { 0x0F, 0x1F, 0x40, 0x00, },                                // 4
-        { 0x0F, 0x1F, 0x44, 0x00, 0x00, },                          // 5
-        { 0x66, 0x0F, 0x1F, 0x44, 0x00, 0x00, },                    // 6
-        { 0x0F, 0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, },              // 7
-        { 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, },        // 8
-        { 0x66, 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00 },   // 9-byte NOP
-    };
-
-    // Start from delivering the longest possible NOPs, then proceed with shorter ones
-    for (unsigned nopSize=9; nopSize!=0; nopSize--) {
-        while(howMany>=nopSize) {
-            const unsigned char* nopBytes = nops[nopSize];
-            for (unsigned i=0; i<nopSize; i++) {
-                stream[i] = nopBytes[i];
-            }
-            stream += nopSize;
-            howMany -= nopSize;
-        }
-    }
-    char* end = stream + howMany;
-    return end;
-}
-
-char * EncoderBase::prefix(char* stream, InstPrefix pref)
-{
-    if (pref== InstPrefix_Null) {
-        // nothing to do
-        return stream;
-    }
-    *stream = (char)pref;
-    return stream + 1;
-}
-
-
-/**
- *
- */
-bool EncoderBase::extAllowed(OpndExt opndExt, OpndExt instExt) {
-    if (instExt == opndExt || instExt == OpndExt_Any || opndExt == OpndExt_Any) {
-            return true;
-    }
-//asm("int3");
-assert(0);
-    return false;
-}
-
-static bool try_match(const EncoderBase::OpcodeDesc& odesc,
-                      const EncoderBase::Operands& opnds, bool strict) {
-
-    assert(odesc.roles.count == opnds.count());
-
-    for(unsigned j=0; j<odesc.roles.count; j++) {
-        // - the location must match exactly
-        if ((odesc.opnds[j].kind & opnds[j].kind()) != opnds[j].kind()) {
-            return false;
-        }
-        if (strict) {
-            // the size must match exactly
-            if (odesc.opnds[j].size != opnds[j].size()) {
-                return false;
-            }
-        }
-        else {
-            // must match only for def operands, and dont care about use ones
-            // situations like 'mov r8, imm32/mov r32, imm8' so the
-            // destination operand defines the overall size
-            if (EncoderBase::getOpndRoles(odesc.roles, j) & OpndRole_Def) {
-                if (odesc.opnds[j].size != opnds[j].size()) {
-                    return false;
-                }
-            }
-        }
-    }
-    return true;
-}
-
-//
-//Subhash implementaion - may be useful in case of many misses during fast
-//opcode lookup.
-//
-
-#ifdef ENCODER_USE_SUBHASH
-static unsigned subHash[32];
-
-static unsigned find(Mnemonic mn, unsigned hash)
-{
-    unsigned key = hash % COUNTOF(subHash);
-    unsigned pack = subHash[key];
-    unsigned _hash = pack & 0xFFFF;
-    if (_hash != hash) {
-        stat.miss(mn);
-        return EncoderBase::NOHASH;
-    }
-    unsigned _mn = (pack >> 24)&0xFF;
-    if (_mn != _mn) {
-        stat.miss(mn);
-        return EncoderBase::NOHASH;
-    }
-    unsigned idx = (pack >> 16) & 0xFF;
-    stat.hit(mn);
-    return idx;
-}
-
-static void put(Mnemonic mn, unsigned hash, unsigned idx)
-{
-    unsigned pack = hash | (idx<<16) | (mn << 24);
-    unsigned key = hash % COUNTOF(subHash);
-    subHash[key] = pack;
-}
-#endif
-
-const EncoderBase::OpcodeDesc *
-EncoderBase::lookup(Mnemonic mn, const Operands& opnds)
-{
-    const unsigned hash = opnds.hash();
-    unsigned opcodeIndex = opcodesHashMap[mn][hash];
-#ifdef ENCODER_USE_SUBHASH
-    if (opcodeIndex == NOHASH) {
-        opcodeIndex = find(mn, hash);
-    }
-#endif
-
-    if (opcodeIndex == NOHASH) {
-        // fast-path did no work. try to lookup sequentially
-        const OpcodeDesc * odesc = opcodes[mn];
-        int idx = -1;
-        bool found = false;
-        for (idx=0; !odesc[idx].last; idx++) {
-            const OpcodeDesc& opcode = odesc[idx];
-            if (opcode.platf == OpcodeInfo::decoder) {
-                continue;
-            }
-            if (opcode.roles.count != opnds.count()) {
-                continue;
-            }
-            if (try_match(opcode, opnds, true)) {
-                found = true;
-                break;
-            }
-        }
-        if (!found) {
-            for (idx=0; !odesc[idx].last; idx++) {
-                const OpcodeDesc& opcode = odesc[idx];
-                if (opcode.platf == OpcodeInfo::decoder) {
-                    continue;
-                }
-                if (opcode.roles.count != opnds.count()) {
-                    continue;
-                }
-                if (try_match(opcode, opnds, false)) {
-                    found = true;
-                    break;
-                }
-            }
-        }
-        assert(found);
-        opcodeIndex = idx;
-#ifdef ENCODER_USE_SUBHASH
-        put(mn, hash, opcodeIndex);
-#endif
-    }
-    assert(opcodeIndex != NOHASH);
-    const OpcodeDesc * odesc = &opcodes[mn][opcodeIndex];
-    assert(!odesc->last);
-    assert(odesc->roles.count == opnds.count());
-    assert(odesc->platf != OpcodeInfo::decoder);
-#if !defined(_EM64T_)
-    // tuning was done for IA32 only, so no size restriction on EM64T
-    //assert(sizeof(OpcodeDesc)==128);
-#endif
-    return odesc;
-}
-
-char* EncoderBase::getOpndLocation(int index) {
-     assert(index < 3);
-     return curRelOpnd[index];
-}
-
-
-Mnemonic EncoderBase::str2mnemonic(const char * mn_name)
-{
-    for (unsigned m = 1; m<Mnemonic_Count; m++) {
-        if (!strcmpi(mnemonics[m].name, mn_name)) {
-            return (Mnemonic)m;
-        }
-    }
-    return Mnemonic_Null;
-}
-
-static const char * conditionStrings[ConditionMnemonic_Count] = {
-    "O",
-    "NO",
-    "B",
-    "AE",
-    "Z",
-    "NZ",
-    "BE",
-    "A",
-
-    "S",
-    "NS",
-    "P",
-    "NP",
-    "L",
-    "GE",
-    "LE",
-    "G",
-};
-
-const char * getConditionString(ConditionMnemonic cm) {
-    return conditionStrings[cm];
-}
-
-static const struct {
-        char            sizeString[12];
-        OpndSize        size;
-}
-sizes[] = {
-    { "Sz8", OpndSize_8 },
-    { "Sz16", OpndSize_16 },
-    { "Sz32", OpndSize_32 },
-    { "Sz64", OpndSize_64 },
-#if !defined(TESTING_ENCODER)
-    { "Sz80", OpndSize_80 },
-    { "Sz128", OpndSize_128 },
-#endif
-    { "SzAny", OpndSize_Any },
-};
-
-
-OpndSize getOpndSize(const char * sizeString)
-{
-    assert(sizeString);
-    for (unsigned i = 0; i<COUNTOF(sizes); i++) {
-        if (!strcmpi(sizeString, sizes[i].sizeString)) {
-            return sizes[i].size;
-        }
-    }
-    return OpndSize_Null;
-}
-
-const char * getOpndSizeString(OpndSize size) {
-    for( unsigned i = 0; i<COUNTOF(sizes); i++ ) {
-        if( sizes[i].size==size ) {
-            return sizes[i].sizeString;
-        }
-    }
-    return NULL;
-}
-
-static const struct {
-    char            kindString[16];
-    OpndKind        kind;
-}
-kinds[] = {
-    { "Null", OpndKind_Null },
-    { "GPReg", OpndKind_GPReg },
-    { "SReg", OpndKind_SReg },
-    { "FPReg", OpndKind_FPReg },
-    { "XMMReg", OpndKind_XMMReg },
-#ifdef _HAVE_MMX_
-    { "MMXReg", OpndKind_MMXReg },
-#endif
-    { "StatusReg", OpndKind_StatusReg },
-    { "Reg", OpndKind_Reg },
-    { "Imm", OpndKind_Imm },
-    { "Mem", OpndKind_Mem },
-    { "Any", OpndKind_Any },
-};
-
-const char * getOpndKindString(OpndKind kind)
-{
-    for (unsigned i = 0; i<COUNTOF(kinds); i++) {
-        if (kinds[i].kind==kind) {
-            return kinds[i].kindString;
-        }
-    }
-    return NULL;
-}
-
-OpndKind getOpndKind(const char * kindString)
-{
-    assert(kindString);
-    for (unsigned i = 0; i<COUNTOF(kinds); i++) {
-        if (!strcmpi(kindString, kinds[i].kindString)) {
-            return kinds[i].kind;
-        }
-    }
-    return OpndKind_Null;
-}
-
-/**
- * A mapping between register string representation and its RegName constant.
- */
-static const struct {
-        char    regstring[7];
-        RegName regname;
-}
-
-registers[] = {
-#ifdef _EM64T_
-    {"RAX",         RegName_RAX},
-    {"RBX",         RegName_RBX},
-    {"RCX",         RegName_RCX},
-    {"RDX",         RegName_RDX},
-    {"RBP",         RegName_RBP},
-    {"RSI",         RegName_RSI},
-    {"RDI",         RegName_RDI},
-    {"RSP",         RegName_RSP},
-    {"R8",          RegName_R8},
-    {"R9",          RegName_R9},
-    {"R10",         RegName_R10},
-    {"R11",         RegName_R11},
-    {"R12",         RegName_R12},
-    {"R13",         RegName_R13},
-    {"R14",         RegName_R14},
-    {"R15",         RegName_R15},
-#endif
-
-    {"EAX",         RegName_EAX},
-    {"ECX",         RegName_ECX},
-    {"EDX",         RegName_EDX},
-    {"EBX",         RegName_EBX},
-    {"ESP",         RegName_ESP},
-    {"EBP",         RegName_EBP},
-    {"ESI",         RegName_ESI},
-    {"EDI",         RegName_EDI},
-#ifdef _EM64T_
-    {"R8D",         RegName_R8D},
-    {"R9D",         RegName_R9D},
-    {"R10D",        RegName_R10D},
-    {"R11D",        RegName_R11D},
-    {"R12D",        RegName_R12D},
-    {"R13D",        RegName_R13D},
-    {"R14D",        RegName_R14D},
-    {"R15D",        RegName_R15D},
-#endif
-
-    {"AX",          RegName_AX},
-    {"CX",          RegName_CX},
-    {"DX",          RegName_DX},
-    {"BX",          RegName_BX},
-    {"SP",          RegName_SP},
-    {"BP",          RegName_BP},
-    {"SI",          RegName_SI},
-    {"DI",          RegName_DI},
-
-    {"AL",          RegName_AL},
-    {"CL",          RegName_CL},
-    {"DL",          RegName_DL},
-    {"BL",          RegName_BL},
-#if !defined(_EM64T_)
-    {"AH",          RegName_AH},
-    {"CH",          RegName_CH},
-    {"DH",          RegName_DH},
-    {"BH",          RegName_BH},
-#else
-    {"SPL",         RegName_SPL},
-    {"BPL",         RegName_BPL},
-    {"SIL",         RegName_SIL},
-    {"DIL",         RegName_DIL},
-    {"R8L",         RegName_R8L},
-    {"R9L",         RegName_R9L},
-    {"R10L",        RegName_R10L},
-    {"R11L",        RegName_R11L},
-    {"R12L",        RegName_R12L},
-    {"R13L",        RegName_R13L},
-    {"R14L",        RegName_R14L},
-    {"R15L",        RegName_R15L},
-#endif
-    {"ES",          RegName_ES},
-    {"CS",          RegName_CS},
-    {"SS",          RegName_SS},
-    {"DS",          RegName_DS},
-    {"FS",          RegName_FS},
-    {"GS",          RegName_GS},
-
-    {"FP0",         RegName_FP0},
-/*
-    {"FP1",         RegName_FP1},
-    {"FP2",         RegName_FP2},
-    {"FP3",         RegName_FP3},
-    {"FP4",         RegName_FP4},
-    {"FP5",         RegName_FP5},
-    {"FP6",         RegName_FP6},
-    {"FP7",         RegName_FP7},
-*/
-    {"FP0S",        RegName_FP0S},
-    {"FP1S",        RegName_FP1S},
-    {"FP2S",        RegName_FP2S},
-    {"FP3S",        RegName_FP3S},
-    {"FP4S",        RegName_FP4S},
-    {"FP5S",        RegName_FP5S},
-    {"FP6S",        RegName_FP6S},
-    {"FP7S",        RegName_FP7S},
-
-    {"FP0D",        RegName_FP0D},
-    {"FP1D",        RegName_FP1D},
-    {"FP2D",        RegName_FP2D},
-    {"FP3D",        RegName_FP3D},
-    {"FP4D",        RegName_FP4D},
-    {"FP5D",        RegName_FP5D},
-    {"FP6D",        RegName_FP6D},
-    {"FP7D",        RegName_FP7D},
-
-    {"XMM0",        RegName_XMM0},
-    {"XMM1",        RegName_XMM1},
-    {"XMM2",        RegName_XMM2},
-    {"XMM3",        RegName_XMM3},
-    {"XMM4",        RegName_XMM4},
-    {"XMM5",        RegName_XMM5},
-    {"XMM6",        RegName_XMM6},
-    {"XMM7",        RegName_XMM7},
-#ifdef _EM64T_
-    {"XMM8",       RegName_XMM8},
-    {"XMM9",       RegName_XMM9},
-    {"XMM10",      RegName_XMM10},
-    {"XMM11",      RegName_XMM11},
-    {"XMM12",      RegName_XMM12},
-    {"XMM13",      RegName_XMM13},
-    {"XMM14",      RegName_XMM14},
-    {"XMM15",      RegName_XMM15},
-#endif
-
-
-    {"XMM0S",       RegName_XMM0S},
-    {"XMM1S",       RegName_XMM1S},
-    {"XMM2S",       RegName_XMM2S},
-    {"XMM3S",       RegName_XMM3S},
-    {"XMM4S",       RegName_XMM4S},
-    {"XMM5S",       RegName_XMM5S},
-    {"XMM6S",       RegName_XMM6S},
-    {"XMM7S",       RegName_XMM7S},
-#ifdef _EM64T_
-    {"XMM8S",       RegName_XMM8S},
-    {"XMM9S",       RegName_XMM9S},
-    {"XMM10S",      RegName_XMM10S},
-    {"XMM11S",      RegName_XMM11S},
-    {"XMM12S",      RegName_XMM12S},
-    {"XMM13S",      RegName_XMM13S},
-    {"XMM14S",      RegName_XMM14S},
-    {"XMM15S",      RegName_XMM15S},
-#endif
-
-    {"XMM0D",       RegName_XMM0D},
-    {"XMM1D",       RegName_XMM1D},
-    {"XMM2D",       RegName_XMM2D},
-    {"XMM3D",       RegName_XMM3D},
-    {"XMM4D",       RegName_XMM4D},
-    {"XMM5D",       RegName_XMM5D},
-    {"XMM6D",       RegName_XMM6D},
-    {"XMM7D",       RegName_XMM7D},
-#ifdef _EM64T_
-    {"XMM8D",       RegName_XMM8D},
-    {"XMM9D",       RegName_XMM9D},
-    {"XMM10D",      RegName_XMM10D},
-    {"XMM11D",      RegName_XMM11D},
-    {"XMM12D",      RegName_XMM12D},
-    {"XMM13D",      RegName_XMM13D},
-    {"XMM14D",      RegName_XMM14D},
-    {"XMM15D",      RegName_XMM15D},
-#endif
-
-    {"EFLGS",       RegName_EFLAGS},
-};
-
-
-const char * getRegNameString(RegName reg)
-{
-    for (unsigned i = 0; i<COUNTOF(registers); i++) {
-        if (registers[i].regname == reg) {
-            return registers[i].regstring;
-        }
-    }
-    return "(null)";
-}
-
-RegName getRegName(const char * regname)
-{
-    if (NULL == regname) {
-        return RegName_Null;
-    }
-
-    for (unsigned i = 0; i<COUNTOF(registers); i++) {
-        if (!strcmpi(regname,registers[i].regstring)) {
-            return registers[i].regname;
-        }
-    }
-    return RegName_Null;
-}
-
-ENCODER_NAMESPACE_END
diff --git a/vm/compiler/codegen/x86/libenc/enc_base.h b/vm/compiler/codegen/x86/libenc/enc_base.h
deleted file mode 100644
index e88443f..0000000
--- a/vm/compiler/codegen/x86/libenc/enc_base.h
+++ /dev/null
@@ -1,745 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-
-/**
- * @file
- * @brief Main encoding routines and structures.
- */
-
-#ifndef __ENC_BASE_H_INCLUDED__
-#define __ENC_BASE_H_INCLUDED__
-
-#include "enc_defs.h"
-
-
-#include <stdlib.h>
-#include <assert.h>
-#include <memory.h>
-
-ENCODER_NAMESPACE_START
-struct MnemonicInfo;
-struct OpcodeInfo;
-struct Rex;
-
-/**
- * @brief Basic facilities for generation of processor's instructions.
- *
- * The class EncoderBase represents the basic facilities for the encoding of
- * processor's instructions on IA32 and EM64T platforms.
- *
- * The class provides general interface to generate the instructions as well
- * as to retrieve some static data about instructions (number of arguments,
- * their roles, etc).
- *
- * Currently, the EncoderBase class is used for both LIL and Jitrino code
- * generators. Each of these code generators has its own wrapper to adapt
- * this general interface for specific needs - see encoder.h for LIL wrappers
- * and Ia32Encoder.h for Jitrino's adapter.
- *
- * Interface is provided through static methods, no instances of EncoderBase
- * to be created.
- *
- * @todo RIP-based addressing on EM64T - it's not yet supported currently.
- */
-class EncoderBase {
-public:
-    class Operands;
-    struct MnemonicDesc;
-    /**
-     * @brief Generates processor's instruction.
-     *
-     * @param stream - a buffer to generate into
-     * @param mn - \link Mnemonic mnemonic \endlink of the instruction
-     * @param opnds - operands for the instruction
-     * @returns (stream + length of the just generated instruction)
-     */
-    static char * encode(char * stream, Mnemonic mn, const Operands& opnds);
-    static char * getOpndLocation(int index);
-
-    /**
-     * @brief Generates the smallest possible number of NOP-s.
-     *
-     * Effectively generates the smallest possible number of instructions,
-     * which are NOP-s for CPU. Normally used to make a code alignment.
-     *
-     * The method inserts exactly number of bytes specified. It's a caller's
-     * responsibility to make sure the buffer is big enough.
-     *
-     * @param stream - buffer where to generate code into, can not be NULL
-     * @param howMany - how many bytes to fill with NOP-s
-     * @return \c (stream+howMany)
-     */
-    static char * nops(char * stream, unsigned howMany);
-
-    /**
-     * @brief Inserts a prefix into the code buffer.
-     *
-     * The method writes no more than one byte into the buffer. This is a
-     * caller's responsibility to make sure the buffer is big enough.
-     *
-     * @param stream - buffer where to insert the prefix
-     * @param pref - prefix to be inserted. If it's InstPrefix_Null, then
-     *        no action performed and return value is \c stream.
-     * @return \c (stream+1) if pref is not InstPrefix_Null, or \c stream
-     *         otherwise
-     */
-     static char * prefix(char* stream, InstPrefix pref);
-
-    /**
-     * @brief Determines if operand with opndExt suites the position with instExt.
-     */
-    static bool extAllowed(OpndExt opndExt, OpndExt instExt);
-
-    /**
-     * @brief Returns MnemonicDesc by the given Mnemonic.
-     */
-    static const MnemonicDesc * getMnemonicDesc(Mnemonic mn)
-    {
-        assert(mn < Mnemonic_Count);
-        return mnemonics + mn;
-    }
-
-    /**
-     * @brief Returns a Mnemonic for the given name.
-     *
-     * The lookup is case insensitive, if no mnemonic found for the given
-     * string, then Mnemonic_Null returned.
-     */
-    static Mnemonic str2mnemonic(const char * mn_name);
-
-    /**
-     * @brief Returns a string representation of the given Mnemonic.
-     *
-     * If invalid mnemonic passed, then the behavior is unpredictable.
-     */
-    static const char * getMnemonicString(Mnemonic mn)
-    {
-        return getMnemonicDesc(mn)->name;
-    }
-
-    static const char * toStr(Mnemonic mn)
-    {
-        return getMnemonicDesc(mn)->name;
-    }
-
-
-    /**
-     * @brief Description of operand.
-     *
-     * Description of an operand in opcode - its kind, size or RegName if
-     * operand must be a particular register.
-     */
-    struct OpndDesc {
-        /**
-         * @brief Location of the operand.
-         *
-         * May be a mask, i.e. OpndKind_Imm|OpndKind_Mem.
-         */
-        OpndKind        kind;
-        /**
-         * @brief Size of the operand.
-         */
-        OpndSize        size;
-        /**
-         * @brief Extention of the operand.
-         */
-        OpndExt         ext;
-        /**
-         * @brief Appropriate RegName if operand must reside on a particular
-         *        register (i.e. CWD/CDQ instructions), RegName_Null
-         *        otherwise.
-         */
-        RegName         reg;
-    };
-
-    /**
-     * @brief Description of operands' roles in instruction.
-     */
-    struct OpndRolesDesc {
-        /**
-         * @brief Total number of operands in the operation.
-         */
-        unsigned                count;
-        /**
-         * @brief Number of defs in the operation.
-         */
-        unsigned                defCount;
-        /**
-         * @brief Number of uses in the operation.
-         */
-        unsigned                useCount;
-        /**
-         * @brief Operand roles, bit-packed.
-         *
-         * A bit-packed info about operands' roles. Each operand's role is
-         * described by two bits, counted from right-to-left - the less
-         * significant bits (0,1) represent operand#0.
-         *
-         * The mask is build by ORing #OpndRole_Def and #OpndRole_Use
-         * appropriately and shifting left, i.e. operand#0's role would be
-         * - '(OpndRole_Def|OpndRole_Use)'
-         * - opnd#1's role would be 'OpndRole_Use<<2'
-         * - and operand#2's role would be, say, 'OpndRole_Def<<4'.
-         */
-        unsigned                roles;
-    };
-
-    /**
-     * @brief Extracts appropriate OpndRole for a given operand.
-     *
-     * The order of operands is left-to-right, i.e. for MOV, it
-     * would be 'MOV op0, op1'
-     */
-    static OpndRole getOpndRoles(OpndRolesDesc ord, unsigned idx)
-    {
-        assert(idx < ord.count);
-        return (OpndRole)(ord.roles>>((ord.count-1-idx)*2) & 0x3);
-    }
-
-    /**
-     * @brief Defines the maximum number of operands for an opcode.
-     *
-     * The 3 mostly comes from IDIV/IMUL which both may have up to
-     * 3 operands.
-     */
-    static const unsigned int MAX_NUM_OPCODE_OPERANDS = 3;
-
-    /**
-     * @brief Info about single opcode - its opcode bytes, operands,
-     *        operands' roles.
-     */
-   union OpcodeDesc {
-       char dummy[128]; // To make total size a power of 2
-
-       struct {
-           /**
-           * @brief Raw opcode bytes.
-           *
-           * 'Raw' opcode bytes which do not require any analysis and are
-           * independent from arguments/sizes/etc (may include opcode size
-           * prefix).
-           */
-           char        opcode[5];
-           unsigned    opcode_len;
-           unsigned    aux0;
-           unsigned    aux1;
-           /**
-           * @brief Info about opcode's operands.
-           */
-           OpndDesc        opnds[MAX_NUM_OPCODE_OPERANDS];
-           unsigned        first_opnd;
-           /**
-           * @brief Info about operands - total number, number of uses/defs,
-           *        operands' roles.
-           */
-           OpndRolesDesc   roles;
-           /**
-           * @brief If not zero, then this is final OpcodeDesc structure in
-           *        the list of opcodes for a given mnemonic.
-           */
-           char            last;
-           char            platf;
-       };
-   };
-public:
-    /**
-     * @brief General info about mnemonic.
-     */
-    struct MnemonicDesc {
-        /**
-        * @brief The mnemonic itself.
-        */
-        Mnemonic        mn;
-        /**
-        * Various characteristics of mnemonic.
-        * @see MF_
-         */
-        unsigned    flags;
-        /**
-         * @brief Operation's operand's count and roles.
-         *
-         * For the operations whose opcodes may use different number of
-         * operands (i.e. IMUL/SHL) either most common value used, or empty
-         * value left.
-         */
-        OpndRolesDesc   roles;
-        /**
-         * @brief Print name of the mnemonic.
-         */
-        const char *    name;
-    };
-
-
-    /**
-     * @brief Magic number, shows a maximum value a hash code can take.
-     *
-     * For meaning and arithmetics see enc_tabl.cpp.
-     *
-     * The value was increased from '5155' to '8192' to make it aligned
-     * for faster access in EncoderBase::lookup().
-     */
-    static const unsigned int               HASH_MAX = 8192; //5155;
-    /**
-     * @brief Empty value, used in hash-to-opcode map to show an empty slot.
-     */
-    static const unsigned char              NOHASH = 0xFF;
-    /**
-     * @brief The name says it all.
-     */
-    static const unsigned char              HASH_BITS_PER_OPERAND = 5;
-
-    /**
-     * @brief Contains info about a single instructions's operand - its
-     *        location, size and a value for immediate or RegName for
-     *        register operands.
-     */
-    class Operand {
-    public:
-        /**
-         * @brief Initializes the instance with empty size and kind.
-         */
-        Operand() : m_kind(OpndKind_Null), m_size(OpndSize_Null), m_ext(OpndExt_None), m_need_rex(false) {}
-        /**
-         * @brief Creates register operand from given RegName.
-         */
-        Operand(RegName reg, OpndExt ext = OpndExt_None) : m_kind(getRegKind(reg)),
-                               m_size(getRegSize(reg)),
-                               m_ext(ext), m_reg(reg)
-        {
-            hash_it();
-        }
-        /**
-         * @brief Creates register operand from given RegName and with the
-         *        specified size and kind.
-         *
-         * Used to speedup Operand creation as there is no need to extract
-         * size and kind from the RegName.
-         * The provided size and kind must match the RegName's ones though.
-         */
-        Operand(OpndSize sz, OpndKind kind, RegName reg, OpndExt ext = OpndExt_None) :
-            m_kind(kind), m_size(sz), m_ext(ext), m_reg(reg)
-        {
-            assert(m_size == getRegSize(reg));
-            assert(m_kind == getRegKind(reg));
-            hash_it();
-        }
-        /**
-         * @brief Creates immediate operand with the given size and value.
-         */
-        Operand(OpndSize size, long long ival, OpndExt ext = OpndExt_None) :
-            m_kind(OpndKind_Imm), m_size(size), m_ext(ext), m_imm64(ival)
-        {
-            hash_it();
-        }
-        /**
-         * @brief Creates immediate operand of OpndSize_32.
-         */
-        Operand(int ival, OpndExt ext = OpndExt_None) :
-            m_kind(OpndKind_Imm), m_size(OpndSize_32), m_ext(ext), m_imm64(ival)
-        {
-            hash_it();
-        }
-        /**
-         * @brief Creates immediate operand of OpndSize_16.
-         */
-        Operand(short ival, OpndExt ext = OpndExt_None) :
-            m_kind(OpndKind_Imm), m_size(OpndSize_16), m_ext(ext), m_imm64(ival)
-        {
-            hash_it();
-        }
-
-        /**
-         * @brief Creates immediate operand of OpndSize_8.
-         */
-        Operand(char ival, OpndExt ext = OpndExt_None) :
-            m_kind(OpndKind_Imm), m_size(OpndSize_8), m_ext(ext), m_imm64(ival)
-        {
-            hash_it();
-        }
-
-        /**
-         * @brief Creates memory operand.
-         */
-        Operand(OpndSize size, RegName base, RegName index, unsigned scale,
-                int disp, OpndExt ext = OpndExt_None) : m_kind(OpndKind_Mem), m_size(size), m_ext(ext)
-        {
-            m_base = base;
-            m_index = index;
-            m_scale = scale;
-            m_disp = disp;
-            hash_it();
-        }
-
-        /**
-         * @brief Creates memory operand with only base and displacement.
-         */
-        Operand(OpndSize size, RegName base, int disp, OpndExt ext = OpndExt_None) :
-            m_kind(OpndKind_Mem), m_size(size), m_ext(ext)
-        {
-            m_base = base;
-            m_index = RegName_Null;
-            m_scale = 0;
-            m_disp = disp;
-            hash_it();
-        }
-        //
-        // general info
-        //
-        /**
-         * @brief Returns kind of the operand.
-         */
-        OpndKind kind(void) const { return m_kind; }
-        /**
-         * @brief Returns size of the operand.
-         */
-        OpndSize size(void) const { return m_size; }
-        /**
-         * @brief Returns extention of the operand.
-         */
-        OpndExt ext(void) const { return m_ext; }
-        /**
-         * @brief Returns hash of the operand.
-         */
-        unsigned hash(void) const { return m_hash; }
-        //
-#ifdef _EM64T_
-        bool need_rex(void) const { return m_need_rex; }
-#else
-        bool need_rex(void) const { return false; }
-#endif
-        /**
-         * @brief Tests whether operand is memory operand.
-         */
-        bool is_mem(void) const { return is_placed_in(OpndKind_Mem); }
-        /**
-         * @brief Tests whether operand is immediate operand.
-         */
-        bool is_imm(void) const { return is_placed_in(OpndKind_Imm); }
-        /**
-         * @brief Tests whether operand is register operand.
-         */
-        bool is_reg(void) const { return is_placed_in(OpndKind_Reg); }
-        /**
-         * @brief Tests whether operand is general-purpose register operand.
-         */
-        bool is_gpreg(void) const { return is_placed_in(OpndKind_GPReg); }
-        /**
-         * @brief Tests whether operand is float-point pseudo-register operand.
-         */
-        bool is_fpreg(void) const { return is_placed_in(OpndKind_FPReg); }
-        /**
-         * @brief Tests whether operand is XMM register operand.
-         */
-        bool is_xmmreg(void) const { return is_placed_in(OpndKind_XMMReg); }
-#ifdef _HAVE_MMX_
-        /**
-         * @brief Tests whether operand is MMX register operand.
-         */
-        bool is_mmxreg(void) const { return is_placed_in(OpndKind_MMXReg); }
-#endif
-        /**
-         * @brief Tests whether operand is signed immediate operand.
-         */
-        //bool is_signed(void) const { assert(is_imm()); return m_is_signed; }
-
-        /**
-         * @brief Returns base of memory operand (RegName_Null if not memory).
-         */
-        RegName base(void) const { return is_mem() ? m_base : RegName_Null; }
-        /**
-         * @brief Returns index of memory operand (RegName_Null if not memory).
-         */
-        RegName index(void) const { return is_mem() ? m_index : RegName_Null; }
-        /**
-         * @brief Returns scale of memory operand (0 if not memory).
-         */
-        unsigned scale(void) const { return is_mem() ? m_scale : 0; }
-        /**
-         * @brief Returns displacement of memory operand (0 if not memory).
-         */
-        int disp(void) const { return is_mem() ? m_disp : 0; }
-        /**
-         * @brief Returns RegName of register operand (RegName_Null if not
-         *        register).
-         */
-        RegName reg(void) const { return is_reg() ? m_reg : RegName_Null; }
-        /**
-         * @brief Returns value of immediate operand (0 if not immediate).
-         */
-        long long imm(void) const { return is_imm() ? m_imm64 : 0; }
-    private:
-        bool is_placed_in(OpndKind kd) const
-        {
-                return kd == OpndKind_Reg ?
-                        m_kind == OpndKind_GPReg ||
-#ifdef _HAVE_MMX_
-                        m_kind == OpndKind_MMXReg ||
-#endif
-                        m_kind == OpndKind_FPReg ||
-                        m_kind == OpndKind_XMMReg
-                        : kd == m_kind;
-        }
-        void hash_it(void)
-        {
-            m_hash = get_size_hash(m_size) | get_kind_hash(m_kind);
-#ifdef _EM64T_
-            m_need_rex = false;
-            if (is_reg() && is_em64t_extra_reg(m_reg)) {
-                m_need_rex = true;
-            }
-            else if (is_mem() && (is_em64t_extra_reg(m_base) ||
-                                  is_em64t_extra_reg(m_index))) {
-                m_need_rex = true;
-            }
-#endif
-        }
-        // general info
-        OpndKind    m_kind;
-        OpndSize    m_size;
-        OpndExt     m_ext;
-        // complex address form support
-        RegName     m_base;
-        RegName     m_index;
-        unsigned    m_scale;
-        union {
-            int         m_disp;
-            RegName     m_reg;
-            long long   m_imm64;
-        };
-        unsigned    m_hash;
-        bool        m_need_rex;
-        friend class EncoderBase::Operands;
-    };
-    /**
-     * @brief Simple container for up to 3 Operand-s.
-     */
-    class Operands {
-    public:
-        Operands(void)
-        {
-            clear();
-        }
-        Operands(const Operand& op0)
-        {
-            clear();
-            add(op0);
-        }
-
-        Operands(const Operand& op0, const Operand& op1)
-        {
-            clear();
-            add(op0); add(op1);
-        }
-
-        Operands(const Operand& op0, const Operand& op1, const Operand& op2)
-        {
-            clear();
-            add(op0); add(op1); add(op2);
-        }
-
-        unsigned count(void) const { return m_count; }
-        unsigned hash(void) const { return m_hash; }
-        const Operand& operator[](unsigned idx) const
-        {
-            assert(idx<m_count);
-            return m_operands[idx];
-        }
-
-        void add(const Operand& op)
-        {
-            assert(m_count < COUNTOF(m_operands));
-            m_hash = (m_hash<<HASH_BITS_PER_OPERAND) | op.hash();
-            m_operands[m_count++] = op;
-            m_need_rex = m_need_rex || op.m_need_rex;
-        }
-#ifdef _EM64T_
-        bool need_rex(void) const { return m_need_rex; }
-#else
-        bool need_rex(void) const { return false; }
-#endif
-        void clear(void)
-        {
-            m_count = 0; m_hash = 0; m_need_rex = false;
-        }
-    private:
-        unsigned    m_count;
-        Operand     m_operands[COUNTOF( ((OpcodeDesc*)NULL)->opnds )];
-        unsigned    m_hash;
-        bool        m_need_rex;
-    };
-public:
-#ifdef _DEBUG
-    /**
-     * Verifies some presumptions about encoding data table.
-     * Called automaticaly during statics initialization.
-     */
-    static int verify(void);
-#endif
-
-private:
-    /**
-     * @brief Returns found OpcodeDesc by the given Mnemonic and operands.
-     */
-    static const OpcodeDesc * lookup(Mnemonic mn, const Operands& opnds);
-    /**
-     * @brief Encodes mod/rm byte.
-     */
-    static char* encodeModRM(char* stream, const Operands& opnds,
-                             unsigned idx, const OpcodeDesc * odesc, Rex * prex);
-    /**
-     * @brief Encodes special things of opcode description - '/r', 'ib', etc.
-     */
-    static char* encode_aux(char* stream, unsigned aux,
-                            const Operands& opnds, const OpcodeDesc * odesc,
-                            unsigned * pargsCount, Rex* prex);
-#ifdef _EM64T_
-    /**
-     * @brief Returns true if the 'reg' argument represents one of the new
-     *        EM64T registers - R8(D)-R15(D).
-     *
-     * The 64 bits versions of 'old-fashion' registers, i.e. RAX are not
-     * considered as 'extra'.
-     */
-    static bool is_em64t_extra_reg(const RegName reg)
-    {
-        if (needs_rex_r(reg)) {
-            return true;
-        }
-        if (RegName_SPL <= reg && reg <= RegName_R15L) {
-            return true;
-        }
-        return false;
-    }
-    static bool needs_rex_r(const RegName reg)
-    {
-        if (RegName_R8 <= reg && reg <= RegName_R15) {
-            return true;
-        }
-        if (RegName_R8D <= reg && reg <= RegName_R15D) {
-            return true;
-        }
-        if (RegName_R8S <= reg && reg <= RegName_R15S) {
-            return true;
-        }
-        if (RegName_R8L <= reg && reg <= RegName_R15L) {
-            return true;
-        }
-        if (RegName_XMM8 <= reg && reg <= RegName_XMM15) {
-            return true;
-        }
-        if (RegName_XMM8D <= reg && reg <= RegName_XMM15D) {
-            return true;
-        }
-        if (RegName_XMM8S <= reg && reg <= RegName_XMM15S) {
-            return true;
-        }
-        return false;
-    }
-    /**
-     * @brief Returns an 'processor's index' of the register - the index
-     *        used to encode the register in ModRM/SIB bytes.
-     *
-     * For the new EM64T registers the 'HW index' differs from the index
-     * encoded in RegName. For old-fashion registers it's effectively the
-     * same as ::getRegIndex(RegName).
-     */
-    static unsigned char getHWRegIndex(const RegName reg)
-    {
-        if (getRegKind(reg) != OpndKind_GPReg) {
-            return getRegIndex(reg);
-        }
-        if (RegName_SPL <= reg && reg<=RegName_DIL) {
-            return getRegIndex(reg);
-        }
-        if (RegName_R8L<= reg && reg<=RegName_R15L) {
-            return getRegIndex(reg) - getRegIndex(RegName_R8L);
-        }
-        return is_em64t_extra_reg(reg) ?
-                getRegIndex(reg)-getRegIndex(RegName_R8D) : getRegIndex(reg);
-    }
-#else
-    static unsigned char getHWRegIndex(const RegName reg)
-    {
-        return getRegIndex(reg);
-    }
-    static bool is_em64t_extra_reg(const RegName reg)
-    {
-        return false;
-    }
-#endif
-public:
-    static unsigned char get_size_hash(OpndSize size) {
-        return (size <= OpndSize_64) ? size_hash[size] : 0xFF;
-    }
-    static unsigned char get_kind_hash(OpndKind kind) {
-        return (kind <= OpndKind_Mem) ? kind_hash[kind] : 0xFF;
-    }
-
-    /**
-     * @brief A table used for the fast computation of hash value.
-     *
-     * A change must be strictly balanced with hash-related functions and data
-     * in enc_base.h/.cpp.
-     */
-    static const unsigned char size_hash[OpndSize_64+1];
-    /**
-     * @brief A table used for the fast computation of hash value.
-     *
-     * A change must be strictly balanced with hash-related functions and data
-     * in enc_base.h/.cpp.
-     */
-    static const unsigned char kind_hash[OpndKind_Mem+1];
-    /**
-     * @brief Maximum number of opcodes used for a single mnemonic.
-     *
-     * No arithmetics behind the number, simply estimated.
-     */
-    static const unsigned int   MAX_OPCODES = 32; //20;
-    /**
-     * @brief Mapping between operands hash code and operands.
-     */
-    static unsigned char    opcodesHashMap[Mnemonic_Count][HASH_MAX];
-    /**
-     * @brief Array of mnemonics.
-     */
-    static MnemonicDesc         mnemonics[Mnemonic_Count];
-    /**
-     * @brief Array of available opcodes.
-     */
-    static OpcodeDesc opcodes[Mnemonic_Count][MAX_OPCODES];
-
-    static int buildTable(void);
-    static void buildMnemonicDesc(const MnemonicInfo * minfo);
-    /**
-     * @brief Computes hash value for the given operands.
-     */
-    static unsigned short getHash(const OpcodeInfo* odesc);
-    /**
-     * @brief Dummy variable, for automatic invocation of buildTable() at
-     *        startup.
-     */
-    static int dummy;
-
-    static char * curRelOpnd[3];
-};
-
-ENCODER_NAMESPACE_END
-
-#endif // ifndef __ENC_BASE_H_INCLUDED__
diff --git a/vm/compiler/codegen/x86/libenc/enc_defs.h b/vm/compiler/codegen/x86/libenc/enc_defs.h
deleted file mode 100644
index 10409d2..0000000
--- a/vm/compiler/codegen/x86/libenc/enc_defs.h
+++ /dev/null
@@ -1,786 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-#ifndef _ENCODER_DEFS_H_
-#define _ENCODER_DEFS_H_
-
-
-// Used to isolate experimental or being tuned encoder into a separate
-// namespace so it can coexist with a stable one in the same bundle.
-#ifdef ENCODER_ISOLATE
-    #define ENCODER_NAMESPACE_START namespace enc_ia32 {
-    #define ENCODER_NAMESPACE_END };
-#else
-    #define ENCODER_NAMESPACE_START
-    #define ENCODER_NAMESPACE_END
-#endif
-
-#include <assert.h>
-#include "enc_defs_ext.h"
-
-#ifndef COUNTOF
-    /**
-     * Number of items in an array.
-     */
-    #define COUNTOF(a)      (sizeof(a)/sizeof(a[0]))
-#endif
-
-#ifdef _EM64T_
-    /**
-     * A stack pointer of default platform's size.
-     */
-    #define REG_STACK       RegName_RSP
-    /**
-     * A max GP register (with a highest index number)
-     */
-    #define REG_MAX         RegName_R15
-    /**
-     * Total number of GP registers including stack pointer.
-     */
-    #define MAX_REGS        15
-#else
-    #define REG_STACK       RegName_ESP
-    #define REG_MAX         RegName_EDI
-    #define MAX_REGS        8
-#endif
-
-ENCODER_NAMESPACE_START
-
-/**
- * A number of bytes 'eaten' by an ordinary PUSH/POP.
- */
-#define STACK_SLOT_SIZE (sizeof(void*))
-
-
-/**
- * A recommended by Intel Arch Manual aligment for instructions that
- * are targets for jmps.
- */
-#define JMP_TARGET_ALIGMENT     (16)
-/**
- * A maximum possible size of native instruction.
- */
-#define MAX_NATIVE_INST_SIZE (15)
-/**
- * The enum OpndKind describes an operand's location - memory, immediate or a register.
- * It can be used as a bit mask.
- */
-typedef enum OpndKind {
-    /**
-     * A change must be balanced with at least the following places:
-     *              Ia32::Constraint-s use the OpndKind as a mask
-     *              encoder.cpp & encoder_master_info.cpp uses OpndKind as an index for hashing
-     *              - perhaps there are much more places
-     *
-     * NOTE: an MMXReg kind is incompatible with the current constraints framework,
-     *              as it's not encoded as a mask.
-     */
-    OpndKind_Null=0,
-    OpndKind_GPReg          = 0x01, OpndKind_MinRegKind = OpndKind_GPReg,
-    OpndKind_SReg           = 0x02,
-#ifdef _HAVE_MMX_
-    OpndKind_MMXReg         = 0x03,
-#endif
-    OpndKind_FPReg          = 0x04,
-    OpndKind_XMMReg         = 0x08,
-    OpndKind_OtherReg       = 0x10,
-    OpndKind_StatusReg      = OpndKind_OtherReg,
-    OpndKind_MaxRegKind     = OpndKind_StatusReg,   // a max existing kind of register
-    OpndKind_MaxReg,                                // -'- + 1 to be used in array defs
-    //
-    OpndKind_Immediate      = 0x20, OpndKind_Imm=OpndKind_Immediate,
-    OpndKind_Memory         = 0x40, OpndKind_Mem=OpndKind_Memory,
-    //
-    OpndKind_Reg            = 0x1F,
-    OpndKind_Any            = 0x7F,
-    // syntetic constants. Normally not used anywhere, but are used for
-    // human-readable showing under the debugger
-    OpndKind_GPReg_Mem      = OpndKind_GPReg|OpndKind_Mem,
-#ifdef _HAVE_MMX_
-    OpndKind_MMXReg_Mem     = OpndKind_MMXReg|OpndKind_Mem,
-#endif
-    OpndKind_XMMReg_Mem     = OpndKind_XMMReg|OpndKind_Mem,
-} OpndKind;
-
-/**
- * Defines type of extention allowed for particular operand.
- * For example imul r32,r_m32,imm8 sign extend imm8 before performing multiplication.
- * To satisfy instruction constraints immediate operand should be either OpndExt_Signed
- * or OpndExt_Any.
- */
-typedef enum OpndExt {
-    OpndExt_None    = 0x0,
-    OpndExt_Signed  = 0x1,
-    OpndExt_Zero    = 0x2,
-    OpndExt_Any     = 0x3,
-}OpndExt;
-
-/**
- * enum OpndRole defines the role of an operand in an instruction
- * Can be used as mask to combine def and use. The complete def+use
- * info can be combined in 2 bits which is used, say in Encoder::OpndRole.
- */
-//TODO: this duplicates an Role used in the Ia32::Inst. That duplicate enum should be removed.
-typedef enum OpndRole {
-    OpndRole_Null=0,
-    OpndRole_Use=0x1,
-    OpndRole_Def=0x2,
-    OpndRole_UseDef=OpndRole_Use|OpndRole_Def,
-    OpndRole_All=0xffff,
-} OpndRole;
-
-
-#define REGNAME(k,s,i) ( ((k & OpndKind_Any)<<24) | ((s & OpndSize_Any)<<16) | (i&0xFF) )
-
-// Gregory -
-// It is critical that all register indexes (3rd number) inside of the
-// following table go in ascending order. That is R8 goes after
-// RDI. It is necessary for decoder when extending registers from RAX-RDI
-// to R8-R15 by simply adding 8 to the index on EM64T architecture
-typedef enum RegName {
-
-    RegName_Null = 0,
-
-#ifdef _EM64T_
-    /*
-    An index part of the RegName-s for RAX-RDI, EAX-ESI, AX-SI and AL-BH is
-    the same as the index used during instructions encoding. The same rule
-    applies for XMM regsters for IA32.
-    For new EM64T registers (both GP and XMM) the index need to be corrected to
-    obtain the index used in processor's instructions.
-    */
-    RegName_RAX = REGNAME(OpndKind_GPReg,OpndSize_64,0),
-    RegName_RCX = REGNAME(OpndKind_GPReg,OpndSize_64,1),
-    RegName_RDX = REGNAME(OpndKind_GPReg,OpndSize_64,2),
-    RegName_RBX = REGNAME(OpndKind_GPReg,OpndSize_64,3),
-    RegName_RSP = REGNAME(OpndKind_GPReg,OpndSize_64,4),
-    RegName_RBP = REGNAME(OpndKind_GPReg,OpndSize_64,5),
-    RegName_RSI = REGNAME(OpndKind_GPReg,OpndSize_64,6),
-    RegName_RDI = REGNAME(OpndKind_GPReg,OpndSize_64,7),
-
-    RegName_R8  = REGNAME(OpndKind_GPReg,OpndSize_64,8),
-    RegName_R9  = REGNAME(OpndKind_GPReg,OpndSize_64,9),
-    RegName_R10 = REGNAME(OpndKind_GPReg,OpndSize_64,10),
-    RegName_R11 = REGNAME(OpndKind_GPReg,OpndSize_64,11),
-    RegName_R12 = REGNAME(OpndKind_GPReg,OpndSize_64,12),
-    RegName_R13 = REGNAME(OpndKind_GPReg,OpndSize_64,13),
-    RegName_R14 = REGNAME(OpndKind_GPReg,OpndSize_64,14),
-    RegName_R15 = REGNAME(OpndKind_GPReg,OpndSize_64,15),
-#endif //~_EM64T_
-
-    RegName_EAX=REGNAME(OpndKind_GPReg,OpndSize_32,0),
-    RegName_ECX=REGNAME(OpndKind_GPReg,OpndSize_32,1),
-    RegName_EDX=REGNAME(OpndKind_GPReg,OpndSize_32,2),
-    RegName_EBX=REGNAME(OpndKind_GPReg,OpndSize_32,3),
-    RegName_ESP=REGNAME(OpndKind_GPReg,OpndSize_32,4),
-    RegName_EBP=REGNAME(OpndKind_GPReg,OpndSize_32,5),
-    RegName_ESI=REGNAME(OpndKind_GPReg,OpndSize_32,6),
-    RegName_EDI=REGNAME(OpndKind_GPReg,OpndSize_32,7),
-
-#ifdef _EM64T_
-    RegName_R8D  = REGNAME(OpndKind_GPReg,OpndSize_32,8),
-    RegName_R9D  = REGNAME(OpndKind_GPReg,OpndSize_32,9),
-    RegName_R10D = REGNAME(OpndKind_GPReg,OpndSize_32,10),
-    RegName_R11D = REGNAME(OpndKind_GPReg,OpndSize_32,11),
-    RegName_R12D = REGNAME(OpndKind_GPReg,OpndSize_32,12),
-    RegName_R13D = REGNAME(OpndKind_GPReg,OpndSize_32,13),
-    RegName_R14D = REGNAME(OpndKind_GPReg,OpndSize_32,14),
-    RegName_R15D = REGNAME(OpndKind_GPReg,OpndSize_32,15),
-#endif //~_EM64T_
-
-    RegName_AX=REGNAME(OpndKind_GPReg,OpndSize_16,0),
-    RegName_CX=REGNAME(OpndKind_GPReg,OpndSize_16,1),
-    RegName_DX=REGNAME(OpndKind_GPReg,OpndSize_16,2),
-    RegName_BX=REGNAME(OpndKind_GPReg,OpndSize_16,3),
-    RegName_SP=REGNAME(OpndKind_GPReg,OpndSize_16,4),
-    RegName_BP=REGNAME(OpndKind_GPReg,OpndSize_16,5),
-    RegName_SI=REGNAME(OpndKind_GPReg,OpndSize_16,6),
-    RegName_DI=REGNAME(OpndKind_GPReg,OpndSize_16,7),
-
-#ifdef _EM64T_
-    RegName_R8S  = REGNAME(OpndKind_GPReg,OpndSize_16,8),
-    RegName_R9S  = REGNAME(OpndKind_GPReg,OpndSize_16,9),
-    RegName_R10S = REGNAME(OpndKind_GPReg,OpndSize_16,10),
-    RegName_R11S = REGNAME(OpndKind_GPReg,OpndSize_16,11),
-    RegName_R12S = REGNAME(OpndKind_GPReg,OpndSize_16,12),
-    RegName_R13S = REGNAME(OpndKind_GPReg,OpndSize_16,13),
-    RegName_R14S = REGNAME(OpndKind_GPReg,OpndSize_16,14),
-    RegName_R15S = REGNAME(OpndKind_GPReg,OpndSize_16,15),
-#endif //~_EM64T_
-
-    RegName_AL=REGNAME(OpndKind_GPReg,OpndSize_8,0),
-    RegName_CL=REGNAME(OpndKind_GPReg,OpndSize_8,1),
-    RegName_DL=REGNAME(OpndKind_GPReg,OpndSize_8,2),
-    RegName_BL=REGNAME(OpndKind_GPReg,OpndSize_8,3),
-    // FIXME: Used in enc_tabl.cpp
-    // AH is not accessible on EM64T, instead encoded register is SPL, so decoded
-    // register will return incorrect enum
-    RegName_AH=REGNAME(OpndKind_GPReg,OpndSize_8,4),
-#if !defined(_EM64T_)
-    RegName_CH=REGNAME(OpndKind_GPReg,OpndSize_8,5),
-    RegName_DH=REGNAME(OpndKind_GPReg,OpndSize_8,6),
-    RegName_BH=REGNAME(OpndKind_GPReg,OpndSize_8,7),
-#else
-    RegName_SPL=REGNAME(OpndKind_GPReg,OpndSize_8,4),
-    RegName_BPL=REGNAME(OpndKind_GPReg,OpndSize_8,5),
-    RegName_SIL=REGNAME(OpndKind_GPReg,OpndSize_8,6),
-    RegName_DIL=REGNAME(OpndKind_GPReg,OpndSize_8,7),
-    RegName_R8L=REGNAME(OpndKind_GPReg,OpndSize_8,8),
-    RegName_R9L=REGNAME(OpndKind_GPReg,OpndSize_8,9),
-    RegName_R10L=REGNAME(OpndKind_GPReg,OpndSize_8,10),
-    RegName_R11L=REGNAME(OpndKind_GPReg,OpndSize_8,11),
-    RegName_R12L=REGNAME(OpndKind_GPReg,OpndSize_8,12),
-    RegName_R13L=REGNAME(OpndKind_GPReg,OpndSize_8,13),
-    RegName_R14L=REGNAME(OpndKind_GPReg,OpndSize_8,14),
-    RegName_R15L=REGNAME(OpndKind_GPReg,OpndSize_8,15),
-#endif
-
-    RegName_ES=REGNAME(OpndKind_SReg,OpndSize_16,0),
-    RegName_CS=REGNAME(OpndKind_SReg,OpndSize_16,1),
-    RegName_SS=REGNAME(OpndKind_SReg,OpndSize_16,2),
-    RegName_DS=REGNAME(OpndKind_SReg,OpndSize_16,3),
-    RegName_FS=REGNAME(OpndKind_SReg,OpndSize_16,4),
-    RegName_GS=REGNAME(OpndKind_SReg,OpndSize_16,5),
-
-    RegName_EFLAGS=REGNAME(OpndKind_StatusReg,OpndSize_32,0),
-
-#if !defined(TESTING_ENCODER)
-    RegName_FP0=REGNAME(OpndKind_FPReg,OpndSize_80,0),
-    RegName_FP1=REGNAME(OpndKind_FPReg,OpndSize_80,1),
-    RegName_FP2=REGNAME(OpndKind_FPReg,OpndSize_80,2),
-    RegName_FP3=REGNAME(OpndKind_FPReg,OpndSize_80,3),
-    RegName_FP4=REGNAME(OpndKind_FPReg,OpndSize_80,4),
-    RegName_FP5=REGNAME(OpndKind_FPReg,OpndSize_80,5),
-    RegName_FP6=REGNAME(OpndKind_FPReg,OpndSize_80,6),
-    RegName_FP7=REGNAME(OpndKind_FPReg,OpndSize_80,7),
-#endif
-    RegName_FP0S=REGNAME(OpndKind_FPReg,OpndSize_32,0),
-    RegName_FP1S=REGNAME(OpndKind_FPReg,OpndSize_32,1),
-    RegName_FP2S=REGNAME(OpndKind_FPReg,OpndSize_32,2),
-    RegName_FP3S=REGNAME(OpndKind_FPReg,OpndSize_32,3),
-    RegName_FP4S=REGNAME(OpndKind_FPReg,OpndSize_32,4),
-    RegName_FP5S=REGNAME(OpndKind_FPReg,OpndSize_32,5),
-    RegName_FP6S=REGNAME(OpndKind_FPReg,OpndSize_32,6),
-    RegName_FP7S=REGNAME(OpndKind_FPReg,OpndSize_32,7),
-
-    RegName_FP0D=REGNAME(OpndKind_FPReg,OpndSize_64,0),
-    RegName_FP1D=REGNAME(OpndKind_FPReg,OpndSize_64,1),
-    RegName_FP2D=REGNAME(OpndKind_FPReg,OpndSize_64,2),
-    RegName_FP3D=REGNAME(OpndKind_FPReg,OpndSize_64,3),
-    RegName_FP4D=REGNAME(OpndKind_FPReg,OpndSize_64,4),
-    RegName_FP5D=REGNAME(OpndKind_FPReg,OpndSize_64,5),
-    RegName_FP6D=REGNAME(OpndKind_FPReg,OpndSize_64,6),
-    RegName_FP7D=REGNAME(OpndKind_FPReg,OpndSize_64,7),
-
-#if !defined(TESTING_ENCODER)
-    RegName_XMM0=REGNAME(OpndKind_XMMReg,OpndSize_128,0),
-    RegName_XMM1=REGNAME(OpndKind_XMMReg,OpndSize_128,1),
-    RegName_XMM2=REGNAME(OpndKind_XMMReg,OpndSize_128,2),
-    RegName_XMM3=REGNAME(OpndKind_XMMReg,OpndSize_128,3),
-    RegName_XMM4=REGNAME(OpndKind_XMMReg,OpndSize_128,4),
-    RegName_XMM5=REGNAME(OpndKind_XMMReg,OpndSize_128,5),
-    RegName_XMM6=REGNAME(OpndKind_XMMReg,OpndSize_128,6),
-    RegName_XMM7=REGNAME(OpndKind_XMMReg,OpndSize_128,7),
-
-#ifdef _EM64T_
-    RegName_XMM8  = REGNAME(OpndKind_XMMReg,OpndSize_128,0),
-    RegName_XMM9  = REGNAME(OpndKind_XMMReg,OpndSize_128,1),
-    RegName_XMM10 = REGNAME(OpndKind_XMMReg,OpndSize_128,2),
-    RegName_XMM11 = REGNAME(OpndKind_XMMReg,OpndSize_128,3),
-    RegName_XMM12 = REGNAME(OpndKind_XMMReg,OpndSize_128,4),
-    RegName_XMM13 = REGNAME(OpndKind_XMMReg,OpndSize_128,5),
-    RegName_XMM14 = REGNAME(OpndKind_XMMReg,OpndSize_128,6),
-    RegName_XMM15 = REGNAME(OpndKind_XMMReg,OpndSize_128,7),
-#endif //~_EM64T_
-
-#endif  // ~TESTING_ENCODER
-
-    RegName_XMM0S=REGNAME(OpndKind_XMMReg,OpndSize_32,0),
-    RegName_XMM1S=REGNAME(OpndKind_XMMReg,OpndSize_32,1),
-    RegName_XMM2S=REGNAME(OpndKind_XMMReg,OpndSize_32,2),
-    RegName_XMM3S=REGNAME(OpndKind_XMMReg,OpndSize_32,3),
-    RegName_XMM4S=REGNAME(OpndKind_XMMReg,OpndSize_32,4),
-    RegName_XMM5S=REGNAME(OpndKind_XMMReg,OpndSize_32,5),
-    RegName_XMM6S=REGNAME(OpndKind_XMMReg,OpndSize_32,6),
-    RegName_XMM7S=REGNAME(OpndKind_XMMReg,OpndSize_32,7),
-#ifdef _EM64T_
-    RegName_XMM8S=REGNAME(OpndKind_XMMReg,OpndSize_32,8),
-    RegName_XMM9S=REGNAME(OpndKind_XMMReg,OpndSize_32,9),
-    RegName_XMM10S=REGNAME(OpndKind_XMMReg,OpndSize_32,10),
-    RegName_XMM11S=REGNAME(OpndKind_XMMReg,OpndSize_32,11),
-    RegName_XMM12S=REGNAME(OpndKind_XMMReg,OpndSize_32,12),
-    RegName_XMM13S=REGNAME(OpndKind_XMMReg,OpndSize_32,13),
-    RegName_XMM14S=REGNAME(OpndKind_XMMReg,OpndSize_32,14),
-    RegName_XMM15S=REGNAME(OpndKind_XMMReg,OpndSize_32,15),
-#endif // ifdef _EM64T_
-    RegName_XMM0D=REGNAME(OpndKind_XMMReg,OpndSize_64,0),
-    RegName_XMM1D=REGNAME(OpndKind_XMMReg,OpndSize_64,1),
-    RegName_XMM2D=REGNAME(OpndKind_XMMReg,OpndSize_64,2),
-    RegName_XMM3D=REGNAME(OpndKind_XMMReg,OpndSize_64,3),
-    RegName_XMM4D=REGNAME(OpndKind_XMMReg,OpndSize_64,4),
-    RegName_XMM5D=REGNAME(OpndKind_XMMReg,OpndSize_64,5),
-    RegName_XMM6D=REGNAME(OpndKind_XMMReg,OpndSize_64,6),
-    RegName_XMM7D=REGNAME(OpndKind_XMMReg,OpndSize_64,7),
-#ifdef _EM64T_
-    RegName_XMM8D=REGNAME(OpndKind_XMMReg,OpndSize_64,8),
-    RegName_XMM9D=REGNAME(OpndKind_XMMReg,OpndSize_64,9),
-    RegName_XMM10D=REGNAME(OpndKind_XMMReg,OpndSize_64,10),
-    RegName_XMM11D=REGNAME(OpndKind_XMMReg,OpndSize_64,11),
-    RegName_XMM12D=REGNAME(OpndKind_XMMReg,OpndSize_64,12),
-    RegName_XMM13D=REGNAME(OpndKind_XMMReg,OpndSize_64,13),
-    RegName_XMM14D=REGNAME(OpndKind_XMMReg,OpndSize_64,14),
-    RegName_XMM15D=REGNAME(OpndKind_XMMReg,OpndSize_64,15),
-#endif // ifdef _EM64T_
-#ifdef _HAVE_MMX_
-    RegName_MMX0=REGNAME(OpndKind_MMXReg,OpndSize_64,0),
-    RegName_MMX1=REGNAME(OpndKind_MMXReg,OpndSize_64,1),
-    RegName_MMX2=REGNAME(OpndKind_MMXReg,OpndSize_64,2),
-    RegName_MMX3=REGNAME(OpndKind_MMXReg,OpndSize_64,3),
-    RegName_MMX4=REGNAME(OpndKind_MMXReg,OpndSize_64,4),
-    RegName_MMX5=REGNAME(OpndKind_MMXReg,OpndSize_64,5),
-    RegName_MMX6=REGNAME(OpndKind_MMXReg,OpndSize_64,6),
-    RegName_MMX7=REGNAME(OpndKind_MMXReg,OpndSize_64,7),
-#endif  // _HAVE_MMX_
-} RegName;
-
-#if 0   // Android x86: use mnemonics defined in enc_defs_ext.h
-/**
- * Conditional mnemonics.
- * The values match the 'real' (==processor's) values of the appropriate
- * condition values used in the opcodes.
- */
-enum ConditionMnemonic {
-
-    ConditionMnemonic_O=0,
-    ConditionMnemonic_NO=1,
-    ConditionMnemonic_B=2, ConditionMnemonic_NAE=ConditionMnemonic_B, ConditionMnemonic_C=ConditionMnemonic_B,
-    ConditionMnemonic_NB=3, ConditionMnemonic_AE=ConditionMnemonic_NB, ConditionMnemonic_NC=ConditionMnemonic_NB,
-    ConditionMnemonic_Z=4, ConditionMnemonic_E=ConditionMnemonic_Z,
-    ConditionMnemonic_NZ=5, ConditionMnemonic_NE=ConditionMnemonic_NZ,
-    ConditionMnemonic_BE=6, ConditionMnemonic_NA=ConditionMnemonic_BE,
-    ConditionMnemonic_NBE=7, ConditionMnemonic_A=ConditionMnemonic_NBE,
-
-    ConditionMnemonic_S=8,
-    ConditionMnemonic_NS=9,
-    ConditionMnemonic_P=10, ConditionMnemonic_PE=ConditionMnemonic_P,
-    ConditionMnemonic_NP=11, ConditionMnemonic_PO=ConditionMnemonic_NP,
-    ConditionMnemonic_L=12, ConditionMnemonic_NGE=ConditionMnemonic_L,
-    ConditionMnemonic_NL=13, ConditionMnemonic_GE=ConditionMnemonic_NL,
-    ConditionMnemonic_LE=14, ConditionMnemonic_NG=ConditionMnemonic_LE,
-    ConditionMnemonic_NLE=15, ConditionMnemonic_G=ConditionMnemonic_NLE,
-    ConditionMnemonic_Count=16
-};
-
-
-#define CCM(prefix,cond) Mnemonic_##prefix##cond=Mnemonic_##prefix##cc+ConditionMnemonic_##cond
-
-//=========================================================================================================
-enum Mnemonic {
-
-Mnemonic_NULL=0, Mnemonic_Null=Mnemonic_NULL,
-Mnemonic_ADC,                           // Add with Carry
-Mnemonic_ADD,                           // Add
-Mnemonic_ADDSD,                         // Add Scalar Double-Precision Floating-Point Values
-Mnemonic_ADDSS,                         // Add Scalar Single-Precision Floating-Point Values
-Mnemonic_AND,                           // Logical AND
-
-Mnemonic_BSF,                           // Bit scan forward
-Mnemonic_BSR,                           // Bit scan reverse
-
-Mnemonic_CALL,                          // Call Procedure
-Mnemonic_CMC,                           // Complement Carry Flag
-Mnemonic_CWD, Mnemonic_CDQ=Mnemonic_CWD,// Convert Word to Doubleword/Convert Doubleword to Qua T dword
-Mnemonic_CMOVcc,                        // Conditional Move
-    CCM(CMOV,O),
-    CCM(CMOV,NO),
-    CCM(CMOV,B), CCM(CMOV,NAE), CCM(CMOV,C),
-    CCM(CMOV,NB), CCM(CMOV,AE), CCM(CMOV,NC),
-    CCM(CMOV,Z), CCM(CMOV,E),
-    CCM(CMOV,NZ), CCM(CMOV,NE),
-    CCM(CMOV,BE), CCM(CMOV,NA),
-    CCM(CMOV,NBE), CCM(CMOV,A),
-
-    CCM(CMOV,S),
-    CCM(CMOV,NS),
-    CCM(CMOV,P), CCM(CMOV,PE),
-    CCM(CMOV,NP), CCM(CMOV,PO),
-    CCM(CMOV,L), CCM(CMOV,NGE),
-    CCM(CMOV,NL), CCM(CMOV,GE),
-    CCM(CMOV,LE), CCM(CMOV,NG),
-    CCM(CMOV,NLE), CCM(CMOV,G),
-
-Mnemonic_CMP,                           // Compare Two Operands
-Mnemonic_CMPXCHG,                       // Compare and exchange
-Mnemonic_CMPXCHG8B,                     // Compare and Exchange 8 Bytes
-Mnemonic_CMPSB,                         // Compare Two Bytes at DS:ESI and ES:EDI
-Mnemonic_CMPSW,                         // Compare Two Words at DS:ESI and ES:EDI
-Mnemonic_CMPSD,                        // Compare Two Doublewords at DS:ESI and ES:EDI
-//
-// double -> float
-Mnemonic_CVTSD2SS,                      // Convert Scalar Double-Precision Floating-Point Value to Scalar Single-Precision Floating-Point Value
-// double -> I_32
-Mnemonic_CVTSD2SI,                      // Convert Scalar Double-Precision Floating-Point Value to Doubleword Integer
-// double [truncated] -> I_32
-Mnemonic_CVTTSD2SI,                     // Convert with Truncation Scalar Double-Precision Floating-Point Value to Signed Doubleword Integer
-//
-// float -> double
-Mnemonic_CVTSS2SD,                      // Convert Scalar Single-Precision Floating-Point Value to Scalar Double-Precision Floating-Point Value
-// float -> I_32
-Mnemonic_CVTSS2SI,                      // Convert Scalar Single-Precision Floating-Point Value to Doubleword Integer
-// float [truncated] -> I_32
-Mnemonic_CVTTSS2SI,                     // Convert with Truncation Scalar Single-Precision Floating-Point Value to Doubleword Integer
-//
-// I_32 -> double
-Mnemonic_CVTSI2SD,                      // Convert Doubleword Integer to Scalar Double-Precision Floating-Point Value
-// I_32 -> float
-Mnemonic_CVTSI2SS,                      // Convert Doubleword Integer to Scalar Single-Precision Floating-Point Value
-
-Mnemonic_COMISD,                        // Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS
-Mnemonic_COMISS,                        // Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS
-Mnemonic_DEC,                           // Decrement by 1
-//Mnemonic_DIV,                         // Unsigned Divide
-Mnemonic_DIVSD,                         // Divide Scalar Double-Precision Floating-Point Values
-Mnemonic_DIVSS,                         // Divide Scalar Single-Precision Floating-Point Values
-
-#ifdef _HAVE_MMX_
-Mnemonic_EMMS,                          // Empty MMX Technology State
-#endif
-
-Mnemonic_ENTER,                         // ENTER-Make Stack Frame for Procedure Parameters
-Mnemonic_FLDCW,                         // Load FPU control word
-Mnemonic_FADDP,
-Mnemonic_FLDZ,
-Mnemonic_FADD,
-Mnemonic_FSUBP,
-Mnemonic_FSUB,
-Mnemonic_FISUB,
-Mnemonic_FMUL,
-Mnemonic_FMULP,
-Mnemonic_FDIVP,
-Mnemonic_FDIV,
-Mnemonic_FUCOMPP,
-Mnemonic_FRNDINT,
-Mnemonic_FNSTCW,                        // Store FPU control word
-Mnemonic_FSTSW,                         // Store FPU status word
-Mnemonic_FNSTSW,                         // Store FPU status word
-//Mnemonic_FDECSTP,                     // Decrement Stack-Top Pointer
-Mnemonic_FILD,                          // Load Integer
-Mnemonic_FLD,                           // Load Floating Point Value
-Mnemonic_FLDLG2,
-Mnemonic_FLDLN2,
-Mnemonic_FLD1,
-
-Mnemonic_FCLEX,                         // Clear Exceptions
-Mnemonic_FCHS,                          // Change sign of ST0
-Mnemonic_FNCLEX,                        // Clear Exceptions
-
-//Mnemonic_FINCSTP,                     // Increment Stack-Top Pointer
-Mnemonic_FIST,                          // Store Integer
-Mnemonic_FISTP,                         // Store Integer, pop FPU stack
-Mnemonic_FISTTP,                        // Store Integer with Truncation
-Mnemonic_FPREM,                         // Partial Remainder
-Mnemonic_FPREM1,                        // Partial Remainder
-Mnemonic_FST,                           // Store Floating Point Value
-Mnemonic_FSTP,                          // Store Floating Point Value and pop the FP stack
-Mnemonic_FSQRT,                         //Computes the square root of the source value in the stack and pop the FP stack
-Mnemonic_FABS,                          //Computes the absolute value of the source value in the stack and pop the FP stack
-Mnemonic_FSIN,                          //Computes the sine of the source value in the stack and pop the FP stack
-Mnemonic_FCOS,                          //Computes the cosine of the source value in the stack and pop the FP stack
-Mnemonic_FPTAN,                         //Computes the tangent of the source value in the stack and pop the FP stack
-Mnemonic_FYL2X,
-Mnemonic_FYL2XP1,
-Mnemonic_F2XM1,
-Mnemonic_FPATAN,
-Mnemonic_FXCH,
-Mnemonic_FSCALE,
-
-Mnemonic_XCHG,
-Mnemonic_DIV,                           // Unsigned Divide
-Mnemonic_IDIV,                          // Signed Divide
-Mnemonic_MUL,                           // Unsigned Multiply
-Mnemonic_IMUL,                          // Signed Multiply
-Mnemonic_INC,                           // Increment by 1
-Mnemonic_INT3,                          // Call break point
-Mnemonic_Jcc,                           // Jump if Condition Is Met
-    CCM(J,O),
-    CCM(J,NO),
-    CCM(J,B), CCM(J,NAE), CCM(J,C),
-    CCM(J,NB), CCM(J,AE), CCM(J,NC),
-    CCM(J,Z), CCM(J,E),
-    CCM(J,NZ), CCM(J,NE),
-    CCM(J,BE), CCM(J,NA),
-    CCM(J,NBE), CCM(J,A),
-    CCM(J,S),
-    CCM(J,NS),
-    CCM(J,P), CCM(J,PE),
-    CCM(J,NP), CCM(J,PO),
-    CCM(J,L), CCM(J,NGE),
-    CCM(J,NL), CCM(J,GE),
-    CCM(J,LE), CCM(J,NG),
-    CCM(J,NLE), CCM(J,G),
-Mnemonic_JMP,                           // Jump
-Mnemonic_LEA,                           // Load Effective Address
-Mnemonic_LEAVE,                         // High Level Procedure Exit
-Mnemonic_LOOP,                          // Loop according to ECX counter
-Mnemonic_LOOPE,                          // Loop according to ECX counter
-Mnemonic_LOOPNE, Mnemonic_LOOPNZ = Mnemonic_LOOPNE, // Loop according to ECX
-Mnemonic_LAHF,                          // Load Flags into AH
-Mnemonic_MOV,                           // Move
-Mnemonic_MOVD,                          // Move Double word
-Mnemonic_MOVQ,                          // Move Quadword
-/*Mnemonic_MOVS,                        // Move Data from String to String*/
-// MOVS is a special case: see encoding table for more details,
-Mnemonic_MOVS8, Mnemonic_MOVS16, Mnemonic_MOVS32, Mnemonic_MOVS64,
-//
-Mnemonic_MOVAPD,                         // Move Scalar Double-Precision Floating-Point Value
-Mnemonic_MOVSD,                         // Move Scalar Double-Precision Floating-Point Value
-Mnemonic_MOVSS,                         // Move Scalar Single-Precision Floating-Point Values
-Mnemonic_MOVSX,                         // Move with Sign-Extension
-Mnemonic_MOVZX,                         // Move with Zero-Extend
-//Mnemonic_MUL,                         // Unsigned Multiply
-Mnemonic_MULSD,                         // Multiply Scalar Double-Precision Floating-Point Values
-Mnemonic_MULSS,                         // Multiply Scalar Single-Precision Floating-Point Values
-Mnemonic_NEG,                           // Two's Complement Negation
-Mnemonic_NOP,                           // No Operation
-Mnemonic_NOT,                           // One's Complement Negation
-Mnemonic_OR,                            // Logical Inclusive OR
-Mnemonic_PREFETCH,                      // prefetch
-
-#ifdef _HAVE_MMX_
-    Mnemonic_PADDQ,                     // Add Packed Quadword Integers
-    Mnemonic_PAND,                      // Logical AND
-    Mnemonic_POR,                       // Bitwise Logical OR
-    Mnemonic_PSUBQ,                     // Subtract Packed Quadword Integers
-#endif
-
-Mnemonic_PXOR,                          // Logical Exclusive OR
-Mnemonic_POP,                           // Pop a Value from the Stack
-Mnemonic_POPFD,                         // Pop a Value of EFLAGS register from the Stack
-Mnemonic_PUSH,                          // Push Word or Doubleword Onto the Stack
-Mnemonic_PUSHFD,                        // Push EFLAGS Doubleword Onto the Stack
-Mnemonic_RET,                           // Return from Procedure
-
-Mnemonic_SETcc,                         // Set Byte on Condition
-    CCM(SET,O),
-    CCM(SET,NO),
-    CCM(SET,B), CCM(SET,NAE), CCM(SET,C),
-    CCM(SET,NB), CCM(SET,AE), CCM(SET,NC),
-    CCM(SET,Z), CCM(SET,E),
-    CCM(SET,NZ), CCM(SET,NE),
-    CCM(SET,BE), CCM(SET,NA),
-    CCM(SET,NBE), CCM(SET,A),
-    CCM(SET,S),
-    CCM(SET,NS),
-    CCM(SET,P), CCM(SET,PE),
-    CCM(SET,NP), CCM(SET,PO),
-    CCM(SET,L), CCM(SET,NGE),
-    CCM(SET,NL), CCM(SET,GE),
-    CCM(SET,LE), CCM(SET,NG),
-    CCM(SET,NLE), CCM(SET,G),
-
-Mnemonic_SAL, Mnemonic_SHL=Mnemonic_SAL,// Shift left
-Mnemonic_SAR,                           // Shift right
-Mnemonic_ROR,                           // Rotate right
-Mnemonic_RCR,                           // Rotate right through CARRY flag
-Mnemonic_ROL,                           // Rotate left
-Mnemonic_RCL,                           // Rotate left through CARRY flag
-Mnemonic_SHR,                           // Unsigned shift right
-Mnemonic_SHRD,                          // Double Precision Shift Right
-Mnemonic_SHLD,                          // Double Precision Shift Left
-
-Mnemonic_SBB,                           // Integer Subtraction with Borrow
-Mnemonic_SUB,                           // Subtract
-Mnemonic_SUBSD,                         // Subtract Scalar Double-Precision Floating-Point Values
-Mnemonic_SUBSS,                         // Subtract Scalar Single-Precision Floating-Point Values
-
-Mnemonic_TEST,                          // Logical Compare
-
-Mnemonic_UCOMISD,                       // Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS
-Mnemonic_UCOMISS,                       // Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS
-
-Mnemonic_XOR,                           // Logical Exclusive OR
-//
-// packed things,
-//
-Mnemonic_XORPD,                         // Bitwise Logical XOR for Double-Precision Floating-Point Values
-Mnemonic_XORPS,                         // Bitwise Logical XOR for Single-Precision Floating-Point Values
-
-Mnemonic_CVTDQ2PD,                      // Convert Packed Doubleword Integers to Packed Double-Precision Floating-Point Values
-Mnemonic_CVTTPD2DQ,                     // Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Doubleword Integers
-
-Mnemonic_CVTDQ2PS,                      // Convert Packed Doubleword Integers to Packed Single-Precision Floating-Point Values
-Mnemonic_CVTTPS2DQ,                     // Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Doubleword Integers
-//
-// String operations
-//
-Mnemonic_STD,                           // Set direction flag
-Mnemonic_CLD,                           // Clear direction flag
-Mnemonic_SCAS,                          // Scan string
-Mnemonic_STOS,                          // Store string
-
-//
-Mnemonic_WAIT,                          // Check pending pending unmasked floating-point exception
-//
-Mnemonic_Count
-};
-
-#undef CCM
-#endif
-
-/**
- * @brief Instruction prefixes, according to arch manual.
- */
-typedef enum InstPrefix {
-    InstPrefix_Null = 0,
-    // Group 1
-    InstPrefix_LOCK = 0xF0,
-    InstPrefix_REPNE = 0xF2,
-    InstPrefix_REPNZ = InstPrefix_REPNE,
-    InstPrefix_REP = 0xF3, InstPrefix_REPZ = InstPrefix_REP,
-    // Group 2
-    InstPrefix_CS = 0x2E,
-    InstPrefix_SS = 0x36,
-    InstPrefix_DS = 0x3E,
-    InstPrefix_ES = 0x26,
-    InstPrefix_FS = 0x64,
-    InstPrefix_GS = 0x65,
-    //
-    InstPrefix_HintTaken = 0x3E,
-    InstPrefix_HintNotTaken = 0x2E,
-    // Group 3
-    InstPrefix_OpndSize = 0x66,
-    // Group 4
-    InstPrefix_AddrSize = 0x67
-} InstPrefix;
-
-inline unsigned getSizeBytes(OpndSize sz)
-{
-    if (sz==OpndSize_64) { return 8; }
-    if (sz==OpndSize_32) { return 4; }
-    if (sz==OpndSize_16) { return 2; }
-    if (sz==OpndSize_8)  { return 1; }
-    assert(false);
-    return 0;
-}
-
-inline bool isRegKind(OpndKind kind)
-{
-    return OpndKind_GPReg<= kind && kind<=OpndKind_MaxRegKind;
-}
-
-/**
- * @brief Returns RegName for a given name.
- *
- * Name is case-insensitive.
- * @param regname - string name of a register
- * @return RegName for the given name, or RegName_Null if name is invalid
- */
-RegName         getRegName(const char * regname);
-/**
- * Constructs RegName from the given OpndKind, size and index.
- */
-inline RegName  getRegName(OpndKind k, OpndSize s, int idx)
-{
-    return (RegName)REGNAME(k,s,idx);
-}
-/**
- * Extracts a bit mask with a bit set at the position of the register's index.
- */
-inline unsigned getRegMask(RegName reg)
-{
-    return 1<<(reg&0xff);
-}
-/**
- * @brief Extracts OpndKind from the RegName.
- */
-inline OpndKind getRegKind(RegName reg)
-{
-    return (OpndKind)(reg>>24);
-}
-/**
- * @brief Extracts OpndSize from RegName.
- */
-inline OpndSize getRegSize(RegName reg)
-{
-    return (OpndSize)((reg>>16)&0xFF);
-}
-/**
- * Extracts an index from the given RegName.
- */
-inline unsigned char getRegIndex(RegName reg)
-{
-    return (unsigned char)(reg&0xFF);
-}
-/**
- * Returns a string name of the given RegName. The name returned is in upper-case.
- * Returns NULL if invalid RegName specified.
- */
-const char *    getRegNameString(RegName reg);
-/**
- * Returns string name of a given OpndSize.
- * Returns NULL if invalid OpndSize passed.
- */
-const char *    getOpndSizeString(OpndSize size);
-/**
- * Returns OpndSize passed by its string representation (case insensitive).
- * Returns OpndSize_Null if invalid string specified.
- * The 'sizeString' can not be NULL.
- */
-OpndSize        getOpndSize(const char * sizeString);
-/**
- * Returns string name of a given OpndKind.
- * Returns NULL if the passed kind is invalid.
- */
-const char *    getOpndKindString(OpndKind kind);
-/**
- * Returns OpndKind found by its string representation (case insensitive).
- * Returns OpndKind_Null if the name is invalid.
- * The 'kindString' can not be NULL.
- */
-OpndKind        getOpndKind(const char * kindString);
-/**
- *
- */
-const char *    getConditionString(ConditionMnemonic cm);
-
-/**
- * Constructs an RegName with the same index and kind, but with a different size from
- * the given RegName (i.e. getRegAlias(EAX, OpndSize_16) => AX; getRegAlias(BL, OpndSize_32) => EBX).
- * The constructed RegName is not checked in any way and thus may be invalid.
- * Note, that the aliasing does not work for at least AH,BH,CH,DH, ESI, EDI, ESP and EBP regs.
- */
-inline RegName getAliasReg(RegName reg, OpndSize sz)
-{
-    return (RegName)REGNAME(getRegKind(reg), sz, getRegIndex(reg));
-}
-
-/**
- * brief Tests two RegName-s of the same kind for equality.
- *
- * @note Does work for 8 bit general purpose registers (AH, AL, BH, BL, etc).
- */
-inline bool equals(RegName r0, RegName r1)
-{
-    return getRegKind(r0) == getRegKind(r1) &&
-           getRegIndex(r0) == getRegIndex(r1);
-}
-
-ENCODER_NAMESPACE_END
-
-#endif  // ifndef _ENCODER_DEFS_H_
diff --git a/vm/compiler/codegen/x86/libenc/enc_defs_ext.h b/vm/compiler/codegen/x86/libenc/enc_defs_ext.h
deleted file mode 100644
index acc61d9..0000000
--- a/vm/compiler/codegen/x86/libenc/enc_defs_ext.h
+++ /dev/null
@@ -1,346 +0,0 @@
-/*
- * Copyright (C) 2012 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef _ENCODER_DEFS_EXT_H_
-#define _ENCODER_DEFS_EXT_H_
-
-
-// Used to isolate experimental or being tuned encoder into a separate
-// namespace so it can coexist with a stable one in the same bundle.
-#ifdef ENCODER_ISOLATE
-    #define ENCODER_NAMESPACE_START namespace enc_ia32 {
-    #define ENCODER_NAMESPACE_END };
-#else
-    #define ENCODER_NAMESPACE_START
-    #define ENCODER_NAMESPACE_END
-#endif
-
-ENCODER_NAMESPACE_START
-typedef enum OpndSize {
-    /**
-     * A change must be balanced with at least the following places:
-     *              Ia32IRConstants.h :: getByteSize() uses some presumptions about OpndSize_ values
-     *              Ia32::Constraint-s use the OpndSize as a mask
-     *              encoder.cpp & encoder_master_info.cpp uses OpndSize as an index for hashing
-     *              - perhaps there are much more places
-     */
-    OpndSize_Null           = 0,
-    OpndSize_8             = 0x01,
-    OpndSize_16            = 0x02,
-    OpndSize_32            = 0x04,
-    OpndSize_64            = 0x08,
-#if !defined(TESTING_ENCODER)
-    OpndSize_80            = 0x10,
-    OpndSize_128           = 0x20,
-#endif
-    OpndSize_Max,
-    OpndSize_Any            = 0x3F,
-    OpndSize_Default        = OpndSize_Any
-} OpndSize;
-
-/**
- * Conditional mnemonics.
- * The values match the 'real' (==processor's) values of the appropriate
- * condition values used in the opcodes.
- */
-typedef enum ConditionMnemonic {
-
-    ConditionMnemonic_O=0,
-    ConditionMnemonic_NO=1,
-    ConditionMnemonic_B=2, ConditionMnemonic_NAE=ConditionMnemonic_B, ConditionMnemonic_C=ConditionMnemonic_B,
-    ConditionMnemonic_NB=3, ConditionMnemonic_AE=ConditionMnemonic_NB, ConditionMnemonic_NC=ConditionMnemonic_NB,
-    ConditionMnemonic_Z=4, ConditionMnemonic_E=ConditionMnemonic_Z,
-    ConditionMnemonic_NZ=5, ConditionMnemonic_NE=ConditionMnemonic_NZ,
-    ConditionMnemonic_BE=6, ConditionMnemonic_NA=ConditionMnemonic_BE,
-    ConditionMnemonic_NBE=7, ConditionMnemonic_A=ConditionMnemonic_NBE,
-
-    ConditionMnemonic_S=8,
-    ConditionMnemonic_NS=9,
-    ConditionMnemonic_P=10, ConditionMnemonic_PE=ConditionMnemonic_P,
-    ConditionMnemonic_NP=11, ConditionMnemonic_PO=ConditionMnemonic_NP,
-    ConditionMnemonic_L=12, ConditionMnemonic_NGE=ConditionMnemonic_L,
-    ConditionMnemonic_NL=13, ConditionMnemonic_GE=ConditionMnemonic_NL,
-    ConditionMnemonic_LE=14, ConditionMnemonic_NG=ConditionMnemonic_LE,
-    ConditionMnemonic_NLE=15, ConditionMnemonic_G=ConditionMnemonic_NLE,
-    ConditionMnemonic_Count=16
-} ConditionMnemonic;
-
-
-#define CCM(prefix,cond) Mnemonic_##prefix##cond=Mnemonic_##prefix##cc+ConditionMnemonic_##cond
-
-//=========================================================================================================
-typedef enum Mnemonic {
-
-Mnemonic_NULL=0, Mnemonic_Null=Mnemonic_NULL,
-Mnemonic_JMP,                           // Jump
-Mnemonic_MOV,                           // Move
-Mnemonic_Jcc,                           // Jump if Condition Is Met
-    CCM(J,O),
-    CCM(J,NO),
-    CCM(J,B), CCM(J,NAE), CCM(J,C),
-    CCM(J,NB), CCM(J,AE), CCM(J,NC),
-    CCM(J,Z), CCM(J,E),
-    CCM(J,NZ), CCM(J,NE),
-    CCM(J,BE), CCM(J,NA),
-    CCM(J,NBE), CCM(J,A),
-    CCM(J,S),
-    CCM(J,NS),
-    CCM(J,P), CCM(J,PE),
-    CCM(J,NP), CCM(J,PO),
-    CCM(J,L), CCM(J,NGE),
-    CCM(J,NL), CCM(J,GE),
-    CCM(J,LE), CCM(J,NG),
-    CCM(J,NLE), CCM(J,G),
-Mnemonic_CALL,                          // Call Procedure
-
-Mnemonic_ADC,                           // Add with Carry
-Mnemonic_ADD,                           // Add
-Mnemonic_ADDSD,                         // Add Scalar Double-Precision Floating-Point Values
-Mnemonic_ADDSS,                         // Add Scalar Single-Precision Floating-Point Values
-Mnemonic_AND,                           // Logical AND
-
-Mnemonic_BSF,                           // Bit scan forward
-Mnemonic_BSR,                           // Bit scan reverse
-
-Mnemonic_CMC,                           // Complement Carry Flag
-Mnemonic_CWD, Mnemonic_CDQ=Mnemonic_CWD,// Convert Word to Doubleword/Convert Doubleword to Qua T dword
-Mnemonic_CMOVcc,                        // Conditional Move
-    CCM(CMOV,O),
-    CCM(CMOV,NO),
-    CCM(CMOV,B), CCM(CMOV,NAE), CCM(CMOV,C),
-    CCM(CMOV,NB), CCM(CMOV,AE), CCM(CMOV,NC),
-    CCM(CMOV,Z), CCM(CMOV,E),
-    CCM(CMOV,NZ), CCM(CMOV,NE),
-    CCM(CMOV,BE), CCM(CMOV,NA),
-    CCM(CMOV,NBE), CCM(CMOV,A),
-
-    CCM(CMOV,S),
-    CCM(CMOV,NS),
-    CCM(CMOV,P), CCM(CMOV,PE),
-    CCM(CMOV,NP), CCM(CMOV,PO),
-    CCM(CMOV,L), CCM(CMOV,NGE),
-    CCM(CMOV,NL), CCM(CMOV,GE),
-    CCM(CMOV,LE), CCM(CMOV,NG),
-    CCM(CMOV,NLE), CCM(CMOV,G),
-
-Mnemonic_CMP,                           // Compare Two Operands
-Mnemonic_CMPXCHG,                       // Compare and exchange
-Mnemonic_CMPXCHG8B,                     // Compare and Exchange 8 Bytes
-Mnemonic_CMPSB,                         // Compare Two Bytes at DS:ESI and ES:EDI
-Mnemonic_CMPSW,                         // Compare Two Words at DS:ESI and ES:EDI
-Mnemonic_CMPSD,                        // Compare Two Doublewords at DS:ESI and ES:EDI
-//
-// double -> float
-Mnemonic_CVTSD2SS,                      // Convert Scalar Double-Precision Floating-Point Value to Scalar Single-Precision Floating-Point Value
-// double -> I_32
-Mnemonic_CVTSD2SI,                      // Convert Scalar Double-Precision Floating-Point Value to Doubleword Integer
-// double [truncated] -> I_32
-Mnemonic_CVTTSD2SI,                     // Convert with Truncation Scalar Double-Precision Floating-Point Value to Signed Doubleword Integer
-//
-// float -> double
-Mnemonic_CVTSS2SD,                      // Convert Scalar Single-Precision Floating-Point Value to Scalar Double-Precision Floating-Point Value
-// float -> I_32
-Mnemonic_CVTSS2SI,                      // Convert Scalar Single-Precision Floating-Point Value to Doubleword Integer
-// float [truncated] -> I_32
-Mnemonic_CVTTSS2SI,                     // Convert with Truncation Scalar Single-Precision Floating-Point Value to Doubleword Integer
-//
-// I_32 -> double
-Mnemonic_CVTSI2SD,                      // Convert Doubleword Integer to Scalar Double-Precision Floating-Point Value
-// I_32 -> float
-Mnemonic_CVTSI2SS,                      // Convert Doubleword Integer to Scalar Single-Precision Floating-Point Value
-
-Mnemonic_COMISD,                        // Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS
-Mnemonic_COMISS,                        // Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS
-Mnemonic_DEC,                           // Decrement by 1
-//Mnemonic_DIV,                         // Unsigned Divide
-Mnemonic_DIVSD,                         // Divide Scalar Double-Precision Floating-Point Values
-Mnemonic_DIVSS,                         // Divide Scalar Single-Precision Floating-Point Values
-
-#ifdef _HAVE_MMX_
-Mnemonic_EMMS,                          // Empty MMX Technology State
-#endif
-
-Mnemonic_ENTER,                         // ENTER-Make Stack Frame for Procedure Parameters
-Mnemonic_FLDCW,                         // Load FPU control word
-Mnemonic_FADDP,
-Mnemonic_FLDZ,
-Mnemonic_FADD,
-Mnemonic_FSUBP,
-Mnemonic_FSUB,
-Mnemonic_FISUB,
-Mnemonic_FMUL,
-Mnemonic_FMULP,
-Mnemonic_FDIVP,
-Mnemonic_FDIV,
-Mnemonic_FUCOM,
-Mnemonic_FUCOMI,
-Mnemonic_FUCOMP,
-Mnemonic_FUCOMIP,
-Mnemonic_FUCOMPP,
-Mnemonic_FRNDINT,
-Mnemonic_FNSTCW,                        // Store FPU control word
-Mnemonic_FSTSW,                         // Store FPU status word
-Mnemonic_FNSTSW,                         // Store FPU status word
-//Mnemonic_FDECSTP,                     // Decrement Stack-Top Pointer
-Mnemonic_FILD,                          // Load Integer
-Mnemonic_FLD,                           // Load Floating Point Value
-Mnemonic_FLDLG2,
-Mnemonic_FLDLN2,
-Mnemonic_FLD1,
-
-Mnemonic_FCLEX,                         // Clear Exceptions
-Mnemonic_FCHS,                          // Change sign of ST0
-Mnemonic_FNCLEX,                        // Clear Exceptions
-
-//Mnemonic_FINCSTP,                     // Increment Stack-Top Pointer
-Mnemonic_FIST,                          // Store Integer
-Mnemonic_FISTP,                         // Store Integer, pop FPU stack
-Mnemonic_FISTTP,                        // Store Integer with Truncation
-Mnemonic_FPREM,                         // Partial Remainder
-Mnemonic_FPREM1,                        // Partial Remainder
-Mnemonic_FST,                           // Store Floating Point Value
-Mnemonic_FSTP,                          // Store Floating Point Value and pop the FP stack
-Mnemonic_FSQRT,                         //Computes the square root of the source value in the stack and pop the FP stack
-Mnemonic_FABS,                          //Computes the absolute value of the source value in the stack and pop the FP stack
-Mnemonic_FSIN,                          //Computes the sine of the source value in the stack and pop the FP stack
-Mnemonic_FCOS,                          //Computes the cosine of the source value in the stack and pop the FP stack
-Mnemonic_FPTAN,                         //Computes the tangent of the source value in the stack and pop the FP stack
-Mnemonic_FYL2X,
-Mnemonic_FYL2XP1,
-Mnemonic_F2XM1,
-Mnemonic_FPATAN,
-Mnemonic_FXCH,
-Mnemonic_FSCALE,
-
-Mnemonic_XCHG,
-Mnemonic_DIV,                           // Unsigned Divide
-Mnemonic_IDIV,                          // Signed Divide
-Mnemonic_MUL,                           // Unsigned Multiply
-Mnemonic_IMUL,                          // Signed Multiply
-Mnemonic_INC,                           // Increment by 1
-Mnemonic_INT3,                          // Call break point
-
-Mnemonic_LEA,                           // Load Effective Address
-Mnemonic_LEAVE,                         // High Level Procedure Exit
-Mnemonic_LOOP,                          // Loop according to ECX counter
-Mnemonic_LOOPE,                          // Loop according to ECX counter
-Mnemonic_LOOPNE, Mnemonic_LOOPNZ = Mnemonic_LOOPNE, // Loop according to ECX
-Mnemonic_LAHF,                          // Load Flags into AH
-Mnemonic_MOVD,                          // Move Double word
-Mnemonic_MOVQ,                          // Move Quadword
-/*Mnemonic_MOVS,                        // Move Data from String to String*/
-// MOVS is a special case: see encoding table for more details,
-Mnemonic_MOVS8, Mnemonic_MOVS16, Mnemonic_MOVS32, Mnemonic_MOVS64,
-//
-Mnemonic_MOVAPD,                         // Move Scalar Double-Precision Floating-Point Value
-Mnemonic_MOVSD,                         // Move Scalar Double-Precision Floating-Point Value
-Mnemonic_MOVSS,                         // Move Scalar Single-Precision Floating-Point Values
-Mnemonic_MOVSX,                         // Move with Sign-Extension
-Mnemonic_MOVZX,                         // Move with Zero-Extend
-//Mnemonic_MUL,                         // Unsigned Multiply
-Mnemonic_MULSD,                         // Multiply Scalar Double-Precision Floating-Point Values
-Mnemonic_MULSS,                         // Multiply Scalar Single-Precision Floating-Point Values
-Mnemonic_NEG,                           // Two's Complement Negation
-Mnemonic_NOP,                           // No Operation
-Mnemonic_NOT,                           // One's Complement Negation
-Mnemonic_OR,                            // Logical Inclusive OR
-Mnemonic_PREFETCH,                      // prefetch
-
-#if 1 //def _HAVE_MMX_
-    Mnemonic_PADDQ,                     // Add Packed Quadword Integers
-    Mnemonic_PAND,                      // Logical AND
-    Mnemonic_POR,                       // Bitwise Logical OR
-    Mnemonic_PSUBQ,                     // Subtract Packed Quadword Integers
-#endif
-Mnemonic_PANDN,
-Mnemonic_PSLLQ,
-Mnemonic_PSRLQ,
-Mnemonic_PXOR,                          // Logical Exclusive OR
-Mnemonic_POP,                           // Pop a Value from the Stack
-Mnemonic_POPFD,                         // Pop a Value of EFLAGS register from the Stack
-Mnemonic_PUSH,                          // Push Word or Doubleword Onto the Stack
-Mnemonic_PUSHFD,                        // Push EFLAGS Doubleword Onto the Stack
-Mnemonic_RET,                           // Return from Procedure
-
-Mnemonic_SETcc,                         // Set Byte on Condition
-    CCM(SET,O),
-    CCM(SET,NO),
-    CCM(SET,B), CCM(SET,NAE), CCM(SET,C),
-    CCM(SET,NB), CCM(SET,AE), CCM(SET,NC),
-    CCM(SET,Z), CCM(SET,E),
-    CCM(SET,NZ), CCM(SET,NE),
-    CCM(SET,BE), CCM(SET,NA),
-    CCM(SET,NBE), CCM(SET,A),
-    CCM(SET,S),
-    CCM(SET,NS),
-    CCM(SET,P), CCM(SET,PE),
-    CCM(SET,NP), CCM(SET,PO),
-    CCM(SET,L), CCM(SET,NGE),
-    CCM(SET,NL), CCM(SET,GE),
-    CCM(SET,LE), CCM(SET,NG),
-    CCM(SET,NLE), CCM(SET,G),
-
-Mnemonic_SAL, Mnemonic_SHL=Mnemonic_SAL,// Shift left
-Mnemonic_SAR,                           // Unsigned shift right
-Mnemonic_ROR,                           // Rotate right
-Mnemonic_RCR,                           // Rotate right through CARRY flag
-Mnemonic_ROL,                           // Rotate left
-Mnemonic_RCL,                           // Rotate left through CARRY flag
-Mnemonic_SHR,                           // Signed shift right
-Mnemonic_SHRD,                          // Double Precision Shift Right
-Mnemonic_SHLD,                          // Double Precision Shift Left
-
-Mnemonic_SBB,                           // Integer Subtraction with Borrow
-Mnemonic_SUB,                           // Subtract
-Mnemonic_SUBSD,                         // Subtract Scalar Double-Precision Floating-Point Values
-Mnemonic_SUBSS,                         // Subtract Scalar Single-Precision Floating-Point Values
-
-Mnemonic_TEST,                          // Logical Compare
-
-Mnemonic_UCOMISD,                       // Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS
-Mnemonic_UCOMISS,                       // Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS
-
-Mnemonic_XOR,                           // Logical Exclusive OR
-//
-// packed things,
-//
-Mnemonic_XORPD,                         // Bitwise Logical XOR for Double-Precision Floating-Point Values
-Mnemonic_XORPS,                         // Bitwise Logical XOR for Single-Precision Floating-Point Values
-
-Mnemonic_CVTDQ2PD,                      // Convert Packed Doubleword Integers to Packed Double-Precision Floating-Point Values
-Mnemonic_CVTTPD2DQ,                     // Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Doubleword Integers
-
-Mnemonic_CVTDQ2PS,                      // Convert Packed Doubleword Integers to Packed Single-Precision Floating-Point Values
-Mnemonic_CVTTPS2DQ,                     // Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Doubleword Integers
-//
-// String operations
-//
-Mnemonic_STD,                           // Set direction flag
-Mnemonic_CLD,                           // Clear direction flag
-Mnemonic_SCAS,                          // Scan string
-Mnemonic_STOS,                          // Store string
-
-//
-Mnemonic_WAIT,                          // Check pending pending unmasked floating-point exception
-//
-Mnemonic_Count
-} Mnemonic;
-
-#undef CCM
-
-ENCODER_NAMESPACE_END
-
-#endif  // ifndef _ENCODER_DEFS_EXT_H_
diff --git a/vm/compiler/codegen/x86/libenc/enc_prvt.h b/vm/compiler/codegen/x86/libenc/enc_prvt.h
deleted file mode 100644
index 343b161..0000000
--- a/vm/compiler/codegen/x86/libenc/enc_prvt.h
+++ /dev/null
@@ -1,382 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-#ifndef __ENC_PRVT_H_INCLUDED__
-#define __ENC_PRVT_H_INCLUDED__
-
-#include "enc_base.h"
-
-ENCODER_NAMESPACE_START
-/*
- * @file
- * @brief Contains some definitions/constants and other stuff used by the
- *        Encoder internally.
- */
-
-enum OpcodeByteKind {
-    //OpcodeByteKind_Opcode = 0x0000,
-    OpcodeByteKind_ZeroOpcodeByte           = 0x0100,
-    //
-    // The names _SlashR,  _SlahsNum, _ib, _iw, etc
-    // represent the appropriate abbreviations used
-    // in the mnemonic descriptions in the Intel's arch manual.
-    //
-    OpcodeByteKind_SlashR                   = 0x0200,
-    OpcodeByteKind_SlashNum                 = 0x0300,
-    OpcodeByteKind_ib                       = 0x0400,
-    OpcodeByteKind_iw                       = 0x0500,
-    OpcodeByteKind_id                       = 0x0600,
-#ifdef _EM64T_
-    OpcodeByteKind_io                       = 0x0700,
-#endif
-    OpcodeByteKind_cb                       = 0x0800,
-    OpcodeByteKind_cw                       = 0x0900,
-    OpcodeByteKind_cd                       = 0x0A00,
-    //OpcodeByteKind_cp                     = 0x0B00,
-    //OpcodeByteKind_co                     = 0x0C00,
-    //OpcodeByteKind_ct                     = 0x0D00,
-
-    OpcodeByteKind_rb                       = 0x0E00,
-    OpcodeByteKind_rw                       = 0x0F00,
-    OpcodeByteKind_rd                       = 0x1000,
-#ifdef _EM64T_
-    OpcodeByteKind_ro                       = 0x1100,
-    //OpcodeByteKind_REX                    = 0x1200,
-    OpcodeByteKind_REX_W                    = 0x1300,
-#endif
-    OpcodeByteKind_plus_i                   = 0x1400,
-    /**
-        * a special marker, means 'no opcode on the given position'
-        * used in opcodes array, to specify the empty slot, say
-        * to fill an em64t-specific opcode on ia32.
-        * last 'e' made lowercase to avoid a mess with 'F' in
-        * OpcodeByteKind_LAST .
-        */
-    OpcodeByteKind_EMPTY                    = 0xFFFE,
-    /**
-        * a special marker, means 'no more opcodes in the array'
-        * used in in opcodes array to show that there are no more
-        * opcodes in the array for a given mnemonic.
-        */
-    OpcodeByteKind_LAST                     = 0xFFFF,
-    /**
-        * a mask to extract the OpcodeByteKind
-        */
-    OpcodeByteKind_KindMask                 = 0xFF00,
-    /**
-        * a mask to extract the opcode byte when presented
-        */
-    OpcodeByteKind_OpcodeMask               = 0x00FF
-};
-
-#ifdef USE_ENCODER_DEFINES
-
-#define N           {0, 0, 0, 0 }
-#define U           {1, 0, 1, OpndRole_Use }
-#define D           {1, 1, 0, OpndRole_Def }
-#define DU          {1, 1, 1, OpndRole_Def|OpndRole_Use }
-
-#define U_U         {2, 0, 2, OpndRole_Use<<2 | OpndRole_Use }
-#define D_U         {2, 1, 1, OpndRole_Def<<2 | OpndRole_Use }
-#define D_DU        {2, 2, 1, OpndRole_Def<<2 | (OpndRole_Def|OpndRole_Use) }
-#define DU_U        {2, 1, 2, ((OpndRole_Def|OpndRole_Use)<<2 | OpndRole_Use) }
-#define DU_DU       {2, 2, 2, ((OpndRole_Def|OpndRole_Use)<<2 | (OpndRole_Def|OpndRole_Use)) }
-
-#define DU_DU_DU    {3, 3, 3, ((OpndRole_Def|OpndRole_Use)<<4) | ((OpndRole_Def|OpndRole_Use)<<2) | (OpndRole_Def|OpndRole_Use) }
-#define DU_DU_U     {3, 2, 3, (((OpndRole_Def|OpndRole_Use)<<4) | ((OpndRole_Def|OpndRole_Use)<<2) | OpndRole_Use) }
-#define D_DU_U      {3, 2, 2, (((OpndRole_Def)<<4) | ((OpndRole_Def|OpndRole_Use)<<2) | OpndRole_Use) }
-#define D_U_U       {3, 1, 2, (((OpndRole_Def)<<4) | ((OpndRole_Use)<<2) | OpndRole_Use) }
-
-// Special encoding of 0x00 opcode byte. Note: it's all O-s, not zeros.
-#define OxOO        OpcodeByteKind_ZeroOpcodeByte
-
-#define Size16      InstPrefix_OpndSize
-
-#define _r          OpcodeByteKind_SlashR
-
-#define _0          OpcodeByteKind_SlashNum|0
-#define _1          OpcodeByteKind_SlashNum|1
-#define _2          OpcodeByteKind_SlashNum|2
-#define _3          OpcodeByteKind_SlashNum|3
-#define _4          OpcodeByteKind_SlashNum|4
-#define _5          OpcodeByteKind_SlashNum|5
-#define _6          OpcodeByteKind_SlashNum|6
-#define _7          OpcodeByteKind_SlashNum|7
-
-// '+i' for floating-point instructions
-#define _i          OpcodeByteKind_plus_i
-
-
-#define ib          OpcodeByteKind_ib
-#define iw          OpcodeByteKind_iw
-#define id          OpcodeByteKind_id
-
-#define cb          OpcodeByteKind_cb
-#define cw          OpcodeByteKind_cw
-#define cd          OpcodeByteKind_cd
-
-#define rb          OpcodeByteKind_rb
-#define rw          OpcodeByteKind_rw
-#define rd          OpcodeByteKind_rd
-
-#define AL          {OpndKind_GPReg, OpndSize_8, OpndExt_Any, RegName_AL}
-#define AH          {OpndKind_GPReg, OpndSize_8, OpndExt_Any, RegName_AH}
-#define AX          {OpndKind_GPReg, OpndSize_16, OpndExt_Any, RegName_AX}
-#define EAX         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_EAX}
-#ifdef _EM64T_
-    #define RAX     {OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RAX }
-#endif
-
-#define CL          {OpndKind_GPReg, OpndSize_8, OpndExt_Any, RegName_CL}
-#define ECX         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_ECX}
-#ifdef _EM64T_
-    #define RCX         {OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RCX}
-#endif
-
-#define DX          {OpndKind_GPReg, OpndSize_16, OpndExt_Any, RegName_DX}
-#define EDX         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_EDX}
-#ifdef _EM64T_
-    #define RDX     { OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RDX }
-#endif
-
-#define ESI         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_ESI}
-#ifdef _EM64T_
-    #define RSI     { OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RSI }
-#endif
-
-#define EDI         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_EDI}
-#ifdef _EM64T_
-    #define RDI     { OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RDI }
-#endif
-
-#define r8          {OpndKind_GPReg, OpndSize_8, OpndExt_Any, RegName_Null}
-#define r16         {OpndKind_GPReg, OpndSize_16, OpndExt_Any, RegName_Null}
-#define r32         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_Null}
-#ifdef _EM64T_
-    #define r64     { OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_Null }
-#endif
-
-#define r_m8        {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_8, OpndExt_Any, RegName_Null}
-#define r_m16       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_16, OpndExt_Any, RegName_Null}
-#define r_m32       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_32, OpndExt_Any, RegName_Null}
-
-#define r_m8s        {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_8, OpndExt_Signed, RegName_Null}
-#define r_m16s       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_16, OpndExt_Signed, RegName_Null}
-#define r_m32s       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_32, OpndExt_Signed, RegName_Null}
-
-#define r_m8u        {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_8, OpndExt_Zero, RegName_Null}
-#define r_m16u       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_16, OpndExt_Zero, RegName_Null}
-#define r_m32u       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_32, OpndExt_Zero, RegName_Null}
-
-//'m' was only used in LEA mnemonic, but is replaced with
-// set of exact sizes. See more comments for LEA instruction in TheTable.
-//#define m           {OpndKind_Mem, OpndSize_Null, RegName_Null}
-#define m8          {OpndKind_Mem, OpndSize_8, OpndExt_Any, RegName_Null}
-#define m16         {OpndKind_Mem, OpndSize_16, OpndExt_Any, RegName_Null}
-#define m32         {OpndKind_Mem, OpndSize_32, OpndExt_Any, RegName_Null}
-#define m64         {OpndKind_Mem, OpndSize_64, OpndExt_Any, RegName_Null}
-#ifdef _EM64T_
-    #define r_m64   { (OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_64, OpndExt_Any, RegName_Null }
-#endif
-
-#define imm8        {OpndKind_Imm, OpndSize_8, OpndExt_Any, RegName_Null}
-#define imm16       {OpndKind_Imm, OpndSize_16, OpndExt_Any, RegName_Null}
-#define imm32       {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
-
-#define imm8s        {OpndKind_Imm, OpndSize_8, OpndExt_Signed, RegName_Null}
-#define imm16s       {OpndKind_Imm, OpndSize_16, OpndExt_Signed, RegName_Null}
-#define imm32s       {OpndKind_Imm, OpndSize_32, OpndExt_Signed, RegName_Null}
-
-#define imm8u        {OpndKind_Imm, OpndSize_8, OpndExt_Zero, RegName_Null}
-#define imm16u       {OpndKind_Imm, OpndSize_16, OpndExt_Zero, RegName_Null}
-#define imm32u       {OpndKind_Imm, OpndSize_32, OpndExt_Zero, RegName_Null}
-
-#ifdef _EM64T_
-    #define imm64   {OpndKind_Imm, OpndSize_64, OpndExt_Any, RegName_Null }
-#endif
-
-//FIXME: moff-s are in fact memory refs, but presented as immediate.
-// Need to specify this in OpndDesc.
-#define moff8        {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
-#define moff16       {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
-#define moff32       {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
-#ifdef _EM64T_
-    #define moff64       {OpndKind_Imm, OpndSize_64, OpndExt_Any, RegName_Null}
-#endif
-
-
-#define rel8        {OpndKind_Imm, OpndSize_8, OpndExt_Any, RegName_Null}
-#define rel16       {OpndKind_Imm, OpndSize_16, OpndExt_Any, RegName_Null}
-#define rel32       {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
-
-#define mm64        {OpndKind_MMXReg, OpndSize_64, OpndExt_Any, RegName_Null}
-#define mm_m64      {(OpndKind)(OpndKind_MMXReg|OpndKind_Mem), OpndSize_64, OpndExt_Any, RegName_Null}
-
-#define xmm64       {OpndKind_XMMReg, OpndSize_64, OpndExt_Any, RegName_Null}
-#define xmm_m64     {(OpndKind)(OpndKind_XMMReg|OpndKind_Mem), OpndSize_64, OpndExt_Any, RegName_Null}
-
-#define xmm32       {OpndKind_XMMReg, OpndSize_32, OpndExt_Any, RegName_Null}
-#define xmm_m32     {(OpndKind)(OpndKind_XMMReg|OpndKind_Mem), OpndSize_32, OpndExt_Any, RegName_Null}
-
-#define FP0S        {OpndKind_FPReg, OpndSize_32, OpndExt_Any, RegName_FP0S}
-#define FP0D        {OpndKind_FPReg, OpndSize_64, OpndExt_Any, RegName_FP0D}
-#define FP1S        {OpndKind_FPReg, OpndSize_32, OpndExt_Any, RegName_FP1S}
-#define FP1D        {OpndKind_FPReg, OpndSize_64, OpndExt_Any, RegName_FP1D}
-#define fp32        {OpndKind_FPReg, OpndSize_32, OpndExt_Any, RegName_Null}
-#define fp64        {OpndKind_FPReg, OpndSize_64, OpndExt_Any, RegName_Null}
-
-#ifdef _EM64T_
-    #define io      OpcodeByteKind_io
-    #define REX_W   OpcodeByteKind_REX_W
-
-#endif
-
-#endif // USE_ENCODER_DEFINES
-
-/**
- * @brief Represents the REX part of instruction.
- */
-struct  Rex {
-    unsigned char b : 1;
-    unsigned char x : 1;
-    unsigned char r : 1;
-    unsigned char w : 1;
-    unsigned char dummy : 4;        // must be '0100'b
-    unsigned int  :24;
-};
-
-/**
- * @brief Describes SIB (scale,index,base) byte.
- */
-struct SIB {
-    unsigned char base:3;
-    unsigned char index:3;
-    unsigned char scale:2;
-    unsigned int  padding:24;
-};
-/**
- * @brief Describes ModRM byte.
- */
-struct ModRM
-{
-    unsigned char rm:3;
-    unsigned char reg:3;
-    unsigned char mod:2;
-    unsigned int  padding:24;
-};
-
-
-
-/**
-* exactly the same as EncoderBase::OpcodeDesc, but also holds info about
-* platform on which the opcode is applicable.
-*/
-struct OpcodeInfo {
-    enum platform {
-        /// an opcode is valid on all platforms
-        all,
-        // opcode is valid on IA-32 only
-        em64t,
-        // opcode is valid on Intel64 only
-        ia32,
-        // opcode is added for the sake of disassembling, should not be used in encoding
-        decoder,
-        // only appears in master table, replaced with 'decoder' in hashed version
-        decoder32,
-        // only appears in master table, replaced with 'decoder' in hashed version
-        decoder64,
-    };
-    platform                        platf;
-    unsigned                        opcode[4+1+1];
-    EncoderBase::OpndDesc           opnds[EncoderBase::MAX_NUM_OPCODE_OPERANDS];
-    EncoderBase::OpndRolesDesc      roles;
-};
-
-/**
- * @defgroup MF_ Mnemonic flags
-*/
-
-    /**
- * Operation has no special properties.
-    */
-#define MF_NONE             (0x00000000)
-    /**
- * Operation affects flags
-    */
-#define MF_AFFECTS_FLAGS    (0x00000001)
-    /**
- * Operation uses flags - conditional operations, ADC/SBB/ETC
-    */
-#define MF_USES_FLAGS       (0x00000002)
-    /**
- * Operation is conditional - MOVcc/SETcc/Jcc/ETC
-    */
-#define MF_CONDITIONAL      (0x00000004)
-/**
- * Operation is symmetric - its args can be swapped (ADD/MUL/etc).
- */
-#define MF_SYMMETRIC        (0x00000008)
-/**
- * Operation is XOR-like - XOR, SUB - operations of 'arg,arg' is pure def,
- * without use.
- */
-#define MF_SAME_ARG_NO_USE  (0x00000010)
-
-///@} // ~MNF
-
-/**
- * @see same structure as EncoderBase::MnemonicDesc, but carries
- * MnemonicInfo::OpcodeInfo[] instead of OpcodeDesc[].
- * Only used during prebuilding the encoding tables, thus it's hidden under
- * the appropriate define.
- */
-struct MnemonicInfo {
-    /**
-    * The mnemonic itself
-    */
-    Mnemonic    mn;
-    /**
-     * Various characteristics of mnemonic.
-     * @see MF_
-     */
-    unsigned    flags;
-    /**
-     * Number of args/des/uses/roles for the operation. For the operations
-     * which may use different number of operands (i.e. IMUL/SHL) use the
-     * most common value, or leave '0' if you are sure this info is not
-     * required.
-     */
-    EncoderBase::OpndRolesDesc              roles;
-    /**
-     * Print name of the mnemonic
-     */
-    const char *                            name;
-    /**
-     * Array of opcodes.
-     * The terminating opcode description always have OpcodeByteKind_LAST
-     * at the opcodes[i].opcode[0].
-     * The size of '25' has nothing behind it, just counted the max
-     * number of opcodes currently used (MOV instruction).
-     */
-    OpcodeInfo                              opcodes[25];
-};
-
-ENCODER_NAMESPACE_END
-
-#endif  // ~__ENC_PRVT_H_INCLUDED__
diff --git a/vm/compiler/codegen/x86/libenc/enc_tabl.cpp b/vm/compiler/codegen/x86/libenc/enc_tabl.cpp
deleted file mode 100644
index af20bd8..0000000
--- a/vm/compiler/codegen/x86/libenc/enc_tabl.cpp
+++ /dev/null
@@ -1,1969 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-
-
-#include <assert.h>
-#include <stdio.h>
-#include <stdlib.h> //qsort
-#include <string.h>
-#include <memory.h>
-#include <errno.h>
-#include <stdlib.h>
-
-
-// need to use EM64T-specifics - new registers, defines from enc_prvt, etc...
-#if !defined(_EM64T_)
-    #define UNDEF_EM64T
-    #define _EM64T_
-#endif
-
-#define USE_ENCODER_DEFINES
-#include "enc_prvt.h"
-#include "enc_defs.h"
-
-#ifdef UNDEF_EM64T
-    #undef _EM64T_
-#endif
-
-//Android x86
-#if 0 //!defined(_HAVE_MMX_)
-    #define Mnemonic_PADDQ  Mnemonic_Null
-    #define Mnemonic_PAND   Mnemonic_Null
-    #define Mnemonic_POR    Mnemonic_Null
-    #define Mnemonic_PSUBQ  Mnemonic_Null
-#endif
-
-ENCODER_NAMESPACE_START
-
-
-EncoderBase::MnemonicDesc EncoderBase::mnemonics[Mnemonic_Count];
-EncoderBase::OpcodeDesc EncoderBase::opcodes[Mnemonic_Count][MAX_OPCODES];
-unsigned char EncoderBase::opcodesHashMap[Mnemonic_Count][HASH_MAX];
-
-
-/**
- * @file
- * @brief 'Master' copy of encoding data.
- */
-
-/*
-This file contains a 'master copy' of encoding table - this is the info used
-by both generator of native instructions (EncoderBase class) and by
-disassembling routines. The first one uses an info how to encode the
-instruction, and the second does an opposite - several separate tables are
-built at runtime from this main table.
-
-=============================================================================
-
-The table was designed for easy support and maintenance. Thus, it was made as
-much close as possible to the Intel's IA32 Architecture Manual descriptions.
-The info is based on the latest (at the moment of writing) revision which is
-June 2005, order number 253666-016.
-
-Normally, almost all of opcodes in the 'master' table represented exactly as
-they are shown in the Intel's Architecture manual (well, with slashes
-replaced with underscore). There are several exclusions especially marked.
-
-Normally, to add an opcode/instruction, one only need to copy the whole
-string from the manual, and simply replace '/' with '_'.
-
-I.e., TheManual reads for DEC:
-    (1)     FE /1 DEC r/m8 Valid Valid Decrement r/m8 by 1.
-    (2)     REX + FE /1 DEC r/m8* Valid N.E. Decrement r/m8 by 1.
-    (3)     REX.W + FF /1 DEC r/m64 Valid N.E. Decrement r/m64 by 1.
-
-1. Note, that there is no need to explicitly specify REX-based opcodes for
-    instruction to handle additional registers on EM64T:
-
-    (1)     FE /1 DEC r/m8 Valid Valid Decrement r/m8 by 1.
-    (3)     REX.W + FF /1 DEC r/m64 Valid N.E. Decrement r/m64 by 1.
-
-2. Copy the string, strip off the text comments, replace '/'=>'_'. Note, that
-    the second line is for EM64T only
-
-    (1)     FE /1 DEC r/m8
-    (3)     REX.W + FF /1 DEC r/m64
-
-3. Fill out the mnemonic, opcode parameters parts
-
-    BEGIN_MNEMONIC(DEC, MF_AFFECTS_FLAGS, DU)
-    BEGIN_OPCODES()
-        {OpcodeInfo::all,   {0xFE, _1},         {r_m8},         DU },
-        {OpcodeInfo::em64t, {REX_W, 0xFF, _1},  {r_m64},        DU },
-
-    DU here - one argument, it's used and defined
-
-4. That's it, that simple !
-
-The operand roles (DU here) are used by Jitrino's optimizing engine to
-perform data flow analysis. It also used to store/obtain number of operands.
-
-Special cases are (see the table for details):
-LEA
-Some FPU operations (i.e. FSTP)
-packed things (XORPD, XORPS, CVTDQ2PD, CVTTPD2DQ)
-
-Also, the Jitrino's needs require to specify all operands - including
-implicit ones (see IMUL).
-
-The master table iself does not need to be ordered - it's get sorted before
-processing. It's recommended (though it's not a law) to group similar
-instructions together - i.e. FPU instructions, MMX, etc.
-
-=============================================================================
-
-The encoding engine builds several tables basing on the 'master' one (here
-'mnemonic' is a kind of synonim for 'instruction'):
-
-- list of mnemonics which holds general info about instructions
-    (EncoderBase::mnemonics)
-- an array of opcodes descriptions (EncodeBase::opcodes)
-- a mapping between a hash value and an opcode description record for a given
-    mnemonic (EncoderBase::opcodesHashMap)
-
-The EncoderBase::mnemonics holds general info about instructions.
-The EncoderBase::opcodesHashMap is used for fast opcode selection basing on
-a hash value.
-The EncodeBase::opcodes is used for the encoding itself.
-
-=============================================================================
-The hash value is calculated and used as follows:
-
-JIT-ted code uses the following operand sizes: 8-, 16-, 32- and 64-bits and
-size for an operand can be encoded in just 2 bits.
-
-The following operand locations are available: one of registers - GP, FP,
-MMX, XMM (not taking segment registers), a memory and an immediate, which
-gives us 6 variants and can be enumerated in 3 bits.
-
-As a grand total, the the whole operand's info needed for opcode selection
-can be packed in 5 bits. Taking into account the IMUL mnemonic with its 3
-operands (including implicit ones), we're getting 15 bits per instruction and
-the complete table is about 32768 items per single instruction.
-
-Seems too many, but luckily, the 15 bit limit will never be reached: the
-worst case is IMUL with its 3 operands:
-(IMUL r64, r/m64, imm32)/(IMUL r32, r/m32, imm32).
-So, assigning lowest value to GP register, the max value of hash can be
-reduced.
-
-The hash values to use are:
-sizes:
-        8               -> 11
-        16              -> 10
-        32              -> 01
-        64              -> 00
-locations:
-        gp reg          -> 000
-        memory          -> 001
-        fp reg          -> 010
-        mmx reg         -> 011
-        xmm reg         -> 100
-        immediate       -> 101
-and the grand total for the worst case would be
-[ GP 32] [GP  32] [Imm 32]
-[000-01] [000-01] [101 01] = 1077
-
-However, the implicit operands adds additional value, and the worstest case
-is 'SHLD r_m32, r32, CL=r8'. This gives us the maximum number of:
-
-[mem 32] [GP  32] [GP  8b]
-[001-01] [000-01] [000-11] = 5155.
-
-The max number is pretty big and the hash functions is quite rare, thus it
-is not resonable to use a direct addressing i.e.
-OpcodeDesc[mnemonic][hash_code] - there would be a huge waste of space.
-
-Instead, we use a kind of mapping: the opcodes info is stored in packed
-(here: non rare) array. The max number of opcodes will not exceed 255 for
-each instruction. And we have an index array in which we store a mapping
-between a hash code value and opcode position for each given instruction.
-
-Sounds a bit sophisticated, but in real is simple, the opcode gets selected
-in 2 simple steps:
-
-1. Select [hash,mnemonic] => 'n'.
-
-The array is pretty rare - many cells contain 0xFF which
-means 'invalid hash - no opcode with given characteristics'
-
-char EnbcoderBase::opcodesHashMap[Mnemonic_Count][HASH_MAX] =
-
-+----+----+----+----+----+----+
-| 00 | 05 | FF | FF | 03 | 12 | ...
-|---------+-------------------+
-| 12 | FF | FF |  n | 04 | 25 | ...   <- Mnemonic
-|-----------------------------+
-| FF | 11 | FF | 10 | 13 | .. | ...
-+-----------------------------+
-     ...         ^
-                 |
-                hash
-
-2. Select [n,mnemonic] => 'opcode_desc11'
-
-OpcodeDesc      EncoderBase::opcodes[Mnemonic_Count][MAX_OPCODES] =
-
-+---------------+---------------+---------------+---------------+
-| opcode_desc00 | opcode_desc01 | opcode_desc02 | last_opcode   | ...
-+---------------+---------------+---------------+---------------+
-| opcode_desc10 | opcode_desc11 | last_opcode   | xxx           | <- Mnemonic
-+---------------+---------------+---------------+---------------+
-| opcode_desc20 | opcode_desc21 | opcode_desc22 | opcode_desc23 | ...
-+---------------+---------------+---------------+---------------+
-     ...
-                      ^
-                      |
-                      n
-
-Now, use 'opcode_desc11'.
-
-=============================================================================
-The array of opcodes descriptions (EncodeBase::opcodes) is specially prepared
-to maximize performance - the EncoderBase::encode() is quite hot on client
-applications for the Jitrino/Jitrino.JET.
-The preparation is that opcode descriptions from the 'master' encoding table
-are preprocessed and a special set of OpcodeDesc prepared:
-First, the 'raw' opcode bytes are extracted. Here, 'raw' means the bytes that
-do not depened on any operands values, do not require any analysis and can be
-simply copied into the output buffer during encoding. Also, number of these
-'raw' bytes is counted. The fields are OpcodeDesc::opcode and
-OpcodeDesc::opcode_len.
-
-Then the fisrt non-implicit operand found and its index is stored in
-OpcodeDesc::first_opnd.
-
-The bytes that require processing and analysis ('/r', '+i', etc) are
-extracted and stored in OpcodeDesc::aux0 and OpcodeDesc::aux1 fields.
-
-Here, a special trick is performed:
-    Some opcodes have register/memory operand, but this is not reflected in
-    opcode column - for example, (MOVQ xmm64, xmm_m64). In this case, a fake
-    '_r' added to OpcodeDesc::aux field.
-    Some other opcodes have immediate operands, but this is again not
-    reflected in opcode column - for example, CALL cd or PUSH imm32.
-    In this case, a fake '/cd' or fake '/id' added to appropriate
-    OpcodeDesc::aux field.
-
-The OpcodeDesc::last is non-zero for the final OpcodeDesc record (which does
-not have valid data itself).
-*/
-
-// TODO: To extend flexibility, replace bool fields in MnemonicDesc &
-// MnemonicInfo with a set of flags packed into integer field.
-
-unsigned short EncoderBase::getHash(const OpcodeInfo* odesc)
-{
-    /*
-    NOTE: any changes in the hash computation must be stricty balanced with
-    EncoderBase::Operand::hash_it and EncoderBase::Operands()
-    */
-    unsigned short hash = 0;
-    // The hash computation, uses fast way - table selection instead of if-s.
-    if (odesc->roles.count > 0) {
-        OpndKind kind = odesc->opnds[0].kind;
-        OpndSize size = odesc->opnds[0].size;
-        assert(kind<COUNTOF(kind_hash));
-        assert(size<COUNTOF(size_hash));
-        hash = get_kind_hash(kind) | get_size_hash(size);
-    }
-
-    if (odesc->roles.count > 1) {
-        OpndKind kind = odesc->opnds[1].kind;
-        OpndSize size = odesc->opnds[1].size;
-        assert(kind<COUNTOF(kind_hash));
-        assert(size<COUNTOF(size_hash));
-        hash = (hash<<HASH_BITS_PER_OPERAND) |
-               (get_kind_hash(kind) | get_size_hash(size));
-    }
-
-    if (odesc->roles.count > 2) {
-        OpndKind kind = odesc->opnds[2].kind;
-        OpndSize size = odesc->opnds[2].size;
-        assert(kind<COUNTOF(kind_hash));
-        assert(size<COUNTOF(size_hash));
-        hash = (hash<<HASH_BITS_PER_OPERAND) |
-            (get_kind_hash(kind) | get_size_hash(size));
-    }
-    assert(hash <= HASH_MAX);
-    return hash;
-}
-
-
-#define BEGIN_MNEMONIC(mn, flags, roles)     \
-        { Mnemonic_##mn, flags, roles, #mn,
-#define END_MNEMONIC() },
-#define BEGIN_OPCODES() {
-#define END_OPCODES()   { OpcodeInfo::all, {OpcodeByteKind_LAST}, {}, {0, 0, 0, 0}}}
-
-
-static MnemonicInfo masterEncodingTable[] = {
-//
-// Null
-//
-BEGIN_MNEMONIC(Null, MF_NONE, N)
-BEGIN_OPCODES()
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(LAHF, MF_USES_FLAGS, D)
-BEGIN_OPCODES()
-// TheManual says it's not always supported in em64t mode, thus excluding it
-    {OpcodeInfo::ia32,    {0x9F},         {EAX}, D },
-END_OPCODES()
-END_MNEMONIC()
-//
-// ALU mnemonics - add, adc, or, xor, and, cmp, sub, sbb
-// as they differ only in the opcode extention (/digit) number and
-// in which number the opcode start from, the opcode definitions
-// for those instructions are packed together
-//
-// The 'opcode_starts_from' and 'opcode_ext' in DEFINE_ALU_OPCODES()
-// are enough to define OpcodeInfo::all opcodes and the 'first_opcode'
-// parameter is only due to ADD instruction, which requires an zero opcode
-// byte which, in turn, is coded especially in the current coding scheme.
-//
-
-#define DEFINE_ALU_OPCODES( opc_ext, opcode_starts_from, first_opcode, def_use ) \
-\
-    {OpcodeInfo::decoder,   {opcode_starts_from + 4, ib},           {AL,    imm8},  DU_U },\
-    {OpcodeInfo::decoder,   {Size16, opcode_starts_from + 5, iw},   {AX,    imm16}, DU_U },\
-    {OpcodeInfo::decoder,   {opcode_starts_from + 5, id},           {EAX,   imm32}, DU_U },\
-    {OpcodeInfo::decoder64, {REX_W, opcode_starts_from+5, id},      {RAX,   imm32s},DU_U },\
-\
-    {OpcodeInfo::all,       {0x80, opc_ext, ib},          {r_m8,  imm8},    def_use },\
-    {OpcodeInfo::all,       {Size16, 0x81, opc_ext, iw},  {r_m16, imm16},   def_use },\
-    {OpcodeInfo::all,       {0x81, opc_ext, id},          {r_m32, imm32},   def_use },\
-    {OpcodeInfo::em64t,     {REX_W, 0x81, opc_ext, id},   {r_m64, imm32s},  def_use },\
-\
-    {OpcodeInfo::all,       {Size16, 0x83, opc_ext, ib},  {r_m16, imm8s},   def_use },\
-    {OpcodeInfo::all,       {0x83, opc_ext, ib},          {r_m32, imm8s},   def_use },\
-    {OpcodeInfo::em64t,     {REX_W, 0x83, opc_ext, ib},   {r_m64, imm8s},   def_use },\
-\
-    {OpcodeInfo::all,       {first_opcode,  _r},          {r_m8,  r8},      def_use },\
-\
-    {OpcodeInfo::all,       {Size16, opcode_starts_from+1,  _r},  {r_m16, r16},   def_use },\
-    {OpcodeInfo::all,       {opcode_starts_from+1,  _r},  {r_m32, r32},   def_use },\
-    {OpcodeInfo::em64t,     {REX_W, opcode_starts_from+1, _r},    {r_m64, r64},   def_use },\
-\
-    {OpcodeInfo::all,       {opcode_starts_from+2,  _r},  {r8,    r_m8},  def_use },\
-\
-    {OpcodeInfo::all,       {Size16, opcode_starts_from+3,  _r},  {r16,   r_m16}, def_use },\
-    {OpcodeInfo::all,       {opcode_starts_from+3,  _r},  {r32,   r_m32}, def_use },\
-    {OpcodeInfo::em64t,     {REX_W, opcode_starts_from+3, _r},    {r64,   r_m64}, def_use },
-
-BEGIN_MNEMONIC(ADD, MF_AFFECTS_FLAGS|MF_SYMMETRIC, DU_U)
-BEGIN_OPCODES()
-    DEFINE_ALU_OPCODES(_0, 0x00, OxOO, DU_U )
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(OR, MF_AFFECTS_FLAGS|MF_SYMMETRIC, DU_U)
-BEGIN_OPCODES()
-    DEFINE_ALU_OPCODES(_1, 0x08, 0x08, DU_U )
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(ADC, MF_AFFECTS_FLAGS|MF_USES_FLAGS|MF_SYMMETRIC, DU_U)
-BEGIN_OPCODES()
-    DEFINE_ALU_OPCODES(_2, 0x10, 0x10, DU_U )
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(SBB, MF_AFFECTS_FLAGS|MF_USES_FLAGS, DU_U)
-BEGIN_OPCODES()
-    DEFINE_ALU_OPCODES(_3, 0x18, 0x18, DU_U )
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(AND, MF_AFFECTS_FLAGS|MF_SYMMETRIC, DU_U)
-BEGIN_OPCODES()
-    DEFINE_ALU_OPCODES(_4, 0x20, 0x20, DU_U )
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(SUB, MF_AFFECTS_FLAGS|MF_SAME_ARG_NO_USE, DU_U)
-BEGIN_OPCODES()
-    DEFINE_ALU_OPCODES(_5, 0x28, 0x28, DU_U )
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(XOR, MF_AFFECTS_FLAGS|MF_SYMMETRIC|MF_SAME_ARG_NO_USE, DU_U)
-BEGIN_OPCODES()
-    DEFINE_ALU_OPCODES( _6, 0x30, 0x30, DU_U )
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CMP, MF_AFFECTS_FLAGS, U_U)
-BEGIN_OPCODES()
-    DEFINE_ALU_OPCODES( _7, 0x38, 0x38, U_U )
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CMPXCHG, MF_AFFECTS_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0xB0, _r},           {r_m8, r8, AL},     DU_DU_DU },
-    {OpcodeInfo::all,   {Size16, 0x0F, 0xB1, _r},   {r_m16, r16, AX},   DU_DU_DU },
-    {OpcodeInfo::all,   {0x0F, 0xB1, _r},           {r_m32, r32, EAX},  DU_DU_DU},
-    {OpcodeInfo::em64t, {REX_W, 0x0F, 0xB1, _r},    {r_m64, r64, RAX},  DU_DU_DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CMPXCHG8B, MF_AFFECTS_FLAGS, D)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0xC7, _1},         {m64},     DU },
-END_OPCODES()
-END_MNEMONIC()
-
-#undef DEFINE_ALU_OPCODES
-//
-//
-//
-BEGIN_MNEMONIC(ADDSD, MF_NONE, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF2, 0x0F, 0x58, _r},   {xmm64, xmm_m64},   DU_U},
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(ADDSS, MF_NONE, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0x58, _r},   {xmm32, xmm_m32},   DU_U},
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(BSF, MF_AFFECTS_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0xBC},   {r32, r_m32},   D_U},
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(BSR, MF_AFFECTS_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0xBD},   {r32, r_m32},   D_U},
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(CALL, MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xE8, cd},        {rel32},     U },
-    {OpcodeInfo::ia32,  {Size16, 0xE8, cw}, {rel16},    U },
-    {OpcodeInfo::ia32,  {0xFF, _2},        {r_m32},     U },
-    {OpcodeInfo::em64t, {0xFF, _2},        {r_m64},     U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CMC, MF_USES_FLAGS|MF_AFFECTS_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::decoder,   {0xF5},         {},     N },
-END_OPCODES()
-END_MNEMONIC()
-
-//TODO: Workaround. Actually, it's D_DU, but Jitrino's CG thinks it's D_U
-BEGIN_MNEMONIC(CDQ, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,       {0x99},         {DX, AX},       D_U },
-    {OpcodeInfo::all,       {0x99},         {EDX, EAX},     D_U },
-    {OpcodeInfo::em64t,     {REX_W, 0x99},  {RDX, RAX},     D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-#define DEFINE_CMOVcc_MNEMONIC( cc ) \
-        BEGIN_MNEMONIC(CMOV##cc, MF_USES_FLAGS|MF_CONDITIONAL, DU_U ) \
-BEGIN_OPCODES() \
-    {OpcodeInfo::all,   {Size16, 0x0F, 0x40 + ConditionMnemonic_##cc, _r},  {r16, r_m16},   DU_U }, \
-    {OpcodeInfo::all,   {0x0F, 0x40 + ConditionMnemonic_##cc, _r},          {r32, r_m32},   DU_U }, \
-    {OpcodeInfo::em64t, {REX_W, 0x0F, 0x40 + ConditionMnemonic_##cc, _r},   {r64, r_m64},   DU_U }, \
-END_OPCODES() \
-END_MNEMONIC()
-
-DEFINE_CMOVcc_MNEMONIC(O)
-DEFINE_CMOVcc_MNEMONIC(NO)
-DEFINE_CMOVcc_MNEMONIC(B)
-DEFINE_CMOVcc_MNEMONIC(NB)
-DEFINE_CMOVcc_MNEMONIC(Z)
-DEFINE_CMOVcc_MNEMONIC(NZ)
-DEFINE_CMOVcc_MNEMONIC(BE)
-DEFINE_CMOVcc_MNEMONIC(NBE)
-DEFINE_CMOVcc_MNEMONIC(S)
-DEFINE_CMOVcc_MNEMONIC(NS)
-DEFINE_CMOVcc_MNEMONIC(P)
-DEFINE_CMOVcc_MNEMONIC(NP)
-DEFINE_CMOVcc_MNEMONIC(L)
-DEFINE_CMOVcc_MNEMONIC(NL)
-DEFINE_CMOVcc_MNEMONIC(LE)
-DEFINE_CMOVcc_MNEMONIC(NLE)
-
-#undef DEFINE_CMOVcc_MNEMONIC
-
-/*****************************************************************************
-                                ***** SSE conversion routines *****
-*****************************************************************************/
-//
-// double -> float
-BEGIN_MNEMONIC(CVTSD2SS, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF2, 0x0F, 0x5A, _r},   {xmm32, xmm_m64}, D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-// double -> I_32
-BEGIN_MNEMONIC(CVTSD2SI, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF2, 0x0F, 0x2D, _r},         {r32, xmm_m64}, D_U },
-    {OpcodeInfo::em64t, {REX_W, 0xF2, 0x0F, 0x2D, _r},  {r64, xmm_m64}, D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-// double [truncated] -> I_32
-BEGIN_MNEMONIC(CVTTSD2SI, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF2, 0x0F, 0x2C, _r},         {r32, xmm_m64}, D_U },
-    {OpcodeInfo::em64t, {REX_W, 0xF2, 0x0F, 0x2C, _r},  {r64, xmm_m64}, D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-// float -> double
-BEGIN_MNEMONIC(CVTSS2SD, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0x5A, _r},         {xmm64, xmm_m32}, D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-// float -> I_32
-BEGIN_MNEMONIC(CVTSS2SI, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0x2D, _r},         {r32, xmm_m32}, D_U},
-    {OpcodeInfo::em64t, {REX_W, 0xF3, 0x0F, 0x2D, _r},  {r64, xmm_m32}, D_U},
-END_OPCODES()
-END_MNEMONIC()
-
-// float [truncated] -> I_32
-BEGIN_MNEMONIC(CVTTSS2SI, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0x2C, _r},         {r32, xmm_m32}, D_U},
-    {OpcodeInfo::em64t, {REX_W, 0xF3, 0x0F, 0x2C, _r},  {r64, xmm_m32}, D_U},
-END_OPCODES()
-END_MNEMONIC()
-
-// I_32 -> double
-BEGIN_MNEMONIC(CVTSI2SD, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF2, 0x0F, 0x2A, _r},         {xmm64, r_m32}, D_U},
-    {OpcodeInfo::em64t, {REX_W, 0xF2, 0x0F, 0x2A, _r},  {xmm64, r_m64}, D_U},
-END_OPCODES()
-END_MNEMONIC()
-
-// I_32 -> float
-BEGIN_MNEMONIC(CVTSI2SS, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0x2A, _r},         {xmm32, r_m32}, D_U},
-    {OpcodeInfo::em64t, {REX_W, 0xF3, 0x0F, 0x2A, _r},  {xmm32, r_m64}, D_U},
-END_OPCODES()
-END_MNEMONIC()
-
-//
-// ~ SSE conversions
-//
-
-BEGIN_MNEMONIC(DEC, MF_AFFECTS_FLAGS, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xFE, _1},         {r_m8},     DU },
-
-    {OpcodeInfo::all,   {Size16, 0xFF, _1}, {r_m16},    DU },
-    {OpcodeInfo::all,   {0xFF, _1},         {r_m32},    DU },
-    {OpcodeInfo::em64t, {REX_W, 0xFF, _1},  {r_m64},    DU },
-
-    {OpcodeInfo::ia32,  {Size16, 0x48|rw},  {r16},      DU },
-    {OpcodeInfo::ia32,  {0x48|rd},          {r32},      DU },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(DIVSD, MF_NONE, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all, {0xF2, 0x0F, 0x5E, _r},   {xmm64, xmm_m64},   DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(DIVSS, MF_NONE, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all, {0xF3, 0x0F, 0x5E, _r},   {xmm32, xmm_m32},   DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-/****************************************************************************
-                 ***** FPU operations *****
-****************************************************************************/
-
-BEGIN_MNEMONIC(FADDP, MF_NONE, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDE, 0xC1},       {FP0D}, DU },
-    {OpcodeInfo::all,   {0xDE, 0xC1},       {FP0S}, DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FLDZ,  MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, 0xEE},   {FP0D}, D },
-    {OpcodeInfo::all,   {0xD9, 0xEE},   {FP0S}, D },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FADD,  MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDC, _0},     {FP0D, m64}, DU_U },
-    {OpcodeInfo::all,   {0xD8, _0},     {FP0S, m32}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FSUBP, MF_NONE, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDE, 0xE9},   {FP0D}, DU },
-    {OpcodeInfo::all,   {0xDE, 0xE9},   {FP0S}, DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FSUB,   MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDC, _4},     {FP0D, m64}, DU_U },
-    {OpcodeInfo::all,   {0xD8, _4},     {FP0S, m32}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FISUB,   MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDA, _4},       {FP0S, m32}, DU_U },
-//    {OpcodeInfo::all,   {0xDE, _4},       {FP0S, m16}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-
-BEGIN_MNEMONIC(FMUL,   MF_NONE, DU_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD8, _1},     {FP0S, m32}, DU_U },
-    {OpcodeInfo::all,   {0xDC, _1},     {FP0D, m64}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FMULP, MF_NONE, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDE, 0xC9},   {FP0D}, DU },
-    {OpcodeInfo::all,   {0xDE, 0xC9},   {FP0S}, DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FDIVP, MF_NONE, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDE, 0xF9},   {FP0D}, DU },
-    {OpcodeInfo::all,   {0xDE, 0xF9},   {FP0S}, DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FDIV,   MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDC, _6},     {FP0D, m64}, DU_U },
-    {OpcodeInfo::all,   {0xD8, _6},     {FP0S, m32}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(FUCOM, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDD, 0xE1},         {FP0D, FP1D},    DU_U },
-    {OpcodeInfo::all,   {0xDD, 0xE1},         {FP0S, FP1S},    DU_U },
-    // A little trick: actually, these 2 opcodes take only index of the
-    // needed register. To make the things similar to other instructions
-    // we encode here as if they took FPREG.
-    {OpcodeInfo::all,   {0xDD, 0xE0|_i},    {fp32},         DU },
-    {OpcodeInfo::all,   {0xDD, 0xE0|_i},    {fp64},         DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FUCOMI, MF_NONE, D_U )
-BEGIN_OPCODES()
-    // A little trick: actually, these 2 opcodes take only index of the
-    // needed register. To make the things similar to other instructions
-    // we encode here as if they took FPREG.
-    {OpcodeInfo::all,   {0xDB, 0xE8|_i},    {fp32},         DU },
-    {OpcodeInfo::all,   {0xDB, 0xE8|_i},    {fp64},         DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FUCOMP, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDD, 0xE9},             {FP0D, FP1D},    DU_U },
-    {OpcodeInfo::all,   {0xDD, 0xE9},             {FP0S, FP1S},    DU_U },
-    // A little trick: actually, these 2 opcodes take only index of the
-    // needed register. To make the things similar to other instructions
-    // we encode here as if they took FPREG.
-    {OpcodeInfo::all,   {0xDD, 0xE8|_i},        {fp32},         DU },
-    {OpcodeInfo::all,   {0xDD, 0xE8|_i},        {fp64},         DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FUCOMIP, MF_NONE, D_U )
-BEGIN_OPCODES()
-    // A little trick: actually, these 2 opcodes take only index of the
-    // needed register. To make the things similar to other instructions
-    // we encode here as if they took FPREG.
-    {OpcodeInfo::all,   {0xDF, 0xE8|_i},        {fp32},         DU },
-    {OpcodeInfo::all,   {0xDF, 0xE8|_i},        {fp64},         DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FUCOMPP, MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDA, 0xE9},   {FP0D, FP1D}, DU_U },
-    {OpcodeInfo::all,   {0xDA, 0xE9},   {FP0S, FP1S}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FLDCW, MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, _5},     {m16},  U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FNSTCW, MF_NONE, D)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, _7},     {m16},  D },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FSTSW, MF_NONE, D)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x9B, 0xDF, 0xE0}, {EAX},  D },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FNSTSW, MF_NONE, D)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDF, 0xE0},   {EAX},  D },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FCHS, MF_NONE, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, 0xE0},   {FP0D}, DU },
-    {OpcodeInfo::all,   {0xD9, 0xE0},   {FP0S}, DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FCLEX, MF_NONE, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x9B, 0xDB, 0xE2}, {}, N },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FNCLEX, MF_NONE, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDB, 0xE2},       {}, N },
-END_OPCODES()
-END_MNEMONIC()
-
-//BEGIN_MNEMONIC(FDECSTP, MF_NONE, N)
-//  BEGIN_OPCODES()
-//          {OpcodeInfo::all, {0xD9, 0xF6},       {},     N },
-//  END_OPCODES()
-//END_MNEMONIC()
-
-BEGIN_MNEMONIC(FILD, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDB, _0}, {FP0S, m32},    D_U },
-    {OpcodeInfo::all,   {0xDF, _5}, {FP0D, m64},    D_U },
-    {OpcodeInfo::all,   {0xDB, _0}, {FP0S, m32},    D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-//BEGIN_MNEMONIC(FINCSTP, MF_NONE, N)
-//  BEGIN_OPCODES()
-//          {OpcodeInfo::all, {0xD9, 0xF7},       {},     N },
-//  END_OPCODES()
-//END_MNEMONIC()
-
-BEGIN_MNEMONIC(FIST, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDB, _2}, {m32, FP0S},    D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FISTP, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDB, _3}, {m32, FP0S},    D_U },
-    {OpcodeInfo::all,   {0xDF, _7}, {m64, FP0D},    D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FISTTP, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xDD, _1}, {m64, FP0D},    D_U },
-    {OpcodeInfo::all,   {0xDB, _1}, {m32, FP0S},    D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FRNDINT, MF_NONE, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, 0xFC}, {FP0S},    DU },
-    {OpcodeInfo::all,   {0xD9, 0xFC}, {FP0D},    DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FLD, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, _0}, {FP0S, m32},    D_U },
-    {OpcodeInfo::all,   {0xDD, _0}, {FP0D, m64},    D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FLDLG2, MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, 0xEC}, {FP0S},    D },
-    {OpcodeInfo::all,   {0xD9, 0xEC}, {FP0D},    D },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FLDLN2, MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, 0xED}, {FP0S},    D },
-    {OpcodeInfo::all,   {0xD9, 0xED}, {FP0D},    D },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FLD1, MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, 0xE8}, {FP0S},    D },
-    {OpcodeInfo::all,   {0xD9, 0xE8}, {FP0D},    D },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(FPREM, MF_NONE, N)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xF8},       {},     N },
-  END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FPREM1, MF_NONE, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, 0xF5},       {},     N },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FST, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, _2},         {m32, FP0S},    D_U },
-    {OpcodeInfo::all,   {0xDD, _2},         {m64, FP0D},    D_U },
-    // A little trick: actually, these 2 opcodes take only index of the
-    // needed register. To make the things similar to other instructions
-    // we encode here as if they took FPREG.
-    {OpcodeInfo::all,   {0xDD, 0xD0|_i},    {fp32},         D },
-    {OpcodeInfo::all,   {0xDD, 0xD0|_i},    {fp64},         D },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FSTP, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xD9, _3},             {m32, FP0S},    D_U },
-    {OpcodeInfo::all,   {0xDD, _3},             {m64, FP0D},    D_U },
-    // A little trick: actually, these 2 opcodes take only index of the
-    // needed register. To make the things similar to other instructions
-    // we encode here as if they took FPREG.
-    {OpcodeInfo::all,   {0xDD, 0xD8|_i},        {fp32},         D },
-    {OpcodeInfo::all,   {0xDD, 0xD8|_i},        {fp64},         D },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FSQRT, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xFA},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xFA},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(FYL2X, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xF1},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xF1},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(FYL2XP1, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xF9},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xF9},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(F2XM1, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xF0},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xF0},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FPATAN, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xF3},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xF3},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FXCH, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xC9},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xC9},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FSCALE, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xFD},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xFD},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FABS, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xE1},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xE1},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FSIN, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xFE},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xFE},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FCOS, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xFF},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xFF},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(FPTAN, MF_NONE, DU)
-  BEGIN_OPCODES()
-          {OpcodeInfo::all, {0xD9, 0xF2},       {FP0S},     DU   },
-          {OpcodeInfo::all, {0xD9, 0xF2},       {FP0D},     DU   },
-  END_OPCODES()
-END_MNEMONIC()
-
-//
-// ~ FPU
-//
-
-BEGIN_MNEMONIC(DIV, MF_AFFECTS_FLAGS, DU_DU_U)
-BEGIN_OPCODES()
-#if !defined(_EM64T_)
-    {OpcodeInfo::all,   {0xF6, _6},         {AH, AL, r_m8},     DU_DU_U },
-    {OpcodeInfo::all,   {Size16, 0xF7, _6}, {DX, AX, r_m16},    DU_DU_U },
-#endif
-    {OpcodeInfo::all,   {0xF7, _6},         {EDX, EAX, r_m32},  DU_DU_U },
-    {OpcodeInfo::em64t, {REX_W, 0xF7, _6},  {RDX, RAX, r_m64},  DU_DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(IDIV, MF_AFFECTS_FLAGS, DU_DU_U)
-BEGIN_OPCODES()
-#if !defined(_EM64T_)
-    {OpcodeInfo::all,   {0xF6, _7},         {AH, AL, r_m8},     DU_DU_U },
-    {OpcodeInfo::all,   {Size16, 0xF7, _7}, {DX, AX, r_m16},    DU_DU_U },
-#endif
-    {OpcodeInfo::all,   {0xF7, _7},         {EDX, EAX, r_m32},  DU_DU_U },
-    {OpcodeInfo::em64t, {REX_W, 0xF7, _7},  {RDX, RAX, r_m64},  DU_DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(IMUL, MF_AFFECTS_FLAGS, D_DU_U)
-BEGIN_OPCODES()
-    /*{OpcodeInfo::all,   {0xF6, _5},               {AH, AL,        r_m8},  D_DU_U },
-    {OpcodeInfo::all,     {Size16, 0xF7, _5},       {DX, AX,        r_m16}, D_DU_U },
-    */
-    //
-    {OpcodeInfo::all,     {0xF7, _5},               {EDX, EAX, r_m32},  D_DU_U },
-    //todo: this opcode's hash conflicts with IMUL r64,r_m64 - they're both 0.
-    // this particular is not currently used, so we may safely drop it, but need to
-    // revisit the hash implementation
-    // {OpcodeInfo::em64t,   {REX_W, 0xF7, _5},        {RDX, RAX, r_m64},  D_DU_U },
-    //
-    {OpcodeInfo::all,   {Size16, 0x0F, 0xAF, _r}, {r16,r_m16},        DU_U },
-    {OpcodeInfo::all,   {0x0F, 0xAF, _r},         {r32,r_m32},        DU_U },
-    {OpcodeInfo::em64t, {REX_W, 0x0F, 0xAF, _r},  {r64,r_m64},        DU_U },
-    {OpcodeInfo::all,   {Size16, 0x6B, _r, ib},   {r16,r_m16,imm8s},  D_DU_U },
-    {OpcodeInfo::all,   {0x6B, _r, ib},           {r32,r_m32,imm8s},  D_DU_U },
-    {OpcodeInfo::em64t, {REX_W, 0x6B, _r, ib},    {r64,r_m64,imm8s},  D_DU_U },
-    {OpcodeInfo::all,   {Size16, 0x6B, _r, ib},   {r16,imm8s},        DU_U },
-    {OpcodeInfo::all,   {0x6B, _r, ib},           {r32,imm8s},        DU_U },
-    {OpcodeInfo::em64t, {REX_W, 0x6B, _r, ib},    {r64,imm8s},        DU_U },
-    {OpcodeInfo::all,   {Size16, 0x69, _r, iw},   {r16,r_m16,imm16},  D_U_U },
-    {OpcodeInfo::all,   {0x69, _r, id},           {r32,r_m32,imm32},  D_U_U },
-    {OpcodeInfo::em64t, {REX_W, 0x69, _r, id},    {r64,r_m64,imm32s}, D_U_U },
-    {OpcodeInfo::all,   {Size16, 0x69, _r, iw},   {r16,imm16},        DU_U },
-    {OpcodeInfo::all,   {0x69, _r, id},           {r32,imm32},        DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MUL, MF_AFFECTS_FLAGS, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF6, _4},           {AX, AL, r_m8},     D_DU_U },
-    {OpcodeInfo::all,   {Size16, 0xF7, _4},   {DX, AX, r_m16},    D_DU_U },
-    {OpcodeInfo::all,   {0xF7, _4},           {EDX, EAX, r_m32},  D_DU_U },
-    {OpcodeInfo::em64t, {REX_W, 0xF7, _4},    {RDX, RAX, r_m64},  D_DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(INC, MF_AFFECTS_FLAGS, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xFE, _0},           {r_m8},         DU },
-    {OpcodeInfo::all,   {Size16, 0xFF, _0},   {r_m16},        DU },
-    {OpcodeInfo::all,   {0xFF, _0},           {r_m32},        DU },
-    {OpcodeInfo::em64t, {REX_W, 0xFF, _0},    {r_m64},        DU },
-    {OpcodeInfo::ia32,  {Size16, 0x40|rw},    {r16},          DU },
-    {OpcodeInfo::ia32,  {0x40|rd},            {r32},          DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(INT3, MF_NONE, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xCC},     {},     N },
-END_OPCODES()
-END_MNEMONIC()
-
-#define DEFINE_Jcc_MNEMONIC( cc ) \
-        BEGIN_MNEMONIC(J##cc, MF_USES_FLAGS|MF_CONDITIONAL, U ) \
-BEGIN_OPCODES() \
-    {OpcodeInfo::all,   {0x70 + ConditionMnemonic_##cc, cb },           { rel8 },       U }, \
-    {OpcodeInfo::ia32,  {Size16, 0x0F, 0x80 + ConditionMnemonic_##cc, cw},      { rel16 },      U }, \
-    {OpcodeInfo::all,   {0x0F, 0x80 + ConditionMnemonic_##cc, cd},      { rel32 },      U }, \
-END_OPCODES() \
-END_MNEMONIC()
-
-
-DEFINE_Jcc_MNEMONIC(O)
-DEFINE_Jcc_MNEMONIC(NO)
-DEFINE_Jcc_MNEMONIC(B)
-DEFINE_Jcc_MNEMONIC(NB)
-DEFINE_Jcc_MNEMONIC(Z)
-DEFINE_Jcc_MNEMONIC(NZ)
-DEFINE_Jcc_MNEMONIC(BE)
-DEFINE_Jcc_MNEMONIC(NBE)
-
-DEFINE_Jcc_MNEMONIC(S)
-DEFINE_Jcc_MNEMONIC(NS)
-DEFINE_Jcc_MNEMONIC(P)
-DEFINE_Jcc_MNEMONIC(NP)
-DEFINE_Jcc_MNEMONIC(L)
-DEFINE_Jcc_MNEMONIC(NL)
-DEFINE_Jcc_MNEMONIC(LE)
-DEFINE_Jcc_MNEMONIC(NLE)
-
-#undef DEFINE_Jcc_MNEMONIC
-
-BEGIN_MNEMONIC(JMP, MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xEB, cb},         {rel8},     U },
-    {OpcodeInfo::ia32,  {Size16, 0xE9, cw}, {rel16},    U },
-    {OpcodeInfo::all,   {0xE9, cd},         {rel32},    U },
-    {OpcodeInfo::ia32,  {Size16, 0xFF, _4}, {r_m16},    U },
-    {OpcodeInfo::ia32,  {0xFF, _4},         {r_m32},    U },
-    {OpcodeInfo::em64t, {0xFF, _4},         {r_m64},    U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(LEA, MF_NONE, D_U )
-BEGIN_OPCODES()
-    /*
-    A special case: the LEA instruction itself does not care about size of
-    second operand. This is obviuos why it is, and thus in The Manual, a
-    simple 'm' without size is used.
-    However, in the Jitrino's instrucitons we'll have an operand with a size.
-    Also, the hashing scheme is not supposed to handle OpndSize_Null, and
-    making it to do so will lead to unnecessary complication of hashing
-    scheme. Thus, instead of handling it as a special case, we simply make
-    copies of the opcodes with sizes set.
-        {OpcodeInfo::all,     {0x8D, _r},             {r32, m},       D_U },
-        {OpcodeInfo::em64t, {0x8D, _r},               {r64, m},       D_U },
-    */
-    //Android x86: keep r32, m32 only, otherwise, will have decoding error
-    //{OpcodeInfo::all,   {0x8D, _r},     {r32, m8},      D_U },
-    {OpcodeInfo::em64t, {REX_W, 0x8D, _r},     {r64, m8},      D_U },
-    //{OpcodeInfo::all,   {0x8D, _r},     {r32, m16},     D_U },
-    {OpcodeInfo::em64t, {REX_W, 0x8D, _r},     {r64, m16},     D_U },
-    {OpcodeInfo::all,   {0x8D, _r},     {r32, m32},     D_U },
-    {OpcodeInfo::em64t, {REX_W, 0x8D, _r},     {r64, m32},     D_U },
-    {OpcodeInfo::all,   {0x8D, _r},     {r32, m64},     D_U },
-    {OpcodeInfo::em64t, {REX_W, 0x8D, _r},     {r64, m64},     D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(LOOP, MF_AFFECTS_FLAGS|MF_USES_FLAGS, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xE2, cb},     {ECX, rel8},    DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(LOOPE, MF_AFFECTS_FLAGS|MF_USES_FLAGS, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xE1, cb},     {ECX, rel8},    DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(LOOPNE, MF_AFFECTS_FLAGS|MF_USES_FLAGS, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xE0, cb},     {ECX, rel8},    DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MOV, MF_NONE, D_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x88, _r},         {r_m8,r8},      D_U },
-
-    {OpcodeInfo::all,   {Size16, 0x89, _r}, {r_m16,r16},    D_U },
-    {OpcodeInfo::all,   {0x89, _r},         {r_m32,r32},    D_U },
-    {OpcodeInfo::em64t, {REX_W, 0x89, _r},  {r_m64,r64},    D_U },
-    {OpcodeInfo::all,   {0x8A, _r},         {r8,r_m8},      D_U },
-
-    {OpcodeInfo::all,   {Size16, 0x8B, _r}, {r16,r_m16},    D_U },
-    {OpcodeInfo::all,   {0x8B, _r},         {r32,r_m32},    D_U },
-    {OpcodeInfo::em64t, {REX_W, 0x8B, _r},  {r64,r_m64},    D_U },
-
-    {OpcodeInfo::all,   {0xB0|rb},          {r8,imm8},      D_U },
-
-    {OpcodeInfo::all,   {Size16, 0xB8|rw},  {r16,imm16},    D_U },
-    {OpcodeInfo::all,   {0xB8|rd},          {r32,imm32},    D_U },
-    {OpcodeInfo::em64t, {REX_W, 0xB8|rd},   {r64,imm64},    D_U },
-    {OpcodeInfo::all,   {0xC6, _0},         {r_m8,imm8},    D_U },
-
-    {OpcodeInfo::all,   {Size16, 0xC7, _0}, {r_m16,imm16},  D_U },
-    {OpcodeInfo::all,   {0xC7, _0},         {r_m32,imm32},  D_U },
-    {OpcodeInfo::em64t, {REX_W, 0xC7, _0},  {r_m64,imm32s}, D_U },
-
-    {OpcodeInfo::decoder,   {0xA0},         {AL,  moff8},  D_U },
-    {OpcodeInfo::decoder,   {Size16, 0xA1}, {AX,  moff16},  D_U },
-    {OpcodeInfo::decoder,   {0xA1},         {EAX, moff32},  D_U },
-    //{OpcodeInfo::decoder64,   {REX_W, 0xA1},  {RAX, moff64},  D_U },
-
-    {OpcodeInfo::decoder,   {0xA2},         {moff8, AL},  D_U },
-    {OpcodeInfo::decoder,   {Size16, 0xA3}, {moff16, AX},  D_U },
-    {OpcodeInfo::decoder,   {0xA3},         {moff32, EAX},  D_U },
-    //{OpcodeInfo::decoder64,   {REX_W, 0xA3},  {moff64, RAX},  D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-
-BEGIN_MNEMONIC(XCHG, MF_NONE, DU_DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x87, _r},   {r_m32,r32},    DU_DU },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(MOVQ, MF_NONE, D_U )
-BEGIN_OPCODES()
-#ifdef _HAVE_MMX_
-    {OpcodeInfo::all,   {0x0F, 0x6F, _r},   {mm64, mm_m64}, D_U },
-    {OpcodeInfo::all,   {0x0F, 0x7F, _r},   {mm_m64, mm64}, D_U },
-#endif
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0x7E },  {xmm64, xmm_m64},       D_U },
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xD6 },  {xmm_m64, xmm64},       D_U },
-//    {OpcodeInfo::em64t, {REX_W, 0x66, 0x0F, 0x6E, _r},  {xmm64, r_m64}, D_U },
-//    {OpcodeInfo::em64t, {REX_W, 0x66, 0x0F, 0x7E, _r},  {r_m64, xmm64}, D_U },
-    {OpcodeInfo::em64t, {REX_W, 0x66, 0x0F, 0x6E, _r},  {xmm64, r64}, D_U },
-    {OpcodeInfo::em64t, {REX_W, 0x66, 0x0F, 0x7E, _r},  {r64, xmm64}, D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(MOVD, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x66, 0x0F, 0x6E, _r}, {xmm32, r_m32}, D_U },
-    {OpcodeInfo::all,   {0x66, 0x0F, 0x7E, _r}, {r_m32, xmm32}, D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-//
-// A bunch of MMX instructions
-//
-#ifdef _HAVE_MMX_
-
-BEGIN_MNEMONIC(EMMS, MF_NONE, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0x77},       {},             N },
-END_OPCODES()
-END_MNEMONIC()
-
-#endif
-
-BEGIN_MNEMONIC(PADDQ, MF_NONE, DU_U)
-BEGIN_OPCODES()
-#ifdef _HAVE_MMX_
-    {OpcodeInfo::all,   {0x0F, 0xD4, _r},   {mm64, mm_m64}, DU_U },
-#endif
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xD4, _r},   {xmm64, xmm_m64}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(PAND, MF_NONE, DU_U)
-BEGIN_OPCODES()
-#ifdef _HAVE_MMX_
-    {OpcodeInfo::all,   {0x0F, 0xDB, _r},   {mm64, mm_m64}, DU_U },
-#endif
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xDB, _r},   {xmm64, xmm_m64}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(POR, MF_NONE, DU_U)
-BEGIN_OPCODES()
-#ifdef _HAVE_MMX_
-    {OpcodeInfo::all,   {0x0F, 0xEB, _r},   {mm64, mm_m64}, DU_U },
-#endif
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xEB, _r},   {xmm64, xmm_m64}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(PSUBQ, MF_NONE, DU_U)
-BEGIN_OPCODES()
-#ifdef _HAVE_MMX_
-    {OpcodeInfo::all,   {0x0F, 0xFB, _r},   {mm64, mm_m64}, DU_U },
-#endif
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xFB, _r},   {xmm64, xmm_m64}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(PANDN, MF_NONE, DU_U)
-BEGIN_OPCODES()
-#ifdef _HAVE_MMX_
-    {OpcodeInfo::all,   {0x0F, 0xDF, _r},   {mm64, mm_m64}, DU_U },
-#endif
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xDF, _r}, {xmm64, xmm_m64},   DU_U },
-END_OPCODES()
-END_MNEMONIC()
-BEGIN_MNEMONIC(PSLLQ, MF_NONE, DU_U)
-BEGIN_OPCODES()
-#ifdef _HAVE_MMX_
-    {OpcodeInfo::all,   {0x0F, 0xF3, _r},   {mm64, mm_m64}, DU_U },
-#endif
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xF3, _r}, {xmm64, xmm_m64},   DU_U },
-    {OpcodeInfo::all,   {0x66, 0x0F, 0x73, _6, ib}, {xmm64, imm8},   DU_U },
-END_OPCODES()
-END_MNEMONIC()
-BEGIN_MNEMONIC(PSRLQ, MF_NONE, DU_U)
-BEGIN_OPCODES()
-#ifdef _HAVE_MMX_
-    {OpcodeInfo::all,   {0x0F, 0xD3, _r},   {mm64, mm_m64}, DU_U },
-#endif
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xD3, _r}, {xmm64, xmm_m64},   DU_U },
-    {OpcodeInfo::all,   {0x66, 0x0F, 0x73, _2, ib}, {xmm64, imm8},   DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(PXOR, MF_NONE, DU_U)
-BEGIN_OPCODES()
-#ifdef _HAVE_MMX_
-    {OpcodeInfo::all,   {0x0F, 0xEF, _r},   {mm64, mm_m64}, DU_U },
-#endif
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xEF, _r}, {xmm64, xmm_m64},   DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(MOVAPD, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x66, 0x0F, 0x28, _r},   {xmm64, xmm_m64},   D_U },
-    {OpcodeInfo::all,   {0x66, 0x0F, 0x29, _r},   {xmm_m64, xmm64},   D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(MOVSD, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all, {0xF2, 0x0F, 0x10, _r},   {xmm64, xmm_m64},   D_U },
-    {OpcodeInfo::all, {0xF2, 0x0F, 0x11, _r},   {xmm_m64, xmm64},   D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MOVSS, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all, {0xF3, 0x0F, 0x10, _r},   {xmm32, xmm_m32}, D_U },
-    {OpcodeInfo::all, {0xF3, 0x0F, 0x11, _r},   {xmm_m32, xmm32}, D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MOVSX, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,       {Size16, 0x0F, 0xBE, _r}, {r16, r_m8s},     D_U },
-    {OpcodeInfo::all,       {0x0F, 0xBE, _r},         {r32, r_m8s},     D_U },
-    {OpcodeInfo::em64t,     {REX_W, 0x0F, 0xBE, _r},  {r64, r_m8s},     D_U },
-
-    {OpcodeInfo::all,       {0x0F, 0xBF, _r},         {r32, r_m16s},    D_U },
-    {OpcodeInfo::em64t,     {REX_W, 0x0F, 0xBF, _r},  {r64, r_m16s},    D_U },
-
-    {OpcodeInfo::em64t,     {REX_W, 0x63, _r},        {r64, r_m32s},    D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MOVZX, MF_NONE, D_U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,       {Size16, 0x0F, 0xB6, _r}, {r16, r_m8u},     D_U },
-    {OpcodeInfo::all,       {0x0F, 0xB6, _r},         {r32, r_m8u},     D_U },
-    {OpcodeInfo::em64t,     {REX_W, 0x0F, 0xB6, _r},  {r64, r_m8u},     D_U },
-
-    {OpcodeInfo::all,       {0x0F, 0xB7, _r},         {r32, r_m16u},    D_U },
-    {OpcodeInfo::em64t,     {REX_W, 0x0F, 0xB7, _r},  {r64, r_m16u},    D_U },
-    //workaround to get r/rm32->r64 ZX mov functionality:
-    //simple 32bit reg copying zeros high bits in 64bit reg
-    {OpcodeInfo::em64t,     {0x8B, _r},               {r64, r_m32u},    D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MULSD, MF_NONE, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF2, 0x0F, 0x59, _r}, {xmm64, xmm_m64},   DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MULSS, MF_NONE, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0x59, _r}, {xmm32, xmm_m32}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(NEG, MF_AFFECTS_FLAGS, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF6, _3},         {r_m8},         DU },
-
-    {OpcodeInfo::all,   {Size16, 0xF7, _3}, {r_m16},        DU },
-    {OpcodeInfo::all,   {0xF7, _3},         {r_m32},        DU },
-    {OpcodeInfo::em64t, {REX_W, 0xF7, _3},  {r_m64},        DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(NOP, MF_NONE, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x90}, {},     N },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(NOT, MF_AFFECTS_FLAGS, DU )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF6, _2},           {r_m8},         DU },
-    {OpcodeInfo::all,   {Size16, 0xF7, _2},   {r_m16},        DU },
-    {OpcodeInfo::all,   {0xF7, _2},           {r_m32},        DU },
-    {OpcodeInfo::em64t, {REX_W, 0xF7, _2},    {r_m64},        DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(POP, MF_NONE, D)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {Size16, 0x8F, _0}, {r_m16},    D },
-    {OpcodeInfo::ia32,  {0x8F, _0},         {r_m32},    D },
-    {OpcodeInfo::em64t, {0x8F, _0},         {r_m64},    D },
-
-    {OpcodeInfo::all,   {Size16, 0x58|rw }, {r16},      D },
-    {OpcodeInfo::ia32,  {0x58|rd },         {r32},      D },
-    {OpcodeInfo::em64t, {0x58|rd },         {r64},      D },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(POPFD, MF_AFFECTS_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x9D},     {},         N },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(PREFETCH, MF_NONE, U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0x18, _0},   {m8},         U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(PUSH, MF_NONE, U )
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {Size16, 0xFF, _6}, {r_m16},    U },
-    {OpcodeInfo::ia32,  {0xFF, _6},         {r_m32},    U },
-    {OpcodeInfo::em64t, {0xFF, _6},         {r_m64},    U },
-
-    {OpcodeInfo::all,   {Size16, 0x50|rw }, {r16},      U },
-    {OpcodeInfo::ia32,  {0x50|rd },         {r32},      U },
-    {OpcodeInfo::em64t, {0x50|rd },         {r64},      U },
-
-    {OpcodeInfo::all,   {0x6A},         {imm8},     U },
-    {OpcodeInfo::all,   {Size16, 0x68}, {imm16},    U },
-    {OpcodeInfo::ia32,  {0x68},         {imm32},    U },
-//          {OpcodeInfo::em64t,   {0x68},   {imm64},    U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(PUSHFD, MF_USES_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x9C},             {},        N },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(RET, MF_NONE, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xC3},       {},         N },
-    {OpcodeInfo::all,   {0xC2, iw},   {imm16},    U },
-END_OPCODES()
-END_MNEMONIC()
-
-#define DEFINE_SETcc_MNEMONIC( cc ) \
-        BEGIN_MNEMONIC(SET##cc, MF_USES_FLAGS|MF_CONDITIONAL, DU) \
-BEGIN_OPCODES() \
-    {OpcodeInfo::all,   {0x0F,     0x90 + ConditionMnemonic_##cc}, {r_m8},  DU }, \
-END_OPCODES() \
-END_MNEMONIC()
-
-DEFINE_SETcc_MNEMONIC(O)
-DEFINE_SETcc_MNEMONIC(NO)
-DEFINE_SETcc_MNEMONIC(B)
-DEFINE_SETcc_MNEMONIC(NB)
-DEFINE_SETcc_MNEMONIC(Z)
-DEFINE_SETcc_MNEMONIC(NZ)
-DEFINE_SETcc_MNEMONIC(BE)
-DEFINE_SETcc_MNEMONIC(NBE)
-
-DEFINE_SETcc_MNEMONIC(S)
-DEFINE_SETcc_MNEMONIC(NS)
-DEFINE_SETcc_MNEMONIC(P)
-DEFINE_SETcc_MNEMONIC(NP)
-DEFINE_SETcc_MNEMONIC(L)
-DEFINE_SETcc_MNEMONIC(NL)
-DEFINE_SETcc_MNEMONIC(LE)
-DEFINE_SETcc_MNEMONIC(NLE)
-
-#undef DEFINE_SETcc_MNEMONIC
-
-#define DEFINE_SHIFT_MNEMONIC(nam, slash_num, flags) \
-BEGIN_MNEMONIC(nam, flags, DU_U) \
-BEGIN_OPCODES()\
-    /* D0 & D1 opcodes are added w/o 2nd operand (1) because */\
-    /* they are used for decoding only so only instruction length is needed */\
-    {OpcodeInfo::decoder,   {0xD0, slash_num},            {r_m8/*,const_1*/},   DU },\
-    {OpcodeInfo::all,       {0xD2, slash_num},              {r_m8,  CL},        DU_U },\
-    {OpcodeInfo::all,       {0xC0, slash_num, ib},          {r_m8,  imm8},      DU_U },\
-\
-    {OpcodeInfo::decoder,   {Size16, 0xD1, slash_num},    {r_m16/*,const_1*/},  DU },\
-    {OpcodeInfo::all,       {Size16, 0xD3, slash_num},      {r_m16, CL},        DU_U },\
-    {OpcodeInfo::all,       {Size16, 0xC1, slash_num, ib},  {r_m16, imm8 },     DU_U },\
-\
-    {OpcodeInfo::decoder,   {0xD1, slash_num},              {r_m32/*,const_1*/}, DU },\
-    {OpcodeInfo::decoder64, {REX_W, 0xD1, slash_num},       {r_m64/*,const_1*/}, DU },\
-\
-    {OpcodeInfo::all,       {0xD3, slash_num},              {r_m32, CL},        DU_U },\
-    {OpcodeInfo::em64t,     {REX_W, 0xD3, slash_num},       {r_m64, CL},        DU_U },\
-\
-    {OpcodeInfo::all,       {0xC1, slash_num, ib},          {r_m32, imm8},      DU_U },\
-    {OpcodeInfo::em64t,     {REX_W, 0xC1, slash_num, ib},   {r_m64, imm8},      DU_U },\
-END_OPCODES()\
-END_MNEMONIC()
-
-
-DEFINE_SHIFT_MNEMONIC(ROL, _0, MF_AFFECTS_FLAGS)
-DEFINE_SHIFT_MNEMONIC(ROR, _1, MF_AFFECTS_FLAGS)
-DEFINE_SHIFT_MNEMONIC(RCL, _2, MF_AFFECTS_FLAGS|MF_USES_FLAGS)
-DEFINE_SHIFT_MNEMONIC(RCR, _3, MF_AFFECTS_FLAGS|MF_USES_FLAGS)
-
-DEFINE_SHIFT_MNEMONIC(SAL, _4, MF_AFFECTS_FLAGS)
-DEFINE_SHIFT_MNEMONIC(SHR, _5, MF_AFFECTS_FLAGS)
-DEFINE_SHIFT_MNEMONIC(SAR, _7, MF_AFFECTS_FLAGS)
-
-#undef DEFINE_SHIFT_MNEMONIC
-
-BEGIN_MNEMONIC(SHLD, MF_AFFECTS_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0xA5},   {r_m32, r32, CL}, DU_DU_U },
-    {OpcodeInfo::all,   {0x0F, 0xA4},   {r_m32, r32, imm8}, DU_DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(SHRD, MF_AFFECTS_FLAGS, N)
-// TODO: the def/use info is wrong
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0xAD},   {r_m32, r32, CL}, DU_DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(SUBSD, MF_NONE, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF2, 0x0F, 0x5C, _r}, {xmm64, xmm_m64}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(SUBSS, MF_NONE, DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0x5C, _r}, {xmm32, xmm_m32}, DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(TEST, MF_AFFECTS_FLAGS, U_U)
-BEGIN_OPCODES()
-
-    {OpcodeInfo::decoder,   {0xA8, ib},             { AL, imm8},    U_U },
-    {OpcodeInfo::decoder,   {0xA9, iw},             { AX, imm16},   U_U },
-    {OpcodeInfo::decoder,   {0xA9, id},             { EAX, imm32},  U_U },
-    {OpcodeInfo::decoder64, {REX_W, 0xA9, id},      { RAX, imm32s}, U_U },
-
-    {OpcodeInfo::all,       {0xF6, _0, ib},         {r_m8,imm8},    U_U },
-
-    {OpcodeInfo::all,       {Size16, 0xF7, _0, iw}, {r_m16,imm16},  U_U },
-    {OpcodeInfo::all,       {0xF7, _0, id},         {r_m32,imm32},  U_U },
-    {OpcodeInfo::em64t,     {REX_W, 0xF7, _0, id},  {r_m64,imm32s}, U_U },
-
-    {OpcodeInfo::all,       {0x84, _r},             {r_m8,r8},      U_U },
-
-    {OpcodeInfo::all,       {Size16, 0x85, _r},     {r_m16,r16},    U_U },
-    {OpcodeInfo::all,       {0x85, _r},             {r_m32,r32},    U_U },
-    {OpcodeInfo::em64t,     {REX_W, 0x85, _r},      {r_m64,r64},    U_U },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(UCOMISD, MF_AFFECTS_FLAGS, U_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x66, 0x0F, 0x2E, _r}, {xmm64, xmm_m64}, U_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(UCOMISS, MF_AFFECTS_FLAGS, U_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0x2E, _r},       {xmm32, xmm_m32}, U_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(COMISD, MF_AFFECTS_FLAGS, U_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x66, 0x0F, 0x2F, _r}, {xmm64, xmm_m64}, U_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(COMISS, MF_AFFECTS_FLAGS, U_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x0F, 0x2F, _r},       {xmm32, xmm_m32}, U_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(XORPD, MF_SAME_ARG_NO_USE|MF_SYMMETRIC, DU_U)
-BEGIN_OPCODES()
-    //Note: they're actually 128 bits
-    {OpcodeInfo::all,   {0x66, 0x0F, 0x57, _r},   {xmm64, xmm_m64},   DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(XORPS, MF_SAME_ARG_NO_USE|MF_SYMMETRIC, DU_U)
-BEGIN_OPCODES()
-    //Note: they're actually 128 bits
-    {OpcodeInfo::all,   {0x0F, 0x57, _r},   {xmm32, xmm_m32},       DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CVTDQ2PD, MF_NONE, D_U )
-BEGIN_OPCODES()
-    //Note: they're actually 128 bits
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0xE6}, {xmm64, xmm_m64},   D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CVTDQ2PS, MF_NONE, D_U )
-BEGIN_OPCODES()
-    //Note: they're actually 128 bits
-    {OpcodeInfo::all,   {0x0F, 0x5B, _r},   {xmm32, xmm_m32},   D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CVTTPD2DQ, MF_NONE, D_U )
-BEGIN_OPCODES()
-    //Note: they're actually 128 bits
-    {OpcodeInfo::all,   {0x66, 0x0F, 0xE6}, {xmm64, xmm_m64},   D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CVTTPS2DQ, MF_NONE, D_U )
-BEGIN_OPCODES()
-    //Note: they're actually 128 bits
-    {OpcodeInfo::all,   {0xF3, 0x0F, 0x5B, _r},   {xmm32, xmm_m32},   D_U },
-END_OPCODES()
-END_MNEMONIC()
-
-//
-// String operations
-//
-BEGIN_MNEMONIC(STD, MF_AFFECTS_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xFD},         {},     N },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CLD, MF_AFFECTS_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xFC},         {},     N },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(SCAS, MF_AFFECTS_FLAGS, N)
-// to be symmetric, this mnemonic must have either m32 or RegName_EAX
-// but as long, as Jitrino's CG does not use the mnemonic, leaving it
-// in its natural form
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xAF},         {},     N },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(STOS, MF_AFFECTS_FLAGS, DU_DU_U)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0xAB},         {EDI, ECX, EAX},   DU_DU_U },
-    {OpcodeInfo::all,   {0xAA},         {EDI, ECX, AL},    DU_DU_U },
-    {OpcodeInfo::em64t, {REX_W, 0xAB},  {RDI, RCX, RAX},   DU_DU_U },
-END_OPCODES()
-END_MNEMONIC()
-
-/*
-MOVS and CMPS are the special cases.
-Most the code in both CG and Encoder do not expect 2 memory operands.
-Also, they are not supposed to setup constrains on which register the
-memory reference must reside - m8,m8 or m32,m32 is not the choice.
-We can't use r8,r8 either - will have problem with 8bit EDI, ESI.
-So, as the workaround we do r32,r32 and specify size of the operand through
-the specific mnemonic - the same is in the codegen.
-*/
-BEGIN_MNEMONIC(MOVS8, MF_NONE, DU_DU_DU)
-BEGIN_OPCODES()
-    {OpcodeInfo::ia32,  {0xA4},         {r32,r32,ECX},    DU_DU_DU },
-    {OpcodeInfo::em64t, {0xA4},         {r64,r64,RCX},    DU_DU_DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MOVS16, MF_NONE, DU_DU_DU)
-BEGIN_OPCODES()
-    {OpcodeInfo::ia32,  {Size16, 0xA5}, {r32,r32,ECX},  DU_DU_DU },
-    {OpcodeInfo::em64t, {Size16, 0xA5}, {r64,r64,RCX},  DU_DU_DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MOVS32, MF_NONE, DU_DU_DU)
-BEGIN_OPCODES()
-    {OpcodeInfo::ia32,  {0xA5},         {r32,r32,ECX},  DU_DU_DU },
-    {OpcodeInfo::em64t, {0xA5},         {r64,r64,RCX},  DU_DU_DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(MOVS64, MF_NONE, DU_DU_DU)
-BEGIN_OPCODES()
-    {OpcodeInfo::em64t, {REX_W,0xA5},   {r64,r64,RCX},  DU_DU_DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CMPSB, MF_AFFECTS_FLAGS, DU_DU_DU)
-BEGIN_OPCODES()
-    {OpcodeInfo::ia32,  {0xA6},         {ESI,EDI,ECX},    DU_DU_DU },
-    {OpcodeInfo::em64t, {0xA6},         {RSI,RDI,RCX},    DU_DU_DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CMPSW, MF_AFFECTS_FLAGS, DU_DU_DU)
-BEGIN_OPCODES()
-    {OpcodeInfo::ia32,  {Size16, 0xA7}, {ESI,EDI,ECX},  DU_DU_DU },
-    {OpcodeInfo::em64t, {Size16, 0xA7}, {RSI,RDI,RCX},  DU_DU_DU },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(CMPSD, MF_AFFECTS_FLAGS, DU_DU_DU)
-BEGIN_OPCODES()
-    {OpcodeInfo::ia32,  {0xA7},         {ESI,EDI,ECX},  DU_DU_DU },
-    {OpcodeInfo::em64t, {0xA7},         {RSI,RDI,RCX},  DU_DU_DU },
-END_OPCODES()
-END_MNEMONIC()
-
-
-BEGIN_MNEMONIC(WAIT, MF_AFFECTS_FLAGS, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::all,   {0x9B},         {},       N },
-END_OPCODES()
-END_MNEMONIC()
-
-//
-// ~String operations
-//
-
-//
-//Note: the instructions below added for the sake of disassembling routine.
-// They need to have flags, params and params usage to be defined more precisely.
-//
-BEGIN_MNEMONIC(LEAVE, MF_NONE, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::decoder,   {0xC9},         {},       N },
-END_OPCODES()
-END_MNEMONIC()
-
-BEGIN_MNEMONIC(ENTER, MF_NONE, N)
-BEGIN_OPCODES()
-    {OpcodeInfo::decoder,   {0xC8, iw, ib},           {imm16, imm8},  N },
-END_OPCODES()
-END_MNEMONIC()
-
-};      // ~masterEncodingTable[]
-
-ENCODER_NAMESPACE_END
-
-ENCODER_NAMESPACE_START
-
-static int compareMnemonicInfo(const void* info1, const void* info2)
-{
-    Mnemonic id1, id2;
-
-    id1 = ((const MnemonicInfo*) info1)->mn;
-    id2 = ((const MnemonicInfo*) info2)->mn;
-    if (id1 < id2)
-        return -1;
-    if (id1 > id2)
-        return 1;
-    return 0;
-}
-
-int EncoderBase::buildTable(void)
-{
-    // A check: all mnemonics must be covered
-    assert(COUNTOF(masterEncodingTable) == Mnemonic_Count);
-
-    // sort out the mnemonics so the list become ordered
-    qsort(masterEncodingTable, Mnemonic_Count, sizeof(MnemonicInfo), compareMnemonicInfo);
-
-    //
-    // clear the things
-    //
-    memset(opcodesHashMap, NOHASH, sizeof(opcodesHashMap));
-    memset(opcodes, 0, sizeof(opcodes));
-    //
-    // and, finally, build it
-    for (unsigned i=0; i<Mnemonic_Count; i++) {
-        assert((Mnemonic)i == (masterEncodingTable + i)->mn);
-        buildMnemonicDesc(masterEncodingTable+i);
-    }
-    return 0;
-}
-
-void EncoderBase::buildMnemonicDesc(const MnemonicInfo * minfo)
-{
-    MnemonicDesc& mdesc = mnemonics[minfo->mn];
-    mdesc.mn = minfo->mn;
-    mdesc.flags = minfo->flags;
-    mdesc.roles = minfo->roles;
-    mdesc.name = minfo->name;
-
-    //
-    // fill the used opcodes
-    //
-    for (unsigned i=0, oindex=0; i<COUNTOF(minfo->opcodes); i++) {
-
-        const OpcodeInfo& oinfo = minfo->opcodes[i];
-        OpcodeDesc& odesc = opcodes[minfo->mn][oindex];
-        // last opcode ?
-        if (oinfo.opcode[0] == OpcodeByteKind_LAST) {
-            // mark the opcode 'last', exit
-            odesc.opcode_len = 0;
-            odesc.last = 1;
-            break;
-        }
-        odesc.last = 0;
-#ifdef _EM64T_
-        if (oinfo.platf == OpcodeInfo::ia32) { continue; }
-        if (oinfo.platf == OpcodeInfo::decoder32) { continue; }
-#else
-        if (oinfo.platf == OpcodeInfo::em64t) { continue; }
-        if (oinfo.platf == OpcodeInfo::decoder64) { continue; }
-#endif
-        if (oinfo.platf == OpcodeInfo::decoder64 ||
-            oinfo.platf == OpcodeInfo::decoder32) {
-             odesc.platf = OpcodeInfo::decoder;
-        }
-        else {
-            odesc.platf = (char)oinfo.platf;
-        }
-        //
-        // fill out opcodes
-        //
-        unsigned j = 0;
-        odesc.opcode_len = 0;
-        for(; oinfo.opcode[j]; j++) {
-            unsigned opcod = oinfo.opcode[j];
-            unsigned kind = opcod&OpcodeByteKind_KindMask;
-            if (kind == OpcodeByteKind_REX_W) {
-                odesc.opcode[odesc.opcode_len++] = (unsigned char)0x48;
-                continue;
-            }
-            else if(kind != 0 && kind != OpcodeByteKind_ZeroOpcodeByte) {
-                break;
-            }
-            unsigned lowByte = (opcod & OpcodeByteKind_OpcodeMask);
-            odesc.opcode[odesc.opcode_len++] = (unsigned char)lowByte;
-        }
-        assert(odesc.opcode_len<5);
-        odesc.aux0 = odesc.aux1 = 0;
-        if (oinfo.opcode[j] != 0) {
-            odesc.aux0 = oinfo.opcode[j];
-            assert((odesc.aux0 & OpcodeByteKind_KindMask) != 0);
-            ++j;
-            if(oinfo.opcode[j] != 0) {
-                odesc.aux1 = oinfo.opcode[j];
-                assert((odesc.aux1 & OpcodeByteKind_KindMask) != 0);
-            }
-        }
-        else if (oinfo.roles.count>=2) {
-            if (((oinfo.opnds[0].kind&OpndKind_Mem) &&
-                 (isRegKind(oinfo.opnds[1].kind))) ||
-                ((oinfo.opnds[1].kind&OpndKind_Mem) &&
-                 (isRegKind(oinfo.opnds[0].kind)))) {
-                 // Example: MOVQ xmm1, xmm/m64 has only opcodes
-                 // same with SHRD
-                 // Adding fake /r
-                 odesc.aux0 = _r;
-            }
-        }
-        else if (oinfo.roles.count==1) {
-            if (oinfo.opnds[0].kind&OpndKind_Mem) {
-                 // Example: SETcc r/m8, adding fake /0
-                 odesc.aux0 = _0;
-            }
-        }
-        // check imm
-        if (oinfo.roles.count > 0 &&
-            (oinfo.opnds[0].kind == OpndKind_Imm ||
-            oinfo.opnds[oinfo.roles.count-1].kind == OpndKind_Imm)) {
-            // Example: CALL cd, PUSH imm32 - they fit both opnds[0] and
-            // opnds[oinfo.roles.count-1].
-            // The A3 opcode fits only opnds[0] - it's currently have
-            // MOV imm32, EAX. Looks ridiculous, but this is how the
-            // moffset is currently implemented. Will need to fix together
-            // with other usages of moff.
-            // adding fake /cd or fake /id
-            unsigned imm_opnd_index =
-                oinfo.opnds[0].kind == OpndKind_Imm ? 0 : oinfo.roles.count-1;
-            OpndSize sz = oinfo.opnds[imm_opnd_index].size;
-            unsigned imm_encode, coff_encode;
-            if (sz==OpndSize_8) {imm_encode = ib; coff_encode=cb; }
-            else if (sz==OpndSize_16) {imm_encode = iw; coff_encode=cw;}
-            else if (sz==OpndSize_32) {imm_encode = id; coff_encode=cd; }
-            else if (sz==OpndSize_64) {imm_encode = io; coff_encode=0xCC; }
-            else { assert(false); imm_encode=0xCC; coff_encode=0xCC; }
-            if (odesc.aux1 == 0) {
-                if (odesc.aux0==0) {
-                    odesc.aux0 = imm_encode;
-                }
-                else {
-                    if (odesc.aux0 != imm_encode && odesc.aux0 != coff_encode) {
-                        odesc.aux1 = imm_encode;
-                    }
-                }
-            }
-            else {
-                assert(odesc.aux1==imm_encode);
-            }
-
-        }
-
-        assert(sizeof(odesc.opnds) == sizeof(oinfo.opnds));
-        memcpy(odesc.opnds, oinfo.opnds,
-                sizeof(EncoderBase::OpndDesc)
-                        * EncoderBase::MAX_NUM_OPCODE_OPERANDS);
-        odesc.roles = oinfo.roles;
-        odesc.first_opnd = 0;
-        if (odesc.opnds[0].reg != RegName_Null) {
-            ++odesc.first_opnd;
-            if (odesc.opnds[1].reg != RegName_Null) {
-                ++odesc.first_opnd;
-            }
-        }
-
-        if (odesc.platf == OpcodeInfo::decoder) {
-            // if the opcode is only for decoding info, then do not hash it.
-            ++oindex;
-            continue;
-        }
-
-        //
-        // check whether the operand info is a mask (i.e. r_m*).
-        // in this case, split the info to have separate entries for 'r'
-        // and for 'm'.
-        // the good news is that there can be only one such operand.
-        //
-        int opnd2split = -1;
-        for (unsigned k=0; k<oinfo.roles.count; k++) {
-            if ((oinfo.opnds[k].kind & OpndKind_Mem) &&
-                (OpndKind_Mem != oinfo.opnds[k].kind)) {
-                opnd2split = k;
-                break;
-            }
-        };
-
-        if (opnd2split == -1) {
-            // not a mask, hash it, store it, continue.
-            unsigned short hash = getHash(&oinfo);
-            opcodesHashMap[minfo->mn][hash] = (unsigned char)oindex;
-            ++oindex;
-            continue;
-        };
-
-        OpcodeInfo storeItem = oinfo;
-        unsigned short hash;
-
-        // remove the memory part of the mask, and store only 'r' part
-        storeItem.opnds[opnd2split].kind = (OpndKind)(storeItem.opnds[opnd2split].kind & ~OpndKind_Mem);
-        hash = getHash(&storeItem);
-        if (opcodesHashMap[minfo->mn][hash] == NOHASH) {
-            opcodesHashMap[minfo->mn][hash] = (unsigned char)oindex;
-        }
-        // else {
-        // do not overwrite if there is something there, just check that operands match
-        // the reason is that for some instructions there are several possibilities:
-        // say 'DEC r' may be encode as either '48+r' or 'FF /1', and I believe
-        // the first one is better for 'dec r'.
-        // as we're currently processing an opcode with memory part in operand,
-        // leave already filled items intact, so if there is 'OP reg' there, this
-        // better choice will be left in the table instead of 'OP r_m'
-        // }
-
-        // compute hash of memory-based operand, 'm' part in 'r_m'
-        storeItem.opnds[opnd2split].kind = OpndKind_Mem;
-        hash = getHash(&storeItem);
-        // should not happen: for the r_m opcodes, there is a possibility
-        // that hash value of 'r' part intersects with 'OP r' value, but it's
-        // impossible for 'm' part.
-        assert(opcodesHashMap[minfo->mn][hash] == NOHASH);
-        opcodesHashMap[minfo->mn][hash] = (unsigned char)oindex;
-
-        ++oindex;
-    }
-}
-
-ENCODER_NAMESPACE_END
diff --git a/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp b/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp
deleted file mode 100644
index 024ffa2..0000000
--- a/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp
+++ /dev/null
@@ -1,757 +0,0 @@
-/*
- * Copyright (C) 2012 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include <stdio.h>
-#include <assert.h>
-#include <limits.h>
-#include "cutils/log.h"
-#include "enc_base.h"
-#include "enc_wrapper.h"
-#include "dec_base.h"
-
-//#define PRINT_ENCODER_STREAM
-bool dump_x86_inst = false;
-
-/**
- * @brief Provides mapping between PhysicalReg and RegName used by encoder
- * @param physicalReg The physical register
- * @return Returns encoder's register name
- */
-static RegName mapFromPhysicalReg (int physicalReg)
-{
-    RegName reg = RegName_Null;
-
-    //Get mapping between PhysicalReg and RegName
-    switch (physicalReg)
-    {
-        case PhysicalReg_EAX:
-            reg = RegName_EAX;
-            break;
-        case PhysicalReg_EBX:
-            reg = RegName_EBX;
-            break;
-        case PhysicalReg_ECX:
-            reg = RegName_ECX;
-            break;
-        case PhysicalReg_EDX:
-            reg = RegName_EDX;
-            break;
-        case PhysicalReg_EDI:
-            reg = RegName_EDI;
-            break;
-        case PhysicalReg_ESI:
-            reg = RegName_ESI;
-            break;
-        case PhysicalReg_ESP:
-            reg = RegName_ESP;
-            break;
-        case PhysicalReg_EBP:
-            reg = RegName_EBP;
-            break;
-        case PhysicalReg_XMM0:
-            reg = RegName_XMM0;
-            break;
-        case PhysicalReg_XMM1:
-            reg = RegName_XMM1;
-            break;
-        case PhysicalReg_XMM2:
-            reg = RegName_XMM2;
-            break;
-        case PhysicalReg_XMM3:
-            reg = RegName_XMM3;
-            break;
-        case PhysicalReg_XMM4:
-            reg = RegName_XMM4;
-            break;
-        case PhysicalReg_XMM5:
-            reg = RegName_XMM5;
-            break;
-        case PhysicalReg_XMM6:
-            reg = RegName_XMM6;
-            break;
-        case PhysicalReg_XMM7:
-            reg = RegName_XMM7;
-            break;
-        default:
-            //We have no mapping
-            reg = RegName_Null;
-            break;
-    }
-
-    return reg;
-}
-
-//getRegSize, getAliasReg:
-//OpndSize, RegName, OpndExt: enum enc_defs.h
-inline void add_r(EncoderBase::Operands & args, int physicalReg, OpndSize sz, OpndExt ext = OpndExt_None) {
-    RegName reg = mapFromPhysicalReg (physicalReg);
-    if (sz != getRegSize(reg)) {
-       reg = getAliasReg(reg, sz);
-    }
-    args.add(EncoderBase::Operand(reg, ext));
-}
-inline void add_m(EncoderBase::Operands & args, int baseReg, int disp, OpndSize sz, OpndExt ext = OpndExt_None) {
-    args.add(EncoderBase::Operand(sz,
-                                  mapFromPhysicalReg (baseReg),
-                                  RegName_Null, 0,
-                                  disp, ext));
-}
-inline void add_m_scale(EncoderBase::Operands & args, int baseReg, int indexReg, int scale,
-                        OpndSize sz, OpndExt ext = OpndExt_None) {
-    args.add(EncoderBase::Operand(sz,
-                                  mapFromPhysicalReg (baseReg),
-                                  mapFromPhysicalReg (indexReg), scale,
-                                  0, ext));
-}
-inline void add_m_disp_scale(EncoderBase::Operands & args, int baseReg, int disp, int indexReg, int scale,
-                        OpndSize sz, OpndExt ext = OpndExt_None) {
-    args.add(EncoderBase::Operand(sz,
-                                  mapFromPhysicalReg (baseReg),
-                                  mapFromPhysicalReg (indexReg), scale,
-                                  disp, ext));
-}
-
-inline void add_fp(EncoderBase::Operands & args, unsigned i, bool dbl) {
-    return args.add((RegName)( (dbl ? RegName_FP0D : RegName_FP0S) + i));
-}
-inline void add_imm(EncoderBase::Operands & args, OpndSize sz, int value, bool is_signed) {
-    //assert(n_size != imm.get_size());
-    args.add(EncoderBase::Operand(sz, value,
-             is_signed ? OpndExt_Signed : OpndExt_Zero));
-}
-
-#define MAX_DECODED_STRING_LEN 1024
-char tmpBuffer[MAX_DECODED_STRING_LEN];
-
-void printOperand(const EncoderBase::Operand & opnd) {
-    unsigned int sz;
-    if(!dump_x86_inst) return;
-    sz = strlen(tmpBuffer);
-    if(opnd.size() != OpndSize_32) {
-        const char * opndSizeString = getOpndSizeString(opnd.size());
-
-        if (opndSizeString == NULL) {
-            // If the string that represents operand size is null it means that
-            // the operand size is an invalid value. Although this could be a
-            // problem if instruction is corrupted, technically failing to
-            // disassemble is not fatal. Thus, let's warn but proceed with using
-            // an empty string.
-            ALOGW("JIT-WARNING: Cannot decode instruction operand size.");
-            opndSizeString = "";
-        }
-
-        sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN - sz, "%s ",
-                opndSizeString);
-    }
-    if(opnd.is_mem()) {
-        if(opnd.scale() != 0) {
-            sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz,
-                           "%d(%s,%s,%d)", opnd.disp(),
-                           getRegNameString(opnd.base()),
-                           getRegNameString(opnd.index()), opnd.scale());
-        } else {
-            sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, "%d(%s)",
-                           opnd.disp(), getRegNameString(opnd.base()));
-        }
-    }
-    if(opnd.is_imm()) {
-        sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, "#%x",
-                       (int)opnd.imm());
-    }
-    if(opnd.is_reg()) {
-        sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, "%s",
-                       getRegNameString(opnd.reg()));
-    }
-}
-//TODO: the order of operands
-//to make the printout have the same order as assembly in .S
-//I reverse the order here
-void printDecoderInst(Inst & decInst) {
-    unsigned int sz;
-    if(!dump_x86_inst) return;
-    sz = strlen(tmpBuffer);
-    sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, "%s ",
-                   EncoderBase::toStr(decInst.mn));
-    for(unsigned int k = 0; k < decInst.argc; k++) {
-        if(k > 0) {
-            sz = strlen(tmpBuffer);
-            sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, ", ");
-        }
-        printOperand(decInst.operands[decInst.argc-1-k]);
-    }
-    ALOGE("%s", tmpBuffer);
-}
-void printOperands(EncoderBase::Operands& opnds) {
-    unsigned int sz;
-    if(!dump_x86_inst) return;
-    for(unsigned int k = 0; k < opnds.count(); k++) {
-        if(k > 0) {
-            sz = strlen(tmpBuffer);
-            sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, ", ");
-        }
-        printOperand(opnds[opnds.count()-1-k]);
-    }
-}
-void printEncoderInst(Mnemonic m, EncoderBase::Operands& opnds) {
-    if(!dump_x86_inst) return;
-    snprintf(tmpBuffer, MAX_DECODED_STRING_LEN, "--- ENC %s ",
-             EncoderBase::toStr(m));
-    printOperands(opnds);
-    ALOGE("%s", tmpBuffer);
-}
-int decodeThenPrint(char* stream_start) {
-    if(!dump_x86_inst) return 0;
-    snprintf(tmpBuffer, MAX_DECODED_STRING_LEN, "--- INST @ %p: ",
-             stream_start);
-    Inst decInst;
-    unsigned numBytes = DecoderBase::decode(stream_start, &decInst);
-    printDecoderInst(decInst);
-    return numBytes;
-}
-
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_imm(Mnemonic m, OpndSize size, int imm, char * stream) {
-    EncoderBase::Operands args;
-    //assert(imm.get_size() == size_32);
-    add_imm(args, size, imm, true/*is_signed*/);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT unsigned encoder_get_inst_size(char * stream) {
-    Inst decInst;
-    unsigned numBytes = DecoderBase::decode(stream, &decInst);
-    return numBytes;
-}
-
-extern "C" ENCODER_DECLARE_EXPORT unsigned encoder_get_cur_operand_offset(int opnd_id)
-{
-    return (unsigned)EncoderBase::getOpndLocation(opnd_id);
-}
-
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_update_imm(int imm, char * stream) {
-    Inst decInst;
-    EncoderBase::Operands args;
-
-    //Decode the instruction
-    DecoderBase::decode(stream, &decInst);
-
-    add_imm(args, decInst.operands[0].size(), imm, true/*is_signed*/);
-    char* stream_next = (char *)EncoderBase::encode(stream, decInst.mn, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(decInst.mn, args);
-    decodeThenPrint(stream);
-#endif
-    return stream_next;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem(Mnemonic m, OpndSize size,
-               int disp, int base_reg, bool isBasePhysical, char * stream) {
-    EncoderBase::Operands args;
-    add_m(args, base_reg, disp, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg(Mnemonic m, OpndSize size,
-               int reg, bool isPhysical, LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    if(m == Mnemonic_DIV || m == Mnemonic_IDIV || m == Mnemonic_MUL || m == Mnemonic_IMUL) {
-      add_r(args, 0/*eax*/, size);
-      add_r(args, 3/*edx*/, size);
-    }
-    add_r(args, reg, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-//! \brief Allows for different operand sizes
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_reg_diff_sizes(Mnemonic m, OpndSize srcOpndSize,
-                   int reg, bool isPhysical, OpndSize destOpndSize,
-                   int reg2, bool isPhysical2, LowOpndRegType type, char * stream) {
-    if((m == Mnemonic_MOV || m == Mnemonic_MOVQ || m == Mnemonic_MOVD) && reg == reg2) return stream;
-    EncoderBase::Operands args;
-    add_r(args, reg2, destOpndSize); //destination
-    if(m == Mnemonic_SAL || m == Mnemonic_SHR || m == Mnemonic_SHL || m == Mnemonic_SAR)
-      add_r(args, reg, OpndSize_8);
-    else
-      add_r(args, reg, srcOpndSize);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-//both operands have same size
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_reg(Mnemonic m, OpndSize size,
-                   int reg, bool isPhysical,
-                   int reg2, bool isPhysical2, LowOpndRegType type, char * stream) {
-    return encoder_reg_reg_diff_sizes(m, size, reg, isPhysical, size, reg2, isPhysical2, type, stream);
-}
-//! \brief Allows for different operand sizes
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_to_reg_diff_sizes(Mnemonic m, OpndSize memOpndSize,
-                   int disp, int base_reg, bool isBasePhysical, OpndSize regOpndSize,
-                   int reg, bool isPhysical, LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    add_r(args, reg, regOpndSize);
-    add_m(args, base_reg, disp, memOpndSize);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_reg(Mnemonic m, OpndSize size,
-                   int disp, int base_reg, bool isBasePhysical,
-                   int reg, bool isPhysical, LowOpndRegType type, char * stream) {
-    return encoder_mem_to_reg_diff_sizes(m, size, disp, base_reg, isBasePhysical, size, reg, isPhysical, type, stream);
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_scale_reg(Mnemonic m, OpndSize size,
-                         int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
-                         int reg, bool isPhysical, LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    add_r(args, reg, size);
-    add_m_scale(args, base_reg, index_reg, scale, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_mem_scale(Mnemonic m, OpndSize size,
-                         int reg, bool isPhysical,
-                         int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
-                         LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    add_m_scale(args, base_reg, index_reg, scale, size);
-    add_r(args, reg, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-//! \brief Allows for different operand sizes
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_to_reg_diff_sizes(Mnemonic m, OpndSize memOpndSize,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         OpndSize regOpndSize, int reg, bool isPhysical, LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    add_r(args, reg, regOpndSize);
-    add_m_disp_scale(args, base_reg, disp, index_reg, scale, memOpndSize);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_reg(Mnemonic m, OpndSize size,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         int reg, bool isPhysical, LowOpndRegType type, char * stream) {
-    return encoder_mem_disp_scale_to_reg_diff_sizes(m, size, base_reg, isBasePhysical,
-            disp, index_reg, isIndexPhysical, scale, size, reg, isPhysical,
-            type, stream);
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_movzs_mem_disp_scale_reg(Mnemonic m, OpndSize size,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         int reg, bool isPhysical, LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    add_r(args, reg, OpndSize_32);
-    add_m_disp_scale(args, base_reg, disp, index_reg, scale, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char* encoder_reg_mem_disp_scale(Mnemonic m, OpndSize size,
-                         int reg, bool isPhysical,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         LowOpndRegType type, char* stream) {
-    EncoderBase::Operands args;
-    add_m_disp_scale(args, base_reg, disp, index_reg, scale, size);
-    add_r(args, reg, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_mem(Mnemonic m, OpndSize size,
-                   int reg, bool isPhysical,
-                   int disp, int base_reg, bool isBasePhysical, LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    add_m(args, base_reg, disp, size);
-    add_r(args, reg, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    if (m == Mnemonic_CMPXCHG ){
-       //CMPXCHG require EAX as args
-       add_r(args,PhysicalReg_EAX,size);
-       //Add lock prefix for CMPXCHG, guarantee the atomic of CMPXCHG in multi-core platform
-       stream = (char *)EncoderBase::prefix(stream, InstPrefix_LOCK);
-    }
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_imm_reg(Mnemonic m, OpndSize size,
-                   int imm, int reg, bool isPhysical, LowOpndRegType type, char * stream) {
-    return encoder_imm_reg_diff_sizes(m, size, imm, size, reg, isPhysical, type, stream);
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_imm_reg_diff_sizes(Mnemonic m, OpndSize sizeImm,
-                   int imm, OpndSize sizeReg, int reg, bool isPhysical, LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    add_r(args, reg, sizeReg); //dst
-    if(m == Mnemonic_IMUL) add_r(args, reg, sizeReg); //src CHECK
-    if(m == Mnemonic_SAL || m == Mnemonic_SHR || m == Mnemonic_SHL
-       || m == Mnemonic_SAR || m == Mnemonic_ROR || m == Mnemonic_PSLLQ || m == Mnemonic_PSRLQ)  //fix for shift opcodes
-      add_imm(args, OpndSize_8, imm, true/*is_signed*/);
-    else
-      add_imm(args, sizeImm, imm, true/*is_signed*/);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_update_imm_rm(int imm, char * stream) {
-    Inst decInst;
-    EncoderBase::Operands args;
-
-    //Decode the instruction
-    DecoderBase::decode(stream, &decInst);
-
-    args.add(decInst.operands[0]);
-    add_imm(args, decInst.operands[1].size(), imm, true/*is_signed*/);
-    char* stream_next = (char *)EncoderBase::encode(stream, decInst.mn, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(decInst.mn, args);
-    decodeThenPrint(stream);
-#endif
-    return stream_next;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_imm_mem(Mnemonic m, OpndSize size,
-                   int imm,
-                   int disp, int base_reg, bool isBasePhysical, char * stream) {
-    EncoderBase::Operands args;
-    add_m(args, base_reg, disp, size);
-    if (m == Mnemonic_SAL || m == Mnemonic_SHR || m == Mnemonic_SHL
-        || m == Mnemonic_SAR || m == Mnemonic_ROR)
-        size = OpndSize_8;
-    add_imm(args, size, imm, true);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_fp_mem(Mnemonic m, OpndSize size, int reg,
-                  int disp, int base_reg, bool isBasePhysical, char * stream) {
-    EncoderBase::Operands args;
-    add_m(args, base_reg, disp, size);
-    // a fake FP register as operand
-    add_fp(args, reg, size == OpndSize_64/*is_double*/);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_fp(Mnemonic m, OpndSize size,
-                  int disp, int base_reg, bool isBasePhysical,
-                  int reg, char * stream) {
-    EncoderBase::Operands args;
-    // a fake FP register as operand
-    add_fp(args, reg, size == OpndSize_64/*is_double*/);
-    add_m(args, base_reg, disp, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_return(char * stream) {
-    EncoderBase::Operands args;
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, Mnemonic_RET, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(Mnemonic_RET, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_compare_fp_stack(bool pop, int reg, bool isDouble, char * stream) {
-    //Mnemonic m = pop ? Mnemonic_FUCOMP : Mnemonic_FUCOM;
-    Mnemonic m = pop ? Mnemonic_FUCOMIP : Mnemonic_FUCOMI;
-    //a single operand or 2 operands?
-    //FST ST(i) has a single operand in encoder.inl?
-    EncoderBase::Operands args;
-    add_fp(args, reg, isDouble);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, m, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(m, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_movez_mem_to_reg(OpndSize size,
-                      int disp, int base_reg, bool isBasePhysical,
-                      int reg, bool isPhysical, char * stream) {
-    EncoderBase::Operands args;
-    add_r(args, reg, OpndSize_32);
-    add_m(args, base_reg, disp, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, Mnemonic_MOVZX, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(Mnemonic_MOVZX, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_moves_mem_to_reg(OpndSize size,
-                      int disp, int base_reg, bool isBasePhysical,
-                      int reg, bool isPhysical, char * stream) {
-    EncoderBase::Operands args;
-    add_r(args, reg, OpndSize_32);
-    add_m(args, base_reg, disp, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, Mnemonic_MOVSX, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(Mnemonic_MOVSX, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_movez_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical, int reg2,
-                      bool isPhysical2, LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    add_r(args, reg2, OpndSize_32); //destination
-    add_r(args, reg, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, Mnemonic_MOVZX, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(Mnemonic_MOVZX, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_moves_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical,int reg2,
-                      bool isPhysical2, LowOpndRegType type, char * stream) {
-    EncoderBase::Operands args;
-    add_r(args, reg2, OpndSize_32); //destination
-    add_r(args, reg, size);
-#ifdef PRINT_ENCODER_STREAM
-    char* stream_start = stream;
-#endif
-    stream = (char *)EncoderBase::encode(stream, Mnemonic_MOVSX, args);
-#ifdef PRINT_ENCODER_STREAM
-    printEncoderInst(Mnemonic_MOVSX, args);
-    decodeThenPrint(stream_start);
-#endif
-    return stream;
-}
-
-/**
- * @brief Generates variable sized nop instructions.
- * @param numBytes Number of bytes for the nop instruction. If this value is
- * larger than 9 bytes, more than one nop instruction will be generated.
- * @param stream Instruction stream where to place the nops
- * @return Updated instruction stream pointer after generating the nops
- */
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_nops(unsigned numBytes, char * stream) {
-    return EncoderBase::nops(stream, numBytes);
-}
-
-// Disassemble the operand "opnd" and put the readable format in "strbuf"
-// up to a string length of "len".
-unsigned int DisassembleOperandToBuf(const EncoderBase::Operand& opnd, char* strbuf, unsigned int len)
-{
-    unsigned int sz = 0;
-    if(opnd.size() != OpndSize_32) {
-        const char * opndSizeString = getOpndSizeString(opnd.size());
-
-        if (opndSizeString == NULL) {
-            // If the string that represents operand size is null it means that
-            // the operand size is an invalid value. Although this could be a
-            // problem if instruction is corrupted, technically failing to
-            // disassemble is not fatal. Thus, let's warn but proceed with using
-            // an empty string.
-            ALOGW("JIT-WARNING: Cannot decode instruction operand size.");
-            opndSizeString = "";
-        }
-
-        sz += snprintf(&strbuf[sz], len-sz, "%s ", opndSizeString);
-    }
-    if(opnd.is_mem()) {
-        if(opnd.scale() != 0) {
-            sz += snprintf(&strbuf[sz], len-sz, "%d(%s,%s,%d)", opnd.disp(),
-                           getRegNameString(opnd.base()),
-                           getRegNameString(opnd.index()), opnd.scale());
-        } else {
-            sz += snprintf(&strbuf[sz], len-sz, "%d(%s)",
-                           opnd.disp(), getRegNameString(opnd.base()));
-        }
-    } else if(opnd.is_imm()) {
-        sz += snprintf(&strbuf[sz], len-sz, "#%x", (int)opnd.imm());
-    } else if(opnd.is_reg()) {
-        sz += snprintf(&strbuf[sz], len-sz, "%s",
-                       getRegNameString(opnd.reg()));
-    }
-    return sz;
-}
-
-// Disassemble the instruction "decInst" and put the readable format
-// in "strbuf" up to a string length of "len".
-void DisassembleInstToBuf(Inst& decInst, char* strbuf, unsigned int len)
-{
-    unsigned int sz = 0;
-    int k;
-    sz += snprintf(&strbuf[sz], len-sz, "%s ", EncoderBase::toStr(decInst.mn));
-    if (decInst.argc > 0) {
-        sz += DisassembleOperandToBuf(decInst.operands[decInst.argc-1],
-                                 &strbuf[sz], len-sz);
-        for(k = decInst.argc-2; k >= 0; k--) {
-            sz += snprintf(&strbuf[sz], len-sz, ", ");
-            sz += DisassembleOperandToBuf(decInst.operands[k], &strbuf[sz], len-sz);
-        }
-    }
-}
-
-// Disassmble the x86 instruction pointed to by code pointer "stream."
-// Put the disassemble text in the "strbuf" up to string length "len".
-// Return the code pointer after the disassemble x86 instruction.
-extern "C" ENCODER_DECLARE_EXPORT
-char* decoder_disassemble_instr(char* stream, char* strbuf, unsigned int len)
-{
-    Inst decInst;
-    unsigned numBytes = DecoderBase::decode(stream, &decInst);
-    DisassembleInstToBuf(decInst, strbuf, len);
-    return (stream + numBytes);
-}
-
-/**
- * @brief Physical register char* counterparts
- */
-static const char * PhysicalRegString[] = { "eax", "ebx", "ecx", "edx", "edi",
-        "esi", "esp", "ebp", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5",
-        "xmm6", "xmm7", "st0", "st1", "st2", "st3", "st4", "st5", "st6", "st7",
-        "null"
-        };
-
-/**
- * @brief Scratch register char* counterparts
- */
-static const char * ScratchRegString[] = { "scratch1", "scratch2", "scratch3",
-        "scratch4", "scratch5", "scratch6", "scratch7", "scratch8", "scratch9",
-        "scratch10" };
-
-extern "C" ENCODER_DECLARE_EXPORT
-/**
- * @brief Transform a physical register into its char* counterpart
- * @param reg the PhysicalReg we want to have a char* equivalent
- * @return the register reg in char* form
- */
-const char * physicalRegToString(PhysicalReg reg)
-{
-    if (reg < PhysicalReg_Null) {
-        return PhysicalRegString[reg];
-    } else if (reg >= PhysicalReg_SCRATCH_1 && reg <= PhysicalReg_SCRATCH_10) {
-        return ScratchRegString[reg - PhysicalReg_SCRATCH_1];
-    } else if (reg == PhysicalReg_Null) {
-        return "null";
-    } else {
-        return "corrupted-data";
-    }
-}
diff --git a/vm/compiler/codegen/x86/libenc/enc_wrapper.h b/vm/compiler/codegen/x86/libenc/enc_wrapper.h
deleted file mode 100644
index 727159d..0000000
--- a/vm/compiler/codegen/x86/libenc/enc_wrapper.h
+++ /dev/null
@@ -1,279 +0,0 @@
-/*
- * Copyright (C) 2012 The Android Open Source Project
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *      http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef _VM_ENC_WRAPPER_H_
-#define _VM_ENC_WRAPPER_H_
-
-#include "enc_defs_ext.h"
-
-extern bool dump_x86_inst;
-typedef enum PhysicalReg {
-  // Currently initializing StartOfGPMarker to be 0 in order to match
-  // register index in Reg_No. However, ideally PhysicalReg_Null should
-  // be 0 and the rest moved over.
-  PhysicalReg_StartOfGPMarker = 0,
-  PhysicalReg_EAX = PhysicalReg_StartOfGPMarker,
-  PhysicalReg_EBX, PhysicalReg_ECX, PhysicalReg_EDX,
-  PhysicalReg_EDI, PhysicalReg_ESI, PhysicalReg_ESP, PhysicalReg_EBP,
-  PhysicalReg_EndOfGPMarker = PhysicalReg_EBP,
-
-  PhysicalReg_StartOfXmmMarker,
-  PhysicalReg_XMM0 = PhysicalReg_StartOfXmmMarker,
-  PhysicalReg_XMM1, PhysicalReg_XMM2, PhysicalReg_XMM3,
-  PhysicalReg_XMM4, PhysicalReg_XMM5, PhysicalReg_XMM6, PhysicalReg_XMM7,
-  PhysicalReg_EndOfXmmMarker = PhysicalReg_XMM7,
-
-  PhysicalReg_StartOfX87Marker,
-  PhysicalReg_ST0 = PhysicalReg_StartOfX87Marker,  PhysicalReg_ST1,
-  PhysicalReg_ST2, PhysicalReg_ST3, PhysicalReg_ST4, PhysicalReg_ST5,
-  PhysicalReg_ST6, PhysicalReg_ST7,
-  PhysicalReg_EndOfX87Marker = PhysicalReg_ST7,
-
-  PhysicalReg_Null,
-  //used as scratch logical register in NCG O1
-  //should not overlap with regular logical register, start from 100
-  PhysicalReg_SCRATCH_1 = 100, PhysicalReg_SCRATCH_2, PhysicalReg_SCRATCH_3, PhysicalReg_SCRATCH_4,
-  PhysicalReg_SCRATCH_5, PhysicalReg_SCRATCH_6, PhysicalReg_SCRATCH_7, PhysicalReg_SCRATCH_8,
-  PhysicalReg_SCRATCH_9, PhysicalReg_SCRATCH_10,
-
-  //This should be the last entry
-  PhysicalReg_Last = PhysicalReg_SCRATCH_10
-} PhysicalReg;
-
-typedef enum Reg_No {
-#ifdef _EM64T_
-    rax_reg = 0,rbx_reg,    rcx_reg,    rdx_reg,
-    rdi_reg,    rsi_reg,    rsp_reg,    rbp_reg,
-    r8_reg,     r9_reg,     r10_reg,    r11_reg,
-    r12_reg,    r13_reg,    r14_reg,    r15_reg,
-    xmm0_reg,   xmm1_reg,   xmm2_reg,   xmm3_reg,
-    xmm4_reg,   xmm5_reg,   xmm6_reg,   xmm7_reg,
-    xmm8_reg,   xmm9_reg,   xmm10_reg,  xmm11_reg,
-    xmm12_reg,  xmm13_reg,  xmm14_reg,  xmm15_reg,
-
-#else   // !defined(_EM64T_)
-
-    eax_reg = 0,ebx_reg,    ecx_reg,    edx_reg,
-    edi_reg,    esi_reg,    esp_reg,    ebp_reg,
-    xmm0_reg,   xmm1_reg,   xmm2_reg,   xmm3_reg,
-    xmm4_reg,   xmm5_reg,   xmm6_reg,   xmm7_reg,
-    fs_reg,
-#endif
-    /** @brief Total number of registers.*/
-    n_reg
-} Reg_No;
-//
-// instruction operand sizes: 8,16,32,64 bits
-//
-typedef enum Opnd_Size {
-    size_8 = 0,
-    size_16,
-    size_32,
-    size_64,
-    n_size,
-#ifdef _EM64T_
-    size_platf = size_64
-#else
-    size_platf = size_32
-#endif
-} Opnd_Size;
-
-//
-// opcodes for alu instructions
-//
-typedef enum ALU_Opcode {
-    add_opc = 0,or_opc,     adc_opc,    sbb_opc,
-    and_opc,    sub_opc,    xor_opc,    cmp_opc,
-    mul_opc,    imul_opc,   div_opc,    idiv_opc,
-    sll_opc,    srl_opc,    sra_opc, //shift right arithmetic
-    shl_opc,    shr_opc,
-    sal_opc,    sar_opc,
-    neg_opc,    not_opc,    andn_opc,
-    n_alu
-} ALU_Opcode;
-
-typedef enum ConditionCode {
-    Condition_O     = 0,
-    Condition_NO    = 1,
-    Condition_B     = 2,
-    Condition_NAE   = Condition_B,
-    Condition_C     = Condition_B,
-    Condition_NB    = 3,
-    Condition_AE    = Condition_NB,
-    Condition_NC    = Condition_NB,
-    Condition_Z     = 4,
-    Condition_E     = Condition_Z,
-    Condition_NZ    = 5,
-    Condition_NE    = Condition_NZ,
-    Condition_BE    = 6,
-    Condition_NA    = Condition_BE,
-    Condition_NBE   = 7,
-    Condition_A     = Condition_NBE,
-
-    Condition_S     = 8,
-    Condition_NS    = 9,
-    Condition_P     = 10,
-    Condition_PE    = Condition_P,
-    Condition_NP    = 11,
-    Condition_PO    = Condition_NP,
-    Condition_L     = 12,
-    Condition_NGE   = Condition_L,
-    Condition_NL    = 13,
-    Condition_GE    = Condition_NL,
-    Condition_LE    = 14,
-    Condition_NG    = Condition_LE,
-    Condition_NLE   = 15,
-    Condition_G     = Condition_NLE,
-    Condition_Count = 16
-} ConditionCode;
-
-//
-// prefix code
-//
-typedef enum InstrPrefix {
-    no_prefix,
-    lock_prefix                     = 0xF0,
-    hint_branch_taken_prefix        = 0x2E,
-    hint_branch_not_taken_prefix    = 0x3E,
-    prefix_repne                    = 0xF2,
-    prefix_repnz                    = prefix_repne,
-    prefix_repe                     = 0xF3,
-    prefix_repz                     = prefix_repe,
-    prefix_rep                      = 0xF3,
-    prefix_cs                       = 0x2E,
-    prefix_ss                       = 0x36,
-    prefix_ds                       = 0x3E,
-    prefix_es                       = 0x26,
-    prefix_fs                       = 0x64,
-    prefix_gs                       = 0x65
-} InstrPrefix;
-
-enum LowOpndRegType
-{
-    LowOpndRegType_gp = 0,
-    LowOpndRegType_fs = 1,
-    LowOpndRegType_xmm = 2,
-    LowOpndRegType_fs_s = 3,
-    LowOpndRegType_ss = 4,
-    LowOpndRegType_invalid = 256,
-};
-
-enum LogicalRegType
-{
-    LogicalType_invalid = 0,
-    LowOpndRegType_scratch = 8,
-    LowOpndRegType_temp = 16,
-    LowOpndRegType_hard = 32,
-    LowOpndRegType_virtual = 64,
-    LowOpndRegType_glue = 128,
-};
-
-//if inline, separte enc_wrapper.cpp into two files, one of them is .inl
-//           enc_wrapper.cpp needs to handle both cases
-#ifdef ENCODER_INLINE
-    #define ENCODER_DECLARE_EXPORT inline
-    #include "enc_wrapper.inl"
-#else
-    #define ENCODER_DECLARE_EXPORT
-#endif
-
-#ifdef __cplusplus
-extern "C"
-{
-#endif
-ENCODER_DECLARE_EXPORT char* encoder_imm(Mnemonic m, OpndSize size,
-                  int imm, char* stream);
-ENCODER_DECLARE_EXPORT unsigned encoder_get_inst_size(char * stream);
-ENCODER_DECLARE_EXPORT char* encoder_update_imm(int imm, char * stream);
-ENCODER_DECLARE_EXPORT char* encoder_mem(Mnemonic m, OpndSize size,
-               int disp, int base_reg, bool isBasePhysical, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_reg(Mnemonic m, OpndSize size,
-               int reg, bool isPhysical, LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_reg_reg(Mnemonic m, OpndSize size,
-                   int reg, bool isPhysical,
-                   int reg2, bool isPhysical2, LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_reg_reg_diff_sizes(Mnemonic m, OpndSize srcOpndSize,
-                   int reg, bool isPhysical, OpndSize destOpndSize,
-                   int reg2, bool isPhysical2, LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_mem_reg(Mnemonic m, OpndSize size,
-                   int disp, int base_reg, bool isBasePhysical,
-                   int reg, bool isPhysical, LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_mem_to_reg_diff_sizes(Mnemonic m, OpndSize memOpndSize,
-                   int disp, int base_reg, bool isBasePhysical, OpndSize regOpndSize,
-                   int reg, bool isPhysical, LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_mem_scale_reg(Mnemonic m, OpndSize size,
-                         int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
-                         int reg, bool isPhysical, LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_reg_mem_scale(Mnemonic m, OpndSize size,
-                         int reg, bool isPhysical,
-                         int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
-                         LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_reg(Mnemonic m, OpndSize size,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         int reg, bool isPhysical, LowOpndRegType type, char * stream);
-ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_to_reg_diff_sizes(Mnemonic m, OpndSize memOpndSize,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         OpndSize regOpndSize, int reg, bool isPhysical, LowOpndRegType type, char * stream);
-ENCODER_DECLARE_EXPORT char * encoder_movzs_mem_disp_scale_reg(Mnemonic m, OpndSize size,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         int reg, bool isPhysical, LowOpndRegType type, char * stream);
-ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_to_reg_2(Mnemonic m, OpndSize memOpndSize,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         OpndSize regOpndSize, int reg, bool isPhysical, LowOpndRegType type, char * stream);
-ENCODER_DECLARE_EXPORT char* encoder_reg_mem_disp_scale(Mnemonic m, OpndSize size,
-                         int reg, bool isPhysical,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_reg_mem(Mnemonic m, OpndSize size,
-                   int reg, bool isPhysical,
-                   int disp, int base_reg, bool isBasePhysical, LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_imm_reg(Mnemonic m, OpndSize size,
-                   int imm, int reg, bool isPhysical, LowOpndRegType type, char* stream);
-ENCODER_DECLARE_EXPORT char * encoder_imm_reg_diff_sizes(Mnemonic m, OpndSize sizeImm,
-                   int imm, OpndSize sizeReg, int reg, bool isPhysical, LowOpndRegType type, char * stream);
-ENCODER_DECLARE_EXPORT char * encoder_update_imm_rm(int imm, char * stream);
-ENCODER_DECLARE_EXPORT char* encoder_imm_mem(Mnemonic m, OpndSize size,
-                   int imm,
-                   int disp, int base_reg, bool isBasePhysical, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_fp_mem(Mnemonic m, OpndSize size, int reg,
-                  int disp, int base_reg, bool isBasePhysical, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_mem_fp(Mnemonic m, OpndSize size,
-                  int disp, int base_reg, bool isBasePhysical,
-                  int reg, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_return(char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_compare_fp_stack(bool pop, int reg, bool isDouble, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_movez_mem_to_reg(OpndSize size,
-                      int disp, int base_reg, bool isBasePhysical,
-                      int reg, bool isPhysical, char* stream);
-ENCODER_DECLARE_EXPORT char* encoder_moves_mem_to_reg(OpndSize size,
-                      int disp, int base_reg, bool isBasePhysical,
-                      int reg, bool isPhysical, char* stream);
-ENCODER_DECLARE_EXPORT char * encoder_movez_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical, int reg2,
-                      bool isPhysical2, LowOpndRegType type, char * stream);
-ENCODER_DECLARE_EXPORT char * encoder_moves_reg_to_reg(OpndSize size,
-                      int reg, bool isPhysical, int reg2,
-                      bool isPhysical2, LowOpndRegType type, char * stream);
-ENCODER_DECLARE_EXPORT char * encoder_nops(unsigned numBytes, char * stream);
-ENCODER_DECLARE_EXPORT int decodeThenPrint(char* stream_start);
-ENCODER_DECLARE_EXPORT char* decoder_disassemble_instr(char* stream, char* strbuf, unsigned int len);
-
-//Provide a char* equivalent to a PhysicalReg type
-ENCODER_DECLARE_EXPORT const char * physicalRegToString(PhysicalReg reg);
-#ifdef __cplusplus
-}
-#endif
-#endif // _VM_ENC_WRAPPER_H_
diff --git a/vm/compiler/codegen/x86/libenc/encoder.cpp b/vm/compiler/codegen/x86/libenc/encoder.cpp
deleted file mode 100644
index ef08a4d..0000000
--- a/vm/compiler/codegen/x86/libenc/encoder.cpp
+++ /dev/null
@@ -1,155 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-#include <stdio.h>
-#include <assert.h>
-
-#include <limits.h>
-
-#include "enc_base.h"
-
-#ifdef NO_ENCODER_INLINE
-    #include "encoder.h"
-    #include "encoder.inl"
-#else
-    #define NO_ENCODER_INLINE
-    #include "encoder.h"
-    #undef NO_ENCODER_INLINE
-#endif
-
-
-
-#ifdef _EM64T_
-
-R_Opnd rax_opnd(rax_reg);
-R_Opnd rcx_opnd(rcx_reg);
-R_Opnd rdx_opnd(rdx_reg);
-R_Opnd rbx_opnd(rbx_reg);
-R_Opnd rsp_opnd(rsp_reg);
-R_Opnd rbp_opnd(rbp_reg);
-R_Opnd rsi_opnd(rsi_reg);
-R_Opnd rdi_opnd(rdi_reg);
-
-R_Opnd r8_opnd(r8_reg);
-R_Opnd r9_opnd(r9_reg);
-R_Opnd r10_opnd(r10_reg);
-R_Opnd r11_opnd(r11_reg);
-R_Opnd r12_opnd(r12_reg);
-R_Opnd r13_opnd(r13_reg);
-R_Opnd r14_opnd(r14_reg);
-R_Opnd r15_opnd(r15_reg);
-
-XMM_Opnd xmm8_opnd(xmm8_reg);
-XMM_Opnd xmm9_opnd(xmm9_reg);
-XMM_Opnd xmm10_opnd(xmm10_reg);
-XMM_Opnd xmm11_opnd(xmm11_reg);
-XMM_Opnd xmm12_opnd(xmm12_reg);
-XMM_Opnd xmm13_opnd(xmm13_reg);
-XMM_Opnd xmm14_opnd(xmm14_reg);
-XMM_Opnd xmm15_opnd(xmm15_reg);
-
-#else
-
-R_Opnd eax_opnd(eax_reg);
-R_Opnd ecx_opnd(ecx_reg);
-R_Opnd edx_opnd(edx_reg);
-R_Opnd ebx_opnd(ebx_reg);
-R_Opnd esp_opnd(esp_reg);
-R_Opnd ebp_opnd(ebp_reg);
-R_Opnd esi_opnd(esi_reg);
-R_Opnd edi_opnd(edi_reg);
-
-#endif //_EM64T_
-
-XMM_Opnd xmm0_opnd(xmm0_reg);
-XMM_Opnd xmm1_opnd(xmm1_reg);
-XMM_Opnd xmm2_opnd(xmm2_reg);
-XMM_Opnd xmm3_opnd(xmm3_reg);
-XMM_Opnd xmm4_opnd(xmm4_reg);
-XMM_Opnd xmm5_opnd(xmm5_reg);
-XMM_Opnd xmm6_opnd(xmm6_reg);
-XMM_Opnd xmm7_opnd(xmm7_reg);
-
-
-#define countof(a)      (sizeof(a)/sizeof(a[0]))
-
-extern const RegName map_of_regno_2_regname[];
-extern const OpndSize map_of_EncoderOpndSize_2_RealOpndSize[];
-extern const Mnemonic map_of_alu_opcode_2_mnemonic[];
-extern const Mnemonic map_of_shift_opcode_2_mnemonic[];
-
-const RegName map_of_regno_2_regname [] = {
-#ifdef _EM64T_
-    RegName_RAX,    RegName_RBX,    RegName_RCX,    RegName_RDX,
-    RegName_RDI,    RegName_RSI,    RegName_RSP,    RegName_RBP,
-    RegName_R8,     RegName_R9,     RegName_R10,    RegName_R11,
-    RegName_R12,    RegName_R13,    RegName_R14,    RegName_R15,
-    RegName_XMM0,   RegName_XMM1,   RegName_XMM2,   RegName_XMM3,
-    RegName_XMM4,   RegName_XMM5,   RegName_XMM6,   RegName_XMM7,
-    RegName_XMM8,   RegName_XMM9,   RegName_XMM10,  RegName_XMM11,
-    RegName_XMM12,  RegName_XMM13,   RegName_XMM14, RegName_XMM15,
-
-#else
-    RegName_EAX,    RegName_EBX,    RegName_ECX,    RegName_EDX,
-    RegName_EDI,    RegName_ESI,    RegName_ESP,    RegName_EBP,
-    RegName_XMM0,   RegName_XMM1,   RegName_XMM2,   RegName_XMM3,
-    RegName_XMM4,   RegName_XMM5,   RegName_XMM6,   RegName_XMM7,
-    RegName_FS,
-#endif  // _EM64T_
-
-    RegName_Null,
-};
-
-const OpndSize map_of_EncoderOpndSize_2_RealOpndSize[] = {
-    OpndSize_8, OpndSize_16, OpndSize_32, OpndSize_64, OpndSize_Any
-};
-
-const Mnemonic map_of_alu_opcode_2_mnemonic[] = {
-    //add_opc=0,  or_opc,           adc_opc,        sbb_opc,
-    //and_opc,      sub_opc,        xor_opc,        cmp_opc,
-    //n_alu
-    Mnemonic_ADD,   Mnemonic_OR,    Mnemonic_ADC,   Mnemonic_SBB,
-    Mnemonic_AND,   Mnemonic_SUB,   Mnemonic_XOR,   Mnemonic_CMP,
-};
-
-const Mnemonic map_of_shift_opcode_2_mnemonic[] = {
-    //shld_opc, shrd_opc,
-    // shl_opc, shr_opc, sar_opc, ror_opc, max_shift_opcode=6,
-    // n_shift = 6
-    Mnemonic_SHLD,  Mnemonic_SHRD,
-    Mnemonic_SHL,   Mnemonic_SHR,   Mnemonic_SAR, Mnemonic_ROR
-};
-
-#ifdef _DEBUG
-
-static int debug_check() {
-    // Checks some assumptions.
-
-    // 1. all items of Encoder.h:enum Reg_No  must be mapped plus n_reg->RegName_Null
-    assert(countof(map_of_regno_2_regname) == n_reg + 1);
-    assert(countof(map_of_alu_opcode_2_mnemonic) == n_alu);
-    assert(countof(map_of_shift_opcode_2_mnemonic) == n_shift);
-    return 0;
-}
-
-static int dummy = debug_check();
-
-// can have this - initialization order problems.... static int dummy_run_the_debug_test = debug_check();
-
-#endif
diff --git a/vm/compiler/codegen/x86/libenc/encoder.h b/vm/compiler/codegen/x86/libenc/encoder.h
deleted file mode 100644
index 9ac0219..0000000
--- a/vm/compiler/codegen/x86/libenc/encoder.h
+++ /dev/null
@@ -1,717 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-/**
- * @file
- * @brief Simple interface for generating processor instructions.
- *
- * The interface works for both IA32 and EM64T. By default, only IA32
- * capabilities are presented. To enable EM64T feature, the _EM64T_ macro
- * must be defined (and, of course, a proper library version to be used).
- *
- * The interface is based on the original ia32.h encoder interface,
- * with some simplifications and add-ons - EM64T-specific, SSE and SSE2.
- *
- * The interface mostly intended for existing legacy code like LIL code
- * generator. From the implementation point of view, it's just a wrapper
- * around the EncoderBase functionality.
- */
-
-#ifndef _VM_ENCODER_H_
-#define _VM_ENCODER_H_
-
-#include <limits.h>
-#include "enc_base.h"
-//#include "open/types.h"
-
-#ifdef _EM64T_
-// size of general-purpose value on the stack in bytes
-#define GR_STACK_SIZE 8
-// size of floating-point value on the stack in bytes
-#define FR_STACK_SIZE 8
-
-#if defined(WIN32) || defined(_WIN64)
-    // maximum number of GP registers for inputs
-    const int MAX_GR = 4;
-    // maximum number of FP registers for inputs
-    const int MAX_FR = 4;
-    // WIN64 reserves 4 words for shadow space
-    const int SHADOW = 4 * GR_STACK_SIZE;
-#else
-    // maximum number of GP registers for inputs
-    const int MAX_GR = 6;
-    // maximum number of FP registers for inputs
-    const int MAX_FR = 8;
-    // Linux x64 doesn't reserve shadow space
-    const int SHADOW = 0;
-#endif
-
-#else
-// size of general-purpose value on the stack in bytes
-#define GR_STACK_SIZE 4
-// size of general-purpose value on the stack in bytes
-#define FR_STACK_SIZE 8
-
-// maximum number of GP registers for inputs
-const int MAX_GR = 0;
-// maximum number of FP registers for inputs
-const int MAX_FR = 0;
-#endif
-
-typedef enum Reg_No {
-#ifdef _EM64T_
-    rax_reg = 0,rbx_reg,    rcx_reg,    rdx_reg,
-    rdi_reg,    rsi_reg,    rsp_reg,    rbp_reg,
-    r8_reg,     r9_reg,     r10_reg,    r11_reg,
-    r12_reg,    r13_reg,    r14_reg,    r15_reg,
-    xmm0_reg,   xmm1_reg,   xmm2_reg,   xmm3_reg,
-    xmm4_reg,   xmm5_reg,   xmm6_reg,   xmm7_reg,
-    xmm8_reg,   xmm9_reg,   xmm10_reg,  xmm11_reg,
-    xmm12_reg,  xmm13_reg,  xmm14_reg,  xmm15_reg,
-
-#else   // !defined(_EM64T_)
-
-    eax_reg = 0,ebx_reg,    ecx_reg,    edx_reg,
-    edi_reg,    esi_reg,    esp_reg,    ebp_reg,
-    xmm0_reg,   xmm1_reg,   xmm2_reg,   xmm3_reg,
-    xmm4_reg,   xmm5_reg,   xmm6_reg,   xmm7_reg,
-    fs_reg,
-#endif
-    /** @brief Total number of registers.*/
-    n_reg
-} Reg_No;
-//
-// instruction operand sizes: 8,16,32,64 bits
-//
-typedef enum Opnd_Size {
-    size_8 = 0,
-    size_16,
-    size_32,
-    size_64,
-    n_size,
-#ifdef _EM64T_
-    size_platf = size_64
-#else
-    size_platf = size_32
-#endif
-} Opnd_Size;
-
-//
-// opcodes for alu instructions
-//
-typedef enum ALU_Opcode {
-    add_opc = 0,or_opc,     adc_opc,    sbb_opc,
-    and_opc,    sub_opc,    xor_opc,    cmp_opc,
-    n_alu
-} ALU_Opcode;
-
-//
-// opcodes for shift instructions
-//
-typedef enum Shift_Opcode {
-    shld_opc,   shrd_opc,   shl_opc,    shr_opc,
-    sar_opc,    ror_opc, max_shift_opcode=6,     n_shift = 6
-} Shift_Opcode;
-
-typedef enum ConditionCode {
-    Condition_O     = 0,
-    Condition_NO    = 1,
-    Condition_B     = 2,
-    Condition_NAE   = Condition_B,
-    Condition_C     = Condition_B,
-    Condition_NB    = 3,
-    Condition_AE    = Condition_NB,
-    Condition_NC    = Condition_NB,
-    Condition_Z     = 4,
-    Condition_E     = Condition_Z,
-    Condition_NZ    = 5,
-    Condition_NE    = Condition_NZ,
-    Condition_BE    = 6,
-    Condition_NA    = Condition_BE,
-    Condition_NBE   = 7,
-    Condition_A     = Condition_NBE,
-
-    Condition_S     = 8,
-    Condition_NS    = 9,
-    Condition_P     = 10,
-    Condition_PE    = Condition_P,
-    Condition_NP    = 11,
-    Condition_PO    = Condition_NP,
-    Condition_L     = 12,
-    Condition_NGE   = Condition_L,
-    Condition_NL    = 13,
-    Condition_GE    = Condition_NL,
-    Condition_LE    = 14,
-    Condition_NG    = Condition_LE,
-    Condition_NLE   = 15,
-    Condition_G     = Condition_NLE,
-    Condition_Count = 16
-} ConditionCode;
-
-//
-// prefix code
-//
-typedef enum InstrPrefix {
-    no_prefix,
-    lock_prefix                     = 0xF0,
-    hint_branch_taken_prefix        = 0x2E,
-    hint_branch_not_taken_prefix    = 0x3E,
-    prefix_repne                    = 0xF2,
-    prefix_repnz                    = prefix_repne,
-    prefix_repe                     = 0xF3,
-    prefix_repz                     = prefix_repe,
-    prefix_rep                      = 0xF3,
-    prefix_cs                       = 0x2E,
-    prefix_ss                       = 0x36,
-    prefix_ds                       = 0x3E,
-    prefix_es                       = 0x26,
-    prefix_fs                       = 0x64,
-    prefix_gs                       = 0x65
-} InstrPrefix;
-
-
-//
-// an instruction operand
-//
-class Opnd {
-
-protected:
-    enum Tag { SignedImm, UnsignedImm, Reg, Mem, FP, XMM };
-
-    const Tag  tag;
-
-    Opnd(Tag t): tag(t) {}
-
-public:
-    void * operator new(size_t, void * mem) {
-        return mem;
-    }
-
-    void operator delete(void *) {}
-
-    void operator delete(void *, void *) {}
-
-private:
-    // disallow copying
-    Opnd(const Opnd &): tag(Mem) { assert(false); }
-    Opnd& operator=(const Opnd &) { assert(false); return *this; }
-};
-typedef int I_32;
-class Imm_Opnd: public Opnd {
-
-protected:
-    union {
-#ifdef _EM64T_
-        int64           value;
-        unsigned char   bytes[8];
-#else
-        I_32           value;
-        unsigned char   bytes[4];
-#endif
-    };
-    Opnd_Size           size;
-
-public:
-    Imm_Opnd(I_32 val, bool isSigned = true):
-        Opnd(isSigned ? SignedImm : UnsignedImm), value(val), size(size_32) {
-        if (isSigned) {
-            if (CHAR_MIN <= val && val <= CHAR_MAX) {
-                size = size_8;
-            } else if (SHRT_MIN <= val && val <= SHRT_MAX) {
-                size = size_16;
-            }
-        } else {
-            assert(val >= 0);
-            if (val <= UCHAR_MAX) {
-                size = size_8;
-            } else if (val <= USHRT_MAX) {
-                size = size_16;
-            }
-        }
-    }
-    Imm_Opnd(const Imm_Opnd& that): Opnd(that.tag), value(that.value), size(that.size) {};
-
-#ifdef _EM64T_
-    Imm_Opnd(Opnd_Size sz, int64 val, bool isSigned = true):
-        Opnd(isSigned ? SignedImm : UnsignedImm), value(val), size(sz) {
-#ifndef NDEBUG
-        switch (size) {
-        case size_8:
-            assert(val == (int64)(I_8)val);
-            break;
-        case size_16:
-            assert(val == (int64)(int16)val);
-            break;
-        case size_32:
-            assert(val == (int64)(I_32)val);
-            break;
-        case size_64:
-            break;
-        case n_size:
-            assert(false);
-            break;
-        }
-#endif // NDEBUG
-    }
-
-    int64 get_value() const { return value; }
-
-#else
-
-    Imm_Opnd(Opnd_Size sz, I_32 val, int isSigned = true):
-        Opnd(isSigned ? SignedImm : UnsignedImm), value(val), size(sz) {
-#ifndef NDEBUG
-        switch (size) {
-        case size_8:
-            assert((I_32)val == (I_32)(I_8)val);
-            break;
-        case size_16:
-            assert((I_32)val == (I_32)(int16)val);
-            break;
-        case size_32:
-            break;
-        case size_64:
-        case n_size:
-            assert(false);
-            break;
-        }
-#endif // NDEBUG
-    }
-
-    I_32 get_value() const { return value; }
-
-#endif
-    Opnd_Size get_size() const { return size; }
-    bool      is_signed() const { return tag == SignedImm; }
-};
-
-class RM_Opnd: public Opnd {
-
-public:
-    bool is_reg() const { return tag != SignedImm && tag != UnsignedImm && tag != Mem; }
-
-protected:
-    RM_Opnd(Tag t): Opnd(t) {}
-
-private:
-    // disallow copying
-    RM_Opnd(const RM_Opnd &): Opnd(Reg) { assert(false); }
-};
-
-class R_Opnd: public RM_Opnd {
-
-protected:
-    Reg_No      _reg_no;
-
-public:
-    R_Opnd(Reg_No r): RM_Opnd(Reg), _reg_no(r) {}
-    Reg_No  reg_no() const { return _reg_no; }
-
-private:
-    // disallow copying
-    R_Opnd(const R_Opnd &): RM_Opnd(Reg) { assert(false); }
-};
-
-//
-// a memory operand with displacement
-// Can also serve as a full memory operand with base,index, displacement and scale.
-// Use n_reg to specify 'no register', say, for index.
-class M_Opnd: public RM_Opnd {
-
-protected:
-    Imm_Opnd        m_disp;
-    Imm_Opnd        m_scale;
-    R_Opnd          m_index;
-    R_Opnd          m_base;
-
-public:
-    //M_Opnd(Opnd_Size sz): RM_Opnd(Mem, K_M, sz), m_disp(0), m_scale(0), m_index(n_reg), m_base(n_reg) {}
-    M_Opnd(I_32 disp):
-        RM_Opnd(Mem), m_disp(disp), m_scale(0), m_index(n_reg), m_base(n_reg) {}
-    M_Opnd(Reg_No rbase, I_32 rdisp):
-        RM_Opnd(Mem), m_disp(rdisp), m_scale(0), m_index(n_reg), m_base(rbase) {}
-    M_Opnd(I_32 disp, Reg_No rbase, Reg_No rindex, unsigned scale):
-        RM_Opnd(Mem), m_disp(disp), m_scale(scale), m_index(rindex), m_base(rbase) {}
-    M_Opnd(const M_Opnd & that) : RM_Opnd(Mem),
-        m_disp((int)that.m_disp.get_value()), m_scale((int)that.m_scale.get_value()),
-        m_index(that.m_index.reg_no()), m_base(that.m_base.reg_no())
-        {}
-    //
-    inline const R_Opnd & base(void) const { return m_base; }
-    inline const R_Opnd & index(void) const { return m_index; }
-    inline const Imm_Opnd & scale(void) const { return m_scale; }
-    inline const Imm_Opnd & disp(void) const { return m_disp; }
-};
-
-//
-//  a memory operand with base register and displacement
-//
-class M_Base_Opnd: public M_Opnd {
-
-public:
-    M_Base_Opnd(Reg_No base, I_32 disp) : M_Opnd(disp, base, n_reg, 0) {}
-
-private:
-    // disallow copying - but it leads to ICC errors #734 in encoder.inl
-    // M_Base_Opnd(const M_Base_Opnd &): M_Opnd(0) { assert(false); }
-};
-
-//
-//  a memory operand with base register, scaled index register
-//  and displacement.
-//
-class M_Index_Opnd : public M_Opnd {
-
-public:
-    M_Index_Opnd(Reg_No base, Reg_No index, I_32 disp, unsigned scale):
-        M_Opnd(disp, base, index, scale) {}
-
-private:
-    // disallow copying - but it leads to ICC errors #734 in encoder.inl
-    // M_Index_Opnd(const M_Index_Opnd &): M_Opnd(0) { assert(false); }
-};
-
-class XMM_Opnd : public Opnd {
-
-protected:
-    unsigned        m_idx;
-
-public:
-    XMM_Opnd(unsigned _idx): Opnd(XMM), m_idx(_idx) {};
-    unsigned get_idx( void ) const { return m_idx; };
-
-private:
-    // disallow copying
-    XMM_Opnd(const XMM_Opnd &): Opnd(XMM) { assert(false); }
-};
-
-//
-// operand structures for ia32 registers
-//
-#ifdef _EM64T_
-
-extern R_Opnd rax_opnd;
-extern R_Opnd rcx_opnd;
-extern R_Opnd rdx_opnd;
-extern R_Opnd rbx_opnd;
-extern R_Opnd rdi_opnd;
-extern R_Opnd rsi_opnd;
-extern R_Opnd rsp_opnd;
-extern R_Opnd rbp_opnd;
-
-extern R_Opnd r8_opnd;
-extern R_Opnd r9_opnd;
-extern R_Opnd r10_opnd;
-extern R_Opnd r11_opnd;
-extern R_Opnd r12_opnd;
-extern R_Opnd r13_opnd;
-extern R_Opnd r14_opnd;
-extern R_Opnd r15_opnd;
-
-extern XMM_Opnd xmm8_opnd;
-extern XMM_Opnd xmm9_opnd;
-extern XMM_Opnd xmm10_opnd;
-extern XMM_Opnd xmm11_opnd;
-extern XMM_Opnd xmm12_opnd;
-extern XMM_Opnd xmm13_opnd;
-extern XMM_Opnd xmm14_opnd;
-extern XMM_Opnd xmm15_opnd;
-#else
-
-extern R_Opnd eax_opnd;
-extern R_Opnd ecx_opnd;
-extern R_Opnd edx_opnd;
-extern R_Opnd ebx_opnd;
-extern R_Opnd esp_opnd;
-extern R_Opnd ebp_opnd;
-extern R_Opnd esi_opnd;
-extern R_Opnd edi_opnd;
-
-#endif // _EM64T_
-
-extern XMM_Opnd xmm0_opnd;
-extern XMM_Opnd xmm1_opnd;
-extern XMM_Opnd xmm2_opnd;
-extern XMM_Opnd xmm3_opnd;
-extern XMM_Opnd xmm4_opnd;
-extern XMM_Opnd xmm5_opnd;
-extern XMM_Opnd xmm6_opnd;
-extern XMM_Opnd xmm7_opnd;
-
-#ifdef NO_ENCODER_INLINE
-    #define ENCODER_DECLARE_EXPORT
-#else
-    #define ENCODER_DECLARE_EXPORT inline
-    #include "encoder.inl"
-#endif
-
-// prefix
-ENCODER_DECLARE_EXPORT char * prefix(char * stream, InstrPrefix p);
-
-// stack push and pop instructions
-ENCODER_DECLARE_EXPORT char * push(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * push(char * stream, const Imm_Opnd & imm);
-ENCODER_DECLARE_EXPORT char * pop(char * stream,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
-
-// cmpxchg or xchg
-ENCODER_DECLARE_EXPORT char * cmpxchg(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * xchg(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz = size_platf);
-
-// inc(rement), dec(rement), not, neg(ate) instructions
-ENCODER_DECLARE_EXPORT char * inc(char * stream,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * dec(char * stream,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * _not(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * neg(char * stream,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * nop(char * stream);
-ENCODER_DECLARE_EXPORT char * int3(char * stream);
-
-// alu instructions: add, or, adc, sbb, and, sub, xor, cmp
-ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const M_Opnd & m, const R_Opnd & r, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-
-// test instruction
-ENCODER_DECLARE_EXPORT char * test(char * stream, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * test(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz = size_platf);
-
-// shift instructions: shl, shr, sar, shld, shrd, ror
-ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode opc, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode opc, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode opc, const RM_Opnd & rm, const R_Opnd & r, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode opc, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz = size_platf);
-
-// multiply instructions: mul, imul
-ENCODER_DECLARE_EXPORT char * mul(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const RM_Opnd & rm, const Imm_Opnd& imm, Opnd_Size sz = size_platf);
-
-// divide instructions: div, idiv
-ENCODER_DECLARE_EXPORT char * idiv(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * div(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-
-// data movement: mov
-ENCODER_DECLARE_EXPORT char * mov(char * stream, const M_Opnd & m,  const R_Opnd & r, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * mov(char * stream, const R_Opnd & r,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * mov(char * stream, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
-
-ENCODER_DECLARE_EXPORT char * movsx( char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * movzx( char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-
-ENCODER_DECLARE_EXPORT char * movd(char * stream, const RM_Opnd & rm, const XMM_Opnd & xmm);
-ENCODER_DECLARE_EXPORT char * movd(char * stream, const XMM_Opnd & xmm, const RM_Opnd & rm);
-ENCODER_DECLARE_EXPORT char * movq(char * stream, const RM_Opnd & rm, const XMM_Opnd & xmm);
-ENCODER_DECLARE_EXPORT char * movq(char * stream, const XMM_Opnd & xmm, const RM_Opnd & rm);
-
-// sse mov
-ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const M_Opnd & mem, const XMM_Opnd & xmm, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
-
-// sse add, sub, mul, div
-ENCODER_DECLARE_EXPORT char * sse_add(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_add(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
-
-ENCODER_DECLARE_EXPORT char * sse_sub(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_sub(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
-
-ENCODER_DECLARE_EXPORT char * sse_mul(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_mul(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
-
-ENCODER_DECLARE_EXPORT char * sse_div(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_div(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
-
-// xor, compare
-ENCODER_DECLARE_EXPORT char * sse_xor(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1);
-
-ENCODER_DECLARE_EXPORT char * sse_compare(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_compare(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem, bool dbl);
-
-// sse conversions
-ENCODER_DECLARE_EXPORT char * sse_cvt_si(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_cvtt2si(char * stream, const R_Opnd & reg, const M_Opnd & mem, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_cvtt2si(char * stream, const R_Opnd & reg, const XMM_Opnd & xmm, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_cvt_fp2dq(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_cvt_dq2fp(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
-ENCODER_DECLARE_EXPORT char * sse_d2s(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem64);
-ENCODER_DECLARE_EXPORT char * sse_d2s(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1);
-ENCODER_DECLARE_EXPORT char * sse_s2d(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem32);
-ENCODER_DECLARE_EXPORT char * sse_s2d(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1);
-
-// condition operations
-ENCODER_DECLARE_EXPORT char * cmov(char * stream, ConditionCode cc, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * setcc(char * stream, ConditionCode cc, const RM_Opnd & rm8);
-
-// load effective address: lea
-ENCODER_DECLARE_EXPORT char * lea(char * stream, const R_Opnd & r, const M_Opnd & m, Opnd_Size sz = size_platf);
-ENCODER_DECLARE_EXPORT char * cdq(char * stream);
-ENCODER_DECLARE_EXPORT char * wait(char * stream);
-
-// control-flow instructions
-ENCODER_DECLARE_EXPORT char * loop(char * stream, const Imm_Opnd & imm);
-
-// jump with 8-bit relative
-ENCODER_DECLARE_EXPORT char * jump8(char * stream, const Imm_Opnd & imm);
-
-// jump with 32-bit relative
-ENCODER_DECLARE_EXPORT char * jump32(char * stream, const Imm_Opnd & imm);
-
-// register indirect jump
-ENCODER_DECLARE_EXPORT char * jump(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-
-// jump to target address
-ENCODER_DECLARE_EXPORT char *jump(char * stream, char *target);
-
-// jump with displacement
-//char * jump(char * stream, I_32 disp);
-
-// conditional branch with 8-bit branch offset
-ENCODER_DECLARE_EXPORT char * branch8(char * stream, ConditionCode cc, const Imm_Opnd & imm, InstrPrefix prefix = no_prefix);
-
-// conditional branch with 32-bit branch offset
-ENCODER_DECLARE_EXPORT char * branch32(char * stream, ConditionCode cc, const Imm_Opnd & imm, InstrPrefix prefix = no_prefix);
-
-// conditional branch with target label address
-//char * branch(char * stream, ConditionCode cc, const char * target, InstrPrefix prefix = no_prefix);
-
-// conditional branch with displacement immediate
-ENCODER_DECLARE_EXPORT char * branch(char * stream, ConditionCode cc, I_32 disp, InstrPrefix prefix = no_prefix);
-
-// call with displacement
-ENCODER_DECLARE_EXPORT char * call(char * stream, const Imm_Opnd & imm);
-
-// indirect call through register or memory location
-ENCODER_DECLARE_EXPORT char * call(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
-
-// call target address
-ENCODER_DECLARE_EXPORT char * call(char * stream, const char * target);
-
-// return instruction
-ENCODER_DECLARE_EXPORT char * ret(char * stream);
-ENCODER_DECLARE_EXPORT char * ret(char * stream, unsigned short pop);
-ENCODER_DECLARE_EXPORT char * ret(char * stream, const Imm_Opnd & imm);
-
-// string operations
-ENCODER_DECLARE_EXPORT char * set_d(char * stream, bool set);
-ENCODER_DECLARE_EXPORT char * scas(char * stream, unsigned char prefix);
-ENCODER_DECLARE_EXPORT char * stos(char * stream, unsigned char prefix);
-
-// floating-point instructions
-
-// st(0) = st(0) fp_op m{32,64}real
-//!char * fp_op_mem(char * stream, FP_Opcode opc,const M_Opnd& mem,int is_double);
-
-// st(0) = st(0) fp_op st(i)
-//!char *fp_op(char * stream, FP_Opcode opc,unsigned i);
-
-// st(i) = st(i) fp_op st(0)    ; optionally pop stack
-//!char * fp_op(char * stream, FP_Opcode opc,unsigned i,unsigned pop_stk);
-
-// compare st(0),st(1) and pop stack twice
-//!char * fcompp(char * stream);
-ENCODER_DECLARE_EXPORT char * fldcw(char * stream, const M_Opnd & mem);
-ENCODER_DECLARE_EXPORT char * fnstcw(char * stream, const M_Opnd & mem);
-ENCODER_DECLARE_EXPORT char * fnstsw(char * stream);
-//!char * fchs(char * stream);
-//!char * frem(char * stream);
-//!char * fxch(char * stream,unsigned i);
-//!char * fcomip(char * stream, unsigned i);
-
-// load from memory (as fp) into fp register stack
-ENCODER_DECLARE_EXPORT char * fld(char * stream, const M_Opnd & m, bool is_double);
-//!char *fld80(char * stream,const M_Opnd& mem);
-
-// load from memory (as int) into fp register stack
-//!char * fild(char * stream,const M_Opnd& mem,int is_long);
-
-// push st(i) onto fp register stack
-//!char * fld(char * stream,unsigned i);
-
-// push the constants 0.0 and 1.0 onto the fp register stack
-//!char * fldz(char * stream);
-//!char * fld1(char * stream);
-
-// store stack to memory (as int), always popping the stack
-ENCODER_DECLARE_EXPORT char * fist(char * stream, const M_Opnd & mem, bool is_long, bool pop_stk);
-// store stack to to memory (as fp), optionally popping the stack
-ENCODER_DECLARE_EXPORT char * fst(char * stream, const M_Opnd & m, bool is_double, bool pop_stk);
-// store ST(0) to ST(i), optionally popping the stack. Takes 1 clock
-ENCODER_DECLARE_EXPORT char * fst(char * stream, unsigned i, bool pop_stk);
-
-//!char * pushad(char * stream);
-//!char * pushfd(char * stream);
-//!char * popad(char * stream);
-//!char * popfd(char * stream);
-
-// stack frame allocation instructions: enter & leave
-//
-//    enter frame_size
-//
-//    is equivalent to:
-//
-//    push    ebp
-//    mov     ebp,esp
-//    sub     esp,frame_size
-//
-//!char *enter(char * stream,const Imm_Opnd& imm);
-
-// leave
-// is equivalent to:
-//
-// mov        esp,ebp
-// pop        ebp
-//!char *leave(char * stream);
-
-// sahf  loads SF, ZF, AF, PF, and CF flags from eax
-//!char *sahf(char * stream);
-
-// Intrinsic FP math functions
-
-//!char *math_fsin(char * stream);
-//!char *math_fcos(char * stream);
-//!char *math_fabs(char * stream);
-//!char *math_fpatan(char * stream);
-ENCODER_DECLARE_EXPORT char * fprem(char * stream);
-ENCODER_DECLARE_EXPORT char * fprem1(char * stream);
-//!char *math_frndint(char * stream);
-//!char *math_fptan(char * stream);
-
-//
-// Add 1-7 bytes padding, with as few instructions as possible,
-// with no effect on the processor state (e.g., registers, flags)
-//
-//!char *padding(char * stream, unsigned num);
-
-// prolog and epilog code generation
-//- char *prolog(char * stream,unsigned frame_size,unsigned reg_save_mask);
-//- char *epilog(char * stream,unsigned reg_save_mask);
-
-//!extern R_Opnd reg_operand_array[];
-
-// fsave and frstor
-//!char *fsave(char * stream);
-//!char *frstor(char * stream);
-
-// lahf : Load Status Flags into AH Register
-//!char *lahf(char * stream);
-
-// mfence : Memory Fence
-//!char *mfence(char * stream);
-
-#endif // _VM_ENCODER_H_
diff --git a/vm/compiler/codegen/x86/libenc/encoder.inl b/vm/compiler/codegen/x86/libenc/encoder.inl
deleted file mode 100644
index ec72097..0000000
--- a/vm/compiler/codegen/x86/libenc/encoder.inl
+++ /dev/null
@@ -1,863 +0,0 @@
-/*
- *  Licensed to the Apache Software Foundation (ASF) under one or more
- *  contributor license agreements.  See the NOTICE file distributed with
- *  this work for additional information regarding copyright ownership.
- *  The ASF licenses this file to You under the Apache License, Version 2.0
- *  (the "License"); you may not use this file except in compliance with
- *  the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- *  Unless required by applicable law or agreed to in writing, software
- *  distributed under the License is distributed on an "AS IS" BASIS,
- *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- *  See the License for the specific language governing permissions and
- *  limitations under the License.
- */
-/**
- * @author Alexander V. Astapchuk
- */
-#include <stdio.h>
-#include <assert.h>
-#include <limits.h>
-
-extern const RegName map_of_regno_2_regname[];
-extern const OpndSize map_of_EncoderOpndSize_2_RealOpndSize[];
-extern const Mnemonic map_of_alu_opcode_2_mnemonic[];
-extern const Mnemonic map_of_shift_opcode_2_mnemonic[];
-
-// S_ stands for 'Signed'
-extern const Mnemonic S_map_of_condition_code_2_branch_mnemonic[];
-// U_ stands for 'Unsigned'
-extern const Mnemonic U_map_of_condition_code_2_branch_mnemonic[];
-
-inline static RegName map_reg(Reg_No r) {
-    assert(r >= 0 && r <= n_reg);
-    return map_of_regno_2_regname[r];
-}
-
-inline static OpndSize map_size(Opnd_Size o_size) {
-    assert(o_size >= 0 && o_size <= n_size);
-    return map_of_EncoderOpndSize_2_RealOpndSize[o_size];
-}
-
-inline static Mnemonic map_alu(ALU_Opcode alu) {
-    assert(alu >= 0 && alu < n_alu);
-    return map_of_alu_opcode_2_mnemonic[alu];
-}
-
-inline static Mnemonic map_shift(Shift_Opcode shc) {
-    assert(shc >= 0 && shc < n_shift);
-    return map_of_shift_opcode_2_mnemonic[shc];
-}
-
-inline bool fit8(int64 val) {
-    return (CHAR_MIN <= val) && (val <= CHAR_MAX);
-}
-
-inline bool fit32(int64 val) {
-    return (INT_MIN <= val) && (val <= INT_MAX);
-}
-
-inline static void add_r(EncoderBase::Operands & args, const R_Opnd & r, Opnd_Size sz, OpndExt ext = OpndExt_None) {
-    RegName reg = map_reg(r.reg_no());
-    if (sz != n_size) {
-        OpndSize size = map_size(sz);
-        if (size != getRegSize(reg)) {
-            reg = getAliasReg(reg, size);
-        }
-    }
-    args.add(EncoderBase::Operand(reg, ext));
-}
-
-inline static void add_m(EncoderBase::Operands & args, const M_Opnd & m, Opnd_Size sz, OpndExt ext = OpndExt_None) {
-        assert(n_size != sz);
-        args.add(EncoderBase::Operand(map_size(sz),
-            map_reg(m.base().reg_no()), map_reg(m.index().reg_no()),
-            (unsigned)m.scale().get_value(), (int)m.disp().get_value(), ext));
-}
-
-inline static void add_rm(EncoderBase::Operands & args, const RM_Opnd & rm, Opnd_Size sz, OpndExt ext = OpndExt_None) {
-    rm.is_reg() ? add_r(args, (R_Opnd &)rm, sz, ext) : add_m(args, (M_Opnd &)rm, sz, ext);
-}
-
-inline static void add_xmm(EncoderBase::Operands & args, const XMM_Opnd & xmm, bool dbl) {
-    // Gregory -
-    // XMM registers indexes in Reg_No enum are shifted by xmm0_reg, their indexes
-    // don't start with 0, so it is necessary to subtract xmm0_reg index from
-    // xmm.get_idx() value
-    assert(xmm.get_idx() >= xmm0_reg);
-    return args.add((RegName)( (dbl ? RegName_XMM0D : RegName_XMM0S) + xmm.get_idx() -
-            xmm0_reg));
-}
-
-inline static void add_fp(EncoderBase::Operands & args, unsigned i, bool dbl) {
-    return args.add((RegName)( (dbl ? RegName_FP0D : RegName_FP0S) + i));
-}
-
-inline static void add_imm(EncoderBase::Operands & args, const Imm_Opnd & imm) {
-    assert(n_size != imm.get_size());
-    args.add(EncoderBase::Operand(map_size(imm.get_size()), imm.get_value(),
-        imm.is_signed() ? OpndExt_Signed : OpndExt_Zero));
-}
-
-ENCODER_DECLARE_EXPORT char * prefix(char * stream, InstrPrefix p) {
-    *stream = (char)p;
-    return stream + 1;
-}
-
-// stack push and pop instructions
-ENCODER_DECLARE_EXPORT char * push(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_PUSH, args);
-}
-
-ENCODER_DECLARE_EXPORT char * push(char * stream, const Imm_Opnd & imm) {
-    EncoderBase::Operands args;
-#ifdef _EM64T_
-    add_imm(args, imm);
-#else
-    // we need this workaround to be compatible with the former ia32 encoder implementation
-    add_imm(args, Imm_Opnd(size_32, imm.get_value()));
-#endif
-    return EncoderBase::encode(stream, Mnemonic_PUSH, args);
-}
-
-ENCODER_DECLARE_EXPORT char * pop(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_POP, args);
-}
-
-// cmpxchg or xchg
-ENCODER_DECLARE_EXPORT char * cmpxchg(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    add_r(args, r, sz);
-    RegName implicitReg = getAliasReg(RegName_EAX, map_size(sz));
-    args.add(implicitReg);
-    return (char*)EncoderBase::encode(stream, Mnemonic_CMPXCHG, args);
-}
-
-ENCODER_DECLARE_EXPORT char * xchg(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    add_r(args, r, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_XCHG, args);
-}
-
-// inc(rement), dec(rement), not, neg(ate) instructions
-ENCODER_DECLARE_EXPORT char * inc(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_INC, args);
-}
-
-ENCODER_DECLARE_EXPORT char * dec(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_DEC, args);
-}
-
-ENCODER_DECLARE_EXPORT char * _not(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_NOT, args);
-}
-
-ENCODER_DECLARE_EXPORT char * neg(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_NEG, args);
-}
-
-ENCODER_DECLARE_EXPORT char * nop(char * stream) {
-    EncoderBase::Operands args;
-    return (char*)EncoderBase::encode(stream, Mnemonic_NOP, args);
-}
-
-ENCODER_DECLARE_EXPORT char * int3(char * stream) {
-    EncoderBase::Operands args;
-    return (char*)EncoderBase::encode(stream, Mnemonic_INT3, args);
-}
-
-// alu instructions: add, or, adc, sbb, and, sub, xor, cmp
-ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, map_alu(opc), args);
-};
-
-ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const M_Opnd & m, const R_Opnd & r, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, m, sz);
-    add_rm(args, r, sz);
-    return (char*)EncoderBase::encode(stream, map_alu(opc), args);
-}
-
-ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, r, sz);
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, map_alu(opc), args);
-}
-
-// test instruction
-ENCODER_DECLARE_EXPORT char * test(char * stream, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    assert(imm.get_size() <= sz);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, Mnemonic_TEST, args);
-}
-
-ENCODER_DECLARE_EXPORT char * test(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    add_r(args, r, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_TEST, args);
-}
-
-// shift instructions: shl, shr, sar, shld, shrd
-ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode shc, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, map_shift(shc), args);
-}
-
-ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode shc, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    args.add(RegName_CL);
-    return (char*)EncoderBase::encode(stream, map_shift(shc), args);
-}
-
-ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode shc, const RM_Opnd & rm,
-                            const R_Opnd & r, const Imm_Opnd & imm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    assert(shc == shld_opc || shc == shrd_opc);
-    add_rm(args, rm, sz);
-    add_r(args, r, sz);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, map_shift(shc), args);
-}
-
-ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode shc, const RM_Opnd & rm,
-                            const R_Opnd & r, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    assert(shc == shld_opc || shc == shrd_opc);
-    add_rm(args, rm, sz);
-    add_r(args, r, sz);
-    args.add(RegName_CL);
-    return (char*)EncoderBase::encode(stream, map_shift(shc), args);
-}
-
-// multiply instructions: mul, imul
-ENCODER_DECLARE_EXPORT char * mul(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    args.add(RegName_EDX);
-    args.add(RegName_EAX);
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MUL, args);
-}
-
-ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_r(args, r, sz);
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_IMUL, args);
-}
-
-ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const Imm_Opnd & imm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_r(args, r, sz);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, Mnemonic_IMUL, args);
-}
-
-ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const RM_Opnd & rm,
-                           const Imm_Opnd & imm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_r(args, r, sz);
-    add_rm(args, rm, sz);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, Mnemonic_IMUL, args);
-}
-
-// divide instructions: div, idiv
-ENCODER_DECLARE_EXPORT char * idiv(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-#ifdef _EM64T_
-    add_r(args, rdx_opnd, sz);
-    add_r(args, rax_opnd, sz);
-#else
-    add_r(args, edx_opnd, sz);
-    add_r(args, eax_opnd, sz);
-#endif
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_IDIV, args);
-}
-
-ENCODER_DECLARE_EXPORT char * div(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-#ifdef _EM64T_
-    add_r(args, rdx_opnd, sz);
-    add_r(args, rax_opnd, sz);
-#else
-    add_r(args, edx_opnd, sz);
-    add_r(args, eax_opnd, sz);
-#endif
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_DIV, args);
-}
-
-// data movement: mov
-ENCODER_DECLARE_EXPORT char * mov(char * stream, const M_Opnd & m, const R_Opnd & r, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_m(args, m, sz);
-    add_r(args, r, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MOV, args);
-}
-
-ENCODER_DECLARE_EXPORT char * mov(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_r(args, r, sz);
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MOV, args);
-}
-
-ENCODER_DECLARE_EXPORT char * mov(char * stream, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MOV, args);
-}
-
-ENCODER_DECLARE_EXPORT char * movd(char * stream, const RM_Opnd & rm, const XMM_Opnd & xmm) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, size_32);
-    add_xmm(args, xmm, false);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MOVD, args);
-}
-
-ENCODER_DECLARE_EXPORT char * movd(char * stream, const XMM_Opnd & xmm, const RM_Opnd & rm) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm, false);
-    add_rm(args, rm, size_32);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MOVD, args);
-}
-
-ENCODER_DECLARE_EXPORT char * movq(char * stream, const RM_Opnd & rm, const XMM_Opnd & xmm) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, size_64);
-    add_xmm(args, xmm, true);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MOVQ, args);
-}
-
-ENCODER_DECLARE_EXPORT char * movq(char * stream, const XMM_Opnd & xmm, const RM_Opnd & rm) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm, true);
-    add_rm(args, rm, size_64);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MOVQ, args);
-}
-
-ENCODER_DECLARE_EXPORT char * movsx(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_r(args, r, n_size);
-    add_rm(args, rm, sz, OpndExt_Signed);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MOVSX, args);
-}
-
-ENCODER_DECLARE_EXPORT char * movzx(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_r(args, r, n_size);
-    // movzx r64, r/m32 is not available on em64t
-    // mov r32, r/m32 should zero out upper bytes
-    assert(sz <= size_16);
-    add_rm(args, rm, sz, OpndExt_Zero);
-    return (char*)EncoderBase::encode(stream, Mnemonic_MOVZX, args);
-}
-
-// sse mov
-ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm, dbl);
-    add_m(args, mem, dbl ? size_64 : size_32);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MOVSD : Mnemonic_MOVSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const M_Opnd &  mem, const XMM_Opnd & xmm, bool dbl) {
-    EncoderBase::Operands args;
-    add_m(args, mem, dbl ? size_64 : size_32);
-    add_xmm(args, xmm, dbl);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MOVSD : Mnemonic_MOVSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, dbl);
-    add_xmm(args, xmm1, dbl);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MOVSD : Mnemonic_MOVSS, args );
-}
-
-// sse add, sub, mul, div
-ENCODER_DECLARE_EXPORT char * sse_add(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm, dbl);
-    add_m(args, mem, dbl ? size_64 : size_32);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_ADDSD : Mnemonic_ADDSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_add(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, dbl);
-    add_xmm(args, xmm1, dbl);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_ADDSD : Mnemonic_ADDSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_sub(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm, dbl);
-    add_m(args, mem, dbl ? size_64 : size_32);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_SUBSD : Mnemonic_SUBSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_sub(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, dbl);
-    add_xmm(args, xmm1, dbl);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_SUBSD : Mnemonic_SUBSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_mul( char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm, dbl);
-    add_m(args, mem, dbl ? size_64 : size_32);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MULSD : Mnemonic_MULSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_mul(char * stream, const XMM_Opnd& xmm0, const XMM_Opnd& xmm1, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args,  xmm0, dbl);
-    add_xmm(args,  xmm1, dbl);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MULSD : Mnemonic_MULSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_div(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm, dbl);
-    add_m(args, mem, dbl ? size_64 : size_32);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_DIVSD : Mnemonic_DIVSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_div(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, dbl);
-    add_xmm(args, xmm1, dbl);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_DIVSD : Mnemonic_DIVSS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_xor(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, true);
-    add_xmm(args, xmm1, true);
-    return (char*)EncoderBase::encode(stream, Mnemonic_PXOR, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_compare(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, true);
-    add_xmm(args, xmm1, true);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_COMISD : Mnemonic_COMISS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_compare(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, dbl);
-    add_m(args, mem, dbl ? size_64 : size_32);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_COMISD : Mnemonic_COMISS, args);
-}
-
-// sse conversions
-ENCODER_DECLARE_EXPORT char * sse_cvt_si(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm, dbl);
-    add_m(args, mem, size_32);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_CVTSI2SD : Mnemonic_CVTSI2SS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_cvtt2si(char * stream, const R_Opnd & reg, const M_Opnd & mem, bool dbl) {
-    EncoderBase::Operands args;
-    add_rm(args, reg, size_32);
-    add_m(args, mem, dbl ? size_64 : size_32);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_CVTTSD2SI : Mnemonic_CVTTSS2SI, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_cvtt2si(char * stream, const R_Opnd & reg, const XMM_Opnd & xmm, bool dbl) {
-    EncoderBase::Operands args;
-    add_rm(args, reg, size_32);
-    add_xmm(args, xmm, dbl);
-    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_CVTTSD2SI : Mnemonic_CVTTSS2SI, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_cvt_fp2dq(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, dbl);
-    add_xmm(args, xmm1, dbl);
-    return (char*)EncoderBase::encode(stream, dbl ?  Mnemonic_CVTTPD2DQ : Mnemonic_CVTTPS2DQ, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_cvt_dq2fp(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, dbl);
-    add_xmm(args, xmm1, dbl);
-    return (char*)EncoderBase::encode(stream, dbl ?  Mnemonic_CVTDQ2PD : Mnemonic_CVTDQ2PS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_d2s(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem64) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, false);
-    add_m(args, mem64, size_64);
-    return (char*)EncoderBase::encode(stream, Mnemonic_CVTSD2SS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_d2s(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, false);
-    add_xmm(args, xmm1, true);
-    return (char*)EncoderBase::encode(stream, Mnemonic_CVTSD2SS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_s2d(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem32) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, true);
-    add_m(args, mem32, size_32);
-    return (char*)EncoderBase::encode(stream, Mnemonic_CVTSS2SD, args);
-}
-
-ENCODER_DECLARE_EXPORT char * sse_s2d(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1) {
-    EncoderBase::Operands args;
-    add_xmm(args, xmm0, true);
-    add_xmm(args, xmm1, false);
-    return (char*)EncoderBase::encode(stream, Mnemonic_CVTSS2SD, args);
-}
-
-// condition operations
-ENCODER_DECLARE_EXPORT char *cmov(char * stream, ConditionCode cc, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_r(args, r, sz);
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, (Mnemonic)(Mnemonic_CMOVcc + cc), args);
-}
-
-ENCODER_DECLARE_EXPORT char * setcc(char * stream, ConditionCode cc, const RM_Opnd & rm8) {
-    EncoderBase::Operands args;
-    add_rm(args, rm8, size_8);
-    return (char*)EncoderBase::encode(stream, (Mnemonic)(Mnemonic_SETcc + cc), args);
-}
-
-// load effective address: lea
-ENCODER_DECLARE_EXPORT char * lea(char * stream, const R_Opnd & r, const M_Opnd & m, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_r(args, r, sz);
-    add_m(args, m, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_LEA, args);
-}
-
-ENCODER_DECLARE_EXPORT char * cdq(char * stream) {
-    EncoderBase::Operands args;
-    args.add(RegName_EDX);
-    args.add(RegName_EAX);
-    return (char*)EncoderBase::encode(stream, Mnemonic_CDQ, args);
-}
-
-ENCODER_DECLARE_EXPORT char * wait(char * stream) {
-    return (char*)EncoderBase::encode(stream, Mnemonic_WAIT, EncoderBase::Operands());
-}
-
-// control-flow instructions
-
-// loop
-ENCODER_DECLARE_EXPORT char * loop(char * stream, const Imm_Opnd & imm) {
-    EncoderBase::Operands args;
-    assert(imm.get_size() == size_8);
-    args.add(RegName_ECX);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, Mnemonic_LOOP, args);
-}
-
-// jump
-ENCODER_DECLARE_EXPORT char * jump8(char * stream, const Imm_Opnd & imm) {
-    EncoderBase::Operands args;
-    assert(imm.get_size() == size_8);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, Mnemonic_JMP, args);
-}
-
-ENCODER_DECLARE_EXPORT char * jump32(char * stream, const Imm_Opnd & imm) {
-    EncoderBase::Operands args;
-    assert(imm.get_size() == size_32);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, Mnemonic_JMP, args);
-}
-
-ENCODER_DECLARE_EXPORT char * jump(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_JMP, args);
-}
-
-/**
- * @note On EM64T: if target lies beyond 2G (does not fit into 32 bit
- *       offset) then generates indirect jump using RAX (whose content is
- *       destroyed).
- */
-ENCODER_DECLARE_EXPORT char * jump(char * stream, char * target) {
-#ifdef _EM64T_
-    int64 offset = target - stream;
-    // sub 2 bytes for the short version
-    offset -= 2;
-    if (fit8(offset)) {
-        // use 8-bit signed relative form
-        return jump8(stream, Imm_Opnd(size_8, offset));
-    } else if (fit32(offset)) {
-        // sub 5 (3 + 2)bytes for the long version
-        offset -= 3;
-        // use 32-bit signed relative form
-        return jump32(stream, Imm_Opnd(size_32, offset));
-    }
-    // need to use absolute indirect jump
-    stream = mov(stream, rax_opnd, Imm_Opnd(size_64, (int64)target), size_64);
-    return jump(stream, rax_opnd, size_64);
-#else
-    I_32 offset = target - stream;
-    // sub 2 bytes for the short version
-    offset -= 2;
-    if (fit8(offset)) {
-        // use 8-bit signed relative form
-        return jump8(stream, Imm_Opnd(size_8, offset));
-    }
-    // sub 5 (3 + 2) bytes for the long version
-    offset -= 3;
-    // use 32-bit signed relative form
-    return jump32(stream, Imm_Opnd(size_32, offset));
-#endif
-}
-
-// branch
-ENCODER_DECLARE_EXPORT char * branch8(char * stream, ConditionCode cond,
-                                      const Imm_Opnd & imm,
-                                      InstrPrefix pref)
-{
-    if (pref != no_prefix) {
-        assert(pref == hint_branch_taken_prefix || pref == hint_branch_taken_prefix);
-        stream = prefix(stream, pref);
-    }
-    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cond);
-    EncoderBase::Operands args;
-    assert(imm.get_size() == size_8);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, m, args);
-}
-
-ENCODER_DECLARE_EXPORT char * branch32(char * stream, ConditionCode cond,
-                                       const Imm_Opnd & imm,
-                                       InstrPrefix pref)
-{
-    if (pref != no_prefix) {
-        assert(pref == hint_branch_taken_prefix || pref == hint_branch_taken_prefix);
-        stream = prefix(stream, pref);
-    }
-    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cond);
-    EncoderBase::Operands args;
-    assert(imm.get_size() == size_32);
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, m, args);
-}
-
-/*
-ENCODER_DECLARE_EXPORT char * branch(char * stream, ConditionCode cc, const char * target, InstrPrefix prefix) {
-// sub 2 bytes for the short version
-int64 offset = stream-target-2;
-if( fit8(offset) ) {
-return branch8(stream, cc, Imm_Opnd(size_8, (char)offset), is_signed);
-}
-return branch32(stream, cc, Imm_Opnd(size_32, (int)offset), is_signed);
-}
-*/
-
-// call
-ENCODER_DECLARE_EXPORT char * call(char * stream, const Imm_Opnd & imm)
-{
-    EncoderBase::Operands args;
-    add_imm(args, imm);
-    return (char*)EncoderBase::encode(stream, Mnemonic_CALL, args);
-}
-
-ENCODER_DECLARE_EXPORT char * call(char * stream, const RM_Opnd & rm,
-                                   Opnd_Size sz)
-{
-    EncoderBase::Operands args;
-    add_rm(args, rm, sz);
-    return (char*)EncoderBase::encode(stream, Mnemonic_CALL, args);
-}
-
-/**
-* @note On EM64T: if target lies beyond 2G (does not fit into 32 bit
-*       offset) then generates indirect jump using RAX (whose content is
-*       destroyed).
-*/
-ENCODER_DECLARE_EXPORT char * call(char * stream, const char * target)
-{
-#ifdef _EM64T_
-    int64 offset = target - stream;
-    if (fit32(offset)) {
-        offset -= 5; // sub 5 bytes for this instruction
-        Imm_Opnd imm(size_32, offset);
-        return call(stream, imm);
-    }
-    // need to use absolute indirect call
-    stream = mov(stream, rax_opnd, Imm_Opnd(size_64, (int64)target), size_64);
-    return call(stream, rax_opnd, size_64);
-#else
-    I_32 offset = target - stream;
-    offset -= 5; // sub 5 bytes for this instruction
-    Imm_Opnd imm(size_32, offset);
-    return call(stream, imm);
-#endif
-}
-
-// return instruction
-ENCODER_DECLARE_EXPORT char * ret(char * stream)
-{
-    EncoderBase::Operands args;
-    return (char*)EncoderBase::encode(stream, Mnemonic_RET, args);
-}
-
-ENCODER_DECLARE_EXPORT char * ret(char * stream, const Imm_Opnd & imm)
-{
-    EncoderBase::Operands args;
-    // TheManual says imm can be 16-bit only
-    //assert(imm.get_size() <= size_16);
-    args.add(EncoderBase::Operand(map_size(size_16), imm.get_value()));
-    return (char*)EncoderBase::encode(stream, Mnemonic_RET, args);
-}
-
-ENCODER_DECLARE_EXPORT char * ret(char * stream, unsigned short pop)
-{
-    // TheManual says it can only be imm16
-    EncoderBase::Operands args(EncoderBase::Operand(OpndSize_16, pop, OpndExt_Zero));
-    return (char*)EncoderBase::encode(stream, Mnemonic_RET, args);
-}
-
-// floating-point instructions
-ENCODER_DECLARE_EXPORT char * fld(char * stream, const M_Opnd & m,
-                                  bool is_double) {
-    EncoderBase::Operands args;
-    // a fake FP register as operand
-    add_fp(args, 0, is_double);
-    add_m(args, m, is_double ? size_64 : size_32);
-    return (char*)EncoderBase::encode(stream, Mnemonic_FLD, args);
-}
-
-ENCODER_DECLARE_EXPORT char * fist(char * stream, const M_Opnd & mem,
-                                   bool is_long, bool pop_stk)
-{
-    EncoderBase::Operands args;
-    if (pop_stk) {
-        add_m(args, mem, is_long ? size_64 : size_32);
-        // a fake FP register as operand
-        add_fp(args, 0, is_long);
-        return (char*)EncoderBase::encode(stream,  Mnemonic_FISTP, args);
-    }
-    // only 32-bit operands are supported
-    assert(is_long == false);
-    add_m(args, mem, size_32);
-    add_fp(args, 0, false);
-    return (char*)EncoderBase::encode(stream,  Mnemonic_FIST, args);
-}
-
-ENCODER_DECLARE_EXPORT char * fst(char * stream, const M_Opnd & m,
-                                  bool is_double, bool pop_stk)
-{
-    EncoderBase::Operands args;
-    add_m(args, m, is_double ? size_64 : size_32);
-    // a fake FP register as operand
-    add_fp(args, 0, is_double);
-    return (char*)EncoderBase::encode(stream,
-                                    pop_stk ? Mnemonic_FSTP : Mnemonic_FST,
-                                    args);
-}
-
-ENCODER_DECLARE_EXPORT char * fst(char * stream, unsigned i, bool pop_stk)
-{
-    EncoderBase::Operands args;
-    add_fp(args, i, true);
-    return (char*)EncoderBase::encode(stream,
-                                    pop_stk ? Mnemonic_FSTP : Mnemonic_FST,
-                                    args);
-}
-
-ENCODER_DECLARE_EXPORT char * fldcw(char * stream, const M_Opnd & mem) {
-    EncoderBase::Operands args;
-    add_m(args, mem, size_16);
-    return (char*)EncoderBase::encode(stream, Mnemonic_FLDCW, args);
-}
-
-ENCODER_DECLARE_EXPORT char * fnstcw(char * stream, const M_Opnd & mem) {
-    EncoderBase::Operands args;
-    add_m(args, mem, size_16);
-    return (char*)EncoderBase::encode(stream, Mnemonic_FNSTCW, args);
-}
-
-ENCODER_DECLARE_EXPORT char * fnstsw(char * stream)
-{
-    return (char*)EncoderBase::encode(stream, Mnemonic_FNSTCW,
-                                      EncoderBase::Operands());
-}
-
-// string operations
-ENCODER_DECLARE_EXPORT char * set_d(char * stream, bool set) {
-    EncoderBase::Operands args;
-    return (char*)EncoderBase::encode(stream,
-                                      set ? Mnemonic_STD : Mnemonic_CLD,
-                                      args);
-}
-
-ENCODER_DECLARE_EXPORT char * scas(char * stream, unsigned char prefix)
-{
-	EncoderBase::Operands args;
-    if (prefix != no_prefix) {
-        assert(prefix == prefix_repnz || prefix == prefix_repz);
-        *stream = prefix;
-        ++stream;
-    }
-    return (char*)EncoderBase::encode(stream, Mnemonic_SCAS, args);
-}
-
-ENCODER_DECLARE_EXPORT char * stos(char * stream, unsigned char prefix)
-{
-    if (prefix != no_prefix) {
-        assert(prefix == prefix_rep);
-        *stream = prefix;
-        ++stream;
-    }
-
-	EncoderBase::Operands args;
-	return (char*)EncoderBase::encode(stream, Mnemonic_STOS, args);
-}
-
-// Intrinsic FP math functions
-
-ENCODER_DECLARE_EXPORT char * fprem(char * stream) {
-    return (char*)EncoderBase::encode(stream, Mnemonic_FPREM,
-                                      EncoderBase::Operands());
-}
-
-ENCODER_DECLARE_EXPORT char * fprem1(char * stream) {
-    return (char*)EncoderBase::encode(stream, Mnemonic_FPREM1,
-                                      EncoderBase::Operands());
-}
diff --git a/vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp b/vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp
new file mode 100644
index 0000000..bbd420f
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp
@@ -0,0 +1,6310 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file AnalysisO1.cpp
+  \brief This file implements register allocator, constant folding
+*/
+#include "CompilationUnit.h"
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "interp/InterpState.h"
+#include "interp/InterpDefs.h"
+#include "libdex/Leb128.h"
+#include "../../RegisterizationME.h"
+#include "Scheduler.h"
+#include "Singleton.h"
+#include <set>
+
+/* compilation flags to turn on debug printout */
+//#define DEBUG_COMPILE_TABLE
+//#define DEBUG_ALLOC_CONSTRAINT
+//#define DEBUG_REGALLOC
+//#define DEBUG_REG_USED
+//#define DEBUG_REFCOUNT
+//#define DEBUG_REACHING_DEF2
+//#define DEBUG_REACHING_DEF
+//#define DEBUG_LIVE_RANGE
+//#define DEBUG_ENDOFBB
+//#define DEBUG_CONST
+//#define DEBUG_XFER_POINTS
+//#define DEBUG_DSE
+//#define DEBUG_CFG
+//#define DEBUG_GLOBALTYPE
+//#define DEBUG_STATE
+//#define DEBUG_COMPILE_TABLE
+//#define DEBUG_VIRTUAL_INFO
+//#define DEBUG_MERGE_ENTRY
+//#define DEBUG_INVALIDATE
+#define DEBUG_MEMORYVR(X)
+#define DEBUG_MOVE_OPT(X)
+#define DEBUG_SPILL(X)
+
+#include "AnalysisO1.h"
+#include "CompileTable.h"
+
+void dumpCompileTable();
+
+/**
+ * @brief Check whether a type is a virtual register type
+ * @param type the logical type of the register
+ * @return whether type is a virtual register or not
+ */
+bool isVirtualReg(int type) {
+    return ((type & LowOpndRegType_virtual) != 0);
+}
+
+/**
+ * @brief Check whether the logical type represents a temporary
+ * @param type the logical type of the register
+ * @param regNum the register number
+ * @return Returns true if we are looking at a temporary, scratch, or hardcoded register.
+ */
+bool isTemporary (int type, int regNum)
+{
+    return (isVirtualReg (type) == false);
+}
+
+/**
+ * @brief Given a physical register it determines if it is scratch type.
+ * @param reg The physical register.
+ * @return Returns whether the register is scratch
+ */
+static bool isScratchReg (int reg)
+{
+    //A register is a scratch register if its physical type is one of the scratch ones
+    bool isScratch = (reg >= static_cast<int> (PhysicalReg_SCRATCH_1)
+            && reg <= static_cast<int> (PhysicalReg_SCRATCH_10));
+
+    return isScratch;
+}
+
+/** convert type defined in lowering module to type defined in register allocator
+    in lowering module <type, isPhysical>
+    in register allocator: LowOpndRegType_hard LowOpndRegType_virtual LowOpndRegType_scratch
+*/
+int convertType(int type, int reg, bool isPhysical) {
+    int newType = type;
+    if(isPhysical) newType |= LowOpndRegType_hard;
+    if(isVirtualReg(type)) newType |= LowOpndRegType_virtual;
+    else {
+        /* reg number for a VR can exceed PhysicalReg_SCRATCH_1 */
+        if(isScratchReg (reg) == true)
+        {
+            newType |= LowOpndRegType_scratch;
+        }
+    }
+    return newType;
+}
+
+/** return the size of a variable
+ */
+OpndSize getRegSize(int type) {
+    if((type & MASK_FOR_TYPE) == LowOpndRegType_xmm) return OpndSize_64;
+    if((type & MASK_FOR_TYPE) == LowOpndRegType_fs) return OpndSize_64;
+    /* for type _gp, _fs_s, _ss */
+    return OpndSize_32;
+}
+
+/*
+   Overlapping cases between two variables A and B
+   layout for A,B   isAPartiallyOverlapB  isBPartiallyOverlapA
+   1> |__|  |____|         OVERLAP_ALIGN        OVERLAP_B_COVER_A
+      |__|  |____|
+   2> |____|           OVERLAP_B_IS_LOW_OF_A    OVERLAP_B_COVER_LOW_OF_A
+        |__|
+   3> |____|           OVERLAP_B_IS_HIGH_OF_A   OVERLAP_B_COVER_HIGH_OF_A
+      |__|
+   4> |____|      OVERLAP_LOW_OF_A_IS_HIGH_OF_B OVERLAP_B_COVER_LOW_OF_A
+         |____|
+   5>    |____|   OVERLAP_HIGH_OF_A_IS_LOW_OF_B OVERLAP_B_COVER_HIGH_OF_A
+      |____|
+   6>   |__|           OVERLAP_A_IS_LOW_OF_B    OVERLAP_B_COVER_A
+      |____|
+   7> |__|             OVERLAP_A_IS_HIGH_OF_B   OVERLAP_B_COVER_A
+      |____|
+*/
+/** determine the overlapping between variable B and A
+*/
+OverlapCase getBPartiallyOverlapA(int regB, LowOpndRegType tB, int regA, LowOpndRegType tA) {
+    if(getRegSize(tA) == getRegSize(tB) && regA == regB) return OVERLAP_B_COVER_A;
+    if(getRegSize(tA) == OpndSize_64 && getRegSize(tB) == OpndSize_32 && regA == regB) return OVERLAP_B_COVER_LOW_OF_A;
+    if(getRegSize(tA) == OpndSize_64 && getRegSize(tB) == OpndSize_32 && regB == regA + 1) return OVERLAP_B_COVER_HIGH_OF_A;
+    if(getRegSize(tA) == OpndSize_32 && getRegSize(tB) == OpndSize_64 && (regA == regB || regA == regB+1)) return OVERLAP_B_COVER_A;
+    if(getRegSize(tB) == OpndSize_64 && getRegSize(tA) == OpndSize_64 && regA == regB+1) return OVERLAP_B_COVER_LOW_OF_A;
+    if(getRegSize(tB) == OpndSize_64 && getRegSize(tA) == OpndSize_64 && regB == regA+1) return OVERLAP_B_COVER_HIGH_OF_A;
+    return OVERLAP_NO;
+}
+
+/** determine overlapping between variable A and B
+*/
+OverlapCase getAPartiallyOverlapB(int regA, LowOpndRegType tA, int regB, LowOpndRegType tB) {
+    if(getRegSize(tA) == getRegSize(tB) && regA == regB) return OVERLAP_ALIGN;
+    if(getRegSize(tA) == OpndSize_64 && getRegSize(tB) == OpndSize_32 && regA == regB)
+        return OVERLAP_B_IS_LOW_OF_A;
+    if(getRegSize(tA) == OpndSize_64 && getRegSize(tB) == OpndSize_32 && regB == regA+1)
+        return OVERLAP_B_IS_HIGH_OF_A;
+    if(getRegSize(tB) == OpndSize_64 && getRegSize(tA) == OpndSize_64 && regA == regB+1)
+        return OVERLAP_LOW_OF_A_IS_HIGH_OF_B;
+    if(getRegSize(tB) == OpndSize_64 && getRegSize(tA) == OpndSize_64 && regB == regA+1)
+        return OVERLAP_HIGH_OF_A_IS_LOW_OF_B;
+    if(getRegSize(tA) == OpndSize_32 && getRegSize(tB) == OpndSize_64 && regA == regB)
+        return OVERLAP_A_IS_LOW_OF_B;
+    if(getRegSize(tA) == OpndSize_32 && getRegSize(tB) == OpndSize_64 && regA == regB+1)
+        return OVERLAP_A_IS_HIGH_OF_B;
+    return OVERLAP_NO;
+}
+
+/** determine whether variable A fully covers B
+ */
+bool isAFullyCoverB(int regA, LowOpndRegType tA, int regB, LowOpndRegType tB) {
+    if(getRegSize(tB) == OpndSize_32) return true;
+    if(getRegSize(tA) == getRegSize(tB) && regA == regB) return true;
+    return false;
+}
+
+/*
+   RegAccessType accessType
+   1> DefOrUse.accessType
+      can only be D(VR), L(low part of VR), H(high part of VR), N(none)
+      for def, it means which part of the VR is live
+      for use, it means which part of the VR comes from the def
+   2> VirtualRegInfo.accessType
+      for currentInfo, it can only be a combination of U & D
+      for entries in infoBasicBlock, it can be a combination of U & D|L|H
+*/
+
+/*
+   Key data structures used:
+   1> BasicBlock_O1
+      VirtualRegInfo infoBasicBlock[]
+      DefUsePair* defUseTable
+      XferPoint xferPoints[]
+   2> MemoryVRInfo memVRTable[]
+      LiveRange* ranges
+   3> CompileTableEntry compileTable[]
+   4> VirtualRegInfo
+      DefOrUse reachingDefs[3]
+   5> DefUsePair, LiveRange
+*/
+
+//! one entry for each variable used
+
+//! a variable can be virtual register, or a temporary (can be hard-coded)
+CompileTable compileTable;
+//! tables to save the states of register allocation
+regAllocStateEntry2 stateTable2_1[NUM_MEM_VR_ENTRY];
+regAllocStateEntry2 stateTable2_2[NUM_MEM_VR_ENTRY];
+regAllocStateEntry2 stateTable2_3[NUM_MEM_VR_ENTRY];
+regAllocStateEntry2 stateTable2_4[NUM_MEM_VR_ENTRY];
+
+//! array of TempRegInfo to store temporaries accessed by a single bytecode
+TempRegInfo infoByteCodeTemp[MAX_TEMP_REG_PER_BYTECODE];
+int num_temp_regs_per_bytecode;
+//! array of MemoryVRInfo to store whether a VR is in memory
+MemoryVRInfo memVRTable[NUM_MEM_VR_ENTRY];
+int num_memory_vr;
+
+CompilationUnit* currentUnit = NULL;
+
+//! the current basic block
+BasicBlock_O1* currentBB = NULL;
+
+/**
+ * @brief Information for each physical register
+ * @details Initialized during code generation
+ */
+RegisterInfo allRegs[PhysicalReg_Last+1];
+
+//! this array says whether a spill location is used (0 means not used, 1 means used)
+int spillIndexUsed[MAX_SPILL_JIT_IA];
+
+int inGetVR_num = -1;
+int inGetVR_type;
+
+///////////////////////////////////////////////////////////////////////////////
+// FORWARD FUNCTION DECLARATION
+void addExceptionHandler(s4 tmp);
+
+int createCFG(Method* method);
+void dumpVirtualInfoOfBasicBlock(BasicBlock_O1* bb);
+void setTypeOfVR();
+void dumpVirtualInfoOfMethod();
+int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb);
+
+//used in collectInfoOfBasicBlock: getVirtualRegInfo
+int mergeEntry2 (BasicBlock_O1* bb, VirtualRegInfo &currentInfo);
+int sortAllocConstraint(RegAllocConstraint* allocConstraints,
+                        RegAllocConstraint* allocConstraintsSorted, bool fromHighToLow);
+
+//Updates compile table with information about virtual register usage
+static void insertFromVirtualInfo (const VirtualRegInfo &regInfo);
+
+//Updates compile table with information about temporary usage
+static void insertFromTempInfo (const TempRegInfo &tempRegInfo);
+
+static int updateXferPoints (BasicBlock_O1 *bb);
+static int updateLiveTable (BasicBlock_O1 *bb);
+static void handleStartOfBBXferPoints (BasicBlock_O1 *bb);
+void printDefUseTable();
+bool isFirstOfHandler(BasicBlock_O1* bb);
+
+//used in mergeEntry2
+//following functions will not update global data structure
+RegAccessType mergeAccess2(RegAccessType A, RegAccessType B, OverlapCase isBPartiallyOverlapA);
+RegAccessType updateAccess1(RegAccessType A, OverlapCase isAPartiallyOverlapB); //will not update global data structure
+RegAccessType updateAccess2(RegAccessType C1, RegAccessType C2);
+RegAccessType updateAccess3(RegAccessType C, RegAccessType B);
+
+void updateDefUseTable (VirtualRegInfo &currentInfo);
+static int updateReachingDefA (VirtualRegInfo &currentInfo, int indexToA, OverlapCase isBPartiallyOverlapA);
+static int updateReachingDefB1 (VirtualRegInfo &currentInfo, VirtualRegInfo &tmpInfo, int indexToA);
+static int updateReachingDefB2 (VirtualRegInfo &currentInfo, VirtualRegInfo &tmpInfo);
+void updateReachingDefB3 (VirtualRegInfo &currentInfo);
+
+RegAccessType insertAUse(DefUsePair* ptr, int offsetPC, int regNum, LowOpndRegType physicalType);
+DefUsePair* insertADef(BasicBlock_O1 *bb, int offsetPC, int regNum, LowOpndRegType pType, RegAccessType rType);
+RegAccessType insertDefUsePair (VirtualRegInfo &currentInfo, int reachingDefIndex);
+
+//used in updateXferPoints
+int fakeUsageAtEndOfBB (BasicBlock_O1* bb, int vR, int physicalAndLogicalType);
+static int insertLoadXfer(int offset, int regNum, LowOpndRegType pType);
+int searchMemTable(int regNum);
+static int mergeLiveRange(int tableIndex, int rangeStart, int rangeEnd);
+//used in updateLiveTable
+RegAccessType setAccessTypeOfUse(OverlapCase isDefPartiallyOverlapUse, RegAccessType reachingDefLive);
+DefUsePair* searchDefUseTable(int offsetPC, int regNum, LowOpndRegType pType);
+void insertAccess(int tableIndex, LiveRange* startP, int rangeStart);
+
+/**
+ * @brief Checks whether the opcode can branch or switch
+ * @param opcode the Dalvik mnemonic
+ * @return true if opcode can branch or switch
+ */
+static inline bool isCurrentByteCodeJump(Opcode opcode)
+{
+    //Get the opcode flags
+    int flags = dvmCompilerGetOpcodeFlags (opcode);
+
+    //Check whether it can branch or switch
+    return (flags & (kInstrCanBranch | kInstrCanSwitch)) != 0;
+}
+
+/* this function is called before code generation of basic blocks
+   initialize data structure allRegs, which stores information for each physical register,
+   whether it is used, when it was last freed, whether it is callee-saved */
+void initializeAllRegs() {
+    //Initialize entire array
+    memset (allRegs, PhysicalReg_Null, sizeof (allRegs));
+
+    int k;
+    for(k = PhysicalReg_EAX; k <= PhysicalReg_EBP; k++) {
+        allRegs[k].physicalReg = (PhysicalReg) k;
+        if(k == PhysicalReg_EDI || k == PhysicalReg_ESP || k == PhysicalReg_EBP)
+            allRegs[k].isUsed = true;
+        else {
+            allRegs[k].isUsed = false;
+            allRegs[k].freeTimeStamp = -1;
+        }
+        if(k == PhysicalReg_EBX || k == PhysicalReg_EBP || k == PhysicalReg_ESI || k == PhysicalReg_EDI)
+            allRegs[k].isCalleeSaved = true;
+        else
+            allRegs[k].isCalleeSaved = false;
+    }
+    for(k = PhysicalReg_XMM0; k <= PhysicalReg_XMM7; k++) {
+        allRegs[k].physicalReg = (PhysicalReg) k;
+        allRegs[k].isUsed = false;
+        allRegs[k].freeTimeStamp = -1;
+        allRegs[k].isCalleeSaved = false;
+    }
+}
+
+/** sync up allRegs (isUsed & freeTimeStamp) with compileTable
+    global data: RegisterInfo allRegs[PhysicalReg_Null]
+    update allRegs[EAX to XMM7] except EDI,ESP,EBP
+    update RegisterInfo.isUsed & RegisterInfo.freeTimeStamp
+        if the physical register was used and is not used now
+*/
+void syncAllRegs() {
+    int k, k2;
+    for(k = PhysicalReg_EAX; k <= PhysicalReg_XMM7; k++) {
+        if(k == PhysicalReg_EDI || k == PhysicalReg_ESP || k == PhysicalReg_EBP)
+            continue;
+        //check whether the physical register is used by any logical register
+        bool stillUsed = false;
+        for(k2 = 0; k2 < compileTable.size (); k2++) {
+            if(compileTable[k2].physicalReg == k) {
+                stillUsed = true;
+                break;
+            }
+        }
+        if(stillUsed && !allRegs[k].isUsed) {
+            allRegs[k].isUsed = true;
+        }
+        if(!stillUsed && allRegs[k].isUsed) {
+            allRegs[k].isUsed = false;
+        }
+    }
+    return;
+}
+
+/**
+ * @brief Looks through all physical registers and determines what is used
+ * @param outFreeRegisters is a set that is updated with the unused physical registers
+ * @param includeGPs Whether or not to include general purpose registers
+ * @param includeXMMs Whether or not to include XMM registers
+ */
+void findFreeRegisters (std::set<PhysicalReg> &outFreeRegisters, bool includeGPs, bool includeXMMs)
+{
+    if (includeGPs == true)
+    {
+        // Go through all GPs
+        for (int reg = PhysicalReg_StartOfGPMarker; reg <= PhysicalReg_EndOfGPMarker; reg++)
+        {
+            //If it is not used, then we can add it to the list of free registers
+            if (allRegs[reg].isUsed == false)
+            {
+                outFreeRegisters.insert (static_cast<PhysicalReg> (reg));
+            }
+        }
+    }
+
+    if (includeXMMs == true)
+    {
+        // Go through all XMMs
+        for (int reg = PhysicalReg_StartOfXmmMarker; reg <= PhysicalReg_EndOfXmmMarker; reg++)
+        {
+            if (allRegs[reg].isUsed == false)
+            {
+                outFreeRegisters.insert (static_cast<PhysicalReg> (reg));
+            }
+        }
+    }
+}
+
+/**
+ * @brief Given a list of scratch register candidates and a register type,
+ * it returns a scratch register of that type
+ * @param scratchCandidates registers that can be used for scratch
+ * @param type xmm or gp
+ * @return physical register which can be used as scratch
+ */
+PhysicalReg getScratch(const std::set<PhysicalReg> &scratchCandidates, LowOpndRegType type) {
+    if (type != LowOpndRegType_gp && type != LowOpndRegType_xmm) {
+        return PhysicalReg_Null;
+    }
+
+    int start = (
+            type == LowOpndRegType_gp ?
+                    PhysicalReg_StartOfGPMarker : PhysicalReg_StartOfXmmMarker);
+    int end = (
+            type == LowOpndRegType_gp ?
+                    PhysicalReg_EndOfGPMarker : PhysicalReg_EndOfXmmMarker);
+
+    PhysicalReg candidate = PhysicalReg_Null;
+    std::set<PhysicalReg>::const_iterator iter;
+    for (iter = scratchCandidates.begin(); iter != scratchCandidates.end();
+            iter++) {
+        PhysicalReg scratch = *iter;
+        if (scratch >= start && scratch <= end) {
+            candidate = scratch;
+            break;
+        }
+    }
+
+    return candidate;
+}
+
+/**
+ * @brief Given a physical register, it returns its physical type
+ * @param reg physical register to check
+ * @return Returns LowOpndRegType_gp if register general purpose.
+ * Returns LowOpndRegType_xmm if register is XMM. Returns
+ * LowOpndRegType_fs is register is stack register for x87.
+ * Otherwise, returns LowOpndRegType_invalid for anything else.
+ */
+LowOpndRegType getTypeOfRegister(PhysicalReg reg) {
+    if (reg >= PhysicalReg_StartOfGPMarker && reg <= PhysicalReg_EndOfGPMarker)
+        return LowOpndRegType_gp;
+    else if (reg >= PhysicalReg_StartOfXmmMarker
+            && reg <= PhysicalReg_EndOfXmmMarker)
+        return LowOpndRegType_xmm;
+    else if (reg >= PhysicalReg_StartOfX87Marker
+            && reg <= PhysicalReg_EndOfX87Marker)
+        return LowOpndRegType_fs;
+
+    return LowOpndRegType_invalid;
+}
+
+/**
+ * @brief Synchronize the spillIndexUsed table with the informatino from compileTable
+ * @return 0 on success, -1 on error
+ */
+static int updateSpillIndexUsed(void) {
+    int k;
+
+    /* First: for each entry, set the index used to 0, in order to reset it */
+    for(k = 0; k <= MAX_SPILL_JIT_IA - 1; k++) {
+        spillIndexUsed[k] = 0;
+    }
+
+    /* Second: go through each compile entry */
+    for(k = 0; k < compileTable.size (); k++) {
+        /** If it is a virtual register, we skip it, we don't need a special spill region for VRs */
+        if(isVirtualReg(compileTable[k].physicalType)) {
+            continue;
+        }
+
+        /** It might have been spilled, let's see if we have the spill_loc_index filled */
+        if(compileTable[k].spill_loc_index >= 0) {
+
+            /* We do have it, but is the index correct (and will fit in our table) */
+            if(compileTable[k].spill_loc_index > 4 * (MAX_SPILL_JIT_IA - 1)) {
+                ALOGI("JIT_INFO: spill_loc_index is wrong for entry %d: %d\n",
+                      k, compileTable[k].spill_loc_index);
+                SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                return -1;
+            }
+
+            /* The spill index is correct, we use the higher bits as a hash for it and set it to 1 */
+            spillIndexUsed[compileTable[k].spill_loc_index >> 2] = 1;
+        }
+    }
+    return 0;
+}
+
+/**
+ * @brief Inserts high VR entries into compile table.
+ * @details Looks through compile table and for all wide VRs it finds, it ensures that an entry exists for the
+ * high part of the VR. Namely, if it finds that v1 is wide, it ensures that there is entry in compile table for v2.
+ * @return Returns true if it succeed insert high VR for all wide VRs.
+ */
+static bool addHighOfWideVRToCompileTable (void)
+{
+    for (int entry = 0; entry < compileTable.size (); entry++)
+    {
+        //We only need to do the correction for wide VRs
+        if (compileTable[entry].isVirtualReg () == false)
+        {
+            continue;
+        }
+
+        //If we have a 64-bit VR, we should also insert the high bits into the compile table
+        if (compileTable[entry].getSize () == OpndSize_64)
+        {
+            //The high bits of a wide virtual register are available in following register number
+            //For example: wide v0 occupies space on stack for v0 and v1.
+            int highVR = compileTable[entry].getRegisterNumber () + 1;
+
+            int indexHigh = searchCompileTable (LowOpndRegType_virtual | LowOpndRegType_gp, highVR);
+
+            //If we don't have an entry for the high bits, we insert it now.
+            if (indexHigh < 0)
+            {
+                //Create a new entry for the high VR. Since we just care about 32-bits we make it GP type
+                CompileTableEntry newEntry (highVR, LowOpndRegType_virtual | LowOpndRegType_gp);
+
+                //We now copy it to the table
+                compileTable.insert (newEntry);
+            }
+        }
+    }
+
+    //If we make it here we were successful at inserting the high VR
+    return true;
+}
+
+void MemoryVRInfo::reset (void)
+{
+    //Set this entry to invalid register
+    regNum = -1;
+
+    //Now set its state to being in memory because it's certainly not in a
+    //location we are keeping track of
+    inMemory = true;
+
+    //Set null check and bound check to false
+    nullCheckDone = false;
+    boundCheck.checkDone = false;
+
+    //Use invalid register for index VR for bound check
+    boundCheck.indexVR = -1;
+
+    //Zero out information about live ranges
+    num_ranges = 0;
+    ranges = 0;
+
+    //Initalize all delay free requests
+    for (int c = 0; c < VRDELAY_COUNT; c++)
+    {
+        delayFreeCounters[c] = 0;
+    }
+}
+
+/**
+ * @brief Updates the table that keeps the in memory state of VRs to contain new VR.
+ * @param vR The virtual register
+ * @param inMemory The initial inMemory state
+ * @return Returns whether the VR's addition was successful.
+ */
+bool addToMemVRTable (int vR, bool inMemory)
+{
+    //We want to keep track of index in memory table
+    int index = 0;
+
+    //Search mem table for the virtual register we are interested in adding
+    for (; index < num_memory_vr; index++)
+    {
+        if (memVRTable[index].regNum == vR)
+        {
+            break;
+        }
+    }
+
+    //If the index is not the size of table, then it means we have found an entry
+    if (index != num_memory_vr)
+    {
+        //We already have entry for this VR so simply update its memory state
+        memVRTable[index].setInMemoryState (inMemory);
+    }
+    else
+    {
+        //Let's make sure we won't overflow the table if we make this insertion
+        if (num_memory_vr >= NUM_MEM_VR_ENTRY)
+        {
+            ALOGI("JIT_INFO: Index %d exceeds size of memVRTable during addToMemVRTable\n", num_memory_vr);
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return false;
+        }
+
+        //Create the new entry
+        MemoryVRInfo memInfo (vR);
+
+        //Now set the in memory state for our new entry
+        memInfo.setInMemoryState (inMemory);
+
+        //We access the index at end of the table
+        index = num_memory_vr;
+
+        //We are adding an entry so increment the number of entries
+        num_memory_vr++;
+
+        //Finally add it to the table
+        memVRTable[index] = memInfo;
+    }
+
+    //If we made it here everything went well
+    return true;
+}
+
+/**
+ * @brief Initializes the in memory tracking table for virtual registers.
+ * @param bb The basic block which we are looking at for initialization.
+ * @return Returns true if all initialization completed successfully. Otherwise it returns false.
+ */
+static bool initializeMemVRTable (BasicBlock_O1 *bb)
+{
+    //Reset number of entries in table
+    num_memory_vr = 0;
+
+    //Now walk through compile entries so we can track in memory state for every VR.
+    //For wide VRs, the compile table must guarantee an entry for both the low and high VR.
+    for(int entry = 0; entry < compileTable.size (); entry++)
+    {
+        //We can skip any entry that is not a virtual register
+        if (compileTable[entry].isVirtualReg () == false)
+        {
+            continue;
+        }
+
+        //Make it easier to refer to the VR number
+        int vR = compileTable[entry].getRegisterNumber ();
+
+        //Determine if parent said that the VR was in memory
+        bool setToInMemory = bb->associationTable.wasVRInMemory (vR);
+
+        //Now let's add it to the table
+        bool result = addToMemVRTable (vR, setToInMemory);
+
+        if (result == false)
+        {
+            //We simply pass along failure since addToMemVRTable has already set error code
+            return false;
+        }
+
+        DEBUG_MEMORYVR(ALOGD("Initializing state of v%d %sin memory",
+                vR, (setToInMemory ? "" : "NOT ")));
+    }
+
+    //If we made it here we were successful
+    return true;
+}
+
+/**
+ * @brief Initializes constant table.
+ * @param bb The basic block which we are looking at for initialization.
+ * @return Returns true if all initialization completed successfully. Otherwise it returns false.
+ */
+static bool initializeConstVRTable (BasicBlock_O1 *bb)
+{
+    //Reset the number of entries to zero since we are initializing
+    num_const_vr = 0;
+
+    //Now walk through compile entries so we can track the constantness for every VR.
+    //For wide VRs, the compile table must guarantee an entry for both the low and high VR.
+    for (int entry = 0; entry < compileTable.size (); entry++)
+    {
+        //We can skip any entry that is not a virtual register
+        if (compileTable[entry].isVirtualReg () == false)
+        {
+            continue;
+        }
+
+        //Make it easier to refer to the VR number
+        int vR = compileTable[entry].getRegisterNumber ();
+
+        //Determine if the virtual register was constant
+        if (bb->associationTable.wasVRConstant (vR) == true)
+        {
+            //We make space for two value because setVRToConst might access high bits
+            int constValue[2];
+
+            //It was constant so let's get its value
+            constValue[0] = bb->associationTable.getVRConstValue (vR);
+
+            //Paranoid so we set high bits to 0
+            constValue[1] = 0;
+
+            //Set it to constant
+            bool result = setVRToConst (vR, OpndSize_32, constValue);
+
+            //We bail out if we failed to set VR to constant. If setVRToConst failed, it already
+            //set an error message.
+            if (result == false)
+            {
+                return false;
+            }
+        }
+    }
+
+    return true;
+}
+
+/**
+ * @brief Used to add registerized virtual registers as defined at entry into basic block
+ */
+static bool initializeRegisterizeDefs (BasicBlock_O1* bb)
+{
+    //Walk through the compile entries
+    for (CompileTable::const_iterator it = compileTable.begin (); it != compileTable.end (); it++)
+    {
+        const CompileTableEntry &compileEntry = *it;
+
+        //Did we find a virtual register that is in physical register? If yes then we must add a def for it
+        if (compileEntry.isVirtualReg () == true && compileEntry.inPhysicalRegister() == true)
+        {
+            //Add a def for this virtual register coming into the BB
+            VirtualRegInfo regDefineInfo;
+            regDefineInfo.regNum = compileEntry.getRegisterNumber ();
+            regDefineInfo.physicalType = compileEntry.getPhysicalType ();
+            regDefineInfo.accessType = REGACCESS_D;
+            offsetPC = PC_FOR_START_OF_BB;
+
+            //Now add it to the defuse tables
+            int res = mergeEntry2 (bb, regDefineInfo);
+
+            if (res < 0)
+            {
+                //We just pass along the error information
+                return false;
+            }
+        }
+    }
+
+    //If we made it here everything went okay
+    return true;
+}
+
+/**
+ * @brief Initializes entries in the compile table at start of BB.
+ * @details It ensures that it updates compile table based on the given associations from its parent.
+ * @param bb The basic block whose virtual register state should be initialized.
+ * @return Returns true if all initialization completed successfully. Otherwise it returns false.
+ */
+static bool initializeRegStateOfBB (BasicBlock_O1* bb)
+{
+    assert (bb != 0);
+
+    //First we clear the compile table
+    compileTable.clear ();
+
+    //Load associations into compile table
+    if (AssociationTable::syncCompileTableWithAssociations (bb->associationTable) == false)
+    {
+        return false;
+    }
+
+    //Since we loaded associations into compile table now we may have virtual registers that are
+    //in physical registers. Thus we set up defines of all those VRs at entry to the BB.
+    if (initializeRegisterizeDefs (bb) == false)
+    {
+        //Just pass along error information
+        return false;
+    }
+
+    //Collect information about the virtual registers in current BB
+    collectInfoOfBasicBlock (bb);
+
+    //Update compileTable with virtual register information from current BB
+    for (std::vector<VirtualRegInfo>::const_iterator vrInfoIter = bb->infoBasicBlock.begin ();
+            vrInfoIter != bb->infoBasicBlock.end (); vrInfoIter++)
+    {
+        insertFromVirtualInfo (*vrInfoIter);
+    }
+
+    //For each virtual register, we insert fake usage at end of basic block to keep it live
+    for (CompileTable::const_iterator tableIter = compileTable.begin (); tableIter != compileTable.end (); tableIter++)
+    {
+        //Get the compile entry
+        const CompileTableEntry &compileEntry = *tableIter;
+
+        if (compileEntry.isVirtualReg ())
+        {
+            //Calling fakeUsageAtEndOfBB uses offsetPC so we switch it to point to end of basic block
+            offsetPC = PC_FOR_END_OF_BB;
+
+            //Update the defUseTable by assuming a fake usage at end of basic block
+            fakeUsageAtEndOfBB (bb, compileEntry.getRegisterNumber (), compileEntry.getLogicalAndPhysicalTypes ());
+        }
+    }
+
+    //Ensure that we also have an entry in compile table for high bits of a VR
+    if (addHighOfWideVRToCompileTable () == false)
+    {
+        return false;
+    }
+
+    //We are ready to initialize the MemVRTable since compile table has been updated
+    if (initializeMemVRTable (bb) == false)
+    {
+        return false;
+    }
+
+    //We are ready to initialize the constant table since compile table has been updated
+    if (initializeConstVRTable (bb) == false)
+    {
+        return false;
+    }
+
+    //Now let's make sure we synchronize all registers being used
+    syncAllRegs ();
+
+    return true;
+}
+
+
+/**
+ * @brief Constructor for BasicBlock_O1
+ */
+BasicBlock_O1::BasicBlock_O1 (void)
+{
+    //We set the defUseTable to 0 to make sure it doesn't try to free it in the clear function
+    defUseTable = 0;
+    clear (true);
+}
+
+/**
+ * @brief Clear function for BasicBlock_O1
+ * @param allocateLabel do we allocate the label
+ */
+void BasicBlock_O1::clear (bool allocateLabel)
+{
+    // Free defUseTable
+    DefUsePair* ptr = defUseTable;
+    defUseTable = 0;
+
+    //Go through each entry
+    while(ptr != NULL) {
+
+        //Get next
+        DefUsePair* tmp = ptr->next;
+
+        //Free its uses
+        DefOrUseLink* ptrUse = ptr->uses;
+
+        while(ptrUse != NULL) {
+            DefOrUseLink* tmp2 = ptrUse->next;
+            free(ptrUse), ptrUse = 0;
+            ptrUse = tmp2;
+        }
+
+        free(ptr), ptr = 0;
+        ptr = tmp;
+    }
+
+    //Reset variables
+    pc_start = 0;
+    pc_end = 0;
+    streamStart = 0;
+    defUseTable = 0;
+    defUseTail = 0;
+    defUseTail = 0;
+
+    //Clear the vectors
+    xferPoints.clear ();
+    associationTable.clear ();
+    infoBasicBlock.clear ();
+
+    //Allocate label
+    if (allocateLabel == true)
+    {
+        label = static_cast<LowOpBlockLabel *> (dvmCompilerNew (sizeof (*label), true));
+    }
+
+    //Paranoid
+    assert (label != NULL);
+
+    //Default value for the offset
+    label->lop.generic.offset = -1;
+
+    //! The logic below assumes that PhysicalReg_EAX is first entry in
+    //! PhysicalReg enum so let's assert it
+    assert(static_cast<int>(PhysicalReg_EAX) == 0);
+
+    // Initialize allocation constraints
+    for (PhysicalReg reg = PhysicalReg_StartOfGPMarker;
+            reg <= PhysicalReg_EndOfGPMarker;
+            reg = static_cast<PhysicalReg>(reg + 1)) {
+        allocConstraints[static_cast<int>(reg)].physicalReg = reg;
+        allocConstraints[static_cast<int>(reg)].count = 0;
+    }
+}
+
+/**
+ * @brief Free everything in the BasicBlock that requires it
+ */
+void BasicBlock_O1::freeIt (void) {
+    //First call clear
+    clear (false);
+
+    //Now free anything that is C++ oriented
+    std::vector<XferPoint> emptyXfer;
+    xferPoints.swap(emptyXfer);
+
+    std::vector<VirtualRegInfo> emptyVRI;
+    infoBasicBlock.swap(emptyVRI);
+}
+
+/**
+ * @brief Do we have enough of a given class of registers to registerize
+ * @param cUnit the BasicBlock
+ * @param reg the RegisterClass to consider
+ * @param cnt the number of this type we have right now
+ */
+static bool isEnoughRegisterization (CompilationUnit *cUnit, RegisterClass reg, int cnt)
+{
+    // Get the max set in the cUnit
+    const int max = cUnit->maximumRegisterization;
+
+    return cnt > max;
+}
+
+/**
+ * @brief Backend specific checker for possible bail out to VM
+ * @details Returns true if bail out from JIT code is possible
+ *          A bit complex logic:
+ *              If someone proved that bail out is not possible we trust him
+ *              Handle not special cases
+ *              Handle special cases: null check elimination, bound check elimination.
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR to check
+ * @return true if bail out from JIT code is possible
+ */
+bool backendCanBailOut(CompilationUnit *cUnit, MIR *mir)
+{
+    if ((mir->OptimizationFlags & MIR_IGNORE_BAIL_OUT_CHECK) != 0) {
+        return false;
+    }
+
+    /* We miss here some entries which makes sense only for method JIT
+     * TODO: update table if method JIT is enabled
+     */
+    switch (mir->dalvikInsn.opcode) {
+
+    /* Monitor enter/exit - there is a call to dvmLockObject */
+    case OP_MONITOR_ENTER:
+    case OP_MONITOR_EXIT:
+        return true;
+
+    /* possible call to class resolution */
+    case OP_CHECK_CAST:
+    case OP_INSTANCE_OF:
+    case OP_SGET:
+    case OP_SGET_WIDE:
+    case OP_SGET_OBJECT:
+    case OP_SGET_BOOLEAN:
+    case OP_SGET_BYTE:
+    case OP_SGET_CHAR:
+    case OP_SGET_SHORT:
+    case OP_SPUT:
+    case OP_SPUT_WIDE:
+    case OP_SPUT_OBJECT:
+    case OP_SPUT_BOOLEAN:
+    case OP_SPUT_BYTE:
+    case OP_SPUT_CHAR:
+    case OP_SPUT_SHORT:
+    case OP_SGET_VOLATILE:
+    case OP_SPUT_VOLATILE:
+    case OP_SGET_WIDE_VOLATILE:
+    case OP_SPUT_WIDE_VOLATILE:
+    case OP_SGET_OBJECT_VOLATILE:
+    case OP_SPUT_OBJECT_VOLATILE:
+        return true;
+
+    /* memory allocation */
+    case OP_NEW_INSTANCE:
+    case OP_NEW_ARRAY:
+    case OP_FILLED_NEW_ARRAY:
+    case OP_FILLED_NEW_ARRAY_RANGE:
+        return true;
+
+    /* implicit throw */
+    case OP_THROW:
+    case OP_THROW_VERIFICATION_ERROR:
+        return true;
+
+    /* invocation */
+    case OP_INVOKE_VIRTUAL:
+    case OP_INVOKE_SUPER:
+    case OP_INVOKE_DIRECT:
+    case OP_INVOKE_STATIC:
+    case OP_INVOKE_INTERFACE:
+    case OP_INVOKE_VIRTUAL_RANGE:
+    case OP_INVOKE_SUPER_RANGE:
+    case OP_INVOKE_DIRECT_RANGE:
+    case OP_INVOKE_STATIC_RANGE:
+    case OP_INVOKE_INTERFACE_RANGE:
+    case OP_EXECUTE_INLINE:
+    case OP_EXECUTE_INLINE_RANGE:
+    case OP_INVOKE_OBJECT_INIT_RANGE:
+    case OP_INVOKE_VIRTUAL_QUICK:
+    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
+    case OP_INVOKE_SUPER_QUICK:
+    case OP_INVOKE_SUPER_QUICK_RANGE:
+        return true;
+
+    /* Division By Zero */
+    case OP_DIV_INT:
+    case OP_REM_INT:
+    case OP_DIV_LONG:
+    case OP_REM_LONG:
+    case OP_DIV_INT_2ADDR:
+    case OP_REM_INT_2ADDR:
+    case OP_DIV_LONG_2ADDR:
+    case OP_REM_LONG_2ADDR:
+        return true;
+
+    case OP_DIV_INT_LIT16:
+    case OP_REM_INT_LIT16:
+    case OP_DIV_INT_LIT8:
+    case OP_REM_INT_LIT8:
+        return mir->dalvikInsn.vC == 0;
+
+    /* Access an Array index */
+    case OP_AGET:
+    case OP_AGET_WIDE:
+    case OP_AGET_OBJECT:
+    case OP_AGET_BOOLEAN:
+    case OP_AGET_BYTE:
+    case OP_AGET_CHAR:
+    case OP_AGET_SHORT:
+    case OP_APUT:
+    case OP_APUT_WIDE:
+    case OP_APUT_OBJECT:
+    case OP_APUT_BOOLEAN:
+    case OP_APUT_BYTE:
+    case OP_APUT_CHAR:
+    case OP_APUT_SHORT:
+        return ((mir->OptimizationFlags & MIR_IGNORE_RANGE_CHECK) == 0) || ((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0);
+
+    /* Access Object field */
+    case OP_ARRAY_LENGTH:
+    case OP_IGET:
+    case OP_IGET_WIDE:
+    case OP_IGET_OBJECT:
+    case OP_IGET_BOOLEAN:
+    case OP_IGET_BYTE:
+    case OP_IGET_CHAR:
+    case OP_IGET_SHORT:
+    case OP_IPUT:
+    case OP_IPUT_WIDE:
+    case OP_IPUT_OBJECT:
+    case OP_IPUT_BOOLEAN:
+    case OP_IPUT_BYTE:
+    case OP_IPUT_CHAR:
+    case OP_IPUT_SHORT:
+    case OP_IGET_VOLATILE:
+    case OP_IPUT_VOLATILE:
+    case OP_IGET_OBJECT_VOLATILE:
+    case OP_IGET_WIDE_VOLATILE:
+    case OP_IPUT_WIDE_VOLATILE:
+    case OP_IGET_QUICK:
+    case OP_IGET_WIDE_QUICK:
+    case OP_IGET_OBJECT_QUICK:
+    case OP_IPUT_QUICK:
+    case OP_IPUT_WIDE_QUICK:
+    case OP_IPUT_OBJECT_QUICK:
+    case OP_IPUT_OBJECT_VOLATILE:
+        return (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0;
+
+    default:
+        /* All other cases do not bail out
+         * For non-trace JIT the following opcodes should be checked:
+         * OP_CONST_STRING, OP_CONST_STRING_JUMBO, OP_CONST_CLASS.
+         */
+        break;
+    }
+
+    return false;
+}
+
+/**
+ * @brief Handles registerization decision before lowering.
+ * @details If registerization is disabled, set the writeBack vector to all 1s.
+ * Registerization extended MIRs are then removed in order to only registerize
+ * maximum set by the cUnit.
+ * @param cUnit the BasicBlock
+ * @param bb the BasicBlock
+ */
+static void handleRegisterizationPrework (CompilationUnit *cUnit, BasicBlock *bb)
+{
+    //Handle no registerization option first
+    if (gDvmJit.backEndRegisterization == false)
+    {
+        //In this case, we are going to rewrite the requestWriteBack to spilling everything
+        dvmCompilerWriteBackAll (cUnit, bb);
+    }
+
+    //A counter for the kOpRegisterize requests
+    MIR *mir = bb->firstMIRInsn;
+    std::map<RegisterClass, int> counters;
+
+    //Go through the instructions
+    while (mir != 0)
+    {
+        //Did we remove an instruction? Suppose no
+        bool removed = false;
+
+        //If it's a registerize request, we might have to ignore it
+        if (mir->dalvikInsn.opcode == static_cast<Opcode> (kMirOpRegisterize))
+        {
+            //Get the class for it
+            RegisterClass reg = static_cast<RegisterClass> (mir->dalvikInsn.vB);
+
+            //Increment counter
+            counters[reg]++;
+
+            //If we've had enough
+            if (isEnoughRegisterization (cUnit, reg, counters[reg]) == true)
+            {
+                //We are going to remove it, remember it
+                MIR *toRemove = mir;
+                //Remove the instruction but first remember the next one
+                //Note: we are removing this registerization request knowing that the system might request a recompile
+                //TODO: Most likely a flag to ignore it would be better
+                mir = mir->next;
+                //Call the helper function
+                dvmCompilerRemoveMIR (bb, toRemove);
+                //Set removed
+                removed = true;
+            }
+        }
+
+        if (removed == false)
+        {
+            //Go to next
+            mir = mir->next;
+        }
+    }
+}
+
+/**
+ * @brief Parse the BasicBlock and perform pre-lowering work
+ * @param cUnit the BasicBlock
+ * @param bb the BasicBlock
+ */
+static void parseBlock (CompilationUnit *cUnit, BasicBlock *bb)
+{
+    //Always handle registerization request
+    handleRegisterizationPrework (cUnit, bb);
+}
+
+/**
+ * @brief Pre-process BasicBlocks
+ * @details This parses the block to perform some pre-code generation tasks
+ * @param cUnit the Compilation Unit
+ * @param bb the BasicBlock
+ * @return -1 if error happened, 0 otherwise
+ */
+int preprocessingBB (CompilationUnit *cUnit, BasicBlock* bb)
+{
+    //Parse the BasicBlock, we might have some pre work to do
+    parseBlock (cUnit, bb);
+
+    //Everything went well
+    return 0;
+}
+
+void printJitTraceInfoAtRunTime(const Method* method, int offset) {
+    ALOGI("execute trace for %s%s at offset %x", method->clazz->descriptor, method->name, offset);
+}
+
+void startOfTraceO1(const Method* method, int exceptionBlockId, CompilationUnit *cUnit) {
+    compileTable.clear ();
+    currentBB = NULL;
+    currentUnit = cUnit;
+
+    /* initialize data structure allRegs */
+    initializeAllRegs();
+
+// dumpDebuggingInfo is gone in CompilationUnit struct
+#if 0
+    /* add code to dump debugging information */
+    if(cUnit->dumpDebuggingInfo) {
+        move_imm_to_mem(OpndSize_32, cUnit->startOffset, -4, PhysicalReg_ESP, true); //2nd argument: offset
+        move_imm_to_mem(OpndSize_32, (int)currentMethod, -8, PhysicalReg_ESP, true); //1st argument: method
+        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+        typedef void (*vmHelper)(const Method*, int);
+        vmHelper funcPtr = printJitTraceInfoAtRunTime;
+        move_imm_to_reg(OpndSize_32, (int)funcPtr, PhysicalReg_ECX, true);
+        call_reg(PhysicalReg_ECX, true);
+
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    }
+#endif
+}
+
+
+/* Code generation for a basic block defined for JIT
+   We have two data structures for a basic block:
+       BasicBlock defined in vm/compiler by JIT
+       BasicBlock_O1 defined in o1 */
+int codeGenBasicBlockJit(const Method* method, BasicBlock* bb) {
+    // For x86, the BasicBlock should be the specialized one
+    currentBB = reinterpret_cast<BasicBlock_O1 *> (bb);
+
+    // Basic block here also means new native basic block
+    if (gDvmJit.scheduling)
+    {
+        singletonPtr<Scheduler> ()->signalEndOfNativeBasicBlock ();
+    }
+
+    // Finalize this block's association table because we are generating
+    // it and thus any parent of it that hasn't been generated yet must
+    // be aware of this fact.
+    currentBB->associationTable.finalize ();
+
+    // Generate code for this basic block
+    int result = codeGenBasicBlock (method, currentBB);
+
+    // End of managed basic block means end of native basic block
+    if (gDvmJit.scheduling)
+    {
+        singletonPtr<Scheduler> ()->signalEndOfNativeBasicBlock ();
+    }
+
+    currentBB = NULL;
+
+    return result;
+}
+void endOfBasicBlock(BasicBlock* bb) {
+    isScratchPhysical = true;
+    currentBB = NULL;
+}
+
+/**
+ * @brief decide if skip the extended Op whose implementation uses NcgO0 mode
+ * @param opc opcode of the extended MIR
+ * @return return false if the opcode doesn't use NcgO0, otherwise return true
+ */
+bool skipExtendedMir(Opcode opc) {
+    ExtendedMIROpcode extendedOpCode = static_cast<ExtendedMIROpcode> (opc);
+
+    switch (extendedOpCode) {
+        case kMirOpRegisterize:
+            return false;
+        default:
+            return true;
+    }
+}
+
+/** entry point to collect information about virtual registers used in a basic block
+    Initialize data structure BasicBlock_O1
+    The usage information of virtual registers is stoerd in bb->infoBasicBlock
+
+    Global variables accessed: offsetPC, rPC
+*/
+int collectInfoOfBasicBlock (BasicBlock_O1* bb)
+{
+    int seqNum = 0;
+    /* traverse the MIR in basic block
+       sequence number is used to make sure next bytecode will have a larger sequence number */
+    for(MIR * mir = bb->firstMIRInsn; mir; mir = mir->next) {
+        offsetPC = seqNum;
+        mir->seqNum = seqNum++;
+
+        // Skip extended MIRs whose implementation uses NcgO0 mode
+        if ( isExtendedMir(mir->dalvikInsn.opcode) == true &&
+             skipExtendedMir(mir->dalvikInsn.opcode) == true) {
+            continue;
+        }
+
+        //Get information about the VRs in current bytecode
+        VirtualRegInfo infoByteCode[MAX_REG_PER_BYTECODE];
+        int numVRs = getVirtualRegInfo (infoByteCode, mir, true);
+
+        for(int kk = 0; kk < numVRs; kk++) {
+            //Make a copy of current bytecode
+            VirtualRegInfo currentInfo = infoByteCode[kk];
+#ifdef DEBUG_MERGE_ENTRY
+            ALOGI("Call mergeEntry2 at offsetPC %x kk %d VR %d %d\n", offsetPC, kk,
+                  currentInfo.regNum, currentInfo.physicalType);
+#endif
+            int retCode = mergeEntry2(bb, currentInfo); //update defUseTable of the basic block
+            if (retCode < 0)
+                return retCode;
+        }
+
+        //dumpVirtualInfoOfBasicBlock(bb);
+    }//for each bytecode
+
+    bb->pc_end = seqNum;
+
+    //sort allocConstraints of each basic block
+    unsigned int max = bb->infoBasicBlock.size ();
+    for(unsigned int kk = 0; kk < max; kk++) {
+#ifdef DEBUG_ALLOC_CONSTRAINT
+        ALOGI("Sort virtual reg %d type %d -------", bb->infoBasicBlock[kk].regNum,
+              bb->infoBasicBlock[kk].physicalType);
+#endif
+        sortAllocConstraint(bb->infoBasicBlock[kk].allocConstraints,
+                            bb->infoBasicBlock[kk].allocConstraintsSorted, true);
+    }
+#ifdef DEBUG_ALLOC_CONSTRAINT
+    ALOGI("Sort constraints for BB %d --------", bb->bb_index);
+#endif
+    sortAllocConstraint(bb->allocConstraints, bb->allocConstraintsSorted, false);
+    return 0;
+}
+
+/**
+ * @brief Looks through a basic block for any reasons to reject it
+ * @param bb Basic Block to look at
+ * @return true if Basic Block cannot be handled safely by Backend
+ */
+static bool shouldRejectBasicBlock(BasicBlock_O1* bb) {
+    // Assume that we do not want to reject the BB
+    bool shouldReject = false;
+
+    // Set a generic error message in case someone forgets to set a proper one
+    JitCompilationErrors errorIfRejected = kJitErrorCodegen;
+
+    /**
+     * Rejection Scenario 1:
+     * If the basic block has incoming virtual registers that are in physical
+     * registers but we have a usage of that VR in x87, we should reject trace
+     * until we properly handle the Xfer point.
+     */
+
+    // We will need to call getVirtualRegInfo but we do not want to update
+    // register constraints so temporarily set the global currentBB to null
+    BasicBlock_O1 * savedCurrentBB = currentBB;
+    currentBB = NULL;
+
+    std::set<int> registerizedVRs;
+
+    // Find all of the VRs that have been registerized at entry into this BB
+    for (AssociationTable::const_iterator iter = bb->associationTable.begin();
+            iter != bb->associationTable.end(); iter++) {
+        // If there is a physical register for this VR, then it has been
+        // registerized
+        if (iter->second.physicalReg != PhysicalReg_Null) {
+            registerizedVRs.insert(iter->first);
+        }
+    }
+
+    for (MIR * mir = bb->firstMIRInsn; mir != NULL; mir = mir->next) {
+
+        if ( isExtendedMir(mir->dalvikInsn.opcode) == true &&
+             skipExtendedMir(mir->dalvikInsn.opcode) == true) {
+            continue;
+        }
+
+        //Get information about the VRs in current bytecode
+        VirtualRegInfo infoByteCode[MAX_REG_PER_BYTECODE];
+        int numVRs = getVirtualRegInfo (infoByteCode, mir);
+
+        // Go through each VR of the MIR
+        for (int vrIter = 0; vrIter < numVRs; vrIter++) {
+            int VR = infoByteCode[vrIter].regNum;
+            LowOpndRegType type = infoByteCode[vrIter].physicalType;
+
+            // Has this VR been registerized?
+            if (registerizedVRs.find(VR) != registerizedVRs.end()) {
+                // If we will be using x87 for this registerized VR, we cannot
+                // handle
+                if (type == LowOpndRegType_fs || type == LowOpndRegType_fs_s) {
+                    ALOGI("JIT_INFO: Found x87 usage for VR that has been registerized.");
+                    errorIfRejected = kJitErrorBERegisterization;
+                    shouldReject = true;
+                    break;
+                }
+            }
+        }
+
+        // We already know we need to reject, break out of loop
+        if (shouldReject == true) {
+            break;
+        }
+    }
+
+    // Restore currentBB
+    currentBB = savedCurrentBB;
+
+    if (shouldReject) {
+        SET_JIT_ERROR (errorIfRejected);
+    }
+
+    return shouldReject;
+}
+
+/** entry point to generate native code for a O1 basic block
+    There are 3 kinds of virtual registers in a O1 basic block:
+    1> L VR: local within the basic block
+    2> GG VR: is live in other basic blocks,
+              its content is in a pre-defined GPR at the beginning of a basic block
+    3> GL VR: is live in other basic blocks,
+              its content is in the interpreted stack at the beginning of a basic block
+    compileTable is updated with infoBasicBlock at the start of the basic block;
+    Before lowering each bytecode, compileTable is updated with infoByteCodeTemp;
+    At end of the basic block, right before the jump instruction, handles constant VRs and GG VRs
+*/
+int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb)
+{
+    //Eagerly set retCode to 0 since most likely everything will be okay
+    int retCode = 0;
+
+    //We have no MIRs if first MIR pointer is null
+    bool noMirs = (currentBB->firstMIRInsn == NULL);
+
+    // If we should reject the BB, return that it has not been handled
+    if (shouldRejectBasicBlock (bb) == true)
+    {
+        //If rejected, an error message will have been set so we just pass along the error
+        return -1;
+    }
+
+    //We have already loaded the information about VR from each bytecode in this basic block.
+    //Thus we are now ready to finish initializing virtual register state.
+    if (initializeRegStateOfBB (bb) == false)
+    {
+        return -1;
+    }
+
+    //If we do have MIRs, we must update the transfer points and the live table
+    if (noMirs == false)
+    {
+        //Now we update any transfer points between virtual registers that are represented by different types
+        //throughout this same BB
+        retCode = updateXferPoints (bb);
+
+        if (retCode < 0)
+        {
+            //Someone else has set the error so we just pass it along
+            return retCode;
+        }
+
+        //Since we have set up the transfer points, check to see if there are any points at the start of the BB
+        //so that we can handle them right now.
+        handleStartOfBBXferPoints (bb);
+
+        retCode = updateLiveTable (bb);
+
+        if (retCode < 0)
+        {
+            //Someone else has set the error so we just pass it along
+            return retCode;
+        }
+    }
+
+#ifdef DEBUG_REACHING_DEF
+    printDefUseTable();
+#endif
+
+#ifdef DEBUG_COMPILE_TABLE
+    ALOGI("At start of basic block %d (num of VRs %d) -------", bb->bb_index, bb->infoBasicBlock.size ());
+    dumpCompileTable();
+#endif
+
+    //Assume that the last bytecode in this block is not a jump unless proven otherwise
+    bool lastByteCodeIsJump = false;
+
+    //Now walk through the bytecodes to generate code for each
+    for (MIR * mir = bb->firstMIRInsn; mir; mir = mir->next)
+    {
+        int k;
+        offsetPC = mir->seqNum;
+        rPC = const_cast<u2 *>(method->insns) + mir->offset;
+
+        //Skip mirs tagged as being no-ops
+        if ((mir->OptimizationFlags & MIR_INLINED) != 0)
+        {
+            continue;
+        }
+
+        // Handle extended MIRs whose implementation uses NcgO0 mode
+        if ( isExtendedMir(mir->dalvikInsn.opcode) == true &&
+             skipExtendedMir(mir->dalvikInsn.opcode) == true) {
+            handleExtendedMIR (currentUnit, bb, mir);
+            // The rest of logic is for handling mirs that use NCG01 so
+            // we can safely skip
+            continue;
+        }
+
+        //before handling a bytecode, import info of temporary registers to compileTable including refCount
+        num_temp_regs_per_bytecode = getTempRegInfo(infoByteCodeTemp, mir);
+        for(k = 0; k < num_temp_regs_per_bytecode; k++) {
+            if(infoByteCodeTemp[k].versionNum > 0) continue;
+            insertFromTempInfo (infoByteCodeTemp[k]);
+        }
+        startNativeCode(-1, -1);
+        for(k = 0; k <= MAX_SPILL_JIT_IA - 1; k++) spillIndexUsed[k] = 0;
+
+#ifdef DEBUG_COMPILE_TABLE
+        ALOGI("compile table size after importing temporary info %d", num_compile_entries);
+        ALOGI("before one bytecode %d (num of VRs %d) -------", bb->bb_index, bb->infoBasicBlock.size ());
+#endif
+        //set isConst to true for CONST & MOVE MOVE_OBJ?
+        //clear isConst to true for MOVE, MOVE_OBJ, MOVE_RESULT, MOVE_EXCEPTION ...
+        bool isConst = false;
+        int retCode = getConstInfo(bb, mir); //will return 0 if a VR is updated by the bytecode
+        //if the bytecode generates a constant
+        if (retCode == 1)
+            isConst = true;
+        //if something went wrong at getConstInfo. getConstInfo has logged it
+        else if (retCode == -1)
+            return retCode;
+        //otherwise, bytecode does not generate a constant
+
+        //Get information about the VRs in current bytecode
+        VirtualRegInfo infoByteCode[MAX_REG_PER_BYTECODE];
+        int numVRs = getVirtualRegInfo (infoByteCode, mir);
+
+        //call something similar to mergeEntry2, but only update refCount
+        //clear refCount
+        for(k = 0; k < numVRs; k++) {
+            int indexT = searchCompileTable(LowOpndRegType_virtual | infoByteCode[k].physicalType,
+                                            infoByteCode[k].regNum);
+            if(indexT >= 0)
+                compileTable[indexT].refCount = 0;
+        }
+        for(k = 0; k < numVRs; k++) {
+            int indexT = searchCompileTable(LowOpndRegType_virtual | infoByteCode[k].physicalType,
+                                            infoByteCode[k].regNum);
+            if(indexT >= 0)
+                compileTable[indexT].refCount += infoByteCode[k].refCount;
+        } //for k
+        lastByteCodeIsJump = false;
+        if(isConst == false)
+        {
+#ifdef DEBUG_COMPILE_TABLE
+            dumpCompileTable();
+#endif
+            freeShortMap();
+            if (isCurrentByteCodeJump(mir->dalvikInsn.opcode))
+                lastByteCodeIsJump = true;
+
+            //We eagerly assume we don't handle unless proven otherwise.
+            bool notHandled = true;
+
+            if ((int) mir->dalvikInsn.opcode >= (int) kMirOpFirst)
+            {
+                notHandled = (handleExtendedMIR (currentUnit, bb, mir) == false);
+            }
+            else
+            {
+                notHandled = lowerByteCodeJit(method, mir, rPC);
+            }
+
+            if(gDvmJit.codeCacheByteUsed + (stream - streamStart) +
+                 CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
+                 ALOGI("JIT_INFO: Code cache full while lowering bytecode %s", dexGetOpcodeName(mir->dalvikInsn.opcode));
+                 gDvmJit.codeCacheFull = true;
+                 SET_JIT_ERROR(kJitErrorCodeCacheFull);
+                 return -1;
+            }
+
+            if (notHandled){
+                SET_JIT_ERROR(kJitErrorCodegen);
+                return -1;
+            }
+
+            //Check if an error happened while in the bytecode
+            if (IS_ANY_JIT_ERROR_SET()) {
+                SET_JIT_ERROR(kJitErrorCodegen);
+                return -1;
+            }
+
+            updateConstInfo(bb);
+            freeShortMap();
+        } else { //isConst
+            //if this bytecode is the target of a jump, the mapFromBCtoNCG should be updated
+            offsetNCG = stream - streamMethodStart;
+            mapFromBCtoNCG[mir->offset] = offsetNCG;
+#ifdef DEBUG_COMPILE_TABLE
+            ALOGI("Bytecode %s generates a constant and has no side effect\n", dexGetOpcodeName(mir->dalvikInsn.opcode));
+#endif
+        }
+
+        //After each bytecode, make sure the temporaries have refCount of zero.
+        for(k = 0; k < compileTable.size (); k++)
+        {
+            if(compileTable[k].isTemporary ())
+            {
+#ifdef PRINT_WARNING
+                //If warnings are enabled, we need to print a message because remaining ref count greater
+                //than zero means that bytecode visitor reference counts are not correct
+                if (compileTable[k].refCount > 0)
+                {
+                    ALOGD ("JIT_INFO: refCount for a temporary reg %d %d is %d after a bytecode", compileTable[k].regNum,
+                            compileTable[k].physicalType, compileTable[k].refCount);
+                }
+#endif
+                compileTable[k].updateRefCount (0);
+            }
+        }
+
+        //Now that we updated reference counts, let's clear the physical register associations
+        freeReg (false);
+
+#ifdef DEBUG_COMPILE_TABLE
+        ALOGI("After one bytecode BB %d (num of VRs %d)", bb->bb_index, bb->infoBasicBlock.size ());
+#endif
+    }//for each bytecode
+
+#ifdef DEBUG_COMPILE_TABLE
+    dumpCompileTable();
+#endif
+
+    //At the end of a basic block we want to handle VR information. If the BB ended with
+    //jump or switch, then we have nothing to handle because it has already been handled
+    //in the corresponding jumping bytecode.
+    retCode = handleRegistersEndOfBB (lastByteCodeIsJump == false);
+
+    if (retCode < 0)
+    {
+        //Just pass along error information
+        return retCode;
+    }
+
+    // We are done with compile table so clear it now
+    compileTable.clear ();
+
+    //Free live table
+    for (int k = 0; k < num_memory_vr; k++)
+    {
+        LiveRange* ptr2 = memVRTable[k].ranges;
+        while (ptr2 != NULL)
+        {
+            LiveRange* tmpP = ptr2->next;
+            free (ptr2->accessPC);
+            free (ptr2);
+            ptr2 = tmpP;
+        }
+    }
+
+    return 0;
+}
+
+/** update infoBasicBlock & defUseTable
+    input: currentInfo
+    side effect: update currentInfo.reachingDefs
+
+    update entries in infoBasicBlock by calling updateReachingDefA
+    if there is no entry in infoBasicBlock for B, an entry will be created and inserted to infoBasicBlock
+
+    defUseTable is updated to account for the access at currentInfo
+    if accessType of B is U or UD, we call updateReachingDefB to update currentInfo.reachingDefs
+        in order to correctly insert the usage to defUseTable
+*/
+int mergeEntry2 (BasicBlock_O1* bb, VirtualRegInfo &currentInfo)
+{
+    LowOpndRegType typeB = currentInfo.physicalType;
+    int regB = currentInfo.regNum;
+    int jj, k;
+    int jjend = bb->infoBasicBlock.size ();
+    bool isMerged = false;
+    bool hasAlias = false;
+    OverlapCase isBPartiallyOverlapA, isAPartiallyOverlapB;
+    RegAccessType tmpType = REGACCESS_N;
+    currentInfo.num_reaching_defs = 0;
+
+    /* traverse variable A in infoBasicBlock */
+    for(jj = 0; jj < jjend; jj++) {
+        int regA = bb->infoBasicBlock[jj].regNum;
+        LowOpndRegType typeA = bb->infoBasicBlock[jj].physicalType;
+        isBPartiallyOverlapA = getBPartiallyOverlapA(regB, typeB, regA, typeA);
+        isAPartiallyOverlapB = getAPartiallyOverlapB(regA, typeA, regB, typeB);
+        if(regA == regB && typeA == typeB) {
+            /* variable A and B are aligned */
+            bb->infoBasicBlock[jj].accessType = mergeAccess2(bb->infoBasicBlock[jj].accessType, currentInfo.accessType,
+                                                             OVERLAP_B_COVER_A);
+            bb->infoBasicBlock[jj].refCount += currentInfo.refCount;
+            /* copy reaching defs of variable B from variable A */
+            currentInfo.num_reaching_defs = bb->infoBasicBlock[jj].num_reaching_defs;
+            for(k = 0; k < currentInfo.num_reaching_defs; k++)
+                currentInfo.reachingDefs[k] = bb->infoBasicBlock[jj].reachingDefs[k];
+            updateDefUseTable (currentInfo); //use currentInfo to update defUseTable
+            int retCode = updateReachingDefA (currentInfo, jj, OVERLAP_B_COVER_A); //update reachingDefs of A
+            if (retCode < 0)
+                return -1;
+            isMerged = true;
+            hasAlias = true;
+            if(typeB == LowOpndRegType_gp) {
+                //merge allocConstraints
+                for(k = 0; k < 8; k++) {
+                    bb->infoBasicBlock[jj].allocConstraints[k].count += currentInfo.allocConstraints[k].count;
+                }
+            }
+        }
+        else if(isBPartiallyOverlapA != OVERLAP_NO) {
+            tmpType = updateAccess2(tmpType, updateAccess1(bb->infoBasicBlock[jj].accessType, isAPartiallyOverlapB));
+            bb->infoBasicBlock[jj].accessType = mergeAccess2(bb->infoBasicBlock[jj].accessType, currentInfo.accessType,
+                                                             isBPartiallyOverlapA);
+#ifdef DEBUG_MERGE_ENTRY
+            ALOGI("Update accessType in case 2: VR %d %d accessType %d", regA, typeA, bb->infoBasicBlock[jj].accessType);
+#endif
+            hasAlias = true;
+            if(currentInfo.accessType == REGACCESS_U || currentInfo.accessType == REGACCESS_UD) {
+                VirtualRegInfo tmpInfo;
+
+                /* update currentInfo.reachingDefs */
+                int retCode = updateReachingDefB1 (currentInfo, tmpInfo, jj);
+                if (retCode < 0)
+                    return retCode;
+                retCode = updateReachingDefB2 (currentInfo, tmpInfo);
+                if (retCode < 0)
+                    return retCode;
+            }
+            int retCode = updateReachingDefA (currentInfo, jj, isBPartiallyOverlapA);
+            if (retCode < 0)
+                return retCode;
+        }
+        else {
+            //even if B does not overlap with A, B can affect the reaching defs of A
+            //for example, B is a def of "v0", A is "v1"
+            //  B can kill some reaching defs of A or affect the accessType of a reaching def
+            int retCode = updateReachingDefA (currentInfo, jj, OVERLAP_NO); //update reachingDefs of A
+            if (retCode < 0)
+                return -1;
+        }
+    }//for each variable A in infoBasicBlock
+    if(!isMerged) {
+        /* create a new entry in infoBasicBlock */
+        VirtualRegInfo info;
+        info.refCount = currentInfo.refCount;
+        info.physicalType = typeB;
+        if(hasAlias)
+            info.accessType = updateAccess3(tmpType, currentInfo.accessType);
+        else
+            info.accessType = currentInfo.accessType;
+#ifdef DEBUG_MERGE_ENTRY
+        ALOGI("Update accessType in case 3: VR %d %d accessType %d", regB, typeB, info.accessType);
+#endif
+        info.regNum = regB;
+        for(k = 0; k < 8; k++)
+            info.allocConstraints[k] = currentInfo.allocConstraints[k];
+#ifdef DEBUG_MERGE_ENTRY
+        ALOGI("isMerged is false, call updateDefUseTable");
+#endif
+        updateDefUseTable (currentInfo); //use currentInfo to update defUseTable
+        updateReachingDefB3 (currentInfo); //update currentInfo.reachingDefs if currentInfo defines variable B
+
+        //copy from currentInfo.reachingDefs to info
+        info.num_reaching_defs = currentInfo.num_reaching_defs;
+        for(k = 0; k < currentInfo.num_reaching_defs; k++)
+            info.reachingDefs[k] = currentInfo.reachingDefs[k];
+#ifdef DEBUG_MERGE_ENTRY
+        ALOGI("Try to update reaching defs for VR %d %d", regB, typeB);
+        for(k = 0; k < info.num_reaching_defs; k++)
+            ALOGI("reaching def %d @ %d for VR %d %d access %d", k, currentInfo.reachingDefs[k].offsetPC,
+                  currentInfo.reachingDefs[k].regNum, currentInfo.reachingDefs[k].physicalType,
+                  currentInfo.reachingDefs[k].accessType);
+#endif
+        //Push it in the vector
+        bb->infoBasicBlock.push_back (info);
+
+        if(bb->infoBasicBlock.size () >= MAX_REG_PER_BASICBLOCK) {
+            ALOGI("JIT_INFO: Number of VRs (%d) in a basic block, exceed maximum (%d)\n", bb->infoBasicBlock.size (), MAX_REG_PER_BASICBLOCK);
+            SET_JIT_ERROR(kJitErrorMaxVR);
+            return -1;
+        }
+    }
+    return 0;
+}
+
+/**
+ * @brief update reaching defs for infoBasicBlock[indexToA]
+ * @details use currentInfo.reachingDefs to update reaching defs for variable A
+ * @param currentInfo the current considered VirtualRegInfo
+ * @param indexToA Index of variable A
+ * @param isBPartiallyOverlapA the type of overlap
+ * @return -1 if error, 0 otherwise
+ */
+static int updateReachingDefA (VirtualRegInfo &currentInfo, int indexToA, OverlapCase isBPartiallyOverlapA)
+{
+    if(indexToA < 0) return 0;
+    int k, k2;
+    OverlapCase isBPartiallyOverlapDef;
+    if(currentInfo.accessType == REGACCESS_U) {
+        return 0; //no update to reachingDefs of the VR
+    }
+    /* access in currentInfo is DU, D, or UD */
+    if(isBPartiallyOverlapA == OVERLAP_B_COVER_A) {
+        /* from this point on, the reachingDefs for variable A is a single def to currentInfo at offsetPC */
+        currentBB->infoBasicBlock[indexToA].num_reaching_defs = 1;
+        currentBB->infoBasicBlock[indexToA].reachingDefs[0].offsetPC = offsetPC;
+        currentBB->infoBasicBlock[indexToA].reachingDefs[0].regNum = currentInfo.regNum;
+        currentBB->infoBasicBlock[indexToA].reachingDefs[0].physicalType = currentInfo.physicalType;
+        currentBB->infoBasicBlock[indexToA].reachingDefs[0].accessType = REGACCESS_D;
+#ifdef DEBUG_REACHING_DEF
+        ALOGI("Single reaching def @ %d for VR %d %d", offsetPC, currentInfo.regNum, currentInfo.physicalType);
+#endif
+        return 0;
+    }
+    /* update reachingDefs for variable A to get rid of dead defs */
+    /* Bug fix: it is possible that more than one reaching defs need to be removed
+                after one reaching def is removed, num_reaching_defs--, but k should not change
+    */
+    for(k = 0; k < currentBB->infoBasicBlock[indexToA].num_reaching_defs; ) {
+        /* remove one reaching def in one interation of the loop */
+        //check overlapping between def & B
+        isBPartiallyOverlapDef = getBPartiallyOverlapA(currentInfo.regNum, currentInfo.physicalType,
+                                                       currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum,
+                                                       currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType);
+#ifdef DEBUG_REACHING_DEF
+        ALOGI("DEBUG B %d %d def %d %d %d", currentInfo.regNum, currentInfo.physicalType,
+              currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum,
+              currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType,
+              currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType);
+#endif
+        /* cases where one def nees to be removed:
+           if B fully covers def, def is removed
+           if B overlaps high half of def & def's accessType is H, def is removed
+           if B overlaps low half of def & def's accessType is L, def is removed
+        */
+        if((isBPartiallyOverlapDef == OVERLAP_B_COVER_HIGH_OF_A &&
+            currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType == REGACCESS_H) ||
+           (isBPartiallyOverlapDef == OVERLAP_B_COVER_LOW_OF_A &&
+            currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType == REGACCESS_L) ||
+           isBPartiallyOverlapDef == OVERLAP_B_COVER_A
+           ) { //remove def
+            //shift from k+1 to end
+            for(k2 = k+1; k2 < currentBB->infoBasicBlock[indexToA].num_reaching_defs; k2++)
+                currentBB->infoBasicBlock[indexToA].reachingDefs[k2-1] = currentBB->infoBasicBlock[indexToA].reachingDefs[k2];
+            currentBB->infoBasicBlock[indexToA].num_reaching_defs--;
+        }
+        /*
+           if B overlaps high half of def & def's accessType is not H --> update accessType of def
+        */
+        else if(isBPartiallyOverlapDef == OVERLAP_B_COVER_HIGH_OF_A &&
+                currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType != REGACCESS_H) {
+            //low half is still valid
+            if(getRegSize(currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType) == OpndSize_32)
+                currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType = REGACCESS_D;
+            else
+                currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType = REGACCESS_L;
+#ifdef DEBUG_REACHING_DEF
+            ALOGI("DEBUG: set accessType of def to L");
+#endif
+            k++;
+        }
+        /*
+           if B overlaps low half of def & def's accessType is not L --> update accessType of def
+        */
+        else if(isBPartiallyOverlapDef == OVERLAP_B_COVER_LOW_OF_A &&
+                currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType != REGACCESS_L) {
+            //high half of def is still valid
+            currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType = REGACCESS_H;
+#ifdef DEBUG_REACHING_DEF
+            ALOGI("DEBUG: set accessType of def to H");
+#endif
+            k++;
+        }
+        else {
+            k++;
+        }
+    }//for k
+    if(isBPartiallyOverlapA != OVERLAP_NO) {
+        //insert the def to variable @ currentInfo
+        k = currentBB->infoBasicBlock[indexToA].num_reaching_defs;
+        if(k >= 3) {
+            ALOGI("JIT_INFO: more than 3 reaching defs at updateReachingDefA");
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return -1;
+        }
+        currentBB->infoBasicBlock[indexToA].reachingDefs[k].offsetPC = offsetPC;
+        currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum = currentInfo.regNum;
+        currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType = currentInfo.physicalType;
+        currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType = REGACCESS_D;
+        currentBB->infoBasicBlock[indexToA].num_reaching_defs++;
+    }
+#ifdef DEBUG_REACHING_DEF2
+    ALOGI("IN updateReachingDefA for VR %d %d", currentBB->infoBasicBlock[indexToA].regNum,
+          currentBB->infoBasicBlock[indexToA].physicalType);
+    for(k = 0; k < currentBB->infoBasicBlock[indexToA].num_reaching_defs; k++)
+        ALOGI("Reaching def %d @ %d for VR %d %d access %d", k,
+              currentBB->infoBasicBlock[indexToA].reachingDefs[k].offsetPC,
+              currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum,
+              currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType,
+              currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType);
+#endif
+    return 0;
+}
+
+/**
+ * @brief updateReachingDefB1
+ * @details Given a variable B in currentInfo, updates its reaching defs
+ * by checking reaching defs of variable A in currentBB->infoBasicBlock[indexToA]
+ * The result is stored in tmpInfo.reachingDefs
+ * @param currentInfo the current considered VirtualRegInfo
+ * @param tmpInfo the temporary information
+ * @param indexToA Index of variable A
+ * @return -1 if error, 0 otherwise
+ */
+static int updateReachingDefB1 (VirtualRegInfo &currentInfo, VirtualRegInfo &tmpInfo, int indexToA)
+{
+    if(indexToA < 0) return 0;
+    int k;
+    tmpInfo.num_reaching_defs = 0;
+    for(k = 0; k < currentBB->infoBasicBlock[indexToA].num_reaching_defs; k++) {
+        /* go through reachingDefs of variable A @currentBB->infoBasicBlock[indexToA]
+           for each def, check whether it overlaps with variable B @currentInfo
+               if the def overlaps with variable B, insert it to tmpInfo.reachingDefs
+        */
+        OverlapCase isDefPartiallyOverlapB = getAPartiallyOverlapB(
+                                                 currentBB->infoBasicBlock[indexToA].reachingDefs[k].regNum,
+                                                 currentBB->infoBasicBlock[indexToA].reachingDefs[k].physicalType,
+                                                 currentInfo.regNum, currentInfo.physicalType
+                                                 );
+        bool insert1 = false; //whether to insert the def to tmpInfo.reachingDefs
+        if(isDefPartiallyOverlapB == OVERLAP_ALIGN ||
+           isDefPartiallyOverlapB == OVERLAP_A_IS_LOW_OF_B ||
+           isDefPartiallyOverlapB == OVERLAP_A_IS_HIGH_OF_B) {
+            /* B aligns with def */
+            /* def is low half of B, def is high half of B
+               in these two cases, def is 32 bits */
+            insert1 = true;
+        }
+        RegAccessType deftype = currentBB->infoBasicBlock[indexToA].reachingDefs[k].accessType;
+        if(isDefPartiallyOverlapB == OVERLAP_B_IS_LOW_OF_A ||
+           isDefPartiallyOverlapB == OVERLAP_LOW_OF_A_IS_HIGH_OF_B) {
+            /* B is the low half of def */
+            /* the low half of def is the high half of B */
+            if(deftype != REGACCESS_H) insert1 = true;
+        }
+        if(isDefPartiallyOverlapB == OVERLAP_B_IS_HIGH_OF_A ||
+           isDefPartiallyOverlapB == OVERLAP_HIGH_OF_A_IS_LOW_OF_B) {
+            /* B is the high half of def */
+            /* the high half of def is the low half of B */
+            if(deftype != REGACCESS_L) insert1 = true;
+        }
+        if(insert1) {
+            if(tmpInfo.num_reaching_defs >= 3) {
+                ALOGI("JIT_INFO: more than 3 reaching defs for tmpInfo at updateReachingDefB1");
+                SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                return -1;
+            }
+            tmpInfo.reachingDefs[tmpInfo.num_reaching_defs] = currentBB->infoBasicBlock[indexToA].reachingDefs[k];
+            tmpInfo.num_reaching_defs++;
+#ifdef DEBUG_REACHING_DEF2
+            ALOGI("Insert from entry %d %d: index %d", currentBB->infoBasicBlock[indexToA].regNum,
+                  currentBB->infoBasicBlock[indexToA].physicalType, k);
+#endif
+        }
+    }
+    return 0;
+}
+
+//! \brief updateReachingDefB2
+//! \details update currentInfo.reachingDefs by merging
+//! currentInfo.reachingDefs with tmpInfo.reachingDefs
+//! \return -1 if error, 0 otherwise
+static int updateReachingDefB2 (VirtualRegInfo &currentInfo, VirtualRegInfo &tmpInfo)
+{
+    int k, k2;
+    for(k2 = 0; k2 < tmpInfo.num_reaching_defs; k2++ ) {
+        bool merged = false;
+        for(k = 0; k < currentInfo.num_reaching_defs; k++) {
+            /* check whether it is the same def, if yes, do nothing */
+            if(currentInfo.reachingDefs[k].regNum == tmpInfo.reachingDefs[k2].regNum &&
+               currentInfo.reachingDefs[k].physicalType == tmpInfo.reachingDefs[k2].physicalType) {
+                merged = true;
+                if(currentInfo.reachingDefs[k].offsetPC != tmpInfo.reachingDefs[k2].offsetPC) {
+                    ALOGI("JIT_INFO: defs on the same VR %d %d with different offsetPC %d vs %d",
+                          currentInfo.reachingDefs[k].regNum, currentInfo.reachingDefs[k].physicalType,
+                          currentInfo.reachingDefs[k].offsetPC, tmpInfo.reachingDefs[k2].offsetPC);
+                    SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                    return -1;
+                }
+                if(currentInfo.reachingDefs[k].accessType != tmpInfo.reachingDefs[k2].accessType) {
+                    ALOGI("JIT_INFO: defs on the same VR %d %d with different accessType\n",
+                          currentInfo.reachingDefs[k].regNum, currentInfo.reachingDefs[k].physicalType);
+                    SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                    return -1;
+                }
+                break;
+            }
+        }
+        if(!merged) {
+            if(currentInfo.num_reaching_defs >= 3) {
+                ALOGI("JIT_INFO: more than 3 reaching defs for currentInfo at updateReachingDefB2\n");
+                SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                return -1;
+            }
+            currentInfo.reachingDefs[currentInfo.num_reaching_defs] = tmpInfo.reachingDefs[k2];
+            currentInfo.num_reaching_defs++;
+        }
+    }
+    return 0;
+}
+
+//!update currentInfo.reachingDefs with currentInfo if variable is defined in currentInfo
+
+//!
+void updateReachingDefB3 (VirtualRegInfo &currentInfo)
+{
+    if(currentInfo.accessType == REGACCESS_U) {
+        return; //no need to update currentInfo.reachingDefs
+    }
+    currentInfo.num_reaching_defs = 1;
+    currentInfo.reachingDefs[0].regNum = currentInfo.regNum;
+    currentInfo.reachingDefs[0].physicalType = currentInfo.physicalType;
+    currentInfo.reachingDefs[0].offsetPC = offsetPC;
+    currentInfo.reachingDefs[0].accessType = REGACCESS_D;
+}
+
+/** update defUseTable by checking currentInfo
+*/
+void updateDefUseTable (VirtualRegInfo &currentInfo)
+{
+    /* no access */
+    if(currentInfo.accessType == REGACCESS_N) return;
+    /* define then use, or define only */
+    if(currentInfo.accessType == REGACCESS_DU || currentInfo.accessType == REGACCESS_D) {
+        /* insert a definition at offsetPC to variable @ currentInfo */
+        DefUsePair* ptr = insertADef(currentBB, offsetPC, currentInfo.regNum, currentInfo.physicalType, REGACCESS_D);
+        if(currentInfo.accessType != REGACCESS_D) {
+             /* if access is define then use, insert a use at offsetPC */
+            insertAUse(ptr, offsetPC, currentInfo.regNum, currentInfo.physicalType);
+        }
+        return;
+    }
+    /* use only or use then define
+       check the reaching defs for the usage */
+    int k;
+    bool isLCovered = false, isHCovered = false, isDCovered = false;
+    for(k = 0; k < currentInfo.num_reaching_defs; k++) {
+        /* insert a def currentInfo.reachingDefs[k] and a use of variable at offsetPC */
+        RegAccessType useType = insertDefUsePair (currentInfo, k);
+        if(useType == REGACCESS_D) isDCovered = true;
+        if(useType == REGACCESS_L) isLCovered = true;
+        if(useType == REGACCESS_H) isHCovered = true;
+    }
+    OpndSize useSize = getRegSize(currentInfo.physicalType);
+    if((!isDCovered) && (!isLCovered)) {
+        /* the low half of variable is not defined in the basic block
+           so insert a def to the low half at START of the basic block */
+        insertDefUsePair(currentInfo, -1);
+    }
+    if(useSize == OpndSize_64 && (!isDCovered) && (!isHCovered)) {
+        /* the high half of variable is not defined in the basic block
+           so insert a def to the high half at START of the basic block */
+        insertDefUsePair(currentInfo, -2);
+    }
+    if(currentInfo.accessType == REGACCESS_UD) {
+        /* insert a def at offsetPC to variable @ currentInfo */
+        insertADef(currentBB, offsetPC, currentInfo.regNum, currentInfo.physicalType, REGACCESS_D);
+        return;
+    }
+}
+
+//! \brief insertAUse
+//! \details Insert a use at offsetPC of given variable at end of DefUsePair
+//! \param ptr The DefUsePair
+//! \param offsetPC
+//! \param regNum
+//! \param physicalType
+//! \return useType
+RegAccessType insertAUse(DefUsePair* ptr, int offsetPC, int regNum, LowOpndRegType physicalType) {
+    DefOrUseLink* tLink = (DefOrUseLink*)malloc(sizeof(DefOrUseLink));
+    if(tLink == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertAUse");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return REGACCESS_UNKNOWN;
+    }
+    tLink->offsetPC = offsetPC;
+    tLink->regNum = regNum;
+    tLink->physicalType = physicalType;
+    tLink->next = NULL;
+    if(ptr->useTail != NULL)
+        ptr->useTail->next = tLink;
+    ptr->useTail = tLink;
+    if(ptr->uses == NULL)
+        ptr->uses = tLink;
+    ptr->num_uses++;
+
+    //check whether the def is partially overlapping with the variable
+    OverlapCase isDefPartiallyOverlapB = getBPartiallyOverlapA(ptr->def.regNum,
+                                                       ptr->def.physicalType,
+                                                       regNum, physicalType);
+    RegAccessType useType = setAccessTypeOfUse(isDefPartiallyOverlapB, ptr->def.accessType);
+    tLink->accessType = useType;
+    return useType;
+}
+
+/**
+ * @brief Insert a definition
+ * @details insert a def to currentBB->defUseTable, update currentBB->defUseTail if necessary
+ * @param bb the BasicBlock_O1
+ * @param offsetPC the PC offset
+ * @param regNum the register number
+ * @param pType Physical type
+ * @param rType Register access type
+ * @return the new inserted DefUsePair
+ */
+DefUsePair* insertADef (BasicBlock_O1 *bb, int offsetPC, int regNum, LowOpndRegType pType, RegAccessType rType)
+{
+    DefUsePair* ptr = (DefUsePair*)malloc(sizeof(DefUsePair));
+    if(ptr == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertADef");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return NULL;
+    }
+
+    //First initialize the information we keep for defuse
+    ptr->next = NULL;
+    ptr->def.offsetPC = offsetPC;
+    ptr->def.regNum = regNum;
+    ptr->def.physicalType = pType;
+    ptr->def.accessType = rType;
+    ptr->num_uses = 0;
+    ptr->useTail = NULL;
+    ptr->uses = NULL;
+
+    //Now add this to the end of our defUse chain
+    if (bb->defUseTail != NULL)
+    {
+        bb->defUseTail->next = ptr;
+    }
+
+    bb->defUseTail = ptr;
+
+    //If this is the first entry, we must make the start point to it
+    if (bb->defUseTable == NULL)
+    {
+        bb->defUseTable = ptr;
+    }
+
+#ifdef DEBUG_REACHING_DEF
+    ALOGI("Insert a def at %d to defUseTable for VR %d %d", offsetPC,
+          regNum, pType);
+#endif
+    return ptr;
+}
+
+/** insert a def to defUseTable, then insert a use of variable @ currentInfo
+    if reachingDefIndex >= 0, the def is currentInfo.reachingDefs[index]
+    if reachingDefIndex is -1, the low half is defined at START of the basic block
+    if reachingDefIndex is -2, the high half is defined at START of the basic block
+*/
+RegAccessType insertDefUsePair (VirtualRegInfo &currentInfo, int reachingDefIndex)
+{
+    int k = reachingDefIndex;
+    DefUsePair* tableIndex = NULL;
+    DefOrUse theDef;
+    theDef.regNum = 0;
+    if(k < 0) {
+        /* def at start of the basic blcok */
+        theDef.offsetPC = PC_FOR_START_OF_BB;
+        theDef.accessType = REGACCESS_D;
+        if(k == -1) //low half of variable
+            theDef.regNum = currentInfo.regNum;
+        if(k == -2) //high half of variable
+            theDef.regNum = currentInfo.regNum+1;
+        theDef.physicalType = LowOpndRegType_gp;
+    }
+    else {
+        theDef = currentInfo.reachingDefs[k];
+    }
+    tableIndex = searchDefUseTable(theDef.offsetPC, theDef.regNum, theDef.physicalType);
+    if(tableIndex == NULL) //insert an entry
+        tableIndex = insertADef(currentBB, theDef.offsetPC, theDef.regNum, theDef.physicalType, theDef.accessType);
+    else
+        tableIndex->def.accessType = theDef.accessType;
+    RegAccessType useType = insertAUse(tableIndex, offsetPC, currentInfo.regNum, currentInfo.physicalType);
+    return useType;
+}
+
+/* @brief insert a XFER_MEM_TO_XMM to currentBB->xferPoints
+ *
+ * @params offset offsetPC of the transfer location
+ * @params regNum Register number
+ * @params pType Physical type of the reg
+ *
+ * @return -1 if error occurred, 0 otherwise
+ */
+static int insertLoadXfer(int offset, int regNum, LowOpndRegType pType) {
+    //check whether it is already in currentBB->xferPoints
+    unsigned int k, max;
+    max = currentBB->xferPoints.size ();
+    for(k = 0; k < max; k++) {
+        if(currentBB->xferPoints[k].xtype == XFER_MEM_TO_XMM &&
+           currentBB->xferPoints[k].offsetPC == offset &&
+           currentBB->xferPoints[k].regNum == regNum &&
+           currentBB->xferPoints[k].physicalType == pType)
+            return 0;
+    }
+
+    //We are going to create a new one
+    XferPoint point;
+    point.xtype = XFER_MEM_TO_XMM;
+    point.regNum = regNum;
+    point.offsetPC = offset;
+    point.physicalType = pType;
+#ifdef DEBUG_XFER_POINTS
+    ALOGI("Insert to xferPoints %d: XFER_MEM_TO_XMM of VR %d %d at %d", max, regNum, pType, offset);
+#endif
+
+    //Insert new point
+    currentBB->xferPoints.push_back (point);
+
+    //Paranoid
+    if(max + 1 >= MAX_XFER_PER_BB) {
+        ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", max + 1, MAX_XFER_PER_BB);
+        SET_JIT_ERROR(kJitErrorMaxXferPoints);
+        return -1;
+    }
+    return 0;
+}
+
+/** update defUseTable by assuming a fake usage at END of a basic block for variable @ currentInfo
+    create a fake usage at end of a basic block for variable B (currentInfo.physicalType, currentInfo.regNum)
+    get reaching def info for variable B and store the info in currentInfo.reachingDefs
+        for each virtual register (variable A) accessed in the basic block
+            update reaching defs of B by checking reaching defs of variable A
+    update defUseTable
+*/
+int fakeUsageAtEndOfBB (BasicBlock_O1* bb, int vR, int physicalAndLogicalType)
+{
+    VirtualRegInfo currentInfo;
+
+    //Get the register number
+    currentInfo.regNum = vR;
+
+    //TODO The cast here is invalid because it creates an invalid LowOpndRegType.
+    //However, this invalidity seems to be required for properly creating usedef chains.
+    currentInfo.physicalType = static_cast<LowOpndRegType> (physicalAndLogicalType);
+
+    currentInfo.accessType = REGACCESS_U;
+    LowOpndRegType typeB = currentInfo.physicalType;
+    int regB = currentInfo.regNum;
+    unsigned int jj;
+    int k;
+    currentInfo.num_reaching_defs = 0;
+    unsigned int max = bb->infoBasicBlock.size ();
+    for(jj = 0; jj < max; jj++) {
+        int regA = bb->infoBasicBlock[jj].regNum;
+        LowOpndRegType typeA = bb->infoBasicBlock[jj].physicalType;
+        OverlapCase isBPartiallyOverlapA = getBPartiallyOverlapA(regB, typeB, regA, typeA);
+        if(regA == regB && typeA == typeB) {
+            /* copy reachingDefs from variable A */
+            currentInfo.num_reaching_defs = bb->infoBasicBlock[jj].num_reaching_defs;
+            for(k = 0; k < currentInfo.num_reaching_defs; k++)
+                currentInfo.reachingDefs[k] = bb->infoBasicBlock[jj].reachingDefs[k];
+            break;
+        }
+        else if(isBPartiallyOverlapA != OVERLAP_NO) {
+            VirtualRegInfo tmpInfo;
+
+            /* B overlaps with A */
+            /* update reaching defs of variable B by checking reaching defs of bb->infoBasicBlock[jj] */
+            int retCode = updateReachingDefB1 (currentInfo, tmpInfo, jj);
+            if (retCode < 0)
+                return retCode;
+            retCode = updateReachingDefB2 (currentInfo, tmpInfo); //merge currentInfo with tmpInfo
+            if (retCode < 0)
+                return retCode;
+        }
+    }
+    /* update defUseTable by checking currentInfo */
+    updateDefUseTable (currentInfo);
+    return 0;
+}
+
+/**
+ * @brief Update xferPoints of currentBB
+ * @return -1 on error, 0 otherwise
+*/
+int updateXferPoints (BasicBlock_O1 *bb)
+{
+    //First clear the XferPoints
+    bb->xferPoints.clear ();
+
+    //Get a local version of defUseTable
+    DefUsePair* ptr = bb->defUseTable;
+
+    /* Traverse the def-use chain of the basic block */
+    while(ptr != NULL) {
+        LowOpndRegType defType = ptr->def.physicalType;
+        //if definition is for a variable of 32 bits
+        if(getRegSize(defType) == OpndSize_32) {
+            /* check usages of the definition, whether it reaches a GPR, a XMM, a FS, or a SS */
+            bool hasGpUsage = false;
+            bool hasGpUsage2 = false; //not a fake usage
+            bool hasXmmUsage = false;
+            bool hasFSUsage = false;
+            bool hasSSUsage = false;
+
+            //Get the uses
+            DefOrUseLink* ptrUse = ptr->uses;
+            while(ptrUse != NULL) {
+                if(ptrUse->physicalType == LowOpndRegType_gp) {
+                    hasGpUsage = true;
+                    if(ptrUse->offsetPC != PC_FOR_END_OF_BB)
+                        hasGpUsage2 = true;
+                }
+                if(ptrUse->physicalType == LowOpndRegType_ss) hasSSUsage = true;
+                if(ptrUse->physicalType == LowOpndRegType_fs ||
+                   ptrUse->physicalType == LowOpndRegType_fs_s)
+                    hasFSUsage = true;
+                if(ptrUse->physicalType == LowOpndRegType_xmm) {
+                    hasXmmUsage = true;
+                }
+                if(ptrUse->physicalType == LowOpndRegType_xmm ||
+                   ptrUse->physicalType == LowOpndRegType_ss) {
+                    /* if a 32-bit definition reaches a xmm usage or a SS usage,
+                       insert a XFER_MEM_TO_XMM */
+                    int retCode = insertLoadXfer(ptrUse->offsetPC,
+                                   ptrUse->regNum, LowOpndRegType_xmm);
+                    if (retCode < 0)
+                        return retCode;
+                }
+                ptrUse = ptrUse->next;
+            }
+            if(((hasXmmUsage || hasFSUsage || hasSSUsage) && defType == LowOpndRegType_gp) ||
+               (hasGpUsage && defType == LowOpndRegType_fs) ||
+               (defType == LowOpndRegType_ss && (hasGpUsage || hasXmmUsage || hasFSUsage))) {
+                /* insert a transfer if def is on a GPR, usage is on a XMM, FS or SS
+                                     if def is on a FS, usage is on a GPR
+                                     if def is on a SS, usage is on a GPR, XMM or FS
+                   transfer type is XFER_DEF_TO_GP_MEM if a real GPR usage exisits
+                   transfer type is XFER_DEF_TO_GP otherwise*/
+                XferPoint point;
+                point.offsetPC = ptr->def.offsetPC;
+                point.regNum = ptr->def.regNum;
+                point.physicalType = ptr->def.physicalType;
+                if(hasGpUsage2) { //create an entry XFER_DEF_TO_GP_MEM
+                    point.xtype = XFER_DEF_TO_GP_MEM;
+                }
+                else { //create an entry XFER_DEF_TO_MEM
+                    point.xtype = XFER_DEF_TO_MEM;
+                }
+                point.tableIndex = 0;
+#ifdef DEBUG_XFER_POINTS
+                ALOGI("Insert XFER %d at def %d: V%d %d", bb->xferPoints.size (), ptr->def.offsetPC, ptr->def.regNum, defType);
+#endif
+
+                //Push new point
+                bb->xferPoints.push_back (point);
+
+                if(bb->xferPoints.size () >= MAX_XFER_PER_BB) {
+                    ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", bb->xferPoints.size (), MAX_XFER_PER_BB);
+                    SET_JIT_ERROR(kJitErrorMaxXferPoints);
+                    return -1;
+                }
+            }
+        }
+        else { /* def is on 64 bits */
+            bool hasGpUsageOfL = false; //exist a GPR usage of the low half
+            bool hasGpUsageOfH = false; //exist a GPR usage of the high half
+            bool hasGpUsageOfL2 = false;
+            bool hasGpUsageOfH2 = false;
+            bool hasMisaligned = false;
+            bool hasAligned = false;
+            bool hasFSUsage = false;
+            bool hasSSUsage = false;
+
+            //Get the uses
+            DefOrUseLink* ptrUse = ptr->uses;
+            while(ptrUse != NULL) {
+                if(ptrUse->physicalType == LowOpndRegType_gp &&
+                   ptrUse->regNum == ptr->def.regNum) {
+                    hasGpUsageOfL = true;
+                    if(ptrUse->offsetPC != PC_FOR_END_OF_BB)
+                        hasGpUsageOfL2 = true;
+                }
+                if(ptrUse->physicalType == LowOpndRegType_gp &&
+                   ptrUse->regNum == ptr->def.regNum + 1) {
+                    hasGpUsageOfH = true;
+                    if(ptrUse->offsetPC != PC_FOR_END_OF_BB)
+                        hasGpUsageOfH2 = true;
+                }
+                if(ptrUse->physicalType == LowOpndRegType_xmm &&
+                   ptrUse->regNum == ptr->def.regNum) {
+                    hasAligned = true;
+                    /* if def is on FS and use is on XMM, insert a XFER_MEM_TO_XMM */
+                    if(defType == LowOpndRegType_fs) {
+                        int retCode = insertLoadXfer(ptrUse->offsetPC,
+                                       ptrUse->regNum, LowOpndRegType_xmm);
+                        if (retCode < 0)
+                            return retCode;
+                    }
+                }
+                if(ptrUse->physicalType == LowOpndRegType_fs ||
+                   ptrUse->physicalType == LowOpndRegType_fs_s)
+                    hasFSUsage = true;
+                if(ptrUse->physicalType == LowOpndRegType_xmm &&
+                   ptrUse->regNum != ptr->def.regNum) {
+                    hasMisaligned = true;
+                    /* if use is on XMM and use and def are misaligned, insert a XFER_MEM_TO_XMM */
+                    int retCode = insertLoadXfer(ptrUse->offsetPC,
+                                   ptrUse->regNum, LowOpndRegType_xmm);
+                    if (retCode < 0)
+                        return retCode;
+                }
+                if(ptrUse->physicalType == LowOpndRegType_ss) {
+                    hasSSUsage = true;
+                    /* if use is on SS, insert a XFER_MEM_TO_XMM */
+                    int retCode = insertLoadXfer(ptrUse->offsetPC,
+                                   ptrUse->regNum, LowOpndRegType_ss);
+                    if (retCode < 0)
+                        return retCode;
+                }
+                ptrUse = ptrUse->next;
+            }
+            if(defType == LowOpndRegType_fs && !hasGpUsageOfL && !hasGpUsageOfH) {
+                ptr = ptr->next;
+                continue;
+            }
+            if(defType == LowOpndRegType_xmm && !hasFSUsage &&
+               !hasGpUsageOfL && !hasGpUsageOfH && !hasMisaligned && !hasSSUsage) {
+                ptr = ptr->next;
+                continue;
+            }
+            /* insert a XFER_DEF_IS_XMM */
+            XferPoint point;
+
+            point.regNum = ptr->def.regNum;
+            point.offsetPC = ptr->def.offsetPC;
+            point.physicalType = ptr->def.physicalType;
+            point.xtype = XFER_DEF_IS_XMM;
+            point.vr_gpl = -1;
+            point.vr_gph = -1;
+            if(hasGpUsageOfL2) {
+                point.vr_gpl = ptr->def.regNum;
+            }
+            if(hasGpUsageOfH2) {
+                point.vr_gph = ptr->def.regNum+1;
+            }
+            point.dumpToMem = true;
+            point.dumpToXmm = false; //not used in updateVirtualReg
+            if(hasAligned) {
+                point.dumpToXmm = true;
+            }
+            point.tableIndex = 0;
+#ifdef DEBUG_XFER_POINTS
+            ALOGI("Insert XFER %d at def %d: V%d %d", bb->xferPoints.size (), ptr->def.offsetPC, ptr->def.regNum, defType);
+#endif
+            //Push new point
+            bb->xferPoints.push_back (point);
+
+            if(bb->xferPoints.size () >= MAX_XFER_PER_BB) {
+                ALOGI("JIT_INFO: Number of transfer points (%d) exceed maximum (%d)", bb->xferPoints.size (), MAX_XFER_PER_BB);
+                SET_JIT_ERROR(kJitErrorMaxXferPoints);
+                return -1;
+            }
+        }
+
+        //Get next pointer
+        ptr = ptr->next;
+    } //while ptr
+
+#ifdef DEBUG_XFER_POINTS
+    ALOGI("XFER points for current basic block ------");
+    unsigned int max = bb->xferPoints.size ();
+    for(unsigned int k = 0; k < max; k++) {
+        ALOGI("  at offset %x, VR %d %d: type %d, vr_gpl %d, vr_gph %d, dumpToMem %d, dumpToXmm %d",
+              bb->xferPoints[k].offsetPC, bb->xferPoints[k].regNum,
+              bb->xferPoints[k].physicalType, bb->xferPoints[k].xtype,
+              bb->xferPoints[k].vr_gpl, bb->xferPoints[k].vr_gph,
+              bb->xferPoints[k].dumpToMem, bb->xferPoints[k].dumpToXmm);
+    }
+#endif
+
+    //Report success
+    return 0;
+}
+
+/**
+ * @brief Used to handle type transfer points at the start of the BB
+ * @param bb The current basic block
+ */
+void handleStartOfBBXferPoints (BasicBlock_O1 *bb)
+{
+    //Walk through the BB's transfer points
+    for (std::vector<XferPoint>::const_iterator xferIter = bb->xferPoints.begin (); xferIter != bb->xferPoints.end ();
+            xferIter++)
+    {
+        const XferPoint &transfer = *xferIter;
+
+        //If we have a transfer of VR defined at start of BB and the transfer involves storing back to memory
+        //then we handle it right now.
+        if (transfer.offsetPC == PC_FOR_START_OF_BB
+                && (transfer.xtype == XFER_DEF_TO_MEM
+                        || transfer.xtype == XFER_DEF_TO_GP_MEM
+                        || transfer.xtype == XFER_DEF_IS_XMM))
+        {
+            int vR = transfer.regNum;
+
+            //Look through compile table to find the physical register for this VR
+            for (CompileTable::iterator compileIter = compileTable.begin (); compileIter != compileTable.end ();
+                    compileIter++)
+            {
+                CompileTableEntry &compileEntry = *compileIter;
+
+                //We found what we're looking for when we find the VR we care about in a physical register
+                if (compileEntry.isVirtualReg () == true
+                        && compileEntry.getRegisterNumber () == vR
+                        && compileEntry.inPhysicalRegister( ) == true)
+                {
+                    //Write back the VR to memory
+                    writeBackVR (vR, compileEntry.getPhysicalType (), compileEntry.getPhysicalReg ());
+                }
+            }
+        }
+    }
+}
+
+/* @brief update memVRTable[].ranges by browsing the defUseTable
+ *
+ * @details each virtual register has a list of live ranges, and
+ * each live range has a list of PCs that access the VR
+ *
+ * @return -1 if error happened, 0 otherwise
+ */
+int updateLiveTable (BasicBlock_O1 *bb)
+{
+    int retCode = 0;
+    DefUsePair* ptr = bb->defUseTable;
+    while(ptr != NULL) {
+        bool updateUse = false;
+        if(ptr->num_uses == 0) {
+            ptr->num_uses = 1;
+            ptr->uses = (DefOrUseLink*)malloc(sizeof(DefOrUseLink));
+            if(ptr->uses == NULL) {
+                ALOGI("JIT_INFO: Memory allocation failed in updateLiveTable");
+                SET_JIT_ERROR(kJitErrorMallocFailed);
+                return -1;
+            }
+            ptr->uses->accessType = REGACCESS_D;
+            ptr->uses->regNum = ptr->def.regNum;
+            ptr->uses->offsetPC = ptr->def.offsetPC;
+            ptr->uses->physicalType = ptr->def.physicalType;
+            ptr->uses->next = NULL;
+            ptr->useTail = ptr->uses;
+            updateUse = true;
+        }
+        DefOrUseLink* ptrUse = ptr->uses;
+        while(ptrUse != NULL) {
+            RegAccessType useType = ptrUse->accessType;
+            if(useType == REGACCESS_L || useType == REGACCESS_D) {
+                int indexL = searchMemTable(ptrUse->regNum);
+                if(indexL >= 0) {
+                    retCode = mergeLiveRange(indexL, ptr->def.offsetPC,
+                                   ptrUse->offsetPC); //tableIndex, start PC, end PC
+                    if (retCode < 0)
+                        return retCode;
+                }
+            }
+            if(getRegSize(ptrUse->physicalType) == OpndSize_64 &&
+               (useType == REGACCESS_H || useType == REGACCESS_D)) {
+                int indexH = searchMemTable(ptrUse->regNum+1);
+                if(indexH >= 0) {
+                    retCode = mergeLiveRange(indexH, ptr->def.offsetPC,
+                                   ptrUse->offsetPC);
+                    if (retCode < 0)
+                        return retCode;
+                }
+            }
+            ptrUse = ptrUse->next;
+        }//while ptrUse
+        if(updateUse) {
+            ptr->num_uses = 0;
+            free(ptr->uses);
+            ptr->uses = NULL;
+            ptr->useTail = NULL;
+        }
+        ptr = ptr->next;
+    }//while ptr
+#ifdef DEBUG_LIVE_RANGE
+    ALOGI("LIVE TABLE");
+    for(int k = 0; k < num_memory_vr; k++) {
+        ALOGI("VR %d live ", memVRTable[k].regNum);
+        LiveRange* ptr = memVRTable[k].ranges;
+        while(ptr != NULL) {
+            ALOGI("[%x %x] (", ptr->start, ptr->end);
+            for(int k3 = 0; k3 < ptr->num_access; k3++)
+                ALOGI("%x ", ptr->accessPC[k3]);
+            ALOGI(") ");
+            ptr = ptr->next;
+        }
+        ALOGI("");
+    }
+#endif
+    return 0;
+}
+
+/* @brief Add a live range [rangeStart, rangeEnd] to ranges of memVRTable,
+ * merge to existing live ranges if necessary
+ *
+ * @details ranges are in increasing order of startPC
+ *
+ * @param tableIndex index into memVRTable
+ * @param rangeStart start of live range
+ * @param rangeEnd end of live range
+ *
+ * @return -1 if error, 0 otherwise
+ */
+static int mergeLiveRange(int tableIndex, int rangeStart, int rangeEnd) {
+    if(rangeStart == PC_FOR_START_OF_BB) rangeStart = currentBB->pc_start;
+    if(rangeEnd == PC_FOR_END_OF_BB) rangeEnd = currentBB->pc_end;
+#ifdef DEBUG_LIVE_RANGE
+    ALOGI("LIVERANGE call mergeLiveRange on tableIndex %d with [%x %x]", tableIndex, rangeStart, rangeEnd);
+#endif
+    int startIndex = -1, endIndex = -1;
+    bool startBeforeRange = false, endBeforeRange = false; //before the index or in the range
+    bool startDone = false, endDone = false;
+    LiveRange* ptr = memVRTable[tableIndex].ranges;
+    LiveRange* ptrStart = NULL;
+    LiveRange* ptrStart_prev = NULL;
+    LiveRange* ptrEnd = NULL;
+    LiveRange* ptrEnd_prev = NULL;
+    int k = 0;
+    while(ptr != NULL) {
+        if(!startDone) {
+            if(ptr->start <= rangeStart &&
+               ptr->end >= rangeStart) {
+                startIndex = k;
+                ptrStart = ptr;
+                startBeforeRange = false;
+                startDone = true;
+            }
+            else if(ptr->start > rangeStart) {
+                startIndex = k;
+                ptrStart = ptr;
+                startBeforeRange = true;
+                startDone = true;
+            }
+        }
+        if(!startDone) ptrStart_prev = ptr;
+        if(!endDone) {
+            if(ptr->start <= rangeEnd &&
+               ptr->end >= rangeEnd) {
+                endIndex = k;
+                ptrEnd = ptr;
+                endBeforeRange = false;
+                endDone = true;
+            }
+            else if(ptr->start > rangeEnd) {
+                endIndex = k;
+                ptrEnd = ptr;
+                endBeforeRange = true;
+                endDone = true;
+            }
+        }
+        if(!endDone) ptrEnd_prev = ptr;
+        ptr = ptr->next;
+        k++;
+    } //while
+    if(!startDone) { //both can be NULL
+        startIndex = memVRTable[tableIndex].num_ranges;
+        ptrStart = NULL; //ptrStart_prev should be the last live range
+        startBeforeRange = true;
+    }
+    //if endDone, ptrEnd is not NULL, ptrEnd_prev can be NULL
+    if(!endDone) { //both can be NULL
+        endIndex = memVRTable[tableIndex].num_ranges;
+        ptrEnd = NULL;
+        endBeforeRange = true;
+    }
+    if(startIndex == endIndex && startBeforeRange && endBeforeRange) { //insert at startIndex
+        //3 cases depending on BeforeRange when startIndex == endIndex
+        //insert only if both true
+        //merge otherwise
+        /////////// insert before ptrStart
+        LiveRange* currRange = (LiveRange *)malloc(sizeof(LiveRange));
+        if(ptrStart_prev == NULL) {
+            currRange->next = memVRTable[tableIndex].ranges;
+            memVRTable[tableIndex].ranges = currRange;
+        } else {
+            currRange->next = ptrStart_prev->next;
+            ptrStart_prev->next = currRange;
+        }
+        currRange->start = rangeStart;
+        currRange->end = rangeEnd;
+        currRange->accessPC = (int *)malloc(sizeof(int) * NUM_ACCESS_IN_LIVERANGE);
+        currRange->num_alloc = NUM_ACCESS_IN_LIVERANGE;
+        if(rangeStart != rangeEnd) {
+            currRange->num_access = 2;
+            currRange->accessPC[0] = rangeStart;
+            currRange->accessPC[1] = rangeEnd;
+        } else {
+            currRange->num_access = 1;
+            currRange->accessPC[0] = rangeStart;
+        }
+        memVRTable[tableIndex].num_ranges++;
+#ifdef DEBUG_LIVE_RANGE
+        ALOGI("LIVERANGE insert one live range [%x %x] to tableIndex %d", rangeStart, rangeEnd, tableIndex);
+#endif
+        return 0;
+    }
+    if(!endBeforeRange) { //here ptrEnd is not NULL
+        endIndex++; //next
+        ptrEnd_prev = ptrEnd; //ptrEnd_prev is not NULL
+        ptrEnd = ptrEnd->next; //ptrEnd can be NULL
+    }
+
+    if(endIndex < startIndex+1) {
+        ALOGI("JIT_INFO: mergeLiveRange endIndex %d is less than startIndex %d\n", endIndex, startIndex);
+        SET_JIT_ERROR(kJitErrorMergeLiveRange);
+        return -1;
+    }
+    ///////// use ptrStart & ptrEnd_prev
+    if(ptrStart == NULL || ptrEnd_prev == NULL) {
+        ALOGI("JIT_INFO: mergeLiveRange ptr is NULL\n");
+        SET_JIT_ERROR(kJitErrorMergeLiveRange);
+        return -1;
+    }
+    //endIndex > startIndex (merge the ranges between startIndex and endIndex-1)
+    //update ptrStart
+    if(ptrStart->start > rangeStart)
+        ptrStart->start = rangeStart; //min of old start & rangeStart
+    ptrStart->end = ptrEnd_prev->end; //max of old end & rangeEnd
+    if(rangeEnd > ptrStart->end)
+        ptrStart->end = rangeEnd;
+#ifdef DEBUG_LIVE_RANGE
+    ALOGI("LIVERANGE merge entries for tableIndex %d from %d to %d", tableIndex, startIndex+1, endIndex-1);
+#endif
+    if(ptrStart->num_access <= 0) {
+        ALOGI("JIT_INFO: mergeLiveRange number of access");
+        SET_JIT_ERROR(kJitErrorMergeLiveRange);
+    }
+#ifdef DEBUG_LIVE_RANGE
+    ALOGI("LIVERANGE tableIndex %d startIndex %d num_access %d (", tableIndex, startIndex, ptrStart->num_access);
+    for(k = 0; k < ptrStart->num_access; k++)
+        ALOGI("%x ", ptrStart->accessPC[k]);
+    ALOGI(")");
+#endif
+    ///// go through pointers from ptrStart->next to ptrEnd
+    //from startIndex+1 to endIndex-1
+    ptr = ptrStart->next;
+    while(ptr != NULL && ptr != ptrEnd) {
+        int k2;
+        for(k2 = 0; k2 < ptr->num_access; k2++) { //merge to startIndex
+            insertAccess(tableIndex, ptrStart, ptr->accessPC[k2]);
+        }//k2
+        ptr = ptr->next;
+    }
+    insertAccess(tableIndex, ptrStart, rangeStart);
+    insertAccess(tableIndex, ptrStart, rangeEnd);
+    //remove startIndex+1 to endIndex-1
+    if(startIndex+1 < endIndex) {
+        ptr = ptrStart->next;
+        while(ptr != NULL && ptr != ptrEnd) {
+            LiveRange* tmpP = ptr->next;
+            free(ptr->accessPC);
+            free(ptr);
+            ptr = tmpP;
+        }
+        ptrStart->next = ptrEnd;
+    }
+    memVRTable[tableIndex].num_ranges -= (endIndex - startIndex - 1);
+#ifdef DEBUG_LIVE_RANGE
+    ALOGI("num_ranges for VR %d: %d", memVRTable[tableIndex].regNum, memVRTable[tableIndex].num_ranges);
+#endif
+    return 0;
+}
+
+//! insert an access to a given live range, in order
+
+//!
+void insertAccess(int tableIndex, LiveRange* startP, int rangeStart) {
+    int k3, k4;
+#ifdef DEBUG_LIVE_RANGE
+    ALOGI("LIVERANGE insertAccess %d %x", tableIndex, rangeStart);
+#endif
+    int insertIndex = -1;
+    for(k3 = 0; k3 < startP->num_access; k3++) {
+        if(startP->accessPC[k3] == rangeStart) {
+            return;
+        }
+        if(startP->accessPC[k3] > rangeStart) {
+            insertIndex = k3;
+            break;
+        }
+    }
+
+    //insert here
+    k3 = insertIndex;
+    if(insertIndex == -1) {
+        k3 = startP->num_access;
+    }
+    if(startP->num_access == startP->num_alloc) {
+        int currentAlloc = startP->num_alloc;
+        startP->num_alloc += NUM_ACCESS_IN_LIVERANGE;
+        int* tmpPtr = (int *)malloc(sizeof(int) * startP->num_alloc);
+        for(k4 = 0; k4 < currentAlloc; k4++)
+            tmpPtr[k4] = startP->accessPC[k4];
+        free(startP->accessPC);
+        startP->accessPC = tmpPtr;
+    }
+    //insert accessPC
+    for(k4 = startP->num_access-1; k4 >= k3; k4--)
+        startP->accessPC[k4+1] = startP->accessPC[k4];
+    startP->accessPC[k3] = rangeStart;
+#ifdef DEBUG_LIVE_RANGE
+    ALOGI("LIVERANGE insert %x to tableIndex %d", rangeStart, tableIndex);
+#endif
+    startP->num_access++;
+    return;
+}
+
+/////////////////////////////////////////////////////////////////////
+bool isVRLive(int vA);
+int getSpillIndex (OpndSize size);
+void clearVRToMemory(int regNum, OpndSize size);
+void clearVRNullCheck(int regNum, OpndSize size);
+
+inline int getSpillLocDisp(int offset) {
+#ifdef SPILL_IN_THREAD
+    return offset+offsetof(Thread, spillRegion);;
+#else
+    return offset+offEBP_spill;
+#endif
+}
+#if 0
+/* used if we keep self pointer in a physical register */
+inline int getSpillLocReg(int offset) {
+    return PhysicalReg_Glue;
+}
+#endif
+#ifdef SPILL_IN_THREAD
+inline void loadFromSpillRegion_with_self(OpndSize size, int reg_self, bool selfPhysical, int reg, int offset) {
+    /* only 1 instruction is generated by move_mem_to_reg_noalloc */
+    move_mem_to_reg_noalloc(size,
+                            getSpillLocDisp(offset), reg_self, selfPhysical,
+                            MemoryAccess_SPILL, offset,
+                            reg, true);
+}
+inline void loadFromSpillRegion(OpndSize size, int reg, int offset) {
+    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
+    int reg_self = registerAlloc(LowOpndRegType_scratch, C_SCRATCH_1, isScratchPhysical, false, true);
+    /* only 1 instruction is generated by move_mem_to_reg_noalloc */
+    move_mem_to_reg_noalloc(size,
+                            getSpillLocDisp(offset), reg_self, true,
+                            MemoryAccess_SPILL, offset,
+                            reg, true);
+}
+inline void saveToSpillRegion_with_self(OpndSize size, int selfReg, bool selfPhysical, int reg, int offset) {
+    move_reg_to_mem_noalloc(size,
+                            reg, true,
+                            getSpillLocDisp(offset), selfReg, selfPhysical,
+                            MemoryAccess_SPILL, offset);
+}
+inline void saveToSpillRegion(OpndSize size, int reg, int offset) {
+    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
+    int reg_self = registerAlloc(LowOpndRegType_scratch, C_SCRATCH_1, isScratchPhysical, false);
+    move_reg_to_mem_noalloc(size,
+                            reg, true,
+                            getSpillLocDisp(offset), reg_self, true,
+                            MemoryAccess_SPILL, offset);
+}
+#else
+inline void loadFromSpillRegion(OpndSize size, int reg, int offset) {
+    /* only 1 instruction is generated by move_mem_to_reg_noalloc */
+    move_mem_to_reg_noalloc(size,
+                            getSpillLocDisp(offset), PhysicalReg_EBP, true,
+                            MemoryAccess_SPILL, offset,
+                            reg, true);
+}
+inline void saveToSpillRegion(OpndSize size, int reg, int offset) {
+    move_reg_to_mem_noalloc(size,
+                            reg, true,
+                            getSpillLocDisp(offset), PhysicalReg_EBP, true,
+                            MemoryAccess_SPILL, offset);
+}
+#endif
+
+/**
+ * @brief If VR is dirty, it writes the constant value to the VR on stack
+ * @details If VR is wide, this function should be called separately twice,
+ * once with the low bits and once with the high bits.
+ * @param vR virtual register
+ * @param value constant value of the VR
+ */
+void writeBackConstVR(int vR, int value) {
+    // If VR is already in memory, we do not need to write it back
+    if(isInMemory(vR, OpndSize_32)) {
+        DEBUG_SPILL(ALOGD("Skip dumpImmToMem v%d size %d", vR, size));
+        return;
+    }
+
+    // Do the actual move to set constant to VR in stack
+    set_VR_to_imm_noalloc(vR, OpndSize_32, value);
+
+    // Mark the VR as in memory now
+    setVRMemoryState(vR, OpndSize_32, true);
+}
+
+/**
+ * @brief Writes back VR to memory if dirty.
+ * @param vR virtual register
+ * @param type physical type as noted by compile table
+ * @param physicalReg physical register
+ */
+void writeBackVR(int vR, LowOpndRegType type, int physicalReg) {
+    int physicalType = type & MASK_FOR_TYPE;
+
+    // Paranoid check because we only handle writing back if in either
+    // GP or XMM registers
+    assert((physicalReg >= PhysicalReg_StartOfGPMarker &&
+            physicalReg <= PhysicalReg_EndOfGPMarker) ||
+            (physicalReg >= PhysicalReg_StartOfXmmMarker &&
+                    physicalReg <= PhysicalReg_EndOfXmmMarker));
+
+    // If VR is already in memory, we can skip writing it back
+    if (isInMemory(vR, getRegSize(physicalType))) {
+        DEBUG_SPILL(ALOGD("Skip writeBackVR v%d type %d", vR, physicalType));
+        return;
+    }
+
+    // Handle writing back different types of VRs
+    if (physicalType == LowOpndRegType_gp || physicalType == LowOpndRegType_xmm)
+        set_virtual_reg_noalloc(vR, getRegSize(physicalType), physicalReg,
+                true);
+    if (physicalType == LowOpndRegType_ss)
+        move_ss_reg_to_mem_noalloc(physicalReg, true, 4 * vR, PhysicalReg_FP,
+                true, MemoryAccess_VR, vR);
+
+    // Mark it in memory because we have written it back
+    setVRMemoryState (vR, getRegSize (physicalType), true);
+}
+//! dump part of a 64-bit VR to memory and update inMemory
+
+//! isLow tells whether low half or high half is dumped
+void dumpPartToMem(int reg /*xmm physical reg*/, int vA, bool isLow) {
+    if(isLow) {
+        if(isInMemory(vA, OpndSize_32)) {
+            DEBUG_SPILL(ALOGD("Skip dumpPartToMem isLow %d v%d", isLow, vA));
+            return;
+        }
+    }
+    else {
+        if(isInMemory(vA+1, OpndSize_32)) {
+            DEBUG_SPILL(ALOGD("Skip dumpPartToMem isLow %d v%d", isLow, vA));
+            return;
+        }
+    }
+    if(isLow) {
+        if(!isVRLive(vA)) return;
+    }
+    else {
+        if(!isVRLive(vA+1)) return;
+    }
+    //move part to vA or vA+1
+    if(isLow) {
+        move_ss_reg_to_mem_noalloc(reg, true,
+                                   4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
+    } else {
+        int k = getSpillIndex (OpndSize_64);
+        //H, L in 4*k+4 & 4*k
+#ifdef SPILL_IN_THREAD
+        get_self_pointer(PhysicalReg_SCRATCH_1, isScratchPhysical);
+        saveToSpillRegion_with_self(OpndSize_64, PhysicalReg_SCRATCH_1, isScratchPhysical, reg, 4*k);
+        //update low 32 bits of xmm reg from 4*k+4
+        move_ss_mem_to_reg(NULL,
+                                   getSpillLocDisp(4*k+4), PhysicalReg_SCRATCH_1, isScratchPhysical,
+                                   reg, true);
+#else
+        // right shift high half of xmm to low half xmm
+        dump_imm_reg_noalloc(Mnemonic_PSRLQ, OpndSize_64, 32, reg, true, LowOpndRegType_xmm);
+#endif
+        //move low 32 bits of xmm reg to vA+1
+        move_ss_reg_to_mem_noalloc(reg, true, 4*(vA+1), PhysicalReg_FP, true, MemoryAccess_VR, vA+1);
+    }
+
+    if (isLow)
+    {
+        setVRMemoryState (vA, OpndSize_32, true);
+    }
+    else
+    {
+        setVRMemoryState (vA + 1, OpndSize_32, true);
+    }
+}
+void clearVRBoundCheck(int regNum, OpndSize size);
+//! the content of a VR is no longer in memory or in physical register if the latest content of a VR is constant
+
+//! clear nullCheckDone; if another VR is overlapped with the given VR, the content of that VR is no longer in physical register
+void invalidateVRDueToConst(int reg, OpndSize size) {
+    clearVRToMemory(reg, size); //memory content is out-dated
+    clearVRNullCheck(reg, size);
+    clearVRBoundCheck(reg, size);
+    //check reg,gp reg,ss reg,xmm reg-1,xmm
+    //if size is 64: check reg+1,gp|ss reg+1,xmm
+    int index;
+    //if VR is xmm, check whether we need to dump part of VR to memory
+    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg);
+    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+        ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_xmm);
+#endif
+        if(size == OpndSize_32)
+            dumpPartToMem(compileTable[index].physicalReg, reg, false); //dump high of xmm to memory
+
+        compileTable[index].setPhysicalReg (PhysicalReg_Null);
+    }
+    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg-1);
+    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+        ALOGI("INVALIDATE virtual reg %d type %d", reg-1, LowOpndRegType_xmm);
+#endif
+        dumpPartToMem(compileTable[index].physicalReg, reg-1, true); //dump low of xmm to memory
+
+        compileTable[index].setPhysicalReg (PhysicalReg_Null);
+    }
+    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg);
+    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+        ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_gp);
+#endif
+
+        compileTable[index].setPhysicalReg (PhysicalReg_Null);
+    }
+    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg);
+    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+        ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_ss);
+#endif
+
+        compileTable[index].setPhysicalReg (PhysicalReg_Null);
+    }
+    if(size == OpndSize_64) {
+        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg+1);
+        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_xmm);
+#endif
+            dumpPartToMem(compileTable[index].physicalReg, reg+1, false); //dump high of xmm to memory
+
+            compileTable[index].setPhysicalReg (PhysicalReg_Null);
+        }
+        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg+1);
+        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_gp);
+#endif
+
+            compileTable[index].setPhysicalReg (PhysicalReg_Null);
+        }
+        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg+1);
+        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_ss);
+#endif
+
+            compileTable[index].setPhysicalReg (PhysicalReg_Null);
+        }
+    }
+}
+//! check which physical registers hold out-dated content if there is a def
+
+//! if another VR is overlapped with the given VR, the content of that VR is no longer in physical register
+//! should we update inMemory?
+void invalidateVR(int reg, LowOpndRegType pType) {
+    //def at fs: content of xmm & gp & ss are out-dated (reg-1,xmm reg,xmm reg+1,xmm) (reg,gp|ss reg+1,gp|ss)
+    //def at xmm: content of misaligned xmm & gp are out-dated (reg-1,xmm reg+1,xmm) (reg,gp|ss reg+1,gp|ss)
+    //def at fs_s: content of xmm & gp are out-dated (reg-1,xmm reg,xmm) (reg,gp|ss)
+    //def at gp:   content of xmm is out-dated (reg-1,xmm reg,xmm) (reg,ss)
+    //def at ss:   content of xmm & gp are out-dated (reg-1,xmm reg,xmm) (reg,gp)
+    int index;
+    if(pType != LowOpndRegType_xmm) { //check xmm @reg
+        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg);
+        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+            ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_xmm);
+#endif
+            if(getRegSize(pType) == OpndSize_32)
+                dumpPartToMem(compileTable[index].physicalReg, reg, false); //dump high of xmm to memory
+
+            compileTable[index].setPhysicalReg (PhysicalReg_Null);
+        }
+    }
+    //check misaligned xmm @ reg-1
+    index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg-1);
+    if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+        ALOGI("INVALIDATE virtual reg %d type %d", reg-1, LowOpndRegType_xmm);
+#endif
+        dumpPartToMem(compileTable[index].physicalReg, reg-1, true); //dump low of xmm to memory
+
+        compileTable[index].setPhysicalReg (PhysicalReg_Null);
+    }
+    //check misaligned xmm @ reg+1
+    if(pType == LowOpndRegType_xmm || pType == LowOpndRegType_fs) {
+        //check reg+1,xmm
+        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_xmm, reg+1);
+        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_xmm);
+#endif
+            dumpPartToMem(compileTable[index].physicalReg, reg+1, false); //dump high of xmm to memory
+
+            compileTable[index].setPhysicalReg (PhysicalReg_Null);
+        }
+    }
+    if(pType != LowOpndRegType_gp) {
+        //check reg,gp
+        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg);
+        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+            ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_gp);
+#endif
+
+            compileTable[index].setPhysicalReg (PhysicalReg_Null);
+        }
+    }
+    if(pType == LowOpndRegType_xmm || pType == LowOpndRegType_fs) {
+        //check reg+1,gp
+        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, reg+1);
+        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_gp);
+#endif
+
+            compileTable[index].setPhysicalReg (PhysicalReg_Null);
+        }
+    }
+    if(pType != LowOpndRegType_ss) {
+        //check reg,ss
+        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg);
+        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+            ALOGI("INVALIDATE virtual reg %d type %d", reg, LowOpndRegType_ss);
+#endif
+
+            compileTable[index].setPhysicalReg (PhysicalReg_Null);
+        }
+    }
+    if(pType == LowOpndRegType_xmm || pType == LowOpndRegType_fs) {
+        //check reg+1,ss
+        index = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_ss, reg+1);
+        if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_INVALIDATE
+            ALOGI("INVALIDATE virtual reg %d type %d", reg+1, LowOpndRegType_ss);
+#endif
+
+            compileTable[index].setPhysicalReg (PhysicalReg_Null);
+        }
+    }
+}
+//! bookkeeping when a VR is updated
+
+//! invalidate contents of some physical registers, clear nullCheckDone, and update inMemory;
+//! check whether there exist tranfer points for this bytecode, if yes, perform the transfer
+int updateVirtualReg(int reg, LowOpndRegType pType) {
+    OpndSize size = getRegSize(pType);
+    //WAS only invalidate xmm VRs for the following cases:
+    //if def reaches a use of vA,xmm and (the def is not xmm or is misaligned xmm)
+    //  invalidate "vA,xmm"
+    invalidateVR(reg, pType);
+    clearVRNullCheck(reg, size);
+    clearVRBoundCheck(reg, size);
+    if (pType == LowOpndRegType_fs || pType == LowOpndRegType_fs_s)
+    {
+        setVRMemoryState (reg, size, true);
+    }
+    else
+    {
+        clearVRToMemory (reg, size);
+    }
+
+    unsigned int max = currentBB->xferPoints.size ();
+    for(unsigned int k = 0; k < max; k++) {
+        if(currentBB->xferPoints[k].offsetPC == offsetPC &&
+           currentBB->xferPoints[k].regNum == reg &&
+           currentBB->xferPoints[k].physicalType == pType &&
+           currentBB->xferPoints[k].xtype != XFER_MEM_TO_XMM) {
+            //perform the corresponding action for the def
+            PhysicalReg regAll;
+            if(currentBB->xferPoints[k].xtype == XFER_DEF_IS_XMM) {
+                //def at fs: content of xmm is out-dated
+                //def at xmm: content of misaligned xmm is out-dated
+                //invalidateXmmVR(currentBB->xferPoints[k].tableIndex);
+#ifdef DEBUG_XFER_POINTS
+                if(currentBB->xferPoints[k].dumpToXmm)
+                    ALOGI("XFER set_virtual_reg to xmm: xmm VR %d", reg);
+#endif
+                if(pType == LowOpndRegType_xmm)  {
+#ifdef DEBUG_XFER_POINTS
+                    ALOGI("XFER set_virtual_reg to memory: xmm VR %d", reg);
+#endif
+                    PhysicalReg regAll = (PhysicalReg)checkVirtualReg(reg, LowOpndRegType_xmm, 0 /* do not update*/);
+                    writeBackVR(reg, LowOpndRegType_xmm, regAll);
+                }
+                if(currentBB->xferPoints[k].vr_gpl >= 0) { //
+                }
+                if(currentBB->xferPoints[k].vr_gph >= 0) {
+                }
+            }
+            if((pType == LowOpndRegType_gp || pType == LowOpndRegType_ss) &&
+               (currentBB->xferPoints[k].xtype == XFER_DEF_TO_MEM ||
+                currentBB->xferPoints[k].xtype == XFER_DEF_TO_GP_MEM)) {
+                //the defined gp VR already in register
+                //invalidateXmmVR(currentBB->xferPoints[k].tableIndex);
+                regAll = (PhysicalReg)checkVirtualReg(reg, pType, 0 /* do not update*/);
+                writeBackVR(reg, pType, regAll);
+#ifdef DEBUG_XFER_POINTS
+                ALOGI("XFER set_virtual_reg to memory: gp VR %d", reg);
+#endif
+            }
+            if((pType == LowOpndRegType_fs_s || pType == LowOpndRegType_ss) &&
+               currentBB->xferPoints[k].xtype == XFER_DEF_TO_GP_MEM) {
+            }
+        }
+    }
+    return 0;
+}
+////////////////////////////////////////////////////////////////
+//REGISTER ALLOCATION
+int spillForHardReg(int regNum, int type);
+void decreaseRefCount(int index);
+int getFreeReg(int type, int reg, int indexToCompileTable);
+PhysicalReg spillForLogicalReg(int type, int reg, int indexToCompileTable);
+int unspillLogicalReg(int spill_index, int physicalReg);
+int searchVirtualInfoOfBB(LowOpndRegType type, int regNum, BasicBlock_O1* bb);
+bool isTemp8Bit(int type, int reg);
+bool matchType(int typeA, int typeB);
+int getNextAccess(int compileIndex);
+void dumpCompileTable();
+
+//! allocate a register for a variable
+
+//!if no physical register is free, call spillForLogicalReg to free up a physical register;
+//!if the variable is a temporary and it was spilled, call unspillLogicalReg to load from spill location to the allocated physical register;
+//!if updateRefCount is true, reduce reference count of the variable by 1
+//!if isDest is true, we inform the compileTable about it
+int registerAlloc(int type, int reg, bool isPhysical, bool updateRefCount, bool isDest) {
+#ifdef DEBUG_REGALLOC
+    ALOGI("%p: try to allocate register %d type %d isPhysical %d", currentBB, reg, type, isPhysical);
+#endif
+    if(currentBB == NULL) {
+        if(type & LowOpndRegType_virtual) {
+            return PhysicalReg_Null;
+        }
+        if(isPhysical) return reg; //for helper functions
+        return PhysicalReg_Null;
+    }
+    //ignore EDI, ESP, EBP (glue)
+    if(isPhysical && (reg == PhysicalReg_EDI || reg == PhysicalReg_ESP ||
+                      reg == PhysicalReg_EBP || reg == PhysicalReg_Null))
+        return reg;
+
+    int newType = convertType(type, reg, isPhysical);
+    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
+    int tIndex = searchCompileTable(newType, reg);
+    if(tIndex < 0) {
+      ALOGI("JIT_INFO: reg %d type %d not found in registerAlloc\n", reg, newType);
+      SET_JIT_ERROR(kJitErrorRegAllocFailed);
+      return PhysicalReg_Null;
+    }
+
+    //physical register
+    if(isPhysical) {
+        if(allRegs[reg].isUsed) { //if used by a non hard-coded register
+            spillForHardReg(reg, newType);
+        }
+        allRegs[reg].isUsed = true;
+#ifdef DEBUG_REG_USED
+        ALOGI("REGALLOC: allocate a reg %d", reg);
+#endif
+
+        //Update the physical register
+        compileTable[tIndex].setPhysicalReg (reg);
+        //Update the isWritten field, if isDest is true, set it to true
+        if (isDest == true) {
+            compileTable[tIndex].isWritten = true;
+        }
+
+        if(updateRefCount)
+            decreaseRefCount(tIndex);
+#ifdef DEBUG_REGALLOC
+        ALOGI("REGALLOC: allocate register %d for logical register %d %d",
+               compileTable[tIndex].physicalReg, reg, newType);
+#endif
+        return reg;
+    }
+    //already allocated
+    if(compileTable[tIndex].physicalReg != PhysicalReg_Null) {
+#ifdef DEBUG_REGALLOC
+        ALOGI("already allocated to physical register %d", compileTable[tIndex].physicalReg);
+#endif
+        //Update the isWritten field, if isDest is true, set it to true
+        if (isDest == true) {
+            compileTable[tIndex].isWritten = true;
+        }
+        if(updateRefCount)
+            decreaseRefCount(tIndex);
+        return compileTable[tIndex].physicalReg;
+    }
+
+    //at this point, the logical register is not hard-coded and is mapped to Reg_Null
+    //first check whether there is a free reg
+    //if not, call spillForLogicalReg
+    int index = getFreeReg(newType, reg, tIndex);
+    if(index >= 0 && index < PhysicalReg_Null) {
+        //update compileTable & allRegs
+        compileTable[tIndex].setPhysicalReg (allRegs[index].physicalReg);
+        allRegs[index].isUsed = true;
+#ifdef DEBUG_REG_USED
+        ALOGI("REGALLOC: register %d is free", allRegs[index].physicalReg);
+#endif
+    } else {
+        PhysicalReg allocR = spillForLogicalReg(newType, reg, tIndex);
+        compileTable[tIndex].setPhysicalReg (allocR);
+    }
+    if(compileTable[tIndex].spill_loc_index >= 0) {
+        unspillLogicalReg(tIndex, compileTable[tIndex].physicalReg);
+    }
+
+    //In this case, it's a new register, set isWritten to isDest
+    compileTable[tIndex].isWritten = isDest;
+    if(updateRefCount)
+        decreaseRefCount(tIndex);
+#ifdef DEBUG_REGALLOC
+    ALOGI("REGALLOC: allocate register %d for logical register %d %d",
+           compileTable[tIndex].physicalReg, reg, newType);
+#endif
+    return compileTable[tIndex].physicalReg;
+}
+//!a variable will use a physical register allocated for another variable
+
+//!This is used when MOVE_OPT is on, it tries to alias a virtual register with a temporary to remove a move
+int registerAllocMove(int reg, int type, bool isPhysical, int srcReg, bool isDest) {
+    if(srcReg == PhysicalReg_EDI || srcReg == PhysicalReg_ESP || srcReg == PhysicalReg_EBP) {
+        ALOGI("JIT_INFO: Cannot move from srcReg EDI or ESP or EBP");
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return -1;
+    }
+#ifdef DEBUG_REGALLOC
+    ALOGI("in registerAllocMove: reg %d type %d srcReg %d", reg, type, srcReg);
+#endif
+    int newType = convertType(type, reg, isPhysical);
+    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
+    int index = searchCompileTable(newType, reg);
+    if(index < 0) {
+        ALOGI("JIT_INFO: reg %d type %d not found in registerAllocMove", reg, newType);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return -1;
+    }
+
+    //Update the isWritten field, if isDest is true, set it to true
+    if (isDest == true) {
+        compileTable[index].isWritten = true;
+    }
+    decreaseRefCount(index);
+    compileTable[index].setPhysicalReg (srcReg);
+
+#ifdef DEBUG_REGALLOC
+    ALOGI("REGALLOC: registerAllocMove %d for logical register %d %d",
+           compileTable[index].physicalReg, reg, newType);
+#endif
+    return srcReg;
+}
+
+//! check whether a physical register is available to be used by a variable
+
+//! data structures accessed:
+//! 1> currentBB->infoBasicBlock[index].allocConstraintsSorted
+//!    sorted from high count to low count
+//! 2> currentBB->allocConstraintsSorted
+//!    sorted from low count to high count
+//! 3> allRegs: whether a physical register is available, indexed by PhysicalReg
+//! NOTE: if a temporary variable is 8-bit, only %eax, %ebx, %ecx, %edx can be used
+int getFreeReg(int type, int reg, int indexToCompileTable) {
+    syncAllRegs();
+    /* handles requests for xmm or ss registers */
+    int k;
+    if(((type & MASK_FOR_TYPE) == LowOpndRegType_xmm) ||
+       ((type & MASK_FOR_TYPE) == LowOpndRegType_ss)) {
+        for(k = PhysicalReg_XMM0; k <= PhysicalReg_XMM7; k++) {
+            if(!allRegs[k].isUsed) return k;
+        }
+        return -1;
+    }
+#ifdef DEBUG_REGALLOC
+    ALOGI("USED registers: ");
+    for(k = 0; k < 8; k++)
+        ALOGI("%d used: %d time freed: %d callee-saveld: %d", k, allRegs[k].isUsed,
+             allRegs[k].freeTimeStamp, allRegs[k].isCalleeSaved);
+    ALOGI("");
+#endif
+
+    /* a VR is requesting a physical register */
+    if(isVirtualReg(type)) { //find a callee-saved register
+        int index = searchVirtualInfoOfBB((LowOpndRegType)(type & MASK_FOR_TYPE), reg, currentBB);
+        if(index < 0) {
+            ALOGI("JIT_INFO: VR %d %d not found in infoBasicBlock of currentBB %d (num of VRs %d)",
+                  reg, type, currentBB->id, currentBB->infoBasicBlock.size ());
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
+            return -1;
+        }
+
+        /* check allocConstraints for this VR,
+           return an available physical register with the highest constraint > 0 */
+        for(k = 0; k < 8; k++) {
+            if(currentBB->infoBasicBlock[index].allocConstraintsSorted[k].count == 0) break;
+            int regCandidateT = currentBB->infoBasicBlock[index].allocConstraintsSorted[k].physicalReg;
+            assert(regCandidateT < PhysicalReg_Null);
+            if(!allRegs[regCandidateT].isUsed) return regCandidateT;
+        }
+
+        /* WAS: return an available physical register with the lowest constraint
+           NOW: consider a new factor (freeTime) when there is a tie
+                if 2 available physical registers have the same number of constraints
+                choose the one with smaller free time stamp */
+        int currentCount = -1;
+        int index1 = -1;
+        int smallestTime = -1;
+        for(k = 0; k < 8; k++) {
+            int regCandidateT = currentBB->allocConstraintsSorted[k].physicalReg;
+            assert(regCandidateT < PhysicalReg_Null);
+            if(index1 >= 0 && currentBB->allocConstraintsSorted[k].count > currentCount)
+                break; //candidate has higher count than index1
+            if(!allRegs[regCandidateT].isUsed) {
+                if(index1 < 0) {
+                    index1 = k;
+                    currentCount = currentBB->allocConstraintsSorted[k].count;
+                    smallestTime = allRegs[regCandidateT].freeTimeStamp;
+                } else if(allRegs[regCandidateT].freeTimeStamp < smallestTime) {
+                    index1 = k;
+                    smallestTime = allRegs[regCandidateT].freeTimeStamp;
+                }
+            }
+        }
+        if(index1 >= 0) return currentBB->allocConstraintsSorted[index1].physicalReg;
+        return -1;
+    }
+    /* handle request from a temporary variable */
+    else {
+        bool is8Bit = isTemp8Bit(type, reg);
+
+        /* if the temporary variable is linked to a VR and
+              the VR is not yet allocated to any physical register */
+        int vr_num = compileTable[indexToCompileTable].getLinkedVR ();
+        if(vr_num >= 0) {
+            int index3 = searchCompileTable(LowOpndRegType_gp | LowOpndRegType_virtual, vr_num);
+            if(index3 < 0) {
+                ALOGI("JIT_INFO: Inavlid linkage VR for temporary register %d", vr_num);
+                SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
+                return -1;
+            }
+
+            if(compileTable[index3].physicalReg == PhysicalReg_Null) {
+                int index2 = searchVirtualInfoOfBB(LowOpndRegType_gp, vr_num, currentBB);
+                if(index2 < 0) {
+                    ALOGI("JIT_INFO: In tracing linkage to VR %d", vr_num);
+                    SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                    //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
+                    return -1;
+                }
+#ifdef DEBUG_REGALLOC
+                ALOGI("In getFreeReg for temporary reg %d, trace the linkage to VR %d",
+                     reg, vr_num);
+#endif
+
+                /* check allocConstraints on the VR
+                   return an available physical register with the highest constraint > 0
+                */
+                for(k = 0; k < 8; k++) {
+                    if(currentBB->infoBasicBlock[index2].allocConstraintsSorted[k].count == 0) break;
+                    int regCandidateT = currentBB->infoBasicBlock[index2].allocConstraintsSorted[k].physicalReg;
+#ifdef DEBUG_REGALLOC
+                    ALOGI("check register %d with count %d", regCandidateT,
+                          currentBB->infoBasicBlock[index2].allocConstraintsSorted[k].count);
+#endif
+                    /* if the requesting variable is 8 bit */
+                    if(is8Bit && regCandidateT > PhysicalReg_EDX) continue;
+                    assert(regCandidateT < PhysicalReg_Null);
+                    if(!allRegs[regCandidateT].isUsed) return regCandidateT;
+                }
+            }
+        }
+        /* check allocConstraints of the basic block
+           if 2 available physical registers have the same constraint count,
+              return the non callee-saved physical reg */
+        /* enhancement: record the time when a register is freed (freeTimeStamp)
+                        the purpose is to reduce false dependency
+           priority: constraint count, non callee-saved, time freed
+               let x be the lowest constraint count
+               set A be available callee-saved physical registers with count == x
+               set B be available non callee-saved physical registers with count == x
+               if set B is not null, return the one with smallest free time
+               otherwise, return the one in A with smallest free time
+           To ignore whether it is callee-saved, add all candidates to set A
+        */
+        int setAIndex[8];
+        int num_A = 0;
+        int setBIndex[8];
+        int num_B = 0;
+        int index1 = -1; //points to an available physical reg with lowest count
+        int currentCount = -1;
+        for(k = 0; k < 8; k++) {
+            int regCandidateT = currentBB->allocConstraintsSorted[k].physicalReg;
+            if(is8Bit && regCandidateT > PhysicalReg_EDX) continue;
+
+            if(index1 >= 0 && currentBB->allocConstraintsSorted[k].count > currentCount)
+                break; //candidate has higher count than index1
+            assert(regCandidateT < PhysicalReg_Null);
+            if(!allRegs[regCandidateT].isUsed) {
+                /*To ignore whether it is callee-saved, add all candidates to set A */
+                if(false) {//!allRegs[regCandidateT].isCalleeSaved) { //add to set B
+                    setBIndex[num_B++] = k;
+                } else { //add to set A
+                    setAIndex[num_A++] = k;
+                }
+                if(index1 < 0) {
+                    /* index1 points to a physical reg with lowest count */
+                    index1 = k;
+                    currentCount = currentBB->allocConstraintsSorted[k].count;
+                }
+            }
+        }
+
+        int kk;
+        int smallestTime = -1;
+        index1 = -1;
+        for(kk = 0; kk < num_B; kk++) {
+            k = setBIndex[kk];
+            int regCandidateT = currentBB->allocConstraintsSorted[k].physicalReg;
+            assert(regCandidateT < PhysicalReg_Null);
+            if(kk == 0 || allRegs[regCandidateT].freeTimeStamp < smallestTime) {
+                index1 = k;
+                smallestTime = allRegs[regCandidateT].freeTimeStamp;
+            }
+        }
+        if(index1 >= 0)
+            return currentBB->allocConstraintsSorted[index1].physicalReg;
+        index1 = -1;
+        for(kk = 0; kk < num_A; kk++) {
+            k = setAIndex[kk];
+            int regCandidateT = currentBB->allocConstraintsSorted[k].physicalReg;
+            if(kk == 0 || allRegs[regCandidateT].freeTimeStamp < smallestTime) {
+                index1 = k;
+                smallestTime = allRegs[regCandidateT].freeTimeStamp;
+            }
+        }
+        if(index1 >= 0) return currentBB->allocConstraintsSorted[index1].physicalReg;
+        return -1;
+    }
+    return -1;
+}
+
+//! find a candidate physical register for a variable and spill all variables that are mapped to the candidate
+
+//!
+PhysicalReg spillForLogicalReg(int type, int reg, int indexToCompileTable) {
+    //choose a used register to spill
+    //when a GG virtual register is spilled, write it to interpretd stack, set physicalReg to Null
+    //  at end of the basic block, load spilled GG VR to physical reg
+    //when other types of VR is spilled, write it to interpreted stack, set physicalReg to Null
+    //when a temporary (non-virtual) register is spilled, write it to stack, set physicalReg to Null
+    //can we spill a hard-coded temporary register? YES
+    int k, k2;
+    PhysicalReg allocR;
+
+    //do not try to free a physical reg that is used by more than one logical registers
+    //fix on sep 28, 2009
+    //do not try to spill a hard-coded logical register
+    //do not try to free a physical reg that is outside of the range for 8-bit logical reg
+    /* for each physical register,
+       collect number of non-hardcode entries that are mapped to the physical register */
+    int numOfUses[PhysicalReg_Null];
+    for(k = PhysicalReg_EAX; k < PhysicalReg_Null; k++)
+        numOfUses[k] = 0;
+    for(k = 0; k < compileTable.size (); k++) {
+        if((compileTable[k].physicalReg != PhysicalReg_Null) &&
+           matchType(type, compileTable[k].physicalType) &&
+           (compileTable[k].physicalType & LowOpndRegType_hard) == 0) {
+            numOfUses[compileTable[k].physicalReg]++;
+        }
+    }
+
+    /* candidates: all non-hardcode entries that are mapped to
+           a physical register that is used by only one entry*/
+    bool is8Bit = isTemp8Bit(type, reg);
+    int candidates[compileTable.size ()];
+    int num_cand = 0;
+    for(k = 0; k < compileTable.size (); k++) {
+        if(matchType(type, compileTable[k].physicalType) && compileTable[k].physicalReg != PhysicalReg_Null) {
+            //If we care about 8 bits, we can't have a register over EDX
+            if(is8Bit == true && compileTable[k].physicalReg > PhysicalReg_EDX)
+            {
+                continue;
+            }
+
+            //If we can spill it, ignore it
+            if(gCompilationUnit->getCanSpillRegister (compileTable[k].physicalReg) == false)
+            {
+                continue;
+            }
+
+            //If it isn't a hard register or it only a few uses left, it can be a candidate
+            if((compileTable[k].physicalType & LowOpndRegType_hard) == 0 && numOfUses[compileTable[k].physicalReg] <= 1) {
+                candidates[num_cand++] = k;
+            }
+        }
+    }
+
+    int spill_index = -1;
+
+    /* out of the candates, find a VR that has the furthest next use */
+    int furthestUse = offsetPC;
+    for(k2 = 0; k2 < num_cand; k2++) {
+        k = candidates[k2];
+        if((compileTable[k].physicalReg != PhysicalReg_Null) &&
+           matchType(type, compileTable[k].physicalType) &&
+           isVirtualReg(compileTable[k].physicalType)) {
+            int nextUse = getNextAccess(k);
+            if(spill_index < 0 || nextUse > furthestUse) {
+                spill_index = k;
+                furthestUse = nextUse;
+            }
+        }
+    }
+
+    /* spill the VR with the furthest next use */
+    if(spill_index >= 0) {
+        allocR = (PhysicalReg)spillLogicalReg(spill_index, true);
+        return allocR; //the register is still being used
+    }
+
+    /* spill an entry with the smallest refCount */
+    int baseLeftOver = 0;
+    int index = -1;
+    for(k2 = 0; k2 < num_cand; k2++) {
+        k = candidates[k2];
+        if((compileTable[k].physicalReg != PhysicalReg_Null) &&
+           (compileTable[k].physicalType & LowOpndRegType_hard) == 0 && //not hard-coded
+           matchType(type, compileTable[k].physicalType)) {
+            if((index < 0) || (compileTable[k].refCount < baseLeftOver)) {
+                baseLeftOver = compileTable[k].refCount;
+                index = k;
+            }
+        }
+    }
+    if(index < 0) {
+        dumpCompileTable();
+        ALOGI("JIT_INFO: no register to spill for logical %d %d\n", reg, type);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
+        return PhysicalReg_Null;
+    }
+    allocR = (PhysicalReg)spillLogicalReg(index, true);
+#ifdef DEBUG_REGALLOC
+    ALOGI("SPILL register used by num %d type %d it is a temporary register with refCount %d",
+           compileTable[index].regNum, compileTable[index].physicalType, compileTable[index].refCount);
+#endif
+    return allocR;
+}
+
+/**
+ * @brief Spill a variable to memory, the variable is specified by an index to compileTable
+ * @details If the variable is a temporary, get a spill location that is not in use and spill the content to the spill location;
+ *       If updateTable is true, set physicalReg to Null;
+ * @param spill_index the index into the compile table
+ * @param updateTable do we update the table?
+ * @return Return the physical register that was allocated to the variable
+ */
+int spillLogicalReg(int spill_index, bool updateTable) {
+    if((compileTable[spill_index].physicalType & LowOpndRegType_hard) != 0) {
+        ALOGI("JIT_INFO: can't spill a hard-coded register");
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
+        return -1;
+    }
+
+#ifdef PRINT_WARNING
+    int physicalReg = compileTable[spill_index].physicalReg;
+
+    //If we can't spill it, print out a warning
+    if(gCompilationUnit->getCanSpillReg (physicalReg) == false) {
+        // This scenario can occur whenever a VR is allocated to the
+        // same physical register as a hardcoded temporary
+        ALOGW("Shouldn't spill register %s but going to do it anyway.",
+                physicalRegToString(static_cast<PhysicalReg>(physicalReg)));
+    }
+#endif
+
+    if (compileTable[spill_index].isVirtualReg ())
+    {
+        //Write VR back to memory
+        writeBackVR (compileTable[spill_index].getRegisterNumber (), compileTable[spill_index].getPhysicalType (),
+                compileTable[spill_index].getPhysicalReg ());
+    }
+    else {
+        //If the gCompilationUnit has maximumRegisterization set
+        if (gCompilationUnit->maximumRegisterization > 0)
+        {
+            //Signal the error framework we spilled: this is a warning that can help recompile if possible
+            SET_JIT_ERROR (kJitErrorSpill);
+        }
+        //update spill_loc_index
+        int k = getSpillIndex (compileTable[spill_index].getSize ());
+        compileTable[spill_index].spill_loc_index = 4*k;
+        if(k >= 0)
+            spillIndexUsed[k] = 1;
+        saveToSpillRegion(getRegSize(compileTable[spill_index].physicalType),
+                          compileTable[spill_index].physicalReg, 4*k);
+    }
+    //compileTable[spill_index].physicalReg_prev = compileTable[spill_index].physicalReg;
+#ifdef DEBUG_REGALLOC
+    ALOGI("REGALLOC: SPILL logical reg %d %d with refCount %d allocated to %d",
+           compileTable[spill_index].regNum,
+           compileTable[spill_index].physicalType, compileTable[spill_index].refCount,
+           compileTable[spill_index].physicalReg);
+#endif
+    if(!updateTable) return PhysicalReg_Null;
+
+    int allocR = compileTable[spill_index].physicalReg;
+    compileTable[spill_index].setPhysicalReg (PhysicalReg_Null);
+
+    return allocR;
+}
+//! load a varible from memory to physical register, the variable is specified with an index to compileTable
+
+//!If the variable is a temporary, load from spill location and set the flag for the spill location to not used
+int unspillLogicalReg(int spill_index, int physicalReg) {
+    //can't un-spill to %eax in afterCall!!!
+    //what if GG VR is allocated to %eax!!!
+    if(isVirtualReg(compileTable[spill_index].physicalType)) {
+        get_virtual_reg_noalloc(compileTable[spill_index].regNum,
+                                getRegSize(compileTable[spill_index].physicalType),
+                                physicalReg, true);
+    }
+    else {
+        loadFromSpillRegion(getRegSize(compileTable[spill_index].physicalType),
+                            physicalReg, compileTable[spill_index].spill_loc_index);
+        spillIndexUsed[compileTable[spill_index].spill_loc_index >> 2] = 0;
+        compileTable[spill_index].spill_loc_index = -1;
+    }
+#ifdef DEBUG_REGALLOC
+    ALOGI("REGALLOC: UNSPILL logical reg %d %d with refCount %d", compileTable[spill_index].regNum,
+           compileTable[spill_index].physicalType, compileTable[spill_index].refCount);
+#endif
+    return PhysicalReg_Null;
+}
+
+//!spill a virtual register to memory
+
+//!if the current value of a VR is constant, write immediate to memory;
+//!if the current value of a VR is in a physical register, call spillLogicalReg to dump content of the physical register to memory;
+//!ifupdateTable is true, set the physical register for VR to Null and decrease reference count of the virtual register
+int spillVirtualReg(int vrNum, LowOpndRegType type, bool updateTable) {
+    int index = searchCompileTable(type | LowOpndRegType_virtual, vrNum);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Cannot find VR %d %d in spillVirtualReg", vrNum, type);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return -1;
+    }
+    //check whether it is const
+    int value[2];
+    int isConst = isVirtualRegConstant(vrNum, type, value, false); //do not update refCount
+    if(isConst == 1 || isConst == 3) {
+        writeBackConstVR(vrNum, value[0]);
+    }
+    if(getRegSize(type) == OpndSize_64 && (isConst == 2 || isConst == 3)) {
+        writeBackConstVR(vrNum+1, value[1]);
+    }
+    if(isConst != 3 && compileTable[index].physicalReg != PhysicalReg_Null)
+        spillLogicalReg(index, updateTable);
+    if(updateTable) decreaseRefCount(index);
+    return -1;
+}
+
+/**
+ * @brief Writes virtual register back to memory if it holds a constant value
+ * @param vR virtual register number
+ * @param type the physical type register type that can be associated with
+ * this VR
+ * @return true if the entire VR was written back to memory
+ */
+bool writeBackVRIfConstant(int vR, LowOpndRegType type) {
+    int constantValue[2];
+    bool writtenBack = false;
+
+    // Check if the VR is a constant. This function returns 3 if VR
+    // is 32-bit and constant, or if VR is 64-bit and both high order
+    // bits and low order bits are constant
+    int isConst = isVirtualRegConstant(vR, type, constantValue, false);
+
+    // If the VR is a constant, then write it back to memory
+    if (isConst == 3) {
+        writeBackConstVR(vR, constantValue[0]);
+        writtenBack |= true;
+    }
+
+    // If VR is wide and high order bits are constant, then write them to memory
+    if (getRegSize(type) == OpndSize_64 && isConst == 3) {
+        writeBackConstVR(vR + 1, constantValue[1]);
+        writtenBack |= true;
+    }
+
+    return writtenBack;
+}
+
+//! spill variables that are mapped to physical register (regNum)
+
+//!
+int spillForHardReg(int regNum, int type) {
+    //find an entry that uses the physical register
+    int spill_index = -1;
+    int k;
+    for(k = 0; k < compileTable.size (); k++) {
+        if(compileTable[k].physicalReg == regNum &&
+           matchType(type, compileTable[k].physicalType)) {
+            spill_index = k;
+            if(compileTable[k].regNum == regNum && compileTable[k].physicalType == type)
+                continue;
+            if(inGetVR_num >= 0 && compileTable[k].regNum == inGetVR_num && compileTable[k].physicalType == (type | LowOpndRegType_virtual))
+                continue;
+#ifdef DEBUG_REGALLOC
+            ALOGI("SPILL logical reg %d %d to free hard-coded reg %d %d",
+                   compileTable[spill_index].regNum, compileTable[spill_index].physicalType,
+                   regNum, type);
+            if(compileTable[spill_index].physicalType & LowOpndRegType_hard) dumpCompileTable();
+#endif
+            assert(spill_index < compileTable.size ());
+            spillLogicalReg(spill_index, true);
+        }
+    }
+    return regNum;
+}
+////////////////////////////////////////////////////////////////
+//! update allocConstraints of the current basic block
+
+//! allocConstraints specify how many times a hardcoded register is used in this basic block
+void updateCurrentBBWithConstraints(PhysicalReg reg) {
+    if (currentBB != 0)
+    {
+        if(reg > PhysicalReg_EBP) {
+            ALOGI("JIT_INFO: Register %d out of range in updateCurrentBBWithConstraints\n", reg);
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return;
+        }
+        currentBB->allocConstraints[reg].count++;
+    }
+}
+//! sort allocConstraints and save the result in allocConstraintsSorted
+
+//! allocConstraints specify how many times a virtual register is linked to a hardcode register
+//! it is updated in getVirtualRegInfo and merged by mergeEntry2
+int sortAllocConstraint(RegAllocConstraint* allocConstraints,
+                        RegAllocConstraint* allocConstraintsSorted, bool fromHighToLow) {
+    int ii, jj;
+    int num_sorted = 0;
+    for(jj = 0; jj < 8; jj++) {
+        //figure out where to insert allocConstraints[jj]
+        int count = allocConstraints[jj].count;
+        int regT = allocConstraints[jj].physicalReg;
+        assert(regT < PhysicalReg_Null);
+        int insertIndex = -1;
+        for(ii = 0; ii < num_sorted; ii++) {
+            int regT2 = allocConstraintsSorted[ii].physicalReg;
+            assert(regT2 < PhysicalReg_Null);
+            if(allRegs[regT].isCalleeSaved &&
+               count == allocConstraintsSorted[ii].count) {
+                insertIndex = ii;
+                break;
+            }
+            if((!allRegs[regT].isCalleeSaved) &&
+               count == allocConstraintsSorted[ii].count &&
+               (!allRegs[regT2].isCalleeSaved)) { //skip until found one that is not callee-saved
+                insertIndex = ii;
+                break;
+            }
+            if((fromHighToLow && count > allocConstraintsSorted[ii].count) ||
+               ((!fromHighToLow) && count < allocConstraintsSorted[ii].count)) {
+                insertIndex = ii;
+                break;
+            }
+        }
+        if(insertIndex < 0) {
+            allocConstraintsSorted[num_sorted].physicalReg = (PhysicalReg)regT;
+            allocConstraintsSorted[num_sorted].count = count;
+            num_sorted++;
+        } else {
+            for(ii = num_sorted-1; ii >= insertIndex; ii--) {
+                allocConstraintsSorted[ii+1] = allocConstraintsSorted[ii];
+            }
+            allocConstraintsSorted[insertIndex] = allocConstraints[jj];
+            num_sorted++;
+        }
+    } //for jj
+#ifdef DEBUG_ALLOC_CONSTRAINT
+    for(jj = 0; jj < 8; jj++) {
+        if(allocConstraintsSorted[jj].count > 0)
+            ALOGI("%d: register %d has count %d", jj, allocConstraintsSorted[jj].physicalReg, allocConstraintsSorted[jj].count);
+    }
+#endif
+    return 0;
+}
+
+//! \brief find the entry for a given virtual register in compileTable
+//! \param vA The VR to search for
+//! \param type Register type
+//! \return the virtual reg if found, else -1 as error.
+int findVirtualRegInTable(int vA, LowOpndRegType type) {
+    int k = searchCompileTable(type | LowOpndRegType_virtual, vA);
+    if(k < 0) {
+        ALOGI("JIT_INFO: Couldn't find virtual register %d type %d in compiler table\n", vA, type);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        //Error trickles down to dvmCompilerMIR2LIR, trace is rejected
+        return -1;
+    }
+    return k;
+}
+
+/**
+ * @brief Checks whether the virtual register is constant.
+ * @param regNum The virtual register to check.
+ * @param opndRegType The physical type of this virtual register.
+ * @param valuePtr If non-null, this is updated by function to contain the constant values.
+ * @param updateRefCount When set, lowers reference count in compile table for this VR.
+ * @return Returns information about the constantness of this VR.
+ */
+VirtualRegConstantness isVirtualRegConstant(int regNum, int opndRegType, int *valuePtr, bool updateRefCount)
+{
+    //Determine the size of the VR by looking at its physical type
+    OpndSize size = getRegSize (opndRegType);
+
+    //Use these to keep track of index in the constant table of the VR we are looking for
+    int indexL = -1;
+    int indexH = -1;
+
+    //Iterate through constant table to find the desired virtual register
+    for(int k = 0; k < num_const_vr; k++)
+    {
+#ifdef DEBUG_CONST
+        ALOGI("constVRTable VR %d isConst %d value %x", constVRTable[k].regNum, constVRTable[k].isConst, constVRTable[k].value);
+#endif
+        if(constVRTable[k].regNum == regNum)
+        {
+            indexL = k;
+            continue;
+        }
+
+        if(constVRTable[k].regNum == regNum + 1 && size == OpndSize_64)
+        {
+            indexH = k;
+            continue;
+        }
+    }
+
+    //Eagerly assume that we won't find constantness of this VR
+    bool isConstL = false;
+    bool isConstH = false;
+
+    //If we found an entry in constant table for this VR, check if it is constant.
+    if(indexL >= 0)
+    {
+        isConstL = constVRTable[indexL].isConst;
+    }
+
+    //If we found an entry in constant table for the high part of VR, check if it is constant.
+    if(size == OpndSize_64 && indexH >= 0)
+    {
+        isConstH = constVRTable[indexH].isConst;
+    }
+
+    //Only tell the caller the values of constant if space has been provided for this purpose
+    if (valuePtr != 0)
+    {
+        //Are either the low bits or high bits constant?
+        if (isConstL == true || isConstH == true)
+        {
+            //If the high bits are constant and we care about them, then set value.
+            if (size == OpndSize_64 && isConstH == true)
+            {
+                valuePtr[1] = constVRTable[indexH].value;
+            }
+
+            //Now see if we can set value for the low bits
+            if (isConstL == true)
+            {
+                valuePtr[0] = constVRTable[indexL].value;
+            }
+        }
+    }
+
+    //If we are looking at non-wide VR that is constant or wide VR whose low and high parts are both constant,
+    //then we say that this VR is constant.
+    if((isConstL == true && size == OpndSize_32) || (isConstL == true && isConstH == true))
+    {
+        if(updateRefCount)
+        {
+            //We want to find entry in the compile table that matches the physical type we want.
+            //Since compile table keeps track of physical type along with logical type in same field,
+            //we do a binary bitwise inclusive or including the virtual register type.
+            int indexOrig = searchCompileTable(opndRegType | LowOpndRegType_virtual, regNum);
+
+            if(indexOrig < 0)
+            {
+                //We were not able to find the virtual register in compile table so just set an error
+                //and say that it is not constant.
+                ALOGI("JIT_INFO: Cannot find VR in isVirtualRegConstant num %d type %d\n", regNum, opndRegType);
+                SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                return VR_IS_NOT_CONSTANT;
+            }
+
+            //Decrement the reference count for this VR
+            decreaseRefCount(indexOrig);
+        }
+#ifdef DEBUG_CONST
+        ALOGI("VR %d %d is const case", regNum, type);
+#endif
+        return VR_IS_CONSTANT;
+    }
+    else if (isConstL == true && size != OpndSize_32)
+    {
+        //If the VR is wide and only low part is constant, we return saying that
+        return VR_LOW_IS_CONSTANT;
+    }
+    else if (isConstH == true && size != OpndSize_32)
+    {
+        //If the VR is wide and only high part is constant, we return saying that
+        return VR_HIGH_IS_CONSTANT;
+    }
+    else
+    {
+        //If we make it here, this VR is not constant
+        return VR_IS_NOT_CONSTANT;
+    }
+}
+
+//!update RegAccessType of virtual register vB given RegAccessType of vA
+
+//!RegAccessType can be D, L, H
+//!D means full definition, L means only lower-half is defined, H means only higher half is defined
+//!we say a VR has no exposed usage in a basic block if the accessType is D or DU
+//!we say a VR has exposed usage in a basic block if the accessType is not D nor DU
+//!we say a VR has exposed usage in other basic blocks (hasOtherExposedUsage) if
+//!  there exists another basic block where VR has exposed usage in that basic block
+//!A can be U, D, L, H, UD, UL, UH, DU, LU, HU (merged result)
+//!B can be U, D, UD, DU (an entry for the current bytecode)
+//!input isAPartiallyOverlapB can be any value between -1 to 6
+//!if A is xmm: gp B lower half of A, (isAPartiallyOverlapB is 1)
+//!             gp B higher half of A, (isAPartiallyOverlapB is 2)
+//!             lower half of A covers the higher half of xmm B  (isAPartiallyOverlapB is 4)
+//!             higher half of A covers the lower half of xmm B   (isAPartiallyOverlapB is 3)
+//!if A is gp:  A covers the lower half of xmm B, (isAPartiallyOverlapB is 5)
+//!             A covers the higher half of xmm B (isAPartiallyOverlapB is 6)
+RegAccessType updateAccess1(RegAccessType A, OverlapCase isAPartiallyOverlapB) {
+    if(A == REGACCESS_D || A == REGACCESS_DU || A == REGACCESS_UD) {
+        if(isAPartiallyOverlapB == OVERLAP_ALIGN) return REGACCESS_D;
+        if(isAPartiallyOverlapB == OVERLAP_B_IS_LOW_OF_A || isAPartiallyOverlapB == OVERLAP_B_IS_HIGH_OF_A)
+            return REGACCESS_D;
+        if(isAPartiallyOverlapB == OVERLAP_LOW_OF_A_IS_HIGH_OF_B || isAPartiallyOverlapB == OVERLAP_A_IS_LOW_OF_B)
+            return REGACCESS_L;
+        return REGACCESS_H;
+    }
+    if(A == REGACCESS_L || A == REGACCESS_LU || A == REGACCESS_UL) {
+        if(isAPartiallyOverlapB == OVERLAP_ALIGN || isAPartiallyOverlapB == OVERLAP_A_IS_LOW_OF_B)
+            return REGACCESS_L;
+        if(isAPartiallyOverlapB == OVERLAP_B_IS_LOW_OF_A) return REGACCESS_D;
+        if(isAPartiallyOverlapB == OVERLAP_B_IS_HIGH_OF_A || isAPartiallyOverlapB == OVERLAP_LOW_OF_A_IS_HIGH_OF_B)
+            return REGACCESS_N;
+        if(isAPartiallyOverlapB == OVERLAP_HIGH_OF_A_IS_LOW_OF_B || isAPartiallyOverlapB == OVERLAP_A_IS_HIGH_OF_B)
+            return REGACCESS_H;
+    }
+    if(A == REGACCESS_H || A == REGACCESS_HU || A == REGACCESS_UH) {
+        if(isAPartiallyOverlapB == OVERLAP_ALIGN || isAPartiallyOverlapB == OVERLAP_A_IS_HIGH_OF_B)
+            return REGACCESS_H;
+        if(isAPartiallyOverlapB == OVERLAP_B_IS_LOW_OF_A || isAPartiallyOverlapB == OVERLAP_HIGH_OF_A_IS_LOW_OF_B)
+            return REGACCESS_N;
+        if(isAPartiallyOverlapB == OVERLAP_B_IS_HIGH_OF_A) return REGACCESS_D;
+        if(isAPartiallyOverlapB == OVERLAP_LOW_OF_A_IS_HIGH_OF_B || isAPartiallyOverlapB == OVERLAP_A_IS_LOW_OF_B)
+            return REGACCESS_L;
+    }
+    return REGACCESS_N;
+}
+//! merge RegAccessType C1 with RegAccessType C2
+
+//!C can be N,L,H,D
+RegAccessType updateAccess2(RegAccessType C1, RegAccessType C2) {
+    if(C1 == REGACCESS_D || C2 == REGACCESS_D) return REGACCESS_D;
+    if(C1 == REGACCESS_N) return C2;
+    if(C2 == REGACCESS_N) return C1;
+    if(C1 == REGACCESS_L && C2 == REGACCESS_H) return REGACCESS_D;
+    if(C1 == REGACCESS_H && C2 == REGACCESS_L) return REGACCESS_D;
+    return C1;
+}
+//! merge RegAccessType C with RegAccessType B
+
+//!C can be N,L,H,D
+//!B can be U, D, UD, DU
+RegAccessType updateAccess3(RegAccessType C, RegAccessType B) {
+    if(B == REGACCESS_D || B == REGACCESS_DU) return B; //no exposed usage
+    if(B == REGACCESS_U || B == REGACCESS_UD) {
+        if(C == REGACCESS_N) return B;
+        if(C == REGACCESS_L) return REGACCESS_LU;
+        if(C == REGACCESS_H) return REGACCESS_HU;
+        if(C == REGACCESS_D) return REGACCESS_DU;
+    }
+    return B;
+}
+//! merge RegAccessType A with RegAccessType B
+
+//!argument isBPartiallyOverlapA can be any value between -1 and 2
+//!0 means fully overlapping, 1 means B is the lower half, 2 means B is the higher half
+RegAccessType mergeAccess2(RegAccessType A, RegAccessType B, OverlapCase isBPartiallyOverlapA) {
+    if(A == REGACCESS_UD || A == REGACCESS_UL || A == REGACCESS_UH ||
+       A == REGACCESS_DU || A == REGACCESS_LU || A == REGACCESS_HU) return A;
+    if(A == REGACCESS_D) {
+        if(B == REGACCESS_D) return REGACCESS_D;
+        if(B == REGACCESS_U) return REGACCESS_DU;
+        if(B == REGACCESS_UD) return REGACCESS_DU;
+        if(B == REGACCESS_DU) return B;
+    }
+    if(A == REGACCESS_U) {
+        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_UL;
+        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_UH;
+        if(B == REGACCESS_D && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_UD;
+        if(B == REGACCESS_U) return A;
+        if(B == REGACCESS_UD && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_UL;
+        if(B == REGACCESS_UD && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_UH;
+        if(B == REGACCESS_UD && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_UD;
+        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_UL;
+        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_UH;
+        if(B == REGACCESS_DU && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_UD;
+    }
+    if(A == REGACCESS_L) {
+        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_L;
+        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_D;
+        if(B == REGACCESS_D && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_D;
+        if(B == REGACCESS_U) return REGACCESS_LU;
+        if(B == REGACCESS_UD) return REGACCESS_LU;
+        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_LU;
+        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_DU;
+        if(B == REGACCESS_DU && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_DU;
+    }
+    if(A == REGACCESS_H) {
+        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_D;
+        if(B == REGACCESS_D && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_H;
+        if(B == REGACCESS_D && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_D;
+        if(B == REGACCESS_U) return REGACCESS_HU;
+        if(B == REGACCESS_UD) return REGACCESS_HU;
+        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_LOW_OF_A) return REGACCESS_DU;
+        if(B == REGACCESS_DU && isBPartiallyOverlapA == OVERLAP_B_COVER_HIGH_OF_A) return REGACCESS_HU;
+        if(B == REGACCESS_DU && (isBPartiallyOverlapA == OVERLAP_B_COVER_A)) return REGACCESS_DU;
+    }
+    return REGACCESS_N;
+}
+
+//!determines which part of a use is from a given definition
+
+//!reachingDefLive tells us which part of the def is live at this point
+//!isDefPartiallyOverlapUse can be any value between -1 and 2
+RegAccessType setAccessTypeOfUse(OverlapCase isDefPartiallyOverlapUse, RegAccessType reachingDefLive) {
+    if(isDefPartiallyOverlapUse == OVERLAP_B_COVER_A)
+        return reachingDefLive;
+    if(isDefPartiallyOverlapUse == OVERLAP_B_COVER_LOW_OF_A) { //def covers the low half of use
+        return REGACCESS_L;
+    }
+    if(isDefPartiallyOverlapUse == OVERLAP_B_COVER_HIGH_OF_A) {
+        return REGACCESS_H;
+    }
+    return REGACCESS_N;
+}
+
+//! search currentBB->defUseTable to find a def for regNum at offsetPC
+
+//!
+DefUsePair* searchDefUseTable(int offsetPC, int regNum, LowOpndRegType pType) {
+    DefUsePair* ptr = currentBB->defUseTable;
+    while(ptr != NULL) {
+        if(ptr->def.offsetPC == offsetPC &&
+           ptr->def.regNum == regNum &&
+           ptr->def.physicalType == pType) {
+            return ptr;
+        }
+        ptr = ptr->next;
+    }
+    return NULL;
+}
+void printDefUseTable() {
+    ALOGI("PRINT defUseTable --------");
+    DefUsePair* ptr = currentBB->defUseTable;
+    while(ptr != NULL) {
+        ALOGI("  def @ %x of VR %d %d has %d uses", ptr->def.offsetPC,
+              ptr->def.regNum, ptr->def.physicalType,
+              ptr->num_uses);
+        DefOrUseLink* ptr2 = ptr->uses;
+        while(ptr2 != NULL) {
+            ALOGI("    use @ %x of VR %d %d accessType %d", ptr2->offsetPC,
+                  ptr2->regNum,
+                  ptr2->physicalType,
+                  ptr2->accessType);
+            ptr2 = ptr2->next;
+        }
+        ptr = ptr->next;
+    }
+}
+
+/**
+ * @brief Update a VR use
+ * @param reg the register
+ * @param pType the type we want for the register
+ * @param regAll
+ */
+void updateVRAtUse(int reg, LowOpndRegType pType, int regAll) {
+
+    //Get a local size of xferPoints' size
+    unsigned int max = currentBB->xferPoints.size ();
+
+    //Go through each element
+    for(unsigned int k = 0; k < max; k++) {
+
+        //If the xferPoint matches and says we want memory to xmm
+        if(currentBB->xferPoints[k].offsetPC == offsetPC &&
+           currentBB->xferPoints[k].xtype == XFER_MEM_TO_XMM &&
+           currentBB->xferPoints[k].regNum == reg &&
+           currentBB->xferPoints[k].physicalType == pType) {
+#ifdef DEBUG_XFER_POINTS
+            ALOGI("XFER from memory to xmm %d", reg);
+#endif
+            // If we get to this point, it is possible that we believe we need
+            // to load the wide VR from memory, but in reality this VR might
+            // already be in a physical register.
+
+            // TODO Figure out why this transfer point is inserted even when we already
+            // have VR in xmm.
+
+            int xmmVRType = static_cast<int>(LowOpndRegType_virtual)
+                    | static_cast<int>(LowOpndRegType_xmm);
+            int ssVRType = static_cast<int>(LowOpndRegType_virtual)
+                    | static_cast<int>(LowOpndRegType_ss);
+            bool loadFromMemory = true;
+
+            // Look in compile table for this VR
+            int entry = searchCompileTable(xmmVRType, reg);
+
+            if (entry == -1) {
+                // Single FP VRs can also use xmm, so try looking for this
+                // as well if we haven't already found an entry
+                entry = searchCompileTable(ssVRType, reg);
+            }
+
+            if (entry != -1) {
+                // If we found an entry, check whether its physical register
+                // is not null. If we have a physical register, we shouldn't be
+                // loading from memory
+                if (compileTable[entry].physicalReg != PhysicalReg_Null)
+                    loadFromMemory = false;
+            }
+
+            // Load from memory into the physical register
+            if (loadFromMemory) {
+                move_mem_to_reg_noalloc(OpndSize_64,
+                        4 * currentBB->xferPoints[k].regNum, PhysicalReg_FP,
+                        true, MemoryAccess_VR, currentBB->xferPoints[k].regNum,
+                        regAll, true);
+            }
+        }
+    }
+}
+
+/////////////////////////////////////////////////////////////
+//!search memVRTable for a given virtual register
+
+/**
+ * @brief Search in the memory table for a register
+ * @param regNum the register we are looking for
+ * @return the index for the register, -1 if not found
+ */
+int searchMemTable(int regNum) {
+    int k;
+    for(k = 0; k < num_memory_vr; k++) {
+        if(memVRTable[k].regNum == regNum) {
+            return k;
+        }
+    }
+    ALOGI("JIT_INFO: Can't find VR %d num_memory_vr %d at searchMemTable", regNum, num_memory_vr);
+    return -1;
+}
+/////////////////////////////////////////////////////////////////////////
+// A VR is already in memory && NULL CHECK
+//!check whether the latest content of a VR is in memory
+
+//!
+bool isInMemory(int regNum, OpndSize size) {
+    int indexL = searchMemTable(regNum);
+    int indexH = -1;
+    if(size == OpndSize_64) indexH = searchMemTable(regNum+1);
+    if(indexL < 0) return false;
+    if(size == OpndSize_64 && indexH < 0) return false;
+    if(!memVRTable[indexL].inMemory) return false;
+    if(size == OpndSize_64 && (!memVRTable[indexH].inMemory)) return false;
+    return true;
+}
+
+/**
+ * @brief Used to update the in memory state of a virtual register
+ * @param vR The virtual register
+ * @param size The size of the virtual register
+ * @param inMemory The new in memory state of the virtual register
+ */
+void setVRMemoryState (int vR, OpndSize size, bool inMemory)
+{
+    //Look for the index in the mem table for the virtual register
+    int indexL = searchMemTable (vR);
+
+    //If virtual register is wide, we want to find the index for the high part as well
+    int indexH = -1;
+    if (size == OpndSize_64)
+    {
+        indexH = searchMemTable (vR + 1);
+    }
+
+    if (indexL < 0)
+    {
+        ALOGI ("JIT_INFO: VR %d not in memVRTable at setVRToMemory", vR);
+        SET_JIT_ERROR (kJitErrorRegAllocFailed);
+        return;
+    }
+
+    //Update the in memory state of the VR
+    memVRTable[indexL].setInMemoryState (inMemory);
+
+    DEBUG_MEMORYVR (ALOGD ("Setting state of v%d %sin memory", memVRTable[indexL].vR,
+            (memVRTable[indexL].inMemory ? "" : "NOT ")));
+
+    if (size == OpndSize_64)
+    {
+        if (indexH < 0)
+        {
+            ALOGI ("JIT_INFO: VR %d not in memVRTable at setVRToMemory for upper 64-bits", vR+1);
+            SET_JIT_ERROR (kJitErrorRegAllocFailed);
+            return;
+        }
+
+        //Update the in memory state of the upper 64-bits of the VR
+        memVRTable[indexH].setInMemoryState (inMemory);
+
+        DEBUG_MEMORYVR (ALOGD ("Setting state of v%d %sin memory", memVRTable[indexH].vR,
+                (memVRTable[indexH].inMemory ? "" : "NOT ")));
+    }
+}
+
+//! check whether null check for a VR is performed previously
+
+//!
+bool isVRNullCheck(int regNum, OpndSize size) {
+    if(size != OpndSize_32) {
+        ALOGI("JIT_INFO: isVRNullCheck size is not 32 for register %d", regNum);
+        SET_JIT_ERROR(kJitErrorNullBoundCheckFailed);
+        return false;
+    }
+    int indexL = searchMemTable(regNum);
+    if(indexL < 0) {
+        ALOGI("JIT_INFO: VR %d not in memVRTable at isVRNullCheck", regNum);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    return memVRTable[indexL].nullCheckDone;
+}
+bool isVRBoundCheck(int vr_array, int vr_index) {
+    int indexL = searchMemTable(vr_array);
+    if(indexL < 0) {
+        ALOGI("JIT_INFO: VR %d not in memVRTable at isVRBoundCheck", vr_array);
+        SET_JIT_ERROR(kJitErrorNullBoundCheckFailed);
+        return false;
+    }
+    if(memVRTable[indexL].boundCheck.indexVR == vr_index)
+        return memVRTable[indexL].boundCheck.checkDone;
+    return false;
+}
+//! \brief set nullCheckDone in memVRTable to true
+//!
+//! \param regNum the register number
+//! \param size the register size
+//!
+//! \return -1 if error happened, 0 otherwise
+int setVRNullCheck(int regNum, OpndSize size) {
+    if(size != OpndSize_32) {
+        ALOGI("JIT_INFO: setVRNullCheck size should be 32\n");
+        SET_JIT_ERROR(kJitErrorNullBoundCheckFailed);
+        return -1;
+    }
+    int indexL = searchMemTable(regNum);
+    if(indexL < 0) {
+        ALOGI("JIT_INFO: VR %d not in memVRTable at setVRNullCheck", regNum);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return -1;
+    }
+    memVRTable[indexL].nullCheckDone = true;
+    return 0;
+}
+void setVRBoundCheck(int vr_array, int vr_index) {
+    int indexL = searchMemTable(vr_array);
+    if(indexL < 0) {
+        ALOGI("JIT_INFO: VR %d not in memVRTable at setVRBoundCheck", vr_array);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return;
+    }
+    memVRTable[indexL].boundCheck.indexVR = vr_index;
+    memVRTable[indexL].boundCheck.checkDone = true;
+}
+void clearVRBoundCheck(int regNum, OpndSize size) {
+    int k;
+    for(k = 0; k < num_memory_vr; k++) {
+        if(memVRTable[k].regNum == regNum ||
+           (size == OpndSize_64 && memVRTable[k].regNum == regNum+1)) {
+            memVRTable[k].boundCheck.checkDone = false;
+        }
+        if(memVRTable[k].boundCheck.indexVR == regNum ||
+           (size == OpndSize_64 && memVRTable[k].boundCheck.indexVR == regNum+1)) {
+            memVRTable[k].boundCheck.checkDone = false;
+        }
+    }
+}
+//! set inMemory of memVRTable to false
+
+//!
+void clearVRToMemory(int regNum, OpndSize size) {
+    int indexL = searchMemTable(regNum);
+    int indexH = -1;
+    if(size == OpndSize_64) indexH = searchMemTable(regNum+1);
+    if(indexL >= 0) {
+        memVRTable[indexL].inMemory = false;
+
+        DEBUG_MEMORYVR(ALOGD("Setting state of v%d %sin memory",
+                memVRTable[indexL].regNum,
+                (memVRTable[indexL].inMemory ? "" : "NOT ")));
+    }
+    if(size == OpndSize_64 && indexH >= 0) {
+        memVRTable[indexH].inMemory = false;
+
+        DEBUG_MEMORYVR(ALOGD("Setting state of v%d %sin memory",
+                memVRTable[indexH].regNum,
+                (memVRTable[indexH].inMemory ? "" : "NOT ")));
+    }
+}
+//! set nullCheckDone of memVRTable to false
+
+//!
+void clearVRNullCheck(int regNum, OpndSize size) {
+    int indexL = searchMemTable(regNum);
+    int indexH = -1;
+    if(size == OpndSize_64) indexH = searchMemTable(regNum+1);
+    if(indexL >= 0) {
+        memVRTable[indexL].nullCheckDone = false;
+    }
+    if(size == OpndSize_64 && indexH >= 0) {
+        memVRTable[indexH].nullCheckDone = false;
+    }
+}
+
+//! Extend life for all VRs
+
+//! Affects only VRs, stored in physical reg on last bytecode of their live range
+//! @see VRFreeDelayCounters
+//! @param reason explains what freeing delay request should be canceled.
+//! A single VRFreeDelayCounters index should be used.
+//! @return true if at least one VR changed it's state
+bool requestVRFreeDelayAll(u4 reason) {
+    bool state_changed = false;
+    // Delay only VRs, which could be freed by freeReg
+    for(int k = 0; k < compileTable.size (); k++) {
+
+        if(compileTable[k].physicalReg != PhysicalReg_Null) {
+
+            if(isVirtualReg(compileTable[k].physicalType) == true) {
+                bool freeCrit = isLastByteCodeOfLiveRange(k);
+
+                if(freeCrit == true) {
+                    int res = requestVRFreeDelay(compileTable[k].regNum, reason);
+                    if(res >= 0) {
+                        state_changed = true;
+                    }
+                }
+            }
+        }
+    }
+#ifdef DEBUG_REGALLOC
+    if(state_changed) {
+        ALOGI("requestVRFreeDelayAll: state_changed=%i", state_changed);
+    }
+#endif
+    return state_changed;
+}
+
+//! Cancel request for all VR life extension
+
+//! Affects only VRs, stored in physical reg on last bytecode of theirs live range
+//! @see VRFreeDelayCounters
+//! @param reason explains what freeing delay request should be canceled.
+//! A single VRFreeDelayCounters index should be used.
+//! @return true if at least one VR changed it's state
+bool cancelVRFreeDelayRequestAll(u4 reason) {
+    bool state_changed = false;
+    // Cancel delay for VRs only
+    for(int k = 0; k < compileTable.size (); k++) {
+        if(isVirtualReg(compileTable[k].physicalType) == true) {
+            bool freeCrit = isLastByteCodeOfLiveRange(k);
+
+            if(freeCrit == true) {
+                int res = cancelVRFreeDelayRequest(compileTable[k].regNum, reason);
+                if(res >= 0) {
+                    state_changed = true;
+                }
+            }
+        }
+    }
+#ifdef DEBUG_REGALLOC
+    if(state_changed) {
+        ALOGI("cancelVRFreeDelayRequestAll: state_changed=%i", state_changed);
+    }
+#endif
+    return state_changed;
+}
+
+//! Extend Virtual Register life
+
+//! Requests that the life of a specific virtual register be extended. This ensures
+//! that its mapping to a physical register won't be canceled while the extension
+//! request is valid. NOTE: This does not support 64-bit values (when two adjacent
+//! VRs are used)
+//! @see cancelVRFreeDelayRequest
+//! @see getVRFreeDelayRequested
+//! @see VRFreeDelayCounters
+//! @param regNum is the VR number
+//! @param reason explains why freeing must be delayed.
+//! A single VRFreeDelayCounters index should be used.
+//! @return negative value if request failed
+int requestVRFreeDelay(int regNum, u4 reason) {
+    // TODO Add 64-bit operand support when needed
+    int indexL = searchMemTable(regNum);
+    if(indexL >= 0) {
+        if(reason < VRDELAY_COUNT) {
+#ifdef DEBUG_REGALLOC
+            ALOGI("requestFreeDelay: reason=%i VR=%d count=%i", reason, regNum, memVRTable[indexL].delayFreeCounters[reason]);
+#endif
+            memVRTable[indexL].delayFreeCounters[reason]++;
+        } else {
+            ALOGI("JIT_INFO: At requestVRFreeDelay: reason %i is unknown (VR=%d)", reason, regNum);
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return -1;
+        }
+    } else {
+        ALOGI("JIT_INFO: At requestVRFreeDelay: VR %d not in memVRTable", regNum);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return -1;
+    }
+
+    return indexL;
+}
+
+//! Cancel request for virtual register life extension
+
+//! Cancels any outstanding requests to extended liveness of VR. Additionally,
+//! this ensures that if the VR is no longer life after this point, it will
+//! no longer be associated with a physical register which can then be reused.
+//! NOTE: This does not support 64-bit values (when two adjacent VRs are used)
+//! @see requestVRFreeDelay
+//! @see getVRFreeDelayRequested
+//! @see VRFreeDelayCounters
+//! @param regNum is the VR number
+//! @param reason explains what freeing delay request should be canceled.
+//! A single VRFreeDelayCounters index should be used.
+//! @return negative value if request failed
+int cancelVRFreeDelayRequest(int regNum, u4 reason) {
+    //TODO Add 64-bit operand support when needed
+    bool needCallToFreeReg = false;
+    int indexL = searchMemTable(regNum);
+    if(indexL >= 0) {
+        if(reason < VRDELAY_COUNT) { // don't cancel delay if it wasn't requested
+            if(memVRTable[indexL].delayFreeCounters[reason] > 0) {
+#ifdef DEBUG_REGALLOC
+                ALOGI("cancelVRFreeDelay: reason=%i VR=%d count=%i", reason, regNum, memVRTable[indexL].delayFreeCounters[reason]);
+#endif
+                memVRTable[indexL].delayFreeCounters[reason]--; // only cancel this particular reason, not all others
+
+                // freeReg might want to free this VR now if there is no longer a valid delay
+                needCallToFreeReg = !getVRFreeDelayRequested(regNum);
+            } else {
+                return -1;
+            }
+        } else {
+            ALOGI("JIT_INFO: At cancelVRFreeDelay: reason %i is unknown (VR: %d)", reason, regNum);
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return -1;
+        }
+    }
+    if(needCallToFreeReg)
+        freeReg(false);
+
+    return indexL;
+}
+
+//! Gets status of virtual register free delay request
+
+//! Finds out if there was a delay request for freeing this VR.
+//! NOTE: This does not support 64-bit values (when two adjacent VRs are used)
+//! @see requestVRFreeDelay
+//! @see cancelVRFreeDelayRequest
+//! @param regNum is the VR number
+//! @return true if VR has an active delay request
+bool getVRFreeDelayRequested(int regNum) {
+    //TODO Add 64-bit operand support when needed
+    int indexL = searchMemTable(regNum);
+    if(indexL >= 0) {
+        for(int c=0; c<VRDELAY_COUNT; c++) {
+            if(memVRTable[indexL].delayFreeCounters[c] != 0) {
+                return true;
+            }
+        }
+        return false;
+    }
+    return false;
+}
+
+int current_bc_size = -1;
+
+/**
+ * @brief Search the basic block information for a register number and type
+ * @param type the register type we are looking for (masked with MASK_FOR_TYPE)
+ * @param regNum the register number we are looking for
+ * @param bb the BasicBlock
+ * @return true if found
+ */
+bool isUsedInBB(int regNum, int type, BasicBlock_O1* bb) {
+    unsigned int k, max;
+
+    //Get a local version of the infoBasicBlock's size
+    max = bb->infoBasicBlock.size ();
+
+    //Go through each element, if it matches, return true
+    for(k = 0; k < max; k++) {
+        if(bb->infoBasicBlock[k].physicalType == (type & MASK_FOR_TYPE) && bb->infoBasicBlock[k].regNum == regNum) {
+            return true;
+        }
+    }
+
+    //Report failure
+    return false;
+}
+
+/**
+ * @brief Search the basic block information for a register number and type
+ * @param type the register type we are looking for
+ * @param regNum the register number we are looking for
+ * @param bb the BasicBlock
+ * @return the index in the infoBasicBlock table, -1 if not found
+ */
+int searchVirtualInfoOfBB(LowOpndRegType type, int regNum, BasicBlock_O1* bb) {
+    unsigned int k, max;
+
+    //Get a local version of the infoBasicBlock's size
+    max = bb->infoBasicBlock.size ();
+
+    //Go through each element, if it matches, return the index
+    for(k = 0; k < max; k++) {
+        if(bb->infoBasicBlock[k].physicalType == type && bb->infoBasicBlock[k].regNum == regNum) {
+            return k;
+        }
+    }
+
+    //Not found is not always an error, so don't set any
+    return -1;
+}
+//! return the index to compileTable for a given virtual register
+
+//! return -1 if not found
+int searchCompileTable(int type, int regNum) { //returns the index
+    int k;
+    for(k = 0; k < compileTable.size (); k++) {
+        if(compileTable[k].physicalType == type && compileTable[k].regNum == regNum)
+            return k;
+    }
+
+    //Returning -1 might not always be an error, so we don't set any
+    return -1;
+}
+
+/**
+ * @brief Update the compileTable entry with the new register
+ * @param vR what virtual register are we interested in
+ * @param oldReg the old register that used to be used
+ * @param newReg the new register that is now used
+ * @return whether the update was successful
+ */
+bool updatePhysicalRegForVR(int vR, PhysicalReg oldReg, PhysicalReg newReg) {
+    //Go through the entries in the compile table
+    for (int entry = 0; entry < compileTable.size (); entry++) {
+
+        //If it is a virtual register, the vR we are looking for, and is associated to the old register
+        if (isVirtualReg(compileTable[entry].physicalType)
+                && compileTable[entry].regNum == vR
+                && compileTable[entry].physicalReg == oldReg) {
+
+            //Update it and report success
+            compileTable[entry].setPhysicalReg (newReg);
+
+            return true;
+        }
+    }
+
+    //We did not find it
+    return false;
+}
+
+//!check whether a physical register for a variable with typeA will work for another variable with typeB
+
+//!Type LowOpndRegType_ss is compatible with type LowOpndRegType_xmm
+bool matchType(int typeA, int typeB) {
+    if((typeA & MASK_FOR_TYPE) == (typeB & MASK_FOR_TYPE)) return true;
+    if((typeA & MASK_FOR_TYPE) == LowOpndRegType_ss &&
+       (typeB & MASK_FOR_TYPE) == LowOpndRegType_xmm) return true;
+    if((typeA & MASK_FOR_TYPE) == LowOpndRegType_xmm &&
+       (typeB & MASK_FOR_TYPE) == LowOpndRegType_ss) return true;
+    return false;
+}
+
+//! obsolete
+bool defineFirst(int atype) {
+    if(atype == REGACCESS_D || atype == REGACCESS_L || atype == REGACCESS_H || atype == REGACCESS_DU)
+        return true;
+    return false;
+}
+//!check whether a virtual register is updated in a basic block
+
+//!
+bool notUpdated(RegAccessType atype) {
+    if(atype == REGACCESS_U) return true;
+    return false;
+}
+//!check whether a virtual register has exposed usage within a given basic block
+
+//!
+bool hasExposedUsage2(BasicBlock_O1* bb, int index) {
+    RegAccessType atype = bb->infoBasicBlock[index].accessType;
+    if(atype == REGACCESS_D || atype == REGACCESS_L || atype == REGACCESS_H || atype == REGACCESS_DU)
+        return false;
+    return true;
+}
+//! return the spill location that is not used
+
+//!
+int getSpillIndex (OpndSize size) {
+    int k;
+    for(k = 1; k <= MAX_SPILL_JIT_IA - 1; k++) {
+        if(size == OpndSize_64) {
+            if(k < MAX_SPILL_JIT_IA - 1 && spillIndexUsed[k] == 0 && spillIndexUsed[k+1] == 0)
+                return k;
+        }
+        else if(spillIndexUsed[k] == 0) {
+            return k;
+        }
+    }
+    ALOGI("JIT_INFO: Cannot find spill position in spillLogicalReg\n");
+    SET_JIT_ERROR(kJitErrorRegAllocFailed);
+    return -1;
+}
+
+//!this is called before generating a native code, it resets the spill information
+//!startNativeCode must be paired with endNativeCode
+void startNativeCode(int vr_num, int vr_type) {
+    //Reset the spilling information
+    gCompilationUnit->resetCanSpillRegisters ();
+
+    //Set the inGetVR_num and type now
+    inGetVR_num = vr_num;
+    inGetVR_type = vr_type;
+}
+
+//! called right after generating a native code
+//!It resets the spill information and resets inGetVR_num to -1
+void endNativeCode(void) {
+    //Reset the spilling information
+    gCompilationUnit->resetCanSpillRegisters ();
+
+    //Reset the inGetVR_num now
+    inGetVR_num = -1;
+}
+
+//! touch hardcoded register %ecx and reduce its reference count
+
+//!
+int touchEcx() {
+    //registerAlloc will spill logical reg that is mapped to ecx
+    //registerAlloc will reduce refCount
+    registerAlloc(LowOpndRegType_gp, PhysicalReg_ECX, true, true);
+    return 0;
+}
+//! touch hardcoded register %eax and reduce its reference count
+
+//!
+int touchEax() {
+    registerAlloc(LowOpndRegType_gp, PhysicalReg_EAX, true, true);
+    return 0;
+}
+int touchEsi() {
+    registerAlloc(LowOpndRegType_gp, PhysicalReg_ESI, true, true);
+    return 0;
+}
+int touchXmm1() {
+    registerAlloc(LowOpndRegType_xmm, XMM_1, true, true);
+    return 0;
+}
+int touchEbx() {
+    registerAlloc(LowOpndRegType_gp, PhysicalReg_EBX, true, true);
+    return 0;
+}
+
+//! touch hardcoded register %edx and reduce its reference count
+
+//!
+int touchEdx() {
+    registerAlloc(LowOpndRegType_gp, PhysicalReg_EDX, true, true);
+    return 0;
+}
+
+#ifdef HACK_FOR_DEBUG
+//for debugging purpose, instructions are added at a certain place
+bool hacked = false;
+void hackBug() {
+  if(!hacked && iget_obj_inst == 13) {
+#if 0
+    move_reg_to_reg_noalloc(OpndSize_32, PhysicalReg_EBX, true, PhysicalReg_ECX, true);
+    //move from ebx to ecx & update compileTable for v3
+    int tIndex = searchCompileTable(LowOpndRegType_virtual | LowOpndRegType_gp, 3);
+    if(tIndex < 0) ALOGE("hack can't find VR3");
+    compileTable[tIndex].physicalReg = PhysicalReg_ECX;
+#else
+    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EBX, true, 12, PhysicalReg_FP, true);
+#endif
+  }
+}
+void hackBug2() {
+  if(!hacked && iget_obj_inst == 13) {
+    dump_imm_mem_noalloc(Mnemonic_MOV, OpndSize_32, 0, 12, PhysicalReg_FP, true);
+    hacked = true;
+  }
+}
+#endif
+
+//! this function is called before calling a helper function or a vm function
+int beforeCall(const char* target) { //spill all live registers
+    if(currentBB == NULL) return -1;
+
+    /* special case for ncgGetEIP: this function only updates %edx */
+    if(!strcmp(target, "ncgGetEIP")) {
+        touchEdx();
+        return -1;
+    }
+
+    /* these functions use %eax for the return value */
+    if((!strcmp(target, "dvmInstanceofNonTrivial")) ||
+       (!strcmp(target, "dvmUnlockObject")) ||
+       (!strcmp(target, "dvmAllocObject")) ||
+       (!strcmp(target, "dvmAllocArrayByClass")) ||
+       (!strcmp(target, "dvmAllocPrimitiveArray")) ||
+       (!strcmp(target, "dvmInterpHandleFillArrayData")) ||
+       (!strcmp(target, "dvmFindInterfaceMethodInCache")) ||
+       (!strcmp(target, "dvmNcgHandlePackedSwitch")) ||
+       (!strcmp(target, "dvmNcgHandleSparseSwitch")) ||
+       (!strcmp(target, "dvmCanPutArrayElement")) ||
+       (!strcmp(target, "moddi3")) || (!strcmp(target, "divdi3")) ||
+       (!strcmp(target, "execute_inline"))
+       || (!strcmp(target, "dvmJitToPatchPredictedChain"))
+       || (!strcmp(target, "dvmJitHandlePackedSwitch"))
+       || (!strcmp(target, "dvmJitHandleSparseSwitch"))
+#if defined(WITH_SELF_VERIFICATION)
+       || (!strcmp(target, "selfVerificationLoad"))
+#endif
+       ) {
+        touchEax();
+    }
+
+    //these two functions also use %edx for the return value
+    if((!strcmp(target, "moddi3")) || (!strcmp(target, "divdi3"))) {
+        touchEdx();
+    }
+    if((!strcmp(target, ".new_instance_helper"))) {
+        touchEsi(); touchEax();
+    }
+#if defined(ENABLE_TRACING)
+    if((!strcmp(target, "common_periodicChecks4"))) {
+        touchEdx();
+    }
+#endif
+    if((!strcmp(target, ".const_string_helper"))) {
+        touchEcx(); touchEax();
+    }
+    if((!strcmp(target, ".check_cast_helper"))) {
+        touchEbx(); touchEsi();
+    }
+    if((!strcmp(target, ".instance_of_helper"))) {
+        touchEbx(); touchEsi(); touchEcx();
+    }
+    if((!strcmp(target, ".monitor_enter_helper"))) {
+        touchEbx();
+    }
+    if((!strcmp(target, ".monitor_exit_helper"))) {
+        touchEbx();
+    }
+    if((!strcmp(target, ".aget_wide_helper"))) {
+        touchEbx(); touchEcx(); touchXmm1();
+    }
+    if((!strcmp(target, ".aget_helper")) || (!strcmp(target, ".aget_char_helper")) ||
+       (!strcmp(target, ".aget_short_helper")) || (!strcmp(target, ".aget_bool_helper")) ||
+       (!strcmp(target, ".aget_byte_helper"))) {
+        touchEbx(); touchEcx(); touchEdx();
+    }
+    if((!strcmp(target, ".aput_helper")) || (!strcmp(target, ".aput_char_helper")) ||
+       (!strcmp(target, ".aput_short_helper")) || (!strcmp(target, ".aput_bool_helper")) ||
+       (!strcmp(target, ".aput_byte_helper")) || (!strcmp(target, ".aput_wide_helper"))) {
+        touchEbx(); touchEcx(); touchEdx();
+    }
+    if((!strcmp(target, ".sput_helper")) || (!strcmp(target, ".sput_wide_helper"))) {
+        touchEdx(); touchEax();
+    }
+    if((!strcmp(target, ".sget_helper"))) {
+        touchEdx(); touchEcx();
+    }
+    if((!strcmp(target, ".sget_wide_helper"))) {
+        touchEdx(); touchXmm1();
+    }
+    if((!strcmp(target, ".aput_obj_helper"))) {
+        touchEdx(); touchEcx(); touchEax();
+    }
+    if((!strcmp(target, ".iput_helper")) || (!strcmp(target, ".iput_wide_helper"))) {
+        touchEbx(); touchEcx(); touchEsi();
+    }
+    if((!strcmp(target, ".iget_helper"))) {
+        touchEbx(); touchEcx(); touchEdx();
+    }
+    if((!strcmp(target, ".iget_wide_helper"))) {
+        touchEbx(); touchEcx(); touchXmm1();
+    }
+    if((!strcmp(target, ".new_array_helper"))) {
+        touchEbx(); touchEdx(); touchEax();
+    }
+    if((!strcmp(target, ".invoke_virtual_helper"))) {
+        touchEbx(); touchEcx();
+    }
+    if((!strcmp(target, ".invoke_direct_helper"))) {
+        touchEsi(); touchEcx();
+    }
+    if((!strcmp(target, ".invoke_super_helper"))) {
+        touchEbx(); touchEcx();
+    }
+    if((!strcmp(target, ".invoke_interface_helper"))) {
+        touchEbx(); touchEcx();
+    }
+    if((!strcmp(target, ".invokeMethodNoRange_5_helper")) ||
+       (!strcmp(target, ".invokeMethodNoRange_4_helper"))) {
+        touchEbx(); touchEsi(); touchEax(); touchEdx();
+    }
+    if((!strcmp(target, ".invokeMethodNoRange_3_helper"))) {
+        touchEbx(); touchEsi(); touchEax();
+    }
+    if((!strcmp(target, ".invokeMethodNoRange_2_helper"))) {
+        touchEbx(); touchEsi();
+    }
+    if((!strcmp(target, ".invokeMethodNoRange_1_helper"))) {
+        touchEbx();
+    }
+    if((!strcmp(target, ".invokeMethodRange_helper"))) {
+        touchEdx(); touchEsi();
+    }
+#ifdef DEBUG_REGALLOC
+    ALOGI("enter beforeCall");
+#endif
+
+    freeReg(true); //to avoid spilling dead logical registers
+    int k;
+    for(k = 0; k < compileTable.size (); k++) {
+        if(compileTable[k].physicalReg != PhysicalReg_Null &&
+           (compileTable[k].physicalType & LowOpndRegType_hard) == 0) {
+            /* handles non hardcoded variables that are in physical registers */
+            if(!strcmp(target, "exception")) {
+                /* before throwing an exception
+                   update contents of all VRs in Java stack */
+                if(!isVirtualReg(compileTable[k].physicalType)) continue;
+                /* to have correct GC, we should update contents for L VRs as well */
+                //if(compileTable[k].gType == GLOBALTYPE_L) continue;
+            }
+            if((!strcmp(target, ".const_string_resolve")) ||
+               (!strcmp(target, ".static_field_resolve")) ||
+               (!strcmp(target, ".inst_field_resolve")) ||
+               (!strcmp(target, ".class_resolve")) ||
+               (!strcmp(target, ".direct_method_resolve")) ||
+               (!strcmp(target, ".virtual_method_resolve")) ||
+               (!strcmp(target, ".static_method_resolve"))) {
+               /* physical register %ebx will keep its content
+                  but to have correct GC, we should dump content of a VR
+                     that is mapped to %ebx */
+                if(compileTable[k].physicalReg == PhysicalReg_EBX &&
+                   (!isVirtualReg(compileTable[k].physicalType)))
+                    continue;
+            }
+            if((!strncmp(target, "dvm", 3)) || (!strcmp(target, "moddi3")) ||
+               (!strcmp(target, "divdi3")) ||
+               (!strcmp(target, "fmod")) || (!strcmp(target, "fmodf"))) {
+                /* callee-saved registers (%ebx, %esi, %ebp, %edi) will keep the content
+                   but to have correct GC, we should dump content of a VR
+                      that is mapped to a callee-saved register */
+                if((compileTable[k].physicalReg == PhysicalReg_EBX ||
+                    compileTable[k].physicalReg == PhysicalReg_ESI) &&
+                   (!isVirtualReg(compileTable[k].physicalType)))
+                    continue;
+            }
+
+            if(strncmp(target, "dvmUnlockObject", 15) == 0) {
+               continue;
+            }
+
+#ifdef DEBUG_REGALLOC
+            ALOGI("SPILL logical register %d %d in beforeCall",
+                  compileTable[k].regNum, compileTable[k].physicalType);
+#endif
+            spillLogicalReg(k, true);
+        }
+    }
+
+    cancelVRFreeDelayRequestAll(VRDELAY_CAN_THROW);
+
+#ifdef DEBUG_REGALLOC
+    ALOGI("exit beforeCall");
+#endif
+    return 0;
+}
+int getFreeReg(int type, int reg, int indexToCompileTable);
+//! after calling a helper function or a VM function
+
+//!
+int afterCall(const char* target) { //un-spill
+    if(currentBB == NULL) return -1;
+    if(!strcmp(target, "ncgGetEIP")) return -1;
+
+    return 0;
+}
+//! check whether a temporary is 8-bit
+
+//!
+bool isTemp8Bit(int type, int reg) {
+    if(currentBB == NULL) return false;
+    if(!isTemporary(type, reg)) return false;
+    int k;
+    for(k = 0; k < num_temp_regs_per_bytecode; k++) {
+        if(infoByteCodeTemp[k].physicalType == type &&
+           infoByteCodeTemp[k].regNum == reg) {
+            return infoByteCodeTemp[k].is8Bit;
+        }
+    }
+    ALOGI("JIT_INFO: Could not find reg %d type %d at isTemp8Bit", reg, type);
+    SET_JIT_ERROR(kJitErrorRegAllocFailed);
+    return false;
+}
+
+/* functions to access live ranges of a VR
+   Live range info is stored in memVRTable[].ranges, which is a linked list
+*/
+//! check whether a VR is live at the current bytecode
+
+//!
+bool isVRLive(int vA) {
+    int index = searchMemTable(vA);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find VR %d in memTable at isVRLive", vA);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    LiveRange* ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        if(offsetPC >= ptr->start && offsetPC <= ptr->end) return true;
+        ptr = ptr->next;
+    }
+    return false;
+}
+
+//! check whether the current bytecode is the last access to a VR within a live range
+
+//!for 64-bit VR, return true only when true for both low half and high half
+bool isLastByteCodeOfLiveRange(int compileIndex) {
+    int k = compileIndex;
+    OpndSize tSize = getRegSize(compileTable[k].physicalType);
+    int index;
+    LiveRange* ptr = NULL;
+    if(tSize == OpndSize_32) {
+        /* check live ranges for the VR */
+        index = searchMemTable(compileTable[k].regNum);
+        if(index < 0) {
+            ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at isLastByteCodeOfLiveRange", compileTable[k].regNum);
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return false;
+        }
+        ptr = memVRTable[index].ranges;
+        while(ptr != NULL) {
+            if(offsetPC == ptr->end) return true;
+            ptr = ptr->next;
+        }
+        return false;
+    }
+    /* size of the VR is 64 */
+    /* check live ranges of the low half */
+    index = searchMemTable(compileTable[k].regNum);
+    bool tmpB = false;
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 64-bit VR %d (lower 32) in memTable at isLastByteCodeOfLiveRange", compileTable[k].regNum);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        if(offsetPC == ptr->end) {
+            tmpB = true;
+            break;
+        }
+        ptr = ptr->next;
+    }
+    if(!tmpB) return false;
+    /* check live ranges of the high half */
+    index = searchMemTable(compileTable[k].regNum+1);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 64-bit VR %d (upper 32) in memTable at isLastByteCodeOfLiveRange", compileTable[k].regNum+1);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        if(offsetPC == ptr->end) {
+            return true;
+        }
+        ptr = ptr->next;
+    }
+    return false;
+}
+
+// check if virtual register has loop independent dependence
+bool loopIndepUse(int compileIndex) {
+    int k = compileIndex;
+    OpndSize tSize = getRegSize(compileTable[k].physicalType);
+    int index;
+    bool retCode = false;
+
+    /* check live ranges of the low half */
+    index = searchMemTable(compileTable[k].regNum);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at loopIndepUse", compileTable[k].regNum);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    LiveRange* ptr = memVRTable[index].ranges;
+    if (ptr != NULL && ptr->start > 0)
+        retCode = true;
+    if(!retCode) return false;
+    if(tSize == OpndSize_32) return true;
+
+    /* check for the high half */
+    index = searchMemTable(compileTable[k].regNum+1);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at loopIndepUse", compileTable[k].regNum+1);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    ptr = memVRTable[index].ranges;
+    if (ptr != NULL && ptr->start > 0)
+       return true;
+    return false;
+}
+
+
+//! check whether the current bytecode is in a live range that extends to end of a basic block
+
+//!for 64 bit, return true if true for both low half and high half
+bool reachEndOfBB(int compileIndex) {
+    int k = compileIndex;
+    OpndSize tSize = getRegSize(compileTable[k].physicalType);
+    int index;
+    bool retCode = false;
+    /* check live ranges of the low half */
+    index = searchMemTable(compileTable[k].regNum);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at reachEndOfBB", compileTable[k].regNum);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    LiveRange* ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        if(offsetPC >= ptr->start &&
+           offsetPC <= ptr->end) {
+            if(ptr->end == currentBB->pc_end) {
+                retCode = true;
+            }
+            break;
+        }
+        ptr = ptr->next;
+    }
+    if(!retCode) return false;
+    if(tSize == OpndSize_32) return true;
+    /* check live ranges of the high half */
+    index = searchMemTable(compileTable[k].regNum+1);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at reachEndOfBB", compileTable[k].regNum+1);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        if(offsetPC >= ptr->start &&
+           offsetPC <= ptr->end) {
+            if(ptr->end == currentBB->pc_end) return true;
+            return false;
+        }
+        ptr = ptr->next;
+    }
+#ifdef PRINT_WARNING
+    ALOGW("offsetPC %d not in live range of VR %d", offsetPC, compileTable[k].regNum+1);
+#endif
+    return false;
+}
+
+//!check whether the current bytecode is the next to last access to a VR within a live range
+
+//!for 64 bit, return true if true for both low half and high half
+bool isNextToLastAccess(int compileIndex) {
+    int k = compileIndex;
+    OpndSize tSize = getRegSize(compileTable[k].physicalType);
+    int index;
+    /* check live ranges for the low half */
+    bool retCode = false;
+    index = searchMemTable(compileTable[k].regNum);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at isNextToLastAccess", compileTable[k].regNum);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    LiveRange* ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        int num_access = ptr->num_access;
+
+        if(num_access < 2) {
+           ptr = ptr->next;
+           continue;
+        }
+
+        if(offsetPC == ptr->accessPC[num_access-2]) {
+           retCode = true;
+           break;
+        }
+        ptr = ptr->next;
+    }
+    if(!retCode) return false;
+    if(tSize == OpndSize_32) return true;
+    /* check live ranges for the high half */
+    index = searchMemTable(compileTable[k].regNum+1);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at isNextToLastAccess", compileTable[k].regNum+1);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return false;
+    }
+    ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        int num_access = ptr->num_access;
+
+        if(num_access < 2) {
+           ptr = ptr->next;
+           continue;
+        }
+
+        if(offsetPC == ptr->accessPC[num_access-2]) return true;
+        ptr = ptr->next;
+    }
+    return false;
+}
+
+/** return bytecode offset corresponding to offsetPC
+*/
+int convertOffsetPCtoBytecodeOffset(int offPC) {
+    if(offPC == PC_FOR_START_OF_BB)
+        return currentBB->pc_start;
+    if(offPC == PC_FOR_END_OF_BB)
+        return currentBB->pc_end;
+    for(MIR * mir = currentBB->firstMIRInsn; mir; mir = mir->next) {
+       if(mir->seqNum == offPC)
+         return mir->offset;
+    }
+    return currentBB->pc_end;
+}
+
+/** return the start of the next live range
+    if there does not exist a next live range, return pc_end of the basic block
+    for 64 bits, return the larger one for low half and high half
+    Assume live ranges are sorted in order
+*/
+int getNextLiveRange(int compileIndex) {
+    int k = compileIndex;
+    OpndSize tSize = getRegSize(compileTable[k].physicalType);
+    /* check live ranges of the low half */
+    int index;
+    index = searchMemTable(compileTable[k].regNum);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at getNextLiveRange", compileTable[k].regNum);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return offsetPC;
+    }
+    bool found = false;
+    int nextUse = offsetPC;
+    LiveRange* ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        if(ptr->start > offsetPC) {
+            nextUse = ptr->start;
+            found = true;
+            break;
+        }
+        ptr = ptr->next;
+    }
+    if(!found) return PC_FOR_END_OF_BB;
+    if(tSize == OpndSize_32) return nextUse;
+
+    /* check live ranges of the high half */
+    found = false;
+    index = searchMemTable(compileTable[k].regNum+1);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at getNextLiveRange", compileTable[k].regNum+1);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return offsetPC;
+    }
+    int nextUse2 = offsetPC;
+    ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        if(ptr->start > offsetPC) {
+            nextUse2 = ptr->start;
+            found = true;
+            break;
+        }
+        ptr = ptr->next;
+    }
+    if(!found) return PC_FOR_END_OF_BB;
+    /* return the larger one */
+    return (nextUse2 > nextUse ? nextUse2 : nextUse);
+}
+
+/** return the next access to a variable
+    If variable is 64-bit, get the next access to the lower half and the high half
+        return the eariler one
+*/
+int getNextAccess(int compileIndex) {
+    int k = compileIndex;
+    OpndSize tSize = getRegSize(compileTable[k].physicalType);
+    int index, k3;
+    /* check live ranges of the low half */
+    index = searchMemTable(compileTable[k].regNum);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 32-bit VR %d in memTable at getNextAccess", compileTable[k].regNum);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return offsetPC;
+    }
+    bool found = false;
+    int nextUse = offsetPC;
+    LiveRange* ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        if(offsetPC >= ptr->start &&
+           offsetPC <= ptr->end) {
+            /* offsetPC belongs to this live range */
+            for(k3 = 0; k3 < ptr->num_access; k3++) {
+                if(ptr->accessPC[k3] > offsetPC) {
+                    nextUse = ptr->accessPC[k3];
+                    break;
+                }
+            }
+            found = true;
+            break;
+        }
+        ptr = ptr->next;
+    }
+    if(!found) {
+#ifdef PRINT_WARNING
+        ALOGW("offsetPC %d not in live range of VR %d", offsetPC, compileTable[k].regNum);
+#endif
+    }
+    if(tSize == OpndSize_32) return nextUse;
+
+    /* check live ranges of the high half */
+    found = false;
+    index = searchMemTable(compileTable[k].regNum+1);
+    if(index < 0) {
+        ALOGI("JIT_INFO: Could not find 64-bit VR %d in memTable at getNextAccess", compileTable[k].regNum+1);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return offsetPC;
+    }
+    int nextUse2 = offsetPC;
+    ptr = memVRTable[index].ranges;
+    while(ptr != NULL) {
+        if(offsetPC >= ptr->start &&
+           offsetPC <= ptr->end) {
+            for(k3 = 0; k3 < ptr->num_access; k3++) {
+                if(ptr->accessPC[k3] > offsetPC) {
+                    nextUse2 = ptr->accessPC[k3];
+                    break;
+                }
+            }
+            found = true;
+            break;
+        }
+        ptr = ptr->next;
+    }
+    if(!found) {
+#ifdef PRINT_WARNING
+        ALOGW("offsetPC %d not in live range of VR %d", offsetPC, compileTable[k].regNum+1);
+#endif
+    }
+    /* return the earlier one */
+    if(nextUse2 < nextUse) return nextUse2;
+    return nextUse;
+}
+
+/**
+ * @brief Free variables that are no longer in use.
+ * @param writeBackAllVRs When true, writes back all dirty VRs including
+ * constants
+ * @return Returns value >= 0 on success.
+*/
+int freeReg(bool writeBackAllVRs) {
+    //If the current BasicBlock is 0, we have nothing to do
+    if(currentBB == NULL) {
+        return 0;
+    }
+
+    //If writeBackAllVRs is true, we also spill the constants
+    if (writeBackAllVRs == true) {
+        for (int k = 0; k < num_const_vr; k++) {
+            if (constVRTable[k].isConst) {
+                writeBackConstVR(constVRTable[k].regNum, constVRTable[k].value);
+            }
+        }
+    }
+
+    for(int k = 0; k < compileTable.size (); k++) {
+        if (writeBackAllVRs && isVirtualReg(compileTable[k].physicalType)
+                && compileTable[k].inPhysicalRegister () == true) {
+#ifdef DEBUG_REGALLOC
+            ALOGI("FREE v%d with type %d allocated to %s",
+                    compileTable[k].regNum, compileTable[k].physicalType,
+                    physicalRegToString(static_cast<PhysicalReg>(
+                            compileTable[k].physicalReg)));
+#endif
+
+            spillLogicalReg(k, true);
+        }
+
+        if(compileTable[k].refCount == 0 && compileTable[k].inPhysicalRegister () == true) {
+            bool isTemp = (isVirtualReg(compileTable[k].physicalType) == false);
+
+            if (isTemp) {
+#ifdef DEBUG_REGALLOC
+                ALOGI("FREE temporary %d with type %d allocated to %s",
+                       compileTable[k].regNum, compileTable[k].physicalType,
+                       physicalRegToString(static_cast<PhysicalReg>(
+                               compileTable[k].physicalReg)));
+#endif
+
+                compileTable[k].setPhysicalReg (PhysicalReg_Null);
+
+                if(compileTable[k].spill_loc_index >= 0) {
+                    /* update spill info for temporaries */
+                    spillIndexUsed[compileTable[k].spill_loc_index >> 2] = 0;
+                    compileTable[k].spill_loc_index = -1;
+                    ALOGI("JIT_INFO: free a temporary register with TRSTATE_SPILLED\n");
+                    SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                    return -1;
+                }
+            }
+        }
+    }
+    syncAllRegs(); //sync up allRegs (isUsed & freeTimeStamp) with compileTable
+    return 0;
+}
+
+//! reduce the reference count by 1
+
+//! input: index to compileTable
+void decreaseRefCount(int index) {
+#ifdef DEBUG_REFCOUNT
+    ALOGI("REFCOUNT: %d in decreaseRefCount %d %d", compileTable[index].refCount,
+            compileTable[index].regNum, compileTable[index].physicalType);
+#endif
+    compileTable[index].refCount--;
+    if(compileTable[index].refCount < 0) {
+        ALOGI("JIT_INFO: refCount is negative for REG %d %d at decreaseRefCount",
+                compileTable[index].regNum, compileTable[index].physicalType);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return;
+    }
+}
+//! reduce the reference count of a VR by 1
+
+//! input: reg & type
+int updateRefCount(int reg, LowOpndRegType type) {
+    if(currentBB == NULL) return 0;
+    int index = searchCompileTable(LowOpndRegType_virtual | type, reg);
+    if(index < 0) {
+        ALOGI("JIT_INFO: virtual reg %d type %d not found in updateRefCount\n", reg, type);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return -1;
+    }
+    decreaseRefCount(index);
+    return 0;
+}
+//! reduce the reference count of a variable by 1
+
+//! The variable is named with lowering module's naming mechanism
+int updateRefCount2(int reg, int type, bool isPhysical) {
+    if(currentBB == NULL) return 0;
+    int newType = convertType(type, reg, isPhysical);
+    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
+    int index = searchCompileTable(newType, reg);
+    if(index < 0) {
+        ALOGI("JIT_INFO: reg %d type %d not found in updateRefCount\n", reg, newType);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return -1;
+    }
+    decreaseRefCount(index);
+    return 0;
+}
+
+//! check whether a virtual register is in a physical register
+
+//! If updateRefCount is 0, do not update reference count;
+//!If updateRefCount is 1, update reference count only when VR is in a physical register
+//!If updateRefCount is 2, update reference count
+int checkVirtualReg(int reg, LowOpndRegType type, int updateRefCount) {
+    if(currentBB == NULL) return PhysicalReg_Null;
+    int index = searchCompileTable(LowOpndRegType_virtual | type, reg);
+    if(index < 0) {
+        ALOGI("JIT_INFO: virtual reg %d type %d not found in checkVirtualReg\n", reg, type);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return PhysicalReg_Null;
+    }
+    //reduce reference count
+    if(compileTable[index].physicalReg != PhysicalReg_Null) {
+        if(updateRefCount != 0) decreaseRefCount(index);
+        return compileTable[index].physicalReg;
+    }
+    if(updateRefCount == 2) decreaseRefCount(index);
+    return PhysicalReg_Null;
+}
+//!check whether a temporary can share the same physical register with a VR
+
+//!This is called in get_virtual_reg
+//!If this function returns false, new register will be allocated for this temporary
+bool checkTempReg2(int reg, int type, bool isPhysical, int physicalRegForVR, int vB) {
+    if (isPhysical) {
+        // If temporary is already physical, we cannot share with VR
+        return false;
+    }
+
+    int newType = convertType(type, reg, isPhysical);
+    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
+
+    // Look through all of the temporaries used by this bytecode implementation
+    for(int k = 0; k < num_temp_regs_per_bytecode; k++) {
+
+        if(infoByteCodeTemp[k].physicalType == newType &&
+           infoByteCodeTemp[k].regNum == reg) {
+            // We found a matching temporary
+
+            if (!infoByteCodeTemp[k].is8Bit
+                    || (physicalRegForVR >= PhysicalReg_EAX
+                            && physicalRegForVR <= PhysicalReg_EDX)) {
+                DEBUG_MOVE_OPT(ALOGD("Temp%d can%s share %s with v%d",
+                        reg, infoByteCodeTemp[k].shareWithVR ? "" : " NOT",
+                        physicalRegToString(static_cast<PhysicalReg>(
+                                physicalRegForVR)), vB));
+
+                return infoByteCodeTemp[k].shareWithVR;
+            } else {
+                DEBUG_MOVE_OPT(ALOGD("Temp%d can NOT share %s with v%d",
+                        reg, physicalRegToString(static_cast<PhysicalReg>(
+                                physicalRegForVR)), vB));
+
+                // We cannot share same physical register as VR
+                return false;
+            }
+        }
+    }
+    ALOGI("JIT_INFO: in checkTempReg2 %d %d\n", reg, newType);
+    SET_JIT_ERROR(kJitErrorRegAllocFailed);
+    return false;
+}
+//!check whether a temporary can share the same physical register with a VR
+
+//!This is called in set_virtual_reg
+int checkTempReg(int reg, int type, bool isPhysical, int vrNum) {
+    if(currentBB == NULL) return PhysicalReg_Null;
+
+    int newType = convertType(type, reg, isPhysical);
+    if(newType & LowOpndRegType_scratch) reg = reg - PhysicalReg_SCRATCH_1 + 1;
+    int index = searchCompileTable(newType, reg);
+    if(index < 0) {
+        ALOGI("JIT_INFO: temp reg %d type %d not found in checkTempReg\n", reg, newType);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return PhysicalReg_Null;
+    }
+
+    //a temporary register can share the same physical reg with a VR if registerAllocMove is called
+    //this will cause problem with move bytecode
+    //get_VR(v1, t1) t1 and v1 point to the same physical reg
+    //set_VR(t1, v2) t1 and v2 point to the same physical reg
+    //this will cause v1 and v2 point to the same physical reg
+    //FIX: if this temp reg shares a physical reg with another reg
+    if(compileTable[index].physicalReg != PhysicalReg_Null) {
+        int k;
+        for(k = 0; k < compileTable.size (); k++) {
+            if(k == index) continue;
+            if(compileTable[k].physicalReg == compileTable[index].physicalReg) {
+                return PhysicalReg_Null; //will allocate a register for VR
+            }
+        }
+        decreaseRefCount(index);
+        return compileTable[index].physicalReg;
+    }
+    if(compileTable[index].spill_loc_index >= 0) {
+        //registerAlloc will call unspillLogicalReg (load from memory)
+#ifdef DEBUG_REGALLOC
+        ALOGW("in checkTempReg, the temporary register %d %d was spilled", reg, type);
+#endif
+        //No need for written, we don't write in it yet, just aliasing at worse
+        int regAll = registerAlloc(type, reg, isPhysical, true/* updateRefCount */);
+        return regAll;
+    }
+    return PhysicalReg_Null;
+}
+//!check whether a variable has exposed usage in a basic block
+
+//!It calls hasExposedUsage2
+bool hasExposedUsage(LowOpndRegType type, int regNum, BasicBlock_O1* bb) {
+    int index = searchVirtualInfoOfBB(type, regNum, bb);
+    if(index >= 0 && hasExposedUsage2(bb, index)) {
+        return true;
+    }
+    return false;
+}
+
+/**
+ * @brief Handle the spilling of registers at the end of a BasicBlock.
+ * @param syncChildren If sync children is set to true, then we create or sync association table for child.
+ * @return -1 if error, 0 otherwise
+ */
+int handleRegistersEndOfBB (bool syncChildren)
+{
+    //First we call freeReg to get rid of any temporaries that might be using physical registers.
+    //Since we are at the end of the BB, we don't need to spill the temporaries because we
+    //are done using them.
+    freeReg (false);
+
+    //If it's a jump, then we don't update the association tables. The reason is
+    //that the implementation of jumping bytecode (for example "if" bytecode) will
+    //create association tables for children
+    if (syncChildren == true)
+    {
+        //Update association tables of child. There should technically be just one child
+        //but we generically try to pass our information to all children.
+
+        if (AssociationTable::createOrSyncTable(currentBB, true) == false)
+        {
+            //If syncing failed, the error code will be already set so we just pass along error information
+            return -1;
+        }
+
+        if (AssociationTable::createOrSyncTable(currentBB, false) == false)
+        {
+            //If syncing failed, the error code will be already set so we just pass along error information
+            return -1;
+        }
+    }
+
+    syncAllRegs();
+
+    return 0;
+}
+
+//! get ready for the next version of a hard-coded register
+
+//!set its physicalReg to Null and update its reference count
+int nextVersionOfHardReg(PhysicalReg pReg, int refCount) {
+    int indexT = searchCompileTable(LowOpndRegType_gp | LowOpndRegType_hard, pReg);
+    if(indexT < 0) {
+        ALOGI("JIT_INFO: Physical reg not found at nextVersionOfHardReg");
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return -1;
+    }
+    compileTable[indexT].setPhysicalReg (PhysicalReg_Null);
+#ifdef DEBUG_REFCOUNT
+    ALOGI("REFCOUNT: to %d in nextVersionOfHardReg %d", refCount, pReg);
+#endif
+    compileTable[indexT].refCount = refCount;
+    return 0;
+}
+
+/**
+ * @brief Updates compile table with virtual register information.
+ * @details If compile table already contains information about virtual register, only the
+ * reference counts are updated. Otherwise a new entry is created in the compile table.
+ * @param regInfo Information about the virtual register.
+ */
+void insertFromVirtualInfo (const VirtualRegInfo &regInfo)
+{
+    int vR = regInfo.regNum;
+
+    //We want to find entry in the compile table that matches the physical type we want.
+    //Since compile table keeps track of physical type along with logical type in same field,
+    //we do a binary bitwise inclusive or including the virtual register type.
+    int index = searchCompileTable (LowOpndRegType_virtual | regInfo.physicalType, vR);
+
+    if (index < 0)
+    {
+        //If we get here it means that the VR is not in the compile table
+
+        //Create the new entry
+        CompileTableEntry newEntry (regInfo);
+
+        //Create the new entry and then copy it to the table
+        compileTable.insert (newEntry);
+    }
+    else
+    {
+        //Just update the ref count when we already have an entry
+        compileTable[index].updateRefCount (regInfo.refCount);
+    }
+}
+
+/**
+ * @brief Updates compile table with temporary information.
+ * @param tempRegInfo Information about the temporary.
+ */
+static void insertFromTempInfo (const TempRegInfo &tempRegInfo)
+{
+    int index = searchCompileTable(tempRegInfo.physicalType, tempRegInfo.regNum);
+
+    //If we did not find it in compile table simply insert it.
+    if (index < 0)
+    {
+        CompileTableEntry newEntry (tempRegInfo);
+
+        compileTable.insert (newEntry);
+    }
+    else
+    {
+        //Set the physical register
+        compileTable[index].setPhysicalReg (PhysicalReg_Null);
+
+        //Update the reference count for this temporary
+        compileTable[index].updateRefCount (tempRegInfo.refCount);
+
+        //Create link with the corresponding VR if needed
+        compileTable[index].linkToVR (tempRegInfo.linkageToVR);
+
+        //Reset spill location because this is a new temp which has not been spilled
+        compileTable[index].resetSpillLocation ();
+    }
+}
+
+/** print infoBasicBlock of the given basic block
+*/
+void dumpVirtualInfoOfBasicBlock(BasicBlock_O1* bb) {
+    unsigned int jj, max;
+    ALOGI("Virtual Info for BB%d --------", bb->id);
+    max = bb->infoBasicBlock.size ();
+    for(jj = 0; jj < max; jj++) {
+        ALOGI("regNum %d physicalType %d accessType %d refCount %d def ",
+               bb->infoBasicBlock[jj].regNum, bb->infoBasicBlock[jj].physicalType,
+               bb->infoBasicBlock[jj].accessType, bb->infoBasicBlock[jj].refCount);
+        int k;
+        for(k = 0; k < bb->infoBasicBlock[jj].num_reaching_defs; k++)
+            ALOGI("[%x %d %d %d] ", bb->infoBasicBlock[jj].reachingDefs[k].offsetPC,
+                   bb->infoBasicBlock[jj].reachingDefs[k].regNum,
+                   bb->infoBasicBlock[jj].reachingDefs[k].physicalType,
+                   bb->infoBasicBlock[jj].reachingDefs[k].accessType);
+    }
+}
+
+/** print compileTable
+*/
+void dumpCompileTable() {
+    ALOGD("+++++++++++++++++++++ Compile Table +++++++++++++++++++++");
+    ALOGD("%d entries\t%d memory_vr\t%d const_vr", compileTable.size (),
+            num_memory_vr, num_const_vr);
+    for(int entry = 0; entry < compileTable.size (); entry++) {
+        ALOGD("regNum %d physicalType %d refCount %d physicalReg %s",
+               compileTable[entry].regNum, compileTable[entry].physicalType,
+               compileTable[entry].refCount, physicalRegToString(
+                       static_cast<PhysicalReg>(compileTable[entry].physicalReg)));
+    }
+    for(int entry = 0; entry < num_memory_vr; entry++) {
+        ALOGD("v%d inMemory:%s", memVRTable[entry].regNum,
+                memVRTable[entry].inMemory ? "yes" : "no");
+    }
+
+    for(int entry = 0; entry < num_const_vr; entry++) {
+        ALOGD("v%d isConst:%s value:%d", constVRTable[entry].regNum,
+                constVRTable[entry].isConst ? "yes" : "no",
+                constVRTable[entry].value);
+    }
+    ALOGD("---------------------------------------------------------");
+}
+
+/* BEGIN code to handle state transfers */
+//! save the current state of register allocator to a state table
+
+//!
+void rememberState(int stateNum) {
+#ifdef DEBUG_STATE
+    ALOGI("STATE: remember state %d", stateNum);
+#endif
+    int k;
+    for(k = 0; k < compileTable.size (); k++) {
+        compileTable[k].rememberState (stateNum);
+#ifdef DEBUG_STATE
+        ALOGI("logical reg %d %d mapped to physical reg %d with spill index %d refCount %d",
+               compileTable[k].regNum, compileTable[k].physicalType, compileTable[k].physicalReg,
+               compileTable[k].spill_loc_index, compileTable[k].refCount);
+#endif
+    }
+    for(k = 0; k < num_memory_vr; k++) {
+        if(stateNum == 1) {
+            stateTable2_1[k].regNum = memVRTable[k].regNum;
+            stateTable2_1[k].inMemory = memVRTable[k].inMemory;
+        }
+        else if(stateNum == 2) {
+            stateTable2_2[k].regNum = memVRTable[k].regNum;
+            stateTable2_2[k].inMemory = memVRTable[k].inMemory;
+        }
+        else if(stateNum == 3) {
+            stateTable2_3[k].regNum = memVRTable[k].regNum;
+            stateTable2_3[k].inMemory = memVRTable[k].inMemory;
+        }
+        else if(stateNum == 4) {
+            stateTable2_4[k].regNum = memVRTable[k].regNum;
+            stateTable2_4[k].inMemory = memVRTable[k].inMemory;
+        }
+        else {
+            ALOGI("JIT_INFO: state table overflow at goToState for compileTable\n");
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return;
+        }
+#ifdef DEBUG_STATE
+        ALOGI("virtual reg %d in memory %d", memVRTable[k].regNum, memVRTable[k].inMemory);
+#endif
+    }
+}
+
+//!update current state of register allocator with a state table
+
+//!
+void goToState(int stateNum) {
+    int k;
+#ifdef DEBUG_STATE
+    ALOGI("STATE: go to state %d", stateNum);
+#endif
+    for(k = 0; k < compileTable.size (); k++) {
+        compileTable[k].goToState (stateNum);
+    }
+    int retCode = updateSpillIndexUsed();
+    if (retCode < 0)
+        return;
+    syncAllRegs(); //to sync up allRegs CAN'T call freeReg here
+    //since it will change the state!!!
+    for(k = 0; k < num_memory_vr; k++) {
+        if(stateNum == 1) {
+            memVRTable[k].regNum = stateTable2_1[k].regNum;
+            memVRTable[k].inMemory = stateTable2_1[k].inMemory;
+        }
+        else if(stateNum == 2) {
+            memVRTable[k].regNum = stateTable2_2[k].regNum;
+            memVRTable[k].inMemory = stateTable2_2[k].inMemory;
+        }
+        else if(stateNum == 3) {
+            memVRTable[k].regNum = stateTable2_3[k].regNum;
+            memVRTable[k].inMemory = stateTable2_3[k].inMemory;
+        }
+        else if(stateNum == 4) {
+            memVRTable[k].regNum = stateTable2_4[k].regNum;
+            memVRTable[k].inMemory = stateTable2_4[k].inMemory;
+        }
+        else {
+            ALOGI("JIT_INFO: state table overflow at goToState for memVRTable\n");
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return;
+        }
+        DEBUG_MEMORYVR(ALOGD("Updating state of v%d %sin memory",
+                memVRTable[k].regNum, (memVRTable[k].inMemory ? "" : "NOT ")));
+    }
+}
+typedef struct TransferOrder {
+    int targetReg;
+    int targetSpill;
+    int compileIndex;
+} TransferOrder;
+#define MAX_NUM_DEST 20
+//! a source register is used as a source in transfer
+//! it can have a maximum of MAX_NUM_DEST destinations
+typedef struct SourceReg {
+    int physicalReg;
+    int num_dests; //check bound
+    TransferOrder dsts[MAX_NUM_DEST];
+} SourceReg;
+int num_src_regs = 0; //check bound
+//! physical registers that are used as a source in transfer
+//! we allow a maximum of MAX_NUM_DEST sources in a transfer
+SourceReg srcRegs[MAX_NUM_DEST];
+//! tell us whether a source register is handled already
+bool handledSrc[MAX_NUM_DEST];
+//! in what order should the source registers be handled
+int handledOrder[MAX_NUM_DEST];
+
+//! \brief insert a source register with a single destination
+//!
+//! \param srcPhysical
+//! \param targetReg
+//! \param targetSpill
+//! \param index
+//!
+//! \return -1 on error, 0 otherwise
+int insertSrcReg(int srcPhysical, int targetReg, int targetSpill, int index) {
+    int k = 0;
+    for(k = 0; k < num_src_regs; k++) {
+        if(srcRegs[k].physicalReg == srcPhysical) { //increase num_dests
+            if(srcRegs[k].num_dests >= MAX_NUM_DEST) {
+                ALOGI("JIT_INFO: Exceed number dst regs for a source reg\n");
+                SET_JIT_ERROR(kJitErrorMaxDestRegPerSource);
+                return -1;
+            }
+            srcRegs[k].dsts[srcRegs[k].num_dests].targetReg = targetReg;
+            srcRegs[k].dsts[srcRegs[k].num_dests].targetSpill = targetSpill;
+            srcRegs[k].dsts[srcRegs[k].num_dests].compileIndex = index;
+            srcRegs[k].num_dests++;
+            return 0;
+        }
+    }
+    if(num_src_regs >= MAX_NUM_DEST) {
+        ALOGI("JIT_INFO: Exceed number of source regs\n");
+        SET_JIT_ERROR(kJitErrorMaxDestRegPerSource);
+        return -1;
+    }
+    srcRegs[num_src_regs].physicalReg = srcPhysical;
+    srcRegs[num_src_regs].num_dests = 1;
+    srcRegs[num_src_regs].dsts[0].targetReg = targetReg;
+    srcRegs[num_src_regs].dsts[0].targetSpill = targetSpill;
+    srcRegs[num_src_regs].dsts[0].compileIndex = index;
+    num_src_regs++;
+    return 0;
+}
+
+//! check whether a register is a source and the source is not yet handled
+
+//!
+bool dstStillInUse(int dstReg) {
+    if(dstReg == PhysicalReg_Null) return false;
+    int k;
+    int index = -1;
+    for(k = 0; k < num_src_regs; k++) {
+        if(dstReg == srcRegs[k].physicalReg) {
+            index = k;
+            break;
+        }
+    }
+    if(index < 0) return false; //not in use
+    if(handledSrc[index]) return false; //not in use
+    return true;
+}
+
+//! construct a legal order of the source registers in this transfer
+
+//!
+void constructSrcRegs(int stateNum) {
+    int k;
+    num_src_regs = 0;
+#ifdef DEBUG_STATE
+    ALOGI("IN constructSrcRegs");
+#endif
+
+    for(k = 0; k < compileTable.size (); k++) {
+#ifdef DEBUG_STATE
+        ALOGI("logical reg %d %d mapped to physical reg %d with spill index %d refCount %d",
+               compileTable[k].regNum, compileTable[k].physicalType, compileTable[k].physicalReg,
+               compileTable[k].spill_loc_index, compileTable[k].refCount);
+#endif
+
+        int pType = compileTable[k].physicalType;
+        //ignore hardcoded logical registers
+        if((pType & LowOpndRegType_hard) != 0) continue;
+        //ignore type _fs
+        if((pType & MASK_FOR_TYPE) == LowOpndRegType_fs) continue;
+        if((pType & MASK_FOR_TYPE) == LowOpndRegType_fs_s) continue;
+
+        //GL VR refCount is zero, can't ignore
+        //L VR refCount is zero, ignore
+        //GG VR refCount is zero, can't ignore
+        //temporary refCount is zero, ignore
+
+        /* get the target state */
+        int targetReg = compileTable[k].getStatePhysicalRegister (stateNum);
+        int targetSpill = compileTable[k].getStateSpillLocation (stateNum);
+
+        /* there exists an ordering problem
+           for example:
+             for a VR, move from memory to a physical reg esi
+             for a temporary regsiter, from esi to ecx
+             if we handle VR first, content of the temporary reg. will be overwritten
+           there are 4 cases:
+             I: a variable is currently in memory and its target is in physical reg
+             II: a variable is currently in a register and its target is in memory
+             III: a variable is currently in a different register
+             IV: a variable is currently in a different memory location (for non-VRs)
+           For now, case IV is not handled since it didn't show
+        */
+        if(compileTable[k].physicalReg != targetReg &&
+           isVirtualReg(compileTable[k].physicalType)) {
+            /* handles VR for case I to III */
+
+            if(compileTable[k].physicalReg == PhysicalReg_Null && targetReg != PhysicalReg_Null) {
+                /* handles VR for case I:
+                   insert a xfer order from PhysicalReg_Null to targetReg */
+                 if (insertSrcReg(PhysicalReg_Null, targetReg, targetSpill, k) == -1)
+                     return;
+#ifdef DEBUG_STATE
+                ALOGI("insert for VR Null %d %d %d", targetReg, targetSpill, k);
+#endif
+            }
+
+            if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg != PhysicalReg_Null) {
+                /* handles VR for case III
+                   insert a xfer order from srcReg to targetReg */
+                if (insertSrcReg(compileTable[k].physicalReg, targetReg, targetSpill, k) == -1)
+                    return;
+            }
+
+            if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg == PhysicalReg_Null) {
+                /* handles VR for case II
+                   insert a xfer order from srcReg to memory */
+                if (insertSrcReg(compileTable[k].physicalReg, targetReg, targetSpill, k) == -1)
+                    return;
+            }
+        }
+
+        if(compileTable[k].physicalReg != targetReg &&
+           !isVirtualReg(compileTable[k].physicalType)) {
+            /* handles non-VR for case I to III */
+
+            if(compileTable[k].physicalReg == PhysicalReg_Null && targetReg != PhysicalReg_Null) {
+                /* handles non-VR for case I */
+                if(compileTable[k].spill_loc_index < 0) {
+                    /* this variable is freed, no need to transfer */
+#ifdef DEBUG_STATE
+                    ALOGW("in transferToState spill_loc_index is negative for temporary %d", compileTable[k].regNum);
+#endif
+                } else {
+                    /* insert a xfer order from memory to targetReg */
+#ifdef DEBUG_STATE
+                    ALOGI("insert Null %d %d %d", targetReg, targetSpill, k);
+#endif
+                    if (insertSrcReg(PhysicalReg_Null, targetReg, targetSpill, k) == -1)
+                        return;
+                }
+            }
+
+            if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg != PhysicalReg_Null) {
+                /* handles non-VR for case III
+                   insert a xfer order from srcReg to targetReg */
+                if (insertSrcReg(compileTable[k].physicalReg, targetReg, targetSpill, k) == -1)
+                    return;
+            }
+
+            if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg == PhysicalReg_Null) {
+                /* handles non-VR for case II */
+                if(targetSpill < 0) {
+                    /* this variable is freed, no need to transfer */
+#ifdef DEBUG_STATE
+                    ALOGW("in transferToState spill_loc_index is negative for temporary %d", compileTable[k].regNum);
+#endif
+                } else {
+                    /* insert a xfer order from srcReg to memory */
+                    if (insertSrcReg(compileTable[k].physicalReg, targetReg, targetSpill, k) == -1)
+                        return;
+                }
+            }
+
+        }
+    }//for compile entries
+
+    int k2;
+#ifdef DEBUG_STATE
+    for(k = 0; k < num_src_regs; k++) {
+        ALOGI("SRCREG %d: ", srcRegs[k].physicalReg);
+        for(k2 = 0; k2 < srcRegs[k].num_dests; k2++) {
+            int index = srcRegs[k].dsts[k2].compileIndex;
+            ALOGI("[%d %d %d: %d %d %d] ", srcRegs[k].dsts[k2].targetReg,
+                   srcRegs[k].dsts[k2].targetSpill, srcRegs[k].dsts[k2].compileIndex,
+                   compileTable[index].regNum, compileTable[index].physicalType,
+                   compileTable[index].spill_loc_index);
+        }
+        ALOGI("");
+    }
+#endif
+
+    /* construct an order: xfers from srcReg first, then xfers from memory */
+    int num_handled = 0;
+    int num_in_order = 0;
+    for(k = 0; k < num_src_regs; k++) {
+        if(srcRegs[k].physicalReg == PhysicalReg_Null) {
+            handledSrc[k] = true;
+            num_handled++;
+        } else {
+            handledSrc[k] = false;
+        }
+    }
+    while(num_handled < num_src_regs) {
+        int prev_handled = num_handled;
+        for(k = 0; k < num_src_regs; k++) {
+            if(handledSrc[k]) continue;
+            bool canHandleNow = true;
+            for(k2 = 0; k2 < srcRegs[k].num_dests; k2++) {
+                if(dstStillInUse(srcRegs[k].dsts[k2].targetReg)) {
+                    canHandleNow = false;
+                    break;
+                }
+            }
+            if(canHandleNow) {
+                handledSrc[k] = true;
+                num_handled++;
+                handledOrder[num_in_order] = k;
+                num_in_order++;
+            }
+        } //for k
+        if(num_handled == prev_handled) {
+            ALOGI("JIT_INFO: No progress in selecting order while in constructSrcReg");
+            SET_JIT_ERROR(kJitErrorStateTransfer);
+            return;
+        }
+    } //while
+    for(k = 0; k < num_src_regs; k++) {
+        if(srcRegs[k].physicalReg == PhysicalReg_Null) {
+            handledOrder[num_in_order] = k;
+            num_in_order++;
+        }
+    }
+    if(num_in_order != num_src_regs) {
+        ALOGI("JIT_INFO: num_in_order != num_src_regs while in constructSrcReg");
+        SET_JIT_ERROR(kJitErrorStateTransfer);
+        return;
+    }
+#ifdef DEBUG_STATE
+    ALOGI("ORDER: ");
+    for(k = 0; k < num_src_regs; k++) {
+        ALOGI("%d ", handledOrder[k]);
+    }
+#endif
+}
+//! transfer the state of register allocator to a state specified in a state table
+
+//!
+void transferToState(int stateNum) {
+    freeReg(false); //do not spill GL
+    int k;
+#ifdef DEBUG_STATE
+    ALOGI("STATE: transfer to state %d", stateNum);
+#endif
+    if(stateNum > 4 || stateNum < 1) {
+        ALOGI("JIT_INFO: State table overflow at transferToState");
+        SET_JIT_ERROR(kJitErrorStateTransfer);
+        return;
+    }
+    constructSrcRegs(stateNum);
+    int k4, k3;
+    for(k4 = 0; k4 < num_src_regs; k4++) {
+        int k2 = handledOrder[k4]; //index to srcRegs
+        for(k3 = 0; k3 < srcRegs[k2].num_dests; k3++) {
+            k = srcRegs[k2].dsts[k3].compileIndex;
+            int targetReg = srcRegs[k2].dsts[k3].targetReg;
+            int targetSpill = srcRegs[k2].dsts[k3].targetSpill;
+            if(compileTable[k].physicalReg != targetReg && isVirtualReg(compileTable[k].physicalType)) {
+                OpndSize oSize = getRegSize(compileTable[k].physicalType);
+                bool isSS = ((compileTable[k].physicalType & MASK_FOR_TYPE) == LowOpndRegType_ss);
+                if(compileTable[k].physicalReg == PhysicalReg_Null && targetReg != PhysicalReg_Null) {
+                    if(isSS)
+                        move_ss_mem_to_reg_noalloc(4*compileTable[k].regNum,
+                                                   PhysicalReg_FP, true,
+                                                   MemoryAccess_VR, compileTable[k].regNum,
+                                                   targetReg, true);
+                    else
+                        move_mem_to_reg_noalloc(oSize, 4*compileTable[k].regNum,
+                                                PhysicalReg_FP, true,
+                                                MemoryAccess_VR, compileTable[k].regNum,
+                                                targetReg, true);
+                }
+                if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg != PhysicalReg_Null) {
+                    move_reg_to_reg_noalloc((isSS ? OpndSize_64 : oSize),
+                                            compileTable[k].physicalReg, true,
+                                            targetReg, true);
+                }
+                if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg == PhysicalReg_Null) {
+                    writeBackVR(compileTable[k].regNum, (LowOpndRegType)(compileTable[k].physicalType & MASK_FOR_TYPE),
+                              compileTable[k].physicalReg);
+                }
+            } //VR
+            if(compileTable[k].physicalReg != targetReg && !isVirtualReg(compileTable[k].physicalType)) {
+                OpndSize oSize = getRegSize(compileTable[k].physicalType);
+                if(compileTable[k].physicalReg == PhysicalReg_Null && targetReg != PhysicalReg_Null) {
+                    loadFromSpillRegion(oSize, targetReg,
+                                        compileTable[k].spill_loc_index);
+                }
+                //both are not null, move from one to the other
+                if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg != PhysicalReg_Null) {
+                    move_reg_to_reg_noalloc(oSize, compileTable[k].physicalReg, true,
+                                            targetReg, true);
+                }
+                //current is not null, target is null (move from reg to memory)
+                if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg == PhysicalReg_Null) {
+                    saveToSpillRegion(oSize, compileTable[k].physicalReg, targetSpill);
+                }
+            } //temporary
+        }//for
+    }//for
+    for(k = 0; k < num_memory_vr; k++) {
+        bool targetBool = false;
+        int targetReg = -1;
+        if(stateNum == 1) {
+            targetReg = stateTable2_1[k].regNum;
+            targetBool = stateTable2_1[k].inMemory;
+        }
+        else if(stateNum == 2) {
+            targetReg = stateTable2_2[k].regNum;
+            targetBool = stateTable2_2[k].inMemory;
+        }
+        else if(stateNum == 3) {
+            targetReg = stateTable2_3[k].regNum;
+            targetBool = stateTable2_3[k].inMemory;
+        }
+        else if(stateNum == 4) {
+            targetReg = stateTable2_4[k].regNum;
+            targetBool = stateTable2_4[k].inMemory;
+        }
+        if(targetReg != memVRTable[k].regNum) {
+            ALOGI("JIT_INFO: regNum mismatch in transferToState");
+            SET_JIT_ERROR(kJitErrorStateTransfer);
+            return;
+        }
+        if(targetBool && (!memVRTable[k].inMemory)) {
+            //dump to memory, check entries in compileTable: vA gp vA xmm vA ss
+#ifdef DEBUG_STATE
+            ALOGW("inMemory mismatch for VR %d in transferToState", targetReg);
+#endif
+            bool doneXfer = false;
+
+            int index = searchCompileTable(LowOpndRegType_xmm | LowOpndRegType_virtual, targetReg);
+            if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+                writeBackVR(targetReg, LowOpndRegType_xmm, compileTable[index].physicalReg);
+                doneXfer = true;
+            } else if (index >= 0 && writeBackVRIfConstant(targetReg, LowOpndRegType_xmm) == true) {
+                doneXfer = true;
+            }
+
+            if(!doneXfer) { //vA-1, xmm
+                index = searchCompileTable(LowOpndRegType_xmm | LowOpndRegType_virtual, targetReg-1);
+                if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+                    writeBackVR(targetReg-1, LowOpndRegType_xmm, compileTable[index].physicalReg);
+                    doneXfer = true;
+                }
+                else if (index >= 0 && writeBackVRIfConstant(targetReg - 1, LowOpndRegType_xmm) == true) {
+                    doneXfer = true;
+                }
+            }
+            if(!doneXfer) { //vA gp
+                index = searchCompileTable(LowOpndRegType_gp | LowOpndRegType_virtual, targetReg);
+                if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+                    writeBackVR(targetReg, LowOpndRegType_gp, compileTable[index].physicalReg);
+                    doneXfer = true;
+                }
+                else if (index >= 0 && writeBackVRIfConstant(targetReg, LowOpndRegType_gp) == true) {
+                    doneXfer = true;
+                }
+            }
+            if(!doneXfer) { //vA, ss
+                index = searchCompileTable(LowOpndRegType_ss | LowOpndRegType_virtual, targetReg);
+                if(index >= 0 && compileTable[index].physicalReg != PhysicalReg_Null) {
+                    writeBackVR(targetReg, LowOpndRegType_ss, compileTable[index].physicalReg);
+                    doneXfer = true;
+                }
+                else if (index >= 0 && writeBackVRIfConstant(targetReg, LowOpndRegType_ss) == true) {
+                    doneXfer = true;
+                }
+            }
+            if(!doneXfer) {
+                ALOGI("JIT_INFO: Can't match inMemory state of v%d in "
+                        "transferToState.", targetReg);
+                SET_JIT_ERROR(kJitErrorStateTransfer);
+                return;
+            }
+        }
+        if((!targetBool) && memVRTable[k].inMemory) {
+            //do nothing
+        }
+    }
+#ifdef DEBUG_STATE
+    ALOGI("END transferToState %d", stateNum);
+#endif
+    goToState(stateNum);
+}
+/* END code to handle state transfers */
diff --git a/vm/compiler/codegen/x86/lightcg/AnalysisO1.h b/vm/compiler/codegen/x86/lightcg/AnalysisO1.h
new file mode 100644
index 0000000..93e20ee
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/AnalysisO1.h
@@ -0,0 +1,445 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file AnalysisO1.h
+    \brief A header file to define data structures used by register allocator & const folding
+*/
+#ifndef _DALVIK_NCG_ANALYSISO1_H
+#define _DALVIK_NCG_ANALYSISO1_H
+
+#include <set>
+#include "Dalvik.h"
+#include "enc_wrapper.h"
+#include "Lower.h"
+#ifdef WITH_JIT
+#include "compiler/CompilerIR.h"
+#endif
+#include "RegisterizationBE.h"
+
+//! maximal number of edges per basic block
+#define MAX_NUM_EDGE_PER_BB 300
+//! maximal number of virtual registers per basic block
+#define MAX_REG_PER_BASICBLOCK 140
+//! maximal number of virtual registers per bytecode
+#define MAX_REG_PER_BYTECODE 40
+//! maximal number of virtual registers per method
+#define MAX_REG_PER_METHOD 200
+//! maximal number of temporaries per bytecode
+#define MAX_TEMP_REG_PER_BYTECODE 30
+
+#define MAX_CONST_REG 150
+#define NUM_MEM_VR_ENTRY 140
+
+#define MASK_FOR_TYPE 7 //last 3 bits 111
+
+#define LOOP_COUNT 10
+
+//! maximal number of transfer points per basic block
+#define MAX_XFER_PER_BB 1000  //on Jan 4
+#define PC_FOR_END_OF_BB -999
+#define PC_FOR_START_OF_BB -998
+
+//! various cases of overlapping between 2 variables
+typedef enum OverlapCase {
+  OVERLAP_ALIGN = 0,
+  OVERLAP_B_IS_LOW_OF_A,
+  OVERLAP_B_IS_HIGH_OF_A,
+  OVERLAP_LOW_OF_A_IS_HIGH_OF_B,
+  OVERLAP_HIGH_OF_A_IS_LOW_OF_B,
+  OVERLAP_A_IS_LOW_OF_B,
+  OVERLAP_A_IS_HIGH_OF_B,
+  OVERLAP_B_COVER_A,
+  OVERLAP_B_COVER_LOW_OF_A,
+  OVERLAP_B_COVER_HIGH_OF_A,
+  OVERLAP_NO
+} OverlapCase;
+
+//!access type of a variable
+typedef enum RegAccessType {
+  REGACCESS_D = 0,
+  REGACCESS_U,
+  REGACCESS_DU,
+  REGACCESS_UD,
+  REGACCESS_L,
+  REGACCESS_H,
+  REGACCESS_UL,
+  REGACCESS_UH,
+  REGACCESS_LU,
+  REGACCESS_HU,
+  REGACCESS_N, //no access
+  REGACCESS_UNKNOWN
+} RegAccessType;
+
+//! helper state indexes to determine if freeing VRs needs to be delayed
+enum VRDelayFreeCounters {
+  VRDELAY_NULLCHECK = 0, // used when VR is used for null check and freeing must be delayed
+  VRDELAY_BOUNDCHECK = 1, // used when VR is used for bound check and freeing must be delayed
+  VRDELAY_CAN_THROW = 2, // used when bytecode can throw exception, in fact delays freeing any VR
+  VRDELAY_COUNT = 3, // Count of delay reasons
+};
+
+//!information about a physical register
+typedef struct RegisterInfo {
+  PhysicalReg physicalReg;
+  bool isUsed;
+  bool isCalleeSaved;
+  int freeTimeStamp;
+} RegisterInfo;
+
+//!specifies the weight of a VR allocated to a specific physical register
+//!it is used for GPR VR only
+typedef struct RegAllocConstraint {
+  PhysicalReg physicalReg;
+  int count;
+} RegAllocConstraint;
+
+typedef enum XferType {
+  XFER_MEM_TO_XMM, //for usage
+  XFER_DEF_TO_MEM, //def is gp
+  XFER_DEF_TO_GP_MEM,
+  XFER_DEF_TO_GP,
+  XFER_DEF_IS_XMM //def is xmm
+} XferType;
+typedef struct XferPoint {
+  int tableIndex; //generated from a def-use pair
+  XferType xtype;
+  int offsetPC;
+  int regNum; //get or set VR at offsetPC
+  LowOpndRegType physicalType;
+
+  //if XFER_DEF_IS_XMM
+  int vr_gpl; //a gp VR that uses the lower half of the def
+  int vr_gph;
+  bool dumpToXmm;
+  bool dumpToMem;
+} XferPoint;
+
+//!for def: accessType means which part of the VR defined at offestPC is live now
+//!for use: accessType means which part of the usage comes from the reachingDef
+typedef struct DefOrUse {
+  int offsetPC; //!the program point
+  int regNum; //!access the virtual reg
+  LowOpndRegType physicalType; //!xmm or gp or ss
+  RegAccessType accessType; //!D, L, H, N
+} DefOrUse;
+//!a link list of DefOrUse
+typedef struct DefOrUseLink {
+  int offsetPC;
+  int regNum; //access the virtual reg
+  LowOpndRegType physicalType; //xmm or gp
+  RegAccessType accessType; //D, L, H, N
+  struct DefOrUseLink* next;
+} DefOrUseLink;
+//!pair of def and uses
+typedef struct DefUsePair {
+  DefOrUseLink* uses;
+  DefOrUseLink* useTail;
+  int num_uses;
+  DefOrUse def;
+  struct DefUsePair* next;
+} DefUsePair;
+
+//!information associated with a virtual register
+//!the pair <regNum, physicalType> uniquely determines a variable
+typedef struct VirtualRegInfo {
+   VirtualRegInfo () : regNum (-1), physicalType (LowOpndRegType_invalid), refCount (0),
+           accessType (REGACCESS_UNKNOWN), num_reaching_defs (0)
+  {
+        //Set up allocation constraints for hardcoded registers
+        for (int reg = PhysicalReg_StartOfGPMarker; reg <= PhysicalReg_EndOfGPMarker; reg++)
+        {
+            PhysicalReg pReg = static_cast<PhysicalReg> (reg);
+
+            //For now we know of no constraints for each reg so we set count to zero
+            allocConstraints[reg].physicalReg = pReg;
+            allocConstraints[reg].count = 0;
+            allocConstraintsSorted[reg].physicalReg = pReg;
+            allocConstraintsSorted[reg].count = 0;
+        }
+  }
+
+  int regNum;
+  LowOpndRegType physicalType;
+  int refCount;
+  RegAccessType accessType;
+  RegAllocConstraint allocConstraints[PhysicalReg_EndOfGPMarker + 1];
+  RegAllocConstraint allocConstraintsSorted[PhysicalReg_EndOfGPMarker + 1];
+
+  DefOrUse reachingDefs[3]; //!reaching defs to the virtual register
+  int num_reaching_defs;
+} VirtualRegInfo;
+
+//!information of whether a VR is constant and its value
+typedef struct ConstVRInfo {
+  int regNum;
+  int value;
+  bool isConst;
+} ConstVRInfo;
+
+/**
+ * @class constInfo
+ * @brief information on 64 bit constants and their locations in a trace
+*/
+typedef struct ConstInfo {
+    int valueL;             /**< @brief The lower 32 bits of the constant */
+    int valueH;             /**< @brief The higher 32 bits of the constant */
+    int regNum;             /**< @brief The register number of the constant */
+    int offsetAddr;         /**< @brief The offset from start of instruction */
+    char* streamAddr;       /**< @brief The address of instruction in stream */
+    char* constAddr;        /**< @brief The address of the constant at the end of trace */
+    bool constAlign;        /**< @brief Decide whether to Align constAddr to 16 bytes */
+    struct ConstInfo *next; /**< @brief The pointer to the next 64 bit constant */
+} ConstInfo;
+
+#define NUM_ACCESS_IN_LIVERANGE 10
+//!specifies one live range
+typedef struct LiveRange {
+  int start;
+  int end; //inclusive
+  //all accesses in the live range
+  int num_access;
+  int num_alloc;
+  int* accessPC;
+  struct LiveRange* next;
+} LiveRange;
+typedef struct BoundCheckIndex {
+  int indexVR;
+  bool checkDone;
+} BoundCheckIndex;
+
+/**
+ * @brief Used to keep track of virtual register's in-memory state.
+ */
+typedef struct regAllocStateEntry2 {
+  int regNum;    //!< The virtual register
+  bool inMemory; //!< Whether 4-byte virtual register is in memory
+} regAllocStateEntry2;
+
+/**
+ * @class MemoryVRInfo
+ * @brief information for a virtual register such as live ranges, in memory
+ */
+typedef struct MemoryVRInfo {
+    int regNum;                   /**< @brief The register number */
+    bool inMemory;                /**< @brief Is it in memory or not */
+    bool nullCheckDone;           /**< @brief Has a null check been done for it? */
+    BoundCheckIndex boundCheck;   /**< @brief Bound check information for the VR */
+    int num_ranges;               /**< @brief Number of ranges, used as a size for ranges */
+    LiveRange* ranges;            /**< @brief Live range information for the entry */
+    int delayFreeCounters[VRDELAY_COUNT]; /**< @brief Used with indexes defined by VRDelayFreeCounters enum to delay freeing */
+
+    /**
+     * @brief Default constructor which initializes all fields but sets an invalid virtual register.
+     */
+    MemoryVRInfo (void)
+    {
+        reset ();
+    }
+
+    /**
+     * @brief Initializes all fields and sets a virtual register associated with this information.
+     */
+    MemoryVRInfo (int vR)
+    {
+        reset ();
+        regNum = vR;
+    }
+
+    /**
+     * @brief Used to reset information about the VR to default values.
+     * @details Creates a logically invalid entry.
+     */
+    void reset (void);
+
+    /**
+     * @brief Returns the virtual register represented by this entry.
+     */
+    int getVirtualRegister (void)
+    {
+        return regNum;
+    }
+
+    /**
+     * @brief Sets the virtual register represented by this entry.
+     * @param regNum The virtual register number.
+     */
+    void setVirtualRegister (int regNum)
+    {
+        this->regNum = regNum;
+    }
+
+    /**
+     * @brief Sets the in memory state of this entry which represent specific virtual register.
+     * @param inMemory The in memory state to set to this entry.
+     */
+    void setInMemoryState (bool inMemory)
+    {
+        this->inMemory = inMemory;
+    }
+} MemoryVRInfo;
+
+//!information of a temporary
+//!the pair <regNum, physicalType> uniquely determines a variable
+typedef struct TempRegInfo {
+  int regNum;
+  int physicalType;
+  int refCount;
+  int linkageToVR;
+  int versionNum;
+  bool shareWithVR; //for temp. regs updated by get_virtual_reg
+  bool is8Bit;
+} TempRegInfo;
+struct BasicBlock_O1;
+
+//Forward declaration
+struct LowOpBlockLabel;
+
+//!information associated with a basic block
+struct BasicBlock_O1 : BasicBlock {
+  int pc_start;       //!inclusive
+  int pc_end;
+  char *streamStart;        //Where the code generation started for the BasicBlock
+
+  std::vector<VirtualRegInfo> infoBasicBlock;
+
+  RegAllocConstraint allocConstraints[PhysicalReg_EndOfGPMarker+1]; //# of times a hardcoded register is used in this basic block
+  //a physical register that is used many times has a lower priority to get picked in getFreeReg
+  RegAllocConstraint allocConstraintsSorted[PhysicalReg_EndOfGPMarker+1]; //count from low to high
+
+  DefUsePair* defUseTable;
+  DefUsePair* defUseTail;
+
+  std::vector <XferPoint> xferPoints; //program points where the transfer is required
+
+  AssociationTable associationTable;    //Association table to keep track of physical registers beyond a BasicBlock
+
+  LowOpBlockLabel *label;               //Label for the BasicBlock
+
+  //Constructor
+  BasicBlock_O1 (void);
+  //Clear function: do we allocate the label (default: false)
+  void clear (bool allocateLabel = false);
+
+  //Clear and free everything
+  void freeIt (void);
+};
+
+extern MemoryVRInfo memVRTable[NUM_MEM_VR_ENTRY];
+extern int num_memory_vr;
+extern TempRegInfo infoByteCodeTemp[MAX_TEMP_REG_PER_BYTECODE];
+extern int num_temp_regs_per_bytecode;
+extern VirtualRegInfo infoMethod[MAX_REG_PER_METHOD];
+extern int num_regs_per_method;
+extern BasicBlock_O1* currentBB;
+
+extern int num_const_vr;
+extern ConstVRInfo constVRTable[MAX_CONST_REG];
+
+/**
+ * @brief Provides a mapping between physical type and the size represented.
+ * @param type The physical type represented by LowOpndRegType.
+ * @return Returns size represented by the physical type.
+ */
+OpndSize getRegSize (int type);
+
+void forwardAnalysis(int type);
+
+//functions in bc_visitor.c
+int getConstInfo(BasicBlock_O1* bb, const MIR * currentMIR);
+int getVirtualRegInfo(VirtualRegInfo* infoArray, const MIR * currentMIR, bool updateBBConstraints = false);
+int getTempRegInfo(TempRegInfo* infoArray, const MIR * currentMIR);
+int createCFGHandler(Method* method);
+
+int findVirtualRegInTable(int vA, LowOpndRegType type);
+int searchCompileTable(int type, int regNum);
+void handleJump(BasicBlock_O1* bb_prev, int relOff);
+void connectBasicBlock(BasicBlock_O1* src, BasicBlock_O1* dst);
+int insertWorklist(BasicBlock_O1* bb_prev, int targetOff);
+
+//Collects virtual register usage for BB and sets up defuse tables
+int collectInfoOfBasicBlock (BasicBlock_O1* bb);
+
+void updateCurrentBBWithConstraints(PhysicalReg reg);
+void updateConstInfo(BasicBlock_O1*);
+void invalidateVRDueToConst(int reg, OpndSize size);
+
+//Set a VR to a constant value
+bool setVRToConst(int regNum, OpndSize size, int* tmpValue);
+
+//Set that VR is not a constant
+void setVRToNonConst(int regNum, OpndSize size);
+
+/**
+ * @brief Used the represent the constantness of a virtual register.
+ */
+enum VirtualRegConstantness
+{
+    VR_IS_NOT_CONSTANT = 0,  //!< virtual register is not constant
+    VR_LOW_IS_CONSTANT = 1,  //!< only the low 32-bit of virtual register is constant
+    VR_HIGH_IS_CONSTANT = 2, //!< only the high 32-bit of virtual register is constant
+    VR_IS_CONSTANT = 3       //!< virtual register is entirely constant
+};
+
+//Checks if VR is constant
+VirtualRegConstantness isVirtualRegConstant (int regNum, int opndRegType, int *valuePtr, bool updateRef = false);
+
+//If VR is dirty, it writes the constant value to the VR on stack
+void writeBackConstVR (int vR, int value);
+
+//Writes virtual register back to memory if it holds a constant value
+bool writeBackVRIfConstant (int vR, LowOpndRegType type);
+
+//Write back virtual register to memory when it is in given physical register
+void writeBackVR (int vR, LowOpndRegType type, int physicalReg);
+
+//Check if VR is in memory
+bool isInMemory(int regNum, OpndSize size);
+
+//Update the in memory state of the virtual register
+void setVRMemoryState (int vR, OpndSize size, bool inMemory);
+
+//Find free registers and update the set
+void findFreeRegisters (std::set<PhysicalReg> &outFreeRegisters, bool includeGPs = true, bool includeXMMs = true);
+
+//Get a scratch register of a given type
+PhysicalReg getScratch(const std::set<PhysicalReg> &scratchCandidates, LowOpndRegType type);
+
+//Synchronize all registers in the compileTable
+void syncAllRegs(void);
+
+//Get a type for a given register
+LowOpndRegType getTypeOfRegister(PhysicalReg reg);
+
+//Update the physical register in the compile table from oldReg to newReg
+bool updatePhysicalRegForVR(int vR, PhysicalReg oldReg, PhysicalReg newReg);
+
+//Check whether a type is a virtual register type
+bool isVirtualReg(int type);
+
+//Spill a logical register using a index in the compileTable
+int spillLogicalReg(int spill_index, bool updateTable);
+
+//Search in the memory table for a register
+int searchMemTable(int regNum);
+
+//Adds a virtual register to the memory table
+bool addToMemVRTable (int vR, bool inMemory);
+
+//! check whether the current bytecode is the last access to a VR within a live
+bool isLastByteCodeOfLiveRange(int compileIndex);
+#endif
+
diff --git a/vm/compiler/codegen/x86/lightcg/BytecodeVisitor.cpp b/vm/compiler/codegen/x86/lightcg/BytecodeVisitor.cpp
new file mode 100644
index 0000000..2342613
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/BytecodeVisitor.cpp
@@ -0,0 +1,6421 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file BytecodeVisitor.cpp
+    \brief This file implements visitors of the bytecode
+*/
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "AnalysisO1.h"
+
+#if 0 /* This is dead code and has been disabled. If reenabling,
+         the MIR or opcode must be passed in as a parameter */
+//! Returns size of the current bytecode in u2 unit
+
+//!
+int getByteCodeSize() { //uses inst, unit in u2
+    switch (INST_INST(inst)) {
+    case OP_NOP:
+        return 1;
+    case OP_MOVE:
+    case OP_MOVE_OBJECT:
+        return 1;
+    case OP_MOVE_FROM16:
+    case OP_MOVE_OBJECT_FROM16:
+        return 2;
+    case OP_MOVE_16:
+    case OP_MOVE_OBJECT_16:
+        return 3;
+    case OP_MOVE_WIDE:
+        return 1;
+    case OP_MOVE_WIDE_FROM16:
+        return 2;
+    case OP_MOVE_WIDE_16:
+        return 3;
+    case OP_MOVE_RESULT:
+    case OP_MOVE_RESULT_OBJECT:
+        return 1;
+    case OP_MOVE_RESULT_WIDE:
+        return 1;
+    case OP_MOVE_EXCEPTION:
+        return 1;
+    case OP_RETURN_VOID:
+    case OP_RETURN_VOID_BARRIER:
+        return 1;
+    case OP_RETURN:
+    case OP_RETURN_OBJECT:
+        return 1;
+    case OP_RETURN_WIDE:
+        return 1;
+    case OP_CONST_4:
+        return 1;
+    case OP_CONST_16:
+        return 2;
+    case OP_CONST:
+        return 3;
+    case OP_CONST_HIGH16:
+        return 2;
+    case OP_CONST_WIDE_16:
+        return 2;
+    case OP_CONST_WIDE_32:
+        return 3;
+    case OP_CONST_WIDE:
+        return 5;
+    case OP_CONST_WIDE_HIGH16:
+        return 2;
+    case OP_CONST_STRING:
+        return 2;
+    case OP_CONST_STRING_JUMBO:
+        return 3;
+    case OP_CONST_CLASS:
+        return 2;
+    case OP_MONITOR_ENTER:
+        return 1;
+    case OP_MONITOR_EXIT:
+        return 1;
+    case OP_CHECK_CAST:
+        return 2;
+    case OP_INSTANCE_OF:
+        return 2;
+    case OP_ARRAY_LENGTH:
+        return 1;
+    case OP_NEW_INSTANCE:
+        return 2;
+    case OP_NEW_ARRAY:
+        return 2;
+    case OP_FILLED_NEW_ARRAY:
+        return 3;
+    case OP_FILLED_NEW_ARRAY_RANGE:
+        return 3;
+    case OP_FILL_ARRAY_DATA:
+        return 3;
+    case OP_THROW:
+        return 1;
+    case OP_THROW_VERIFICATION_ERROR:
+        return 2;
+    case OP_GOTO:
+        return 1;
+    case OP_GOTO_16:
+        return 2;
+    case OP_GOTO_32:
+        return 3;
+    case OP_PACKED_SWITCH:
+        return 3;
+    case OP_SPARSE_SWITCH:
+        return 3;
+    case OP_CMPL_FLOAT:
+        return 2;
+    case OP_CMPG_FLOAT:
+        return 2;
+    case OP_CMPL_DOUBLE:
+        return 2;
+    case OP_CMPG_DOUBLE:
+        return 2;
+    case OP_CMP_LONG:
+        return 2;
+    case OP_IF_EQ:
+        return 2;
+    case OP_IF_NE:
+        return 2;
+    case OP_IF_LT:
+        return 2;
+    case OP_IF_GE:
+        return 2;
+    case OP_IF_GT:
+        return 2;
+    case OP_IF_LE:
+        return 2;
+    case OP_IF_EQZ:
+        return 2;
+    case OP_IF_NEZ:
+        return 2;
+    case OP_IF_LTZ:
+        return 2;
+    case OP_IF_GEZ:
+        return 2;
+    case OP_IF_GTZ:
+        return 2;
+    case OP_IF_LEZ:
+        return 2;
+    case OP_AGET:
+        return 2;
+    case OP_AGET_WIDE:
+        return 2;
+    case OP_AGET_OBJECT:
+        return 2;
+    case OP_AGET_BOOLEAN:
+        return 2;
+    case OP_AGET_BYTE:
+        return 2;
+    case OP_AGET_CHAR:
+        return 2;
+    case OP_AGET_SHORT:
+        return 2;
+    case OP_APUT:
+        return 2;
+    case OP_APUT_WIDE:
+        return 2;
+    case OP_APUT_OBJECT:
+        return 2;
+    case OP_APUT_BOOLEAN:
+        return 2;
+    case OP_APUT_BYTE:
+        return 2;
+    case OP_APUT_CHAR:
+        return 2;
+    case OP_APUT_SHORT:
+        return 2;
+    case OP_IGET:
+    case OP_IGET_WIDE:
+    case OP_IGET_OBJECT:
+    case OP_IGET_VOLATILE:
+    case OP_IGET_WIDE_VOLATILE:
+    case OP_IGET_OBJECT_VOLATILE:
+    case OP_IGET_BOOLEAN:
+    case OP_IGET_BYTE:
+    case OP_IGET_CHAR:
+    case OP_IGET_SHORT:
+    case OP_IPUT:
+    case OP_IPUT_WIDE:
+    case OP_IPUT_OBJECT:
+    case OP_IPUT_VOLATILE:
+    case OP_IPUT_WIDE_VOLATILE:
+    case OP_IPUT_OBJECT_VOLATILE:
+    case OP_IPUT_BOOLEAN:
+    case OP_IPUT_BYTE:
+    case OP_IPUT_CHAR:
+    case OP_IPUT_SHORT:
+        return 2;
+    case OP_SGET:
+    case OP_SGET_WIDE:
+    case OP_SGET_OBJECT:
+    case OP_SGET_VOLATILE:
+    case OP_SGET_WIDE_VOLATILE:
+    case OP_SGET_OBJECT_VOLATILE:
+    case OP_SGET_BOOLEAN:
+    case OP_SGET_BYTE:
+    case OP_SGET_CHAR:
+    case OP_SGET_SHORT:
+    case OP_SPUT:
+    case OP_SPUT_WIDE:
+    case OP_SPUT_OBJECT:
+    case OP_SPUT_VOLATILE:
+    case OP_SPUT_WIDE_VOLATILE:
+    case OP_SPUT_OBJECT_VOLATILE:
+    case OP_SPUT_BOOLEAN:
+    case OP_SPUT_BYTE:
+    case OP_SPUT_CHAR:
+    case OP_SPUT_SHORT:
+        return 2;
+    case OP_INVOKE_VIRTUAL:
+    case OP_INVOKE_SUPER:
+    case OP_INVOKE_DIRECT:
+    case OP_INVOKE_STATIC:
+    case OP_INVOKE_INTERFACE:
+    case OP_INVOKE_VIRTUAL_RANGE:
+    case OP_INVOKE_SUPER_RANGE:
+    case OP_INVOKE_DIRECT_RANGE:
+    case OP_INVOKE_STATIC_RANGE:
+    case OP_INVOKE_INTERFACE_RANGE:
+        return 3;
+
+    case OP_NEG_INT:
+    case OP_NOT_INT:
+    case OP_NEG_LONG:
+    case OP_NOT_LONG:
+    case OP_NEG_FLOAT:
+    case OP_NEG_DOUBLE:
+    case OP_INT_TO_LONG:
+    case OP_INT_TO_FLOAT:
+    case OP_INT_TO_DOUBLE:
+    case OP_LONG_TO_INT:
+    case OP_LONG_TO_FLOAT:
+    case OP_LONG_TO_DOUBLE:
+    case OP_FLOAT_TO_INT:
+    case OP_FLOAT_TO_LONG:
+    case OP_FLOAT_TO_DOUBLE:
+    case OP_DOUBLE_TO_INT:
+    case OP_DOUBLE_TO_LONG:
+    case OP_DOUBLE_TO_FLOAT:
+    case OP_INT_TO_BYTE:
+    case OP_INT_TO_CHAR:
+    case OP_INT_TO_SHORT:
+        return 1;
+
+    case OP_ADD_INT:
+    case OP_SUB_INT:
+    case OP_MUL_INT:
+    case OP_DIV_INT:
+    case OP_REM_INT:
+    case OP_AND_INT:
+    case OP_OR_INT:
+    case OP_XOR_INT:
+    case OP_SHL_INT:
+    case OP_SHR_INT:
+    case OP_USHR_INT:
+    case OP_ADD_LONG:
+    case OP_SUB_LONG:
+    case OP_MUL_LONG:
+    case OP_DIV_LONG:
+    case OP_REM_LONG:
+    case OP_AND_LONG:
+    case OP_OR_LONG:
+    case OP_XOR_LONG:
+    case OP_SHL_LONG:
+    case OP_SHR_LONG:
+    case OP_USHR_LONG:
+    case OP_ADD_FLOAT:
+    case OP_SUB_FLOAT:
+    case OP_MUL_FLOAT:
+    case OP_DIV_FLOAT:
+    case OP_REM_FLOAT:
+    case OP_ADD_DOUBLE:
+    case OP_SUB_DOUBLE:
+    case OP_MUL_DOUBLE:
+    case OP_DIV_DOUBLE:
+    case OP_REM_DOUBLE:
+        return 2;
+
+    case OP_ADD_INT_2ADDR:
+    case OP_SUB_INT_2ADDR:
+    case OP_MUL_INT_2ADDR:
+    case OP_DIV_INT_2ADDR:
+    case OP_REM_INT_2ADDR:
+    case OP_AND_INT_2ADDR:
+    case OP_OR_INT_2ADDR:
+    case OP_XOR_INT_2ADDR:
+    case OP_SHL_INT_2ADDR:
+    case OP_SHR_INT_2ADDR:
+    case OP_USHR_INT_2ADDR:
+    case OP_ADD_LONG_2ADDR:
+    case OP_SUB_LONG_2ADDR:
+    case OP_MUL_LONG_2ADDR:
+    case OP_DIV_LONG_2ADDR:
+    case OP_REM_LONG_2ADDR:
+    case OP_AND_LONG_2ADDR:
+    case OP_OR_LONG_2ADDR:
+    case OP_XOR_LONG_2ADDR:
+    case OP_SHL_LONG_2ADDR:
+    case OP_SHR_LONG_2ADDR:
+    case OP_USHR_LONG_2ADDR:
+    case OP_ADD_FLOAT_2ADDR:
+    case OP_SUB_FLOAT_2ADDR:
+    case OP_MUL_FLOAT_2ADDR:
+    case OP_DIV_FLOAT_2ADDR:
+    case OP_REM_FLOAT_2ADDR:
+    case OP_ADD_DOUBLE_2ADDR:
+    case OP_SUB_DOUBLE_2ADDR:
+    case OP_MUL_DOUBLE_2ADDR:
+    case OP_DIV_DOUBLE_2ADDR:
+    case OP_REM_DOUBLE_2ADDR:
+        return 1;
+
+    case OP_ADD_INT_LIT16:
+    case OP_RSUB_INT:
+    case OP_MUL_INT_LIT16:
+    case OP_DIV_INT_LIT16:
+    case OP_REM_INT_LIT16:
+    case OP_AND_INT_LIT16:
+    case OP_OR_INT_LIT16:
+    case OP_XOR_INT_LIT16:
+        return 2;
+
+    case OP_ADD_INT_LIT8:
+    case OP_RSUB_INT_LIT8:
+    case OP_MUL_INT_LIT8:
+    case OP_DIV_INT_LIT8:
+    case OP_REM_INT_LIT8:
+    case OP_AND_INT_LIT8:
+    case OP_OR_INT_LIT8:
+    case OP_XOR_INT_LIT8:
+    case OP_SHL_INT_LIT8:
+    case OP_SHR_INT_LIT8:
+    case OP_USHR_INT_LIT8:
+        return 2;
+
+    case OP_EXECUTE_INLINE:
+    case OP_EXECUTE_INLINE_RANGE:
+        return 3;
+#if FIXME
+    case OP_INVOKE_OBJECT_INIT_RANGE:
+        return 3;
+#endif
+
+    case OP_IGET_QUICK:
+    case OP_IGET_WIDE_QUICK:
+    case OP_IGET_OBJECT_QUICK:
+    case OP_IPUT_QUICK:
+    case OP_IPUT_WIDE_QUICK:
+    case OP_IPUT_OBJECT_QUICK:
+        return 2;
+
+    case OP_INVOKE_VIRTUAL_QUICK:
+    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
+    case OP_INVOKE_SUPER_QUICK:
+    case OP_INVOKE_SUPER_QUICK_RANGE:
+        return 3;
+#ifdef SUPPORT_HLO
+    case kExtInstruction:
+        switch(inst) {
+        case OP_X_AGET_QUICK:
+        case OP_X_AGET_WIDE_QUICK:
+        case OP_X_AGET_OBJECT_QUICK:
+    case OP_X_AGET_BOOLEAN_QUICK:
+    case OP_X_AGET_BYTE_QUICK:
+    case OP_X_AGET_CHAR_QUICK:
+    case OP_X_AGET_SHORT_QUICK:
+    case OP_X_APUT_QUICK:
+    case OP_X_APUT_WIDE_QUICK:
+    case OP_X_APUT_OBJECT_QUICK:
+    case OP_X_APUT_BOOLEAN_QUICK:
+    case OP_X_APUT_BYTE_QUICK:
+    case OP_X_APUT_CHAR_QUICK:
+    case OP_X_APUT_SHORT_QUICK:
+        return 3;
+    case OP_X_DEREF_GET:
+    case OP_X_DEREF_GET_OBJECT:
+    case OP_X_DEREF_GET_WIDE:
+    case OP_X_DEREF_GET_BOOLEAN:
+    case OP_X_DEREF_GET_BYTE:
+    case OP_X_DEREF_GET_CHAR:
+    case OP_X_DEREF_GET_SHORT:
+    case OP_X_DEREF_PUT:
+    case OP_X_DEREF_PUT_WIDE:
+    case OP_X_DEREF_PUT_OBJECT:
+    case OP_X_DEREF_PUT_BOOLEAN:
+    case OP_X_DEREF_PUT_BYTE:
+    case OP_X_DEREF_PUT_CHAR:
+    case OP_X_DEREF_PUT_SHORT:
+        return 2;
+    case OP_X_ARRAY_CHECKS:
+    case OP_X_ARRAY_OBJECT_CHECKS:
+        return 3;
+    case OP_X_CHECK_BOUNDS:
+    case OP_X_CHECK_NULL:
+    case OP_X_CHECK_TYPE:
+        return 2;
+    }
+#endif
+    default:
+        ALOGI("JIT_INFO: JIT does not support getting size of bytecode 0x%hx\n",
+                currentMIR->dalvikInsn.opcode);
+        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+        assert(false && "All opcodes should be supported.");
+        break;
+    }
+    return -1;
+}
+#endif
+
+//! \brief reduces refCount of a virtual register
+//!
+//! \param vA
+//! \param type
+//!
+//! \return -1 on error, 0 otherwise
+static int touchOneVR(int vA, LowOpndRegType type) {
+    int index = searchCompileTable(LowOpndRegType_virtual | type, vA);
+    if(index < 0) {
+        ALOGI("JIT_INFO: virtual reg %d type %d not found in touchOneVR\n", vA, type);
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return -1;
+    }
+    compileTable[index].refCount--;
+    return 0;
+}
+
+/** @brief count of const in worklist  */
+int num_const_worklist;
+
+/** @brief  worklist to update constVRTable later */
+int constWorklist[10];
+
+/**
+  * @brief Clears the list registers with killed constants
+  */
+static void clearConstKills(void) {
+    num_const_worklist = 0;
+}
+
+/**
+  * @brief  Adds a register for which any previous constant
+  *  held is killed by an operation.
+  * @param v a virtual register
+  */
+static void addConstKill(u2 v) {
+    constWorklist[num_const_worklist++] = v;
+}
+
+int num_const_vr; //in a basic block
+//! table to store the constant information for virtual registers
+ConstVRInfo constVRTable[MAX_CONST_REG];
+//! update constVRTable for a given virtual register
+
+//! set "isConst" to false
+void setVRToNonConst(int regNum, OpndSize size) {
+    int k;
+    int indexL = -1;
+    int indexH = -1;
+    for(k = 0; k < num_const_vr; k++) {
+        if(constVRTable[k].regNum == regNum) {
+            indexL = k;
+            continue;
+        }
+        if(constVRTable[k].regNum == regNum + 1 && size == OpndSize_64) {
+            indexH = k;
+            continue;
+        }
+    }
+    if(indexL >= 0) {
+        //remove this entry??
+        constVRTable[indexL].isConst = false;
+    }
+    if(size == OpndSize_64 && indexH >= 0) {
+        constVRTable[indexH].isConst = false;
+    }
+}
+
+/**
+ * @brief Marks a virtual register as being constant.
+ * @param regNum The virtual register number.
+ * @param size The size of the virtual register.
+ * @param tmpValue Array representing the constant values for this VR. It must be the correct size to match
+ * the size argument.
+ * @return Returns true if setting VR to constant succeeded. On failure it returns false.
+ */
+bool setVRToConst (int regNum, OpndSize size, int *tmpValue)
+{
+    assert (tmpValue != 0);
+
+    int k;
+    int indexL = -1;
+    int indexH = -1;
+    for(k = 0; k < num_const_vr; k++) {
+        if(constVRTable[k].regNum == regNum) {
+            indexL = k;
+            continue;
+        }
+        if(constVRTable[k].regNum == regNum + 1 && size == OpndSize_64) {
+            indexH = k;
+            continue;
+        }
+    }
+
+    //Add the entry for the VR to the table if we don't have it
+    if(indexL < 0)
+    {
+        //Now check for possible table overflow. If we don't have an entry for this,
+        //then we must add it. Check now for possible overflow.
+        if (num_const_vr >= MAX_CONST_REG)
+        {
+            ALOGI("JIT_INFO: constVRTable overflows at setVRToConst.");
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return false;
+        }
+
+        indexL = num_const_vr;
+        constVRTable[indexL].regNum = regNum;
+        num_const_vr++;
+    }
+
+    //Now initialize the entry with the constant value
+    constVRTable[indexL].isConst = true;
+    constVRTable[indexL].value = tmpValue[0];
+
+    //If we have a 64-bit VR, we must also initialize the high bits
+    if (size == OpndSize_64)
+    {
+        //Add entry to the table if we don't have it
+        if (indexH < 0)
+        {
+            //Now check for possible table overflow. If we don't have an entry for this,
+            //then we must add it. Check now for possible overflow.
+            if (num_const_vr >= MAX_CONST_REG)
+            {
+                ALOGI("JIT_INFO: constVRTable overflows at setVRToConst.");
+                SET_JIT_ERROR(kJitErrorRegAllocFailed);
+                return false;
+            }
+
+            indexH = num_const_vr;
+            constVRTable[indexH].regNum = regNum + 1;
+            num_const_vr++;
+        }
+
+        //Now initialize the entry with the constant value
+        constVRTable[indexH].isConst = true;
+        constVRTable[indexH].value = tmpValue[1];
+    }
+
+    //This VR just became a constant so invalidate other information we have about it
+    invalidateVRDueToConst (regNum, size);
+
+    //If we make it here we were successful
+    return true;
+}
+
+//! perform work on constWorklist
+
+//!
+void updateConstInfo(BasicBlock_O1* bb) {
+    if(bb == NULL) return;
+    int k;
+    for(k = 0; k < num_const_worklist; k++) {
+        //int indexOrig = constWorklist[k];
+        //compileTable[indexOrig].isConst = false;
+        //int A = compileTable[indexOrig].regNum;
+        //LowOpndRegType type = compileTable[indexOrig].physicalType & MASK_FOR_TYPE;
+        setVRToNonConst(constWorklist[k], OpndSize_32);
+    }
+}
+
+//! \brief check whether the current bytecode generates a const
+//!
+//! \details if yes, update constVRTable; otherwise, update constWorklist
+//! if a bytecode uses vA (const), and updates vA to non const, getConstInfo
+//! will return 0 and update constWorklist to make sure when lowering the
+//! bytecode, vA is treated as constant
+//!
+//! \param bb the BasicBlock_O1 to analyze
+//! \param currentMIR
+//!
+//! \return 1 if the bytecode generates a const, 0 otherwise, and -1 if an
+//! error occured.
+int getConstInfo(BasicBlock_O1* bb, const MIR * currentMIR) {
+    //retCode and success are used to keep track of success of function calls from this function
+    int retCode = 0;
+    bool success = false;
+
+    Opcode inst_op = currentMIR->dalvikInsn.opcode;
+    int vA = 0, vB = 0, v1, v2;
+    u2 BBBB;
+    u2 tmp_u2;
+    s4 tmp_s4;
+    u4 tmp_u4;
+    int entry, tmpValue[2], tmpValue2[2];
+
+    clearConstKills ();
+
+    /* A bytecode with the MIR_INLINED op will be treated as
+     * no-op during codegen */
+    if (currentMIR->OptimizationFlags & MIR_INLINED)
+        return 0; // does NOT generate a constant
+
+    // Check if we need to handle an extended MIR
+    if (currentMIR->dalvikInsn.opcode >= static_cast<Opcode> (kMirOpFirst)) {
+        //Currently no extended MIR generates constants
+        switch (static_cast<ExtendedMIROpcode>(currentMIR->dalvikInsn.opcode)) {
+            default:
+                // No constant is generated
+                return 0;
+        }
+    }
+
+    switch(inst_op) {
+        //for other opcode, if update the register, set isConst to false
+    case OP_MOVE:
+    case OP_MOVE_OBJECT:
+    case OP_MOVE_FROM16:
+    case OP_MOVE_OBJECT_FROM16:
+    case OP_MOVE_16:
+    case OP_MOVE_OBJECT_16:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
+            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+            if (entry < 0)
+                return -1;
+
+            success = setVRToConst(vA, OpndSize_32, tmpValue);
+            if (success == false)
+            {
+                //setVRToConst set an error message when it failed so we just pass along the failure information
+                return -1;
+            }
+
+            compileTable[entry].refCount--;
+            retCode = touchOneVR(vB, LowOpndRegType_gp);
+            if (retCode < 0)
+                return retCode;
+            return 1;
+        } else {
+            addConstKill(vA);
+        }
+        return 0;
+    case OP_MOVE_WIDE:
+    case OP_MOVE_WIDE_FROM16:
+    case OP_MOVE_WIDE_16:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        if(isVirtualRegConstant(vB, LowOpndRegType_xmm, tmpValue, false) == 3) {
+            entry = findVirtualRegInTable(vA, LowOpndRegType_xmm);
+            if (entry < 0)
+                return -1;
+
+            success = setVRToConst(vA, OpndSize_64, tmpValue);
+            if (success == false)
+            {
+                //setVRToConst set an error message when it failed so we just pass along the failure information
+                return -1;
+            }
+
+            compileTable[entry].refCount--;
+            retCode = touchOneVR(vB, LowOpndRegType_xmm);
+            if (retCode < 0)
+                return retCode;
+            return 1;
+        } else {
+            addConstKill(vA);
+            addConstKill(vA+1);
+        }
+        return 0;
+    case OP_MOVE_RESULT:
+    case OP_MOVE_RESULT_OBJECT:
+    case OP_MOVE_EXCEPTION:
+    case OP_CONST_STRING:
+    case OP_CONST_STRING_JUMBO:
+    case OP_CONST_CLASS:
+    case OP_NEW_INSTANCE:
+    case OP_CMPL_FLOAT:
+    case OP_CMPG_FLOAT:
+    case OP_CMPL_DOUBLE:
+    case OP_CMPG_DOUBLE:
+    case OP_AGET:
+    case OP_AGET_OBJECT:
+    case OP_AGET_BOOLEAN:
+    case OP_AGET_BYTE:
+    case OP_AGET_CHAR:
+    case OP_AGET_SHORT:
+    case OP_SGET:
+    case OP_SGET_OBJECT:
+    case OP_SGET_VOLATILE:
+    case OP_SGET_OBJECT_VOLATILE:
+    case OP_SGET_BOOLEAN:
+    case OP_SGET_BYTE:
+    case OP_SGET_CHAR:
+    case OP_SGET_SHORT:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        return 0;
+    case OP_MOVE_RESULT_WIDE:
+    case OP_AGET_WIDE:
+    case OP_SGET_WIDE:
+    case OP_SGET_WIDE_VOLATILE:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+    case OP_INSTANCE_OF:
+    case OP_ARRAY_LENGTH:
+    case OP_NEW_ARRAY:
+    case OP_IGET:
+    case OP_IGET_OBJECT:
+    case OP_IGET_VOLATILE:
+    case OP_IGET_OBJECT_VOLATILE:
+    case OP_IGET_BOOLEAN:
+    case OP_IGET_BYTE:
+    case OP_IGET_CHAR:
+    case OP_IGET_SHORT:
+    case OP_IGET_QUICK:
+    case OP_IGET_OBJECT_QUICK:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        return 0;
+    case OP_IGET_WIDE:
+    case OP_IGET_WIDE_VOLATILE:
+    case OP_IGET_WIDE_QUICK:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+        //TODO: constant folding for float/double/long ALU
+    case OP_ADD_FLOAT:
+    case OP_SUB_FLOAT:
+    case OP_MUL_FLOAT:
+    case OP_DIV_FLOAT:
+    case OP_REM_FLOAT:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        return 0;
+    case OP_ADD_DOUBLE:
+    case OP_SUB_DOUBLE:
+    case OP_MUL_DOUBLE:
+    case OP_DIV_DOUBLE:
+    case OP_REM_DOUBLE:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+    case OP_NEG_FLOAT:
+    case OP_INT_TO_FLOAT:
+    case OP_LONG_TO_FLOAT:
+    case OP_FLOAT_TO_INT:
+    case OP_DOUBLE_TO_INT:
+    case OP_ADD_FLOAT_2ADDR:
+    case OP_SUB_FLOAT_2ADDR:
+    case OP_MUL_FLOAT_2ADDR:
+    case OP_DIV_FLOAT_2ADDR:
+    case OP_REM_FLOAT_2ADDR:
+    case OP_DOUBLE_TO_FLOAT:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        return 0;
+    case OP_FLOAT_TO_LONG:
+    case OP_DOUBLE_TO_LONG:
+    case OP_FLOAT_TO_DOUBLE:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+    case OP_NEG_DOUBLE:
+    case OP_INT_TO_DOUBLE: //fp stack
+    case OP_LONG_TO_DOUBLE:
+    case OP_ADD_DOUBLE_2ADDR:
+    case OP_SUB_DOUBLE_2ADDR:
+    case OP_MUL_DOUBLE_2ADDR:
+    case OP_DIV_DOUBLE_2ADDR:
+    case OP_REM_DOUBLE_2ADDR:
+        //ops on float, double
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+    case OP_NEG_INT:
+    case OP_NOT_INT:
+    case OP_LONG_TO_INT:
+    case OP_INT_TO_BYTE:
+    case OP_INT_TO_CHAR:
+    case OP_INT_TO_SHORT:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
+            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+            if (entry < 0)
+            {
+                return -1;
+            }
+
+            switch (inst_op)
+            {
+                case OP_NEG_INT:
+                    tmpValue[0] = -tmpValue[0];
+                    break;
+                case OP_NOT_INT:
+                    tmpValue[0] = ~tmpValue[0]; //CHECK
+                    break;
+                case OP_LONG_TO_INT:
+                    //Nothing to do for long to int to convert value
+                    break;
+                case OP_INT_TO_BYTE: // sar
+                    tmpValue[0] = (tmpValue[0] << 24) >> 24;
+                    break;
+                case OP_INT_TO_CHAR: //shr
+                    tmpValue[0] = ((unsigned int) (tmpValue[0] << 16)) >> 16;
+                    break;
+                case OP_INT_TO_SHORT: //sar
+                    tmpValue[0] = (tmpValue[0] << 16) >> 16;
+                    break;
+                default:
+                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
+                    SET_JIT_ERROR (kJitErrorConstantFolding);
+                    return -1;
+            }
+
+            success = setVRToConst(vA, OpndSize_32, tmpValue);
+            if (success == false)
+            {
+                //setVRToConst set an error message when it failed so we just pass along the failure information
+                return -1;
+            }
+
+            compileTable[entry].refCount--;
+            retCode = touchOneVR(vB, LowOpndRegType_gp);
+            if (retCode < 0)
+                return retCode;
+#ifdef DEBUG_CONST
+            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
+#endif
+            return 1;
+        }
+        else {
+            addConstKill(vA);
+            return 0;
+        }
+    case OP_NEG_LONG:
+    case OP_NOT_LONG:
+    case OP_INT_TO_LONG:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+    case OP_DIV_INT_2ADDR:
+    case OP_REM_INT_2ADDR:
+    case OP_REM_INT_LIT16:
+    case OP_DIV_INT_LIT16:
+    case OP_REM_INT_LIT8:
+    case OP_DIV_INT_LIT8:
+    case OP_DIV_INT:
+    case OP_REM_INT:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        return 0;
+    case OP_ADD_INT_2ADDR:
+    case OP_SUB_INT_2ADDR:
+    case OP_MUL_INT_2ADDR:
+    case OP_AND_INT_2ADDR:
+    case OP_OR_INT_2ADDR:
+    case OP_XOR_INT_2ADDR:
+    case OP_SHL_INT_2ADDR:
+    case OP_SHR_INT_2ADDR:
+    case OP_USHR_INT_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        if(isVirtualRegConstant(vA, LowOpndRegType_gp, tmpValue, false) == 3 &&
+           isVirtualRegConstant(v2, LowOpndRegType_gp, tmpValue2, false) == 3) {
+            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+            if (entry < 0)
+            {
+                return -1;
+            }
+
+            switch (inst_op)
+            {
+                case OP_ADD_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] + tmpValue2[0];
+                    break;
+                case OP_SUB_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] - tmpValue2[0];
+                    break;
+                case OP_MUL_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] * tmpValue2[0];
+                    break;
+                case OP_DIV_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] / tmpValue2[0];
+                    break;
+                case OP_REM_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] % tmpValue2[0];
+                    break;
+                case OP_AND_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] & tmpValue2[0];
+                    break;
+                case OP_OR_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] | tmpValue2[0];
+                    break;
+                case OP_XOR_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] ^ tmpValue2[0];
+                    break;
+                case OP_SHL_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] << tmpValue2[0];
+                    break;
+                case OP_SHR_INT_2ADDR:
+                    tmpValue[0] = tmpValue[0] >> tmpValue2[0];
+                    break;
+                case OP_USHR_INT_2ADDR:
+                    tmpValue[0] = (unsigned int) tmpValue[0] >> tmpValue2[0];
+                    break;
+                default:
+                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
+                    SET_JIT_ERROR (kJitErrorConstantFolding);
+                    return -1;
+            }
+
+            success = setVRToConst(vA, OpndSize_32, tmpValue);
+            if (success == false)
+            {
+                //setVRToConst set an error message when it failed so we just pass along the failure information
+                return -1;
+            }
+
+            compileTable[entry].refCount--;
+            retCode = touchOneVR(v2, LowOpndRegType_gp);
+            if (retCode < 0)
+                return retCode;
+#ifdef DEBUG_CONST
+            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
+#endif
+            return 1;
+        }
+        else {
+            addConstKill(vA);
+            return 0;
+        }
+    case OP_ADD_INT_LIT16:
+    case OP_RSUB_INT:
+    case OP_MUL_INT_LIT16:
+    case OP_AND_INT_LIT16:
+    case OP_OR_INT_LIT16:
+    case OP_XOR_INT_LIT16:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        tmp_s4 = currentMIR->dalvikInsn.vC;
+        if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
+            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+            if (entry < 0)
+            {
+                return -1;
+            }
+
+            switch (inst_op)
+            {
+                case OP_ADD_INT_LIT16:
+                    tmpValue[0] = tmpValue[0] + tmp_s4;
+                    break;
+                case OP_RSUB_INT:
+                    tmpValue[0] = tmp_s4 - tmpValue[0];
+                    break;
+                case OP_MUL_INT_LIT16:
+                    tmpValue[0] = tmpValue[0] * tmp_s4;
+                    break;
+                case OP_DIV_INT_LIT16:
+                    tmpValue[0] = tmpValue[0] / tmp_s4;
+                    break;
+                case OP_REM_INT_LIT16:
+                    tmpValue[0] = tmpValue[0] % tmp_s4;
+                    break;
+                case OP_AND_INT_LIT16:
+                    tmpValue[0] = tmpValue[0] & tmp_s4;
+                    break;
+                case OP_OR_INT_LIT16:
+                    tmpValue[0] = tmpValue[0] | tmp_s4;
+                    break;
+                case OP_XOR_INT_LIT16:
+                    tmpValue[0] = tmpValue[0] ^ tmp_s4;
+                    break;
+                default:
+                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
+                    SET_JIT_ERROR (kJitErrorConstantFolding);
+                    return -1;
+            }
+
+            success = setVRToConst(vA, OpndSize_32, tmpValue);
+            if (success == false)
+            {
+                //setVRToConst set an error message when it failed so we just pass along the failure information
+                return -1;
+            }
+
+            compileTable[entry].refCount--;
+            retCode = touchOneVR(vB, LowOpndRegType_gp);
+            if (retCode < 0)
+                return retCode;
+#ifdef DEBUG_CONST
+            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
+#endif
+            return 1;
+        }
+        else {
+            addConstKill(vA);
+            return 0;
+        }
+    case OP_ADD_INT:
+    case OP_SUB_INT:
+    case OP_MUL_INT:
+    case OP_AND_INT:
+    case OP_OR_INT:
+    case OP_XOR_INT:
+    case OP_SHL_INT:
+    case OP_SHR_INT:
+    case OP_USHR_INT:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        if(isVirtualRegConstant(v1, LowOpndRegType_gp, tmpValue, false) == 3 &&
+           isVirtualRegConstant(v2, LowOpndRegType_gp, tmpValue2, false) == 3) {
+            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+            if (entry < 0)
+            {
+                return -1;
+            }
+
+            switch (inst_op)
+            {
+                case OP_ADD_INT:
+                    tmpValue[0] = tmpValue[0] + tmpValue2[0];
+                    break;
+                case OP_SUB_INT:
+                    tmpValue[0] = tmpValue[0] - tmpValue2[0];
+                    break;
+                case OP_MUL_INT:
+                    tmpValue[0] = tmpValue[0] * tmpValue2[0];
+                    break;
+                case OP_DIV_INT:
+                    tmpValue[0] = tmpValue[0] / tmpValue2[0];
+                    break;
+                case OP_REM_INT:
+                    tmpValue[0] = tmpValue[0] % tmpValue2[0];
+                    break;
+                case OP_AND_INT:
+                    tmpValue[0] = tmpValue[0] & tmpValue2[0];
+                    break;
+                case OP_OR_INT:
+                    tmpValue[0] = tmpValue[0] | tmpValue2[0];
+                    break;
+                case OP_XOR_INT:
+                    tmpValue[0] = tmpValue[0] ^ tmpValue2[0];
+                    break;
+                case OP_SHL_INT:
+                    tmpValue[0] = tmpValue[0] << tmpValue2[0];
+                    break;
+                case OP_SHR_INT:
+                    tmpValue[0] = tmpValue[0] >> tmpValue2[0];
+                    break;
+                case OP_USHR_INT:
+                    tmpValue[0] = (unsigned int) tmpValue[0] >> tmpValue2[0];
+                    break;
+                default:
+                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
+                    SET_JIT_ERROR (kJitErrorConstantFolding);
+                    return -1;
+            }
+
+            success = setVRToConst(vA, OpndSize_32, tmpValue);
+            if (success == false)
+            {
+                //setVRToConst set an error message when it failed so we just pass along the failure information
+                return -1;
+            }
+
+            compileTable[entry].refCount--;
+            retCode = touchOneVR(v1, LowOpndRegType_gp);
+            if (retCode < 0)
+                return retCode;
+            retCode = touchOneVR(v2, LowOpndRegType_gp);
+            if (retCode < 0)
+                return retCode;
+#ifdef DEBUG_CONST
+            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
+#endif
+            return 1;
+        }
+        else {
+            addConstKill(vA);
+            return 0;
+        }
+    case OP_ADD_INT_LIT8: //INST_AA
+    case OP_RSUB_INT_LIT8:
+    case OP_MUL_INT_LIT8:
+    case OP_AND_INT_LIT8:
+    case OP_OR_INT_LIT8:
+    case OP_XOR_INT_LIT8:
+    case OP_SHL_INT_LIT8:
+    case OP_SHR_INT_LIT8:
+    case OP_USHR_INT_LIT8:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        tmp_s4 = currentMIR->dalvikInsn.vC;
+        if(isVirtualRegConstant(vB, LowOpndRegType_gp, tmpValue, false) == 3) {
+            entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+            if (entry < 0)
+            {
+                return -1;
+            }
+
+            switch (inst_op)
+            {
+                case OP_ADD_INT_LIT8:
+                    tmpValue[0] = tmpValue[0] + tmp_s4;
+                    break;
+                case OP_RSUB_INT_LIT8:
+                    tmpValue[0] = tmp_s4 - tmpValue[0];
+                    break;
+                case OP_MUL_INT_LIT8:
+                    tmpValue[0] = tmpValue[0] * tmp_s4;
+                    break;
+                case OP_DIV_INT_LIT8:
+                    tmpValue[0] = tmpValue[0] / tmp_s4;
+                    break;
+                case OP_REM_INT_LIT8:
+                    tmpValue[0] = tmpValue[0] % tmp_s4;
+                    break;
+                case OP_AND_INT_LIT8:
+                    tmpValue[0] = tmpValue[0] & tmp_s4;
+                    break;
+                case OP_OR_INT_LIT8:
+                    tmpValue[0] = tmpValue[0] | tmp_s4;
+                    break;
+                case OP_XOR_INT_LIT8:
+                    tmpValue[0] = tmpValue[0] ^ tmp_s4;
+                    break;
+                case OP_SHL_INT_LIT8:
+                    tmpValue[0] = tmpValue[0] << tmp_s4;
+                    break;
+                case OP_SHR_INT_LIT8:
+                    tmpValue[0] = tmpValue[0] >> tmp_s4;
+                    break;
+                case OP_USHR_INT_LIT8:
+                    tmpValue[0] = (unsigned int) tmpValue[0] >> tmp_s4;
+                    break;
+                default:
+                    ALOGI ("JIT_INFO: Unsupported constant folding for %d\n", inst_op);
+                    SET_JIT_ERROR (kJitErrorConstantFolding);
+                    return -1;
+            }
+
+            success = setVRToConst(vA, OpndSize_32, tmpValue);
+            if (success == false)
+            {
+                //setVRToConst set an error message when it failed so we just pass along the failure information
+                return -1;
+            }
+
+            compileTable[entry].refCount--;
+            retCode = touchOneVR(vB, LowOpndRegType_gp);
+            if (retCode < 0)
+                return retCode;
+#ifdef DEBUG_CONST
+            ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
+#endif
+            return 1;
+        }
+        else {
+            addConstKill(vA);
+            return 0;
+        }
+    case OP_ADD_LONG:
+    case OP_SUB_LONG:
+    case OP_AND_LONG:
+    case OP_OR_LONG:
+    case OP_XOR_LONG:
+    case OP_MUL_LONG:
+    case OP_DIV_LONG:
+    case OP_REM_LONG:
+    case OP_SHL_LONG:
+    case OP_SHR_LONG:
+    case OP_USHR_LONG:
+        //TODO bytecode is not going to update state registers
+        //constant folding
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+    case OP_CMP_LONG:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        return 0;
+    case OP_ADD_LONG_2ADDR:
+    case OP_SUB_LONG_2ADDR:
+    case OP_AND_LONG_2ADDR:
+    case OP_OR_LONG_2ADDR:
+    case OP_XOR_LONG_2ADDR:
+    case OP_MUL_LONG_2ADDR:
+    case OP_DIV_LONG_2ADDR:
+    case OP_REM_LONG_2ADDR:
+    case OP_SHL_LONG_2ADDR:
+    case OP_SHR_LONG_2ADDR:
+    case OP_USHR_LONG_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+    case OP_CONST_4:
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_s4 = currentMIR->dalvikInsn.vB;
+        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[0] = tmp_s4;
+
+        success = setVRToConst(vA, OpndSize_32, tmpValue);
+        if (success == false)
+        {
+            //setVRToConst set an error message when it failed so we just pass along the failure information
+            return -1;
+        }
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %d", vA, tmp_s4);
+#endif
+        return 1;
+    case OP_CONST_16:
+        BBBB = currentMIR->dalvikInsn.vB;
+        vA = currentMIR->dalvikInsn.vA;
+        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[0] = (s2)BBBB;
+
+        success = setVRToConst(vA, OpndSize_32, tmpValue);
+        if (success == false)
+        {
+            //setVRToConst set an error message when it failed so we just pass along the failure information
+            return -1;
+        }
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
+#endif
+        return 1;
+    case OP_CONST:
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u4 = currentMIR->dalvikInsn.vB;
+        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[0] = (s4)tmp_u4;
+
+        success = setVRToConst(vA, OpndSize_32, tmpValue);
+        if (success == false)
+        {
+            //setVRToConst set an error message when it failed so we just pass along the failure information
+            return -1;
+        }
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
+#endif
+        return 1;
+    case OP_CONST_HIGH16:
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u2 = currentMIR->dalvikInsn.vB;
+        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[0] = ((s4)tmp_u2)<<16;
+
+        success = setVRToConst(vA, OpndSize_32, tmpValue);
+        if (success == false)
+        {
+            //setVRToConst set an error message when it failed so we just pass along the failure information
+            return -1;
+        }
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %d", vA, infoArray[entry].value[0]);
+#endif
+        return 1;
+    case OP_CONST_WIDE_16:
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u2 = currentMIR->dalvikInsn.vB;
+        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[0] = (s2)tmp_u2;
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %x", vA, infoArray[entry].value[0]);
+#endif
+
+        entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[1] = ((s2)tmp_u2)>>31;
+
+        success = setVRToConst(vA, OpndSize_64, tmpValue);
+        if (success == false)
+        {
+            //setVRToConst set an error message when it failed so we just pass along the failure information
+            return -1;
+        }
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %x", vA+1, infoArray[entry].value[0]);
+#endif
+        return 1;
+    case OP_CONST_WIDE_32:
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u4 = currentMIR->dalvikInsn.vB;
+        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[0] = (s4)tmp_u4;
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %x", vA, infoArray[entry].value[0]);
+#endif
+
+        entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[1] = ((s4)tmp_u4)>>31;
+
+        success = setVRToConst(vA, OpndSize_64, tmpValue);
+        if (success == false)
+        {
+            //setVRToConst set an error message when it failed so we just pass along the failure information
+            return -1;
+        }
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %x", vA+1, infoArray[entry].value[0]);
+#endif
+        return 1;
+    case OP_CONST_WIDE:
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u4 = (s4)currentMIR->dalvikInsn.vB_wide;
+        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[0] = (s4)tmp_u4;
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %x", vA, infoArray[entry].value[0]);
+#endif
+
+        tmp_u4 = (s4)(currentMIR->dalvikInsn.vB_wide >> 32);
+        entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[1] = (s4)tmp_u4;
+
+        success = setVRToConst(vA, OpndSize_64, tmpValue);
+        if (success == false)
+        {
+            //setVRToConst set an error message when it failed so we just pass along the failure information
+            return -1;
+        }
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %x", vA+1, infoArray[entry].value[0]);
+#endif
+        return 1;
+    case OP_CONST_WIDE_HIGH16:
+        vA = currentMIR->dalvikInsn.vA;
+        tmp_u2 = currentMIR->dalvikInsn.vB;
+        entry = findVirtualRegInTable(vA, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[0] = 0;
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %x", vA, infoArray[entry].value[0]);
+#endif
+
+        entry = findVirtualRegInTable(vA+1, LowOpndRegType_gp);
+        if (entry < 0)
+            return -1;
+        tmpValue[1] = ((s4)tmp_u2)<<16;
+
+        success = setVRToConst(vA, OpndSize_64, tmpValue);
+        if (success == false)
+        {
+            //setVRToConst set an error message when it failed so we just pass along the failure information
+            return -1;
+        }
+
+        compileTable[entry].refCount--;
+#ifdef DEBUG_CONST
+        ALOGD("getConstInfo: set VR %d to %x", vA+1, infoArray[entry].value[0]);
+#endif
+        return 1;
+#ifdef SUPPORT_HLO
+    case OP_X_AGET_QUICK:
+    case OP_X_AGET_OBJECT_QUICK:
+    case OP_X_AGET_BOOLEAN_QUICK:
+    case OP_X_AGET_BYTE_QUICK:
+    case OP_X_AGET_CHAR_QUICK:
+    case OP_X_AGET_SHORT_QUICK:
+        vA = FETCH(1) & 0xff;
+        addConstKill(vA);
+        return 0;
+    case OP_X_AGET_WIDE_QUICK:
+        vA = FETCH(1) & 0xff;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+    case OP_X_DEREF_GET:
+    case OP_X_DEREF_GET_OBJECT:
+    case OP_X_DEREF_GET_BOOLEAN:
+    case OP_X_DEREF_GET_BYTE:
+    case OP_X_DEREF_GET_CHAR:
+    case OP_X_DEREF_GET_SHORT:
+        vA = FETCH(1) & 0xff;
+        addConstKill(vA);
+        return 0;
+    case OP_X_DEREF_GET_WIDE:
+        vA = FETCH(1) & 0xff;
+        addConstKill(vA);
+        addConstKill(vA+1);
+        return 0;
+#endif
+    default:
+        // Bytecode does not generate a const
+        break;
+    }
+    return 0;
+}
+
+/**
+ * @brief Updates infoArray with virtual registers accessed when lowering the bytecode.
+ * @param infoArray Must be an array of size MAX_REG_PER_BYTECODE. This is updated by function.
+ * @param currentMIR The MIR to examine.
+ * @param updateBBConstraints should we update the BB constraints?
+ * @return Returns the number of registers for the bytecode. Returns -1 in case of error.
+ */
+int getVirtualRegInfo (VirtualRegInfo* infoArray, const MIR * currentMIR, bool updateBBConstraints)
+{
+    u2 inst_op = currentMIR->dalvikInsn.opcode;
+    int vA = 0, vB = 0, vref, vindex;
+    int v1, v2, vD, vE, vF;
+    u2 length, count;
+    int kk, num, num_entry;
+    s2 tmp_s2;
+    int num_regs_per_bytecode = 0;
+    //update infoArray[xx].allocConstraints
+    for(num = 0; num < MAX_REG_PER_BYTECODE; num++) {
+        for(kk = 0; kk < 8; kk++) {
+            infoArray[num].allocConstraints[kk].physicalReg = (PhysicalReg)kk;
+            infoArray[num].allocConstraints[kk].count = 0;
+        }
+    }
+
+    //A bytecode with the inlined flag is treated as a nop so therefore simply return
+    //that we have 0 regs for this bytecode
+    if (currentMIR->OptimizationFlags & MIR_INLINED)
+    {
+        return 0;
+    }
+
+    bool isExtended = false;
+
+    // Check if we need to handle an extended MIR
+    if (currentMIR->dalvikInsn.opcode >= static_cast<Opcode> (kMirOpFirst)) {
+        //We have an extended MIR
+        isExtended = true;
+
+        switch (static_cast<ExtendedMIROpcode>(currentMIR->dalvikInsn.opcode)) {
+            case kMirOpRegisterize:
+                infoArray[0].regNum = currentMIR->dalvikInsn.vA;
+                infoArray[0].refCount = 2;
+
+                //The access type is use and then def because we use the VR when loading it into temporary
+                //and then we alias virtual register to that temporary thus "defining" it.
+                infoArray[0].accessType = REGACCESS_UD;
+
+                //Decide the type depending on the register class
+                switch (static_cast<RegisterClass> (currentMIR->dalvikInsn.vB))
+                {
+                    case kCoreReg:
+                        infoArray[0].physicalType = LowOpndRegType_gp;
+                        break;
+                    case kSFPReg:
+                        infoArray[0].physicalType = LowOpndRegType_ss;
+                        break;
+                    case kDFPReg:
+                        infoArray[0].physicalType = LowOpndRegType_xmm;
+                        break;
+                    default:
+                        ALOGI("JIT_INFO: kMirOpRegisterize does not support regClass %d", currentMIR->dalvikInsn.vB);
+                        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+                        break;
+                }
+                num_regs_per_bytecode = 1;
+                break;
+            default:
+                ALOGI("JIT_INFO: Extended MIR not supported in getVirtualRegInfo");
+                SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+                return -1;
+        }
+    }
+
+    //If we have an extended bytecode, we have nothing else to do
+    if (isExtended == true)
+    {
+        return num_regs_per_bytecode;
+    }
+
+    switch (inst_op) {
+    case OP_NOP:
+        break;
+    case OP_MOVE:
+    case OP_MOVE_OBJECT:
+    case OP_MOVE_FROM16:
+    case OP_MOVE_OBJECT_FROM16:
+    case OP_MOVE_16:
+    case OP_MOVE_OBJECT_16:
+        if(inst_op == OP_MOVE || inst_op == OP_MOVE_OBJECT) {
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
+        }
+        else if(inst_op == OP_MOVE_FROM16 || inst_op == OP_MOVE_OBJECT_FROM16) {
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
+        }
+        else if(inst_op == OP_MOVE_16 || inst_op == OP_MOVE_OBJECT_16) {
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
+        }
+        infoArray[1].regNum = vA; //dst
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = vB; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_MOVE_WIDE:
+    case OP_MOVE_WIDE_FROM16:
+    case OP_MOVE_WIDE_16:
+        if(inst_op == OP_MOVE_WIDE) {
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
+        }
+        else if(inst_op == OP_MOVE_WIDE_FROM16) {
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
+        }
+        else if(inst_op == OP_MOVE_WIDE_16) {
+            vA = currentMIR->dalvikInsn.vA;
+            vB = currentMIR->dalvikInsn.vB;
+        }
+        infoArray[1].regNum = vA; //dst
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        infoArray[0].regNum = vB; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_MOVE_RESULT: //access memory
+    case OP_MOVE_RESULT_OBJECT:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_MOVE_RESULT_WIDE: //note: 2 destinations
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_MOVE_EXCEPTION: //access memory
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_RETURN_VOID:
+    case OP_RETURN_VOID_BARRIER:
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+        }
+        num_regs_per_bytecode = 0;
+        break;
+    case OP_RETURN:
+    case OP_RETURN_OBJECT:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+        }
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_RETURN_WIDE:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_CONST_4:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_CONST_16:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_CONST:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_CONST_HIGH16:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_CONST_WIDE_16:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vA+1;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_CONST_WIDE_32:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vA+1;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_CONST_WIDE:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vA+1;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_CONST_WIDE_HIGH16:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vA+1;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_CONST_STRING:
+    case OP_CONST_STRING_JUMBO:
+    case OP_CONST_CLASS:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+        }
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_MONITOR_ENTER:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_MONITOR_EXIT:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX); //eax is used as return value from c function
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_CHECK_CAST:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_ECX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_INSTANCE_OF:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = vB; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vA; //dst
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+        }
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_ARRAY_LENGTH:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = vB; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vA; //dst
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        //%edx is used in this bytecode, update currentBB->allocConstraints
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_NEW_INSTANCE:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //dst
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_D;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_ECX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_NEW_ARRAY:
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB; //length
+        infoArray[0].regNum = vB; //src
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vA; //dst
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_FILLED_NEW_ARRAY: {//update return value
+        //can use up to 5 registers to fill the content of array
+        length = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.arg[0];
+        v2 = currentMIR->dalvikInsn.arg[1];
+        int v3 = currentMIR->dalvikInsn.arg[2];
+        int v4 = currentMIR->dalvikInsn.arg[3];
+        int v5 = currentMIR->dalvikInsn.arg[4];
+        if(length >= 1) {
+            infoArray[0].regNum = v1; //src
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            infoArray[0].physicalType = LowOpndRegType_gp;
+        }
+        if(length >= 2) {
+            infoArray[1].regNum = v2; //src
+            infoArray[1].refCount = 1;
+            infoArray[1].accessType = REGACCESS_U;
+            infoArray[1].physicalType = LowOpndRegType_gp;
+        }
+        if(length >= 3) {
+            infoArray[2].regNum = v3; //src
+            infoArray[2].refCount = 1;
+            infoArray[2].accessType = REGACCESS_U;
+            infoArray[2].physicalType = LowOpndRegType_gp;
+        }
+        if(length >= 4) {
+            infoArray[3].regNum = v4; //src
+            infoArray[3].refCount = 1;
+            infoArray[3].accessType = REGACCESS_U;
+            infoArray[3].physicalType = LowOpndRegType_gp;
+        }
+        if(length >= 5) {
+            infoArray[4].regNum = v5; //src
+            infoArray[4].refCount = 1;
+            infoArray[4].accessType = REGACCESS_U;
+            infoArray[4].physicalType = LowOpndRegType_gp;
+        }
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = length;
+        break;
+    }
+    case OP_FILLED_NEW_ARRAY_RANGE: {//use "length" virtual registers
+        length = currentMIR->dalvikInsn.vA;
+        u4 vC = currentMIR->dalvikInsn.vC;
+        for(kk = 0; kk < length; kk++) {
+            infoArray[kk].regNum = vC+kk; //src
+            infoArray[kk].refCount = 1;
+            infoArray[kk].accessType = REGACCESS_U;
+            infoArray[kk].physicalType = LowOpndRegType_gp;
+        }
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = length;
+        break;
+    }
+    case OP_FILL_ARRAY_DATA: //update content of array, read memory
+        vA = currentMIR->dalvikInsn.vA; //use virtual register, but has side-effect, update memory
+        infoArray[0].regNum = vA; //use
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_THROW: //update glue->exception
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //use
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_THROW_VERIFICATION_ERROR:
+    case OP_GOTO:
+    case OP_GOTO_16:
+    case OP_GOTO_32:
+        num_regs_per_bytecode = 0;
+        break;
+    case OP_PACKED_SWITCH:
+    case OP_SPARSE_SWITCH:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //use
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 1;
+        break;
+
+    case OP_CMPL_FLOAT: //move 32 bits from memory to lower part of XMM register
+    case OP_CMPG_FLOAT:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        num_regs_per_bytecode = 1;
+        infoArray[0].regNum = v1; //use ss or sd CHECK
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_ss;
+        infoArray[1].regNum = v2; //use
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_ss;
+        num_regs_per_bytecode = 3;
+        num_entry = 2;
+        infoArray[num_entry].regNum = vA; //define
+        infoArray[num_entry].refCount = 3;
+        infoArray[num_entry].accessType = REGACCESS_D;
+        infoArray[num_entry].physicalType = LowOpndRegType_gp;
+        break;
+    case OP_CMPL_DOUBLE: //move 64 bits from memory to lower part of XMM register
+    case OP_CMPG_DOUBLE:
+    case OP_CMP_LONG: //load v1, v1+1, v2, v2+1 to gpr
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        num_regs_per_bytecode = 1;
+        if(inst_op == OP_CMP_LONG) {
+            infoArray[0].regNum = v1; //use
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            infoArray[0].physicalType = LowOpndRegType_gp;
+            infoArray[1].regNum = v1 + 1; //use
+            infoArray[1].refCount = 1;
+            infoArray[1].accessType = REGACCESS_U;
+            infoArray[1].physicalType = LowOpndRegType_gp;
+            infoArray[2].regNum = v2; //use
+            infoArray[2].refCount = 1;
+            infoArray[2].accessType = REGACCESS_U;
+            infoArray[2].physicalType = LowOpndRegType_gp;
+            infoArray[3].regNum = v2 + 1; //use
+            infoArray[3].refCount = 1;
+            infoArray[3].accessType = REGACCESS_U;
+            infoArray[3].physicalType = LowOpndRegType_gp;
+            num_regs_per_bytecode = 5;
+            num_entry = 4;
+            infoArray[num_entry].regNum = vA; //define
+            infoArray[num_entry].refCount = 5;
+            infoArray[num_entry].accessType = REGACCESS_D;
+            infoArray[num_entry].physicalType = LowOpndRegType_gp;
+        }
+        else {
+            infoArray[0].regNum = v1; //use ss or sd CHECK
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            infoArray[0].physicalType = LowOpndRegType_xmm;
+            infoArray[1].regNum = v2; //use
+            infoArray[1].refCount = 1;
+            infoArray[1].accessType = REGACCESS_U;
+            infoArray[1].physicalType = LowOpndRegType_xmm;
+            num_regs_per_bytecode = 3;
+            num_entry = 2;
+            infoArray[num_entry].regNum = vA; //define
+            infoArray[num_entry].refCount = 3;
+            infoArray[num_entry].accessType = REGACCESS_D;
+            infoArray[num_entry].physicalType = LowOpndRegType_gp;
+        }
+        break;
+    case OP_IF_EQ:
+    case OP_IF_NE:
+    case OP_IF_LT:
+    case OP_IF_GE:
+    case OP_IF_GT:
+    case OP_IF_LE:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = vA; //use
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vB;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_IF_EQZ:
+    case OP_IF_NEZ:
+    case OP_IF_LTZ:
+    case OP_IF_GEZ:
+    case OP_IF_GTZ:
+    case OP_IF_LEZ:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = vA; //use
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_AGET:
+    case OP_AGET_WIDE:
+    case OP_AGET_OBJECT:
+    case OP_AGET_BOOLEAN: //movez 8
+    case OP_AGET_BYTE: //moves 8
+    case OP_AGET_CHAR: //movez 16
+    case OP_AGET_SHORT: //moves 16
+        vA = currentMIR->dalvikInsn.vA;
+        vref = currentMIR->dalvikInsn.vB;
+        vindex = currentMIR->dalvikInsn.vC;
+        if(inst_op == OP_AGET_WIDE) {
+            infoArray[2].regNum = vA;
+            infoArray[2].refCount = 1;
+            infoArray[2].accessType = REGACCESS_D;
+            infoArray[2].physicalType = LowOpndRegType_xmm; //64, 128 not used in lowering
+        } else {
+            infoArray[2].regNum = vA;
+            infoArray[2].refCount = 1;
+            infoArray[2].accessType = REGACCESS_D;
+            infoArray[2].physicalType = LowOpndRegType_gp;
+        }
+        infoArray[0].regNum = vref; //use
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vindex; //use
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_APUT:
+    case OP_APUT_WIDE:
+    case OP_APUT_OBJECT:
+    case OP_APUT_BOOLEAN:
+    case OP_APUT_BYTE:
+    case OP_APUT_CHAR:
+    case OP_APUT_SHORT:
+        vA = currentMIR->dalvikInsn.vA;
+        vref = currentMIR->dalvikInsn.vB;
+        vindex = currentMIR->dalvikInsn.vC;
+        if(inst_op == OP_APUT_WIDE) {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            infoArray[0].physicalType = LowOpndRegType_xmm; //64, 128 not used in lowering
+        } else {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            infoArray[0].physicalType = LowOpndRegType_gp;
+        }
+        infoArray[1].regNum = vref; //use
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = vindex; //use
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_U;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        if (inst_op == OP_APUT_OBJECT && updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 3;
+        break;
+
+    case OP_IGET:
+    case OP_IGET_WIDE:
+    case OP_IGET_OBJECT:
+    case OP_IGET_VOLATILE:
+    case OP_IGET_WIDE_VOLATILE:
+    case OP_IGET_OBJECT_VOLATILE:
+    case OP_IGET_BOOLEAN:
+    case OP_IGET_BYTE:
+    case OP_IGET_CHAR:
+    case OP_IGET_SHORT:
+    case OP_IGET_QUICK:
+    case OP_IGET_WIDE_QUICK:
+    case OP_IGET_OBJECT_QUICK:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = vB; //object instance
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+#ifndef WITH_SELF_VERIFICATION
+        if(inst_op == OP_IGET_WIDE || inst_op == OP_IGET_WIDE_QUICK) {
+            infoArray[1].regNum = vA;
+            infoArray[1].refCount = 1;
+            infoArray[1].accessType = REGACCESS_D;
+            infoArray[1].physicalType = LowOpndRegType_xmm; //64
+        } else if(inst_op == OP_IGET_WIDE_VOLATILE) {
+            infoArray[1].regNum = vA;
+            infoArray[1].refCount = 1;
+            infoArray[1].accessType = REGACCESS_D;
+            infoArray[1].physicalType = LowOpndRegType_gp;
+            infoArray[2].regNum = vA+1;
+            infoArray[2].refCount = 1;
+            infoArray[2].accessType = REGACCESS_D;
+            infoArray[2].physicalType = LowOpndRegType_gp;
+            ///Update num regs per bytecode in this case
+            num_regs_per_bytecode = 3;
+        } else {
+            infoArray[1].regNum = vA;
+            infoArray[1].refCount = 1;
+            infoArray[1].accessType = REGACCESS_D;
+            infoArray[1].physicalType = LowOpndRegType_gp;
+        }
+#else
+        if(inst_op == OP_IGET_WIDE || inst_op == OP_IGET_WIDE_QUICK ||
+           inst_op == OP_IGET_WIDE_VOLATILE) {
+            infoArray[1].regNum = vA;
+            infoArray[1].refCount = 1;
+            infoArray[1].accessType = REGACCESS_D;
+            infoArray[1].physicalType = LowOpndRegType_xmm; //64
+        } else {
+            infoArray[1].regNum = vA;
+            infoArray[1].refCount = 1;
+            infoArray[1].accessType = REGACCESS_D;
+            infoArray[1].physicalType = LowOpndRegType_gp;
+        }
+#endif
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        break;
+    case OP_IPUT:
+    case OP_IPUT_WIDE:
+    case OP_IPUT_OBJECT:
+    case OP_IPUT_VOLATILE:
+    case OP_IPUT_WIDE_VOLATILE:
+    case OP_IPUT_OBJECT_VOLATILE:
+    case OP_IPUT_BOOLEAN:
+    case OP_IPUT_BYTE:
+    case OP_IPUT_CHAR:
+    case OP_IPUT_SHORT:
+    case OP_IPUT_QUICK:
+    case OP_IPUT_WIDE_QUICK:
+    case OP_IPUT_OBJECT_QUICK:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        if(inst_op == OP_IPUT_WIDE || inst_op == OP_IPUT_WIDE_QUICK || inst_op == OP_IPUT_WIDE_VOLATILE) {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            infoArray[0].physicalType = LowOpndRegType_xmm; //64
+        } else {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            infoArray[0].physicalType = LowOpndRegType_gp;
+        }
+        infoArray[1].regNum = vB; //object instance
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_SGET:
+    case OP_SGET_WIDE:
+    case OP_SGET_OBJECT:
+    case OP_SGET_VOLATILE:
+    case OP_SGET_WIDE_VOLATILE:
+    case OP_SGET_OBJECT_VOLATILE:
+    case OP_SGET_BOOLEAN:
+    case OP_SGET_BYTE:
+    case OP_SGET_CHAR:
+    case OP_SGET_SHORT:
+        vA = currentMIR->dalvikInsn.vA;
+#ifndef WITH_SELF_VERIFICATION
+        if(inst_op == OP_SGET_WIDE) {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_D;
+            infoArray[0].physicalType = LowOpndRegType_xmm; //64
+        } else if(inst_op == OP_SGET_WIDE_VOLATILE) {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_D;
+            infoArray[0].physicalType = LowOpndRegType_gp;
+            infoArray[1].regNum = vA+1;
+            infoArray[1].refCount = 1;
+            infoArray[1].accessType = REGACCESS_D;
+            infoArray[1].physicalType = LowOpndRegType_gp;
+        } else {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_D;
+            infoArray[0].physicalType = LowOpndRegType_gp;
+        }
+        if(inst_op == OP_SGET_WIDE_VOLATILE)
+            num_regs_per_bytecode = 2;
+        else
+            num_regs_per_bytecode = 1;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+        }
+        break;
+#else
+        if(inst_op == OP_SGET_WIDE || inst_op == OP_SGET_WIDE_VOLATILE) {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_D;
+            infoArray[0].physicalType = LowOpndRegType_xmm; //64
+        }  else {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_D;
+            infoArray[0].physicalType = LowOpndRegType_gp;
+        }
+        num_regs_per_bytecode = 1;
+        updateCurrentBBWithConstraints(PhysicalReg_EAX);
+        break;
+#endif
+    case OP_SPUT:
+    case OP_SPUT_WIDE:
+    case OP_SPUT_OBJECT:
+    case OP_SPUT_VOLATILE:
+    case OP_SPUT_WIDE_VOLATILE:
+    case OP_SPUT_OBJECT_VOLATILE:
+    case OP_SPUT_BOOLEAN:
+    case OP_SPUT_BYTE:
+    case OP_SPUT_CHAR:
+    case OP_SPUT_SHORT:
+        vA = currentMIR->dalvikInsn.vA;
+        if(inst_op == OP_SPUT_WIDE || inst_op == OP_SPUT_WIDE_VOLATILE) {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            infoArray[0].physicalType = LowOpndRegType_xmm; //64
+        } else {
+            infoArray[0].regNum = vA;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            infoArray[0].physicalType = LowOpndRegType_gp;
+        }
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+        }
+        num_regs_per_bytecode = 1;
+        break;
+    case OP_INVOKE_VIRTUAL:
+    case OP_INVOKE_SUPER:
+    case OP_INVOKE_DIRECT:
+    case OP_INVOKE_STATIC:
+    case OP_INVOKE_INTERFACE:
+    case OP_INVOKE_VIRTUAL_QUICK:
+    case OP_INVOKE_SUPER_QUICK:
+        vD = currentMIR->dalvikInsn.arg[0]; //object for virtual,direct & interface
+        count = currentMIR->dalvikInsn.vA;
+        vE = currentMIR->dalvikInsn.arg[1];
+        vF = currentMIR->dalvikInsn.arg[2];
+        vA = currentMIR->dalvikInsn.arg[4]; //5th argument
+
+        for (int vrNum = 0; vrNum < count; vrNum++) {
+            if (vrNum == 0) {
+                infoArray[num_regs_per_bytecode].regNum = currentMIR->dalvikInsn.arg[vrNum];
+                if (inst_op == OP_INVOKE_VIRTUAL_QUICK || inst_op == OP_INVOKE_SUPER_QUICK
+                    || inst_op == OP_INVOKE_VIRTUAL || inst_op == OP_INVOKE_DIRECT
+                    || inst_op == OP_INVOKE_INTERFACE){
+                        infoArray[num_regs_per_bytecode].refCount = 2;
+                    } else {
+                        infoArray[num_regs_per_bytecode].refCount = 1;
+                    }
+                    infoArray[num_regs_per_bytecode].accessType = REGACCESS_U;
+                    infoArray[num_regs_per_bytecode].physicalType = LowOpndRegType_gp;
+                    num_regs_per_bytecode++;
+            } else if ((vrNum + 1 < count) && // Use XMM registers if adjacent VRs are accessed
+                       (currentMIR->dalvikInsn.arg[vrNum] + 1 == currentMIR->dalvikInsn.arg[vrNum + 1])) {
+                infoArray[num_regs_per_bytecode].regNum = currentMIR->dalvikInsn.arg[vrNum];
+                infoArray[num_regs_per_bytecode].refCount = 1;
+                infoArray[num_regs_per_bytecode].accessType = REGACCESS_U;
+                infoArray[num_regs_per_bytecode].physicalType = LowOpndRegType_xmm;
+                // We can now skip the vrNum+1 which represents the rest of the wide VR
+                vrNum++;
+                num_regs_per_bytecode++;
+            } else { // Use gp registers
+                infoArray[num_regs_per_bytecode].regNum = currentMIR->dalvikInsn.arg[vrNum];
+                infoArray[num_regs_per_bytecode].refCount = 1;
+                infoArray[num_regs_per_bytecode].accessType = REGACCESS_U;
+                infoArray[num_regs_per_bytecode].physicalType = LowOpndRegType_gp;
+                num_regs_per_bytecode++;
+            }
+        }
+
+        if (updateBBConstraints == true)
+        {
+            if (inst_op != OP_INVOKE_VIRTUAL_QUICK && inst_op != OP_INVOKE_SUPER_QUICK)
+            {
+                updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            }
+            updateCurrentBBWithConstraints (PhysicalReg_ECX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        break;
+    case OP_INVOKE_VIRTUAL_RANGE:
+    case OP_INVOKE_SUPER_RANGE:
+    case OP_INVOKE_DIRECT_RANGE:
+    case OP_INVOKE_STATIC_RANGE:
+    case OP_INVOKE_INTERFACE_RANGE:
+    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
+    case OP_INVOKE_SUPER_QUICK_RANGE:
+        vD = currentMIR->dalvikInsn.vC;
+        count = currentMIR->dalvikInsn.vA;
+        if(count == 0) {
+            if(inst_op == OP_INVOKE_VIRTUAL_RANGE || inst_op == OP_INVOKE_DIRECT_RANGE ||
+               inst_op == OP_INVOKE_INTERFACE_RANGE || inst_op == OP_INVOKE_VIRTUAL_QUICK_RANGE ||
+               inst_op == OP_INVOKE_SUPER_QUICK_RANGE) {
+                infoArray[0].regNum = vD;
+                infoArray[0].refCount = 1;
+                infoArray[0].accessType = REGACCESS_U;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+            }
+        }
+        if(count > 0) { //same for count > 10
+            for(kk = 0; kk < count; kk++) {
+                infoArray[kk].regNum = vD+kk; //src
+                if(kk == 0 && (inst_op == OP_INVOKE_VIRTUAL_QUICK_RANGE ||
+                               inst_op == OP_INVOKE_SUPER_QUICK_RANGE))
+                    infoArray[kk].refCount = 2;
+                else if(kk == 0 && (inst_op == OP_INVOKE_VIRTUAL_RANGE ||
+                                    inst_op == OP_INVOKE_DIRECT_RANGE ||
+                                    inst_op == OP_INVOKE_INTERFACE_RANGE))
+                    infoArray[kk].refCount = 2;
+                else
+                    infoArray[kk].refCount = 1;
+                infoArray[kk].accessType = REGACCESS_U;
+                infoArray[kk].physicalType = LowOpndRegType_gp;
+            }
+        }
+        if (updateBBConstraints == true)
+        {
+            if (inst_op != OP_INVOKE_VIRTUAL_QUICK_RANGE && inst_op != OP_INVOKE_SUPER_QUICK_RANGE)
+            {
+                updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            }
+            updateCurrentBBWithConstraints (PhysicalReg_ECX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = count;
+        break;
+    case OP_NEG_INT:
+    case OP_NOT_INT:
+    case OP_NEG_FLOAT:
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_NEG_LONG:
+    case OP_NOT_LONG:
+    case OP_NEG_DOUBLE:
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_INT_TO_LONG: //hard-coded registers
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp; //save from %eax
+        infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;
+        infoArray[2].regNum = vA+1;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[2].allocConstraints[PhysicalReg_EDX].count = 1;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_INT_TO_FLOAT: //32 to 32
+    case OP_INT_TO_DOUBLE: //32 to 64
+    case OP_LONG_TO_FLOAT: //64 to 32
+    case OP_LONG_TO_DOUBLE: //64 to 64
+    case OP_FLOAT_TO_DOUBLE: //32 to 64
+    case OP_DOUBLE_TO_FLOAT: //64 to 32
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        if(inst_op == OP_LONG_TO_DOUBLE || inst_op == OP_FLOAT_TO_DOUBLE)
+            infoArray[1].physicalType = LowOpndRegType_fs;
+        else if (inst_op == OP_INT_TO_DOUBLE)
+            infoArray[1].physicalType = LowOpndRegType_xmm;
+        else
+            infoArray[1].physicalType = LowOpndRegType_fs_s;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        if(inst_op == OP_INT_TO_FLOAT || inst_op == OP_FLOAT_TO_DOUBLE)
+            infoArray[0].physicalType = LowOpndRegType_fs_s; //float
+        else if (inst_op == OP_INT_TO_DOUBLE)
+            infoArray[0].physicalType = LowOpndRegType_gp;
+        else
+            infoArray[0].physicalType = LowOpndRegType_fs;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_LONG_TO_INT:
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_FLOAT_TO_INT:
+    case OP_DOUBLE_TO_INT: //for reaching-def analysis
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 3;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_fs_s; //store_int_fp_stack_VR
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        if(inst_op == OP_DOUBLE_TO_INT)
+            infoArray[0].physicalType = LowOpndRegType_fs;
+        else
+            infoArray[0].physicalType = LowOpndRegType_fs_s;
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_FLOAT_TO_LONG:
+    case OP_DOUBLE_TO_LONG:
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 3;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_fs;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        if(inst_op == OP_DOUBLE_TO_LONG)
+            infoArray[0].physicalType = LowOpndRegType_fs;
+        else
+            infoArray[0].physicalType = LowOpndRegType_fs_s;
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_INT_TO_BYTE:
+    case OP_INT_TO_CHAR:
+    case OP_INT_TO_SHORT:
+        vA = currentMIR->dalvikInsn.vA; //destination
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+
+    case OP_ADD_INT:
+    case OP_SUB_INT:
+    case OP_MUL_INT:
+    case OP_AND_INT:
+    case OP_OR_INT:
+    case OP_XOR_INT:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_DIV_INT:
+    case OP_REM_INT:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1; //for v1
+        if(inst_op == OP_REM_INT)
+            infoArray[2].allocConstraints[PhysicalReg_EDX].count = 1;//vA
+        else
+            infoArray[2].allocConstraints[PhysicalReg_EAX].count = 1;//vA
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_SHL_INT:
+    case OP_SHR_INT:
+    case OP_USHR_INT:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = v2; // in ecx
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[1].allocConstraints[PhysicalReg_ECX].count = 1;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_ECX);
+        }
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_ADD_LONG:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = v1+1;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = v2;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_U;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = v2+1;
+        infoArray[3].refCount = 1;
+        infoArray[3].accessType = REGACCESS_U;
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = vA;
+        infoArray[4].refCount = 1;
+        infoArray[4].accessType = REGACCESS_D;
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[5].regNum = vA+1;
+        infoArray[5].refCount = 1;
+        infoArray[5].accessType = REGACCESS_D;
+        infoArray[5].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 6;
+        break;
+    case OP_SUB_LONG:
+    case OP_AND_LONG:
+    case OP_OR_LONG:
+    case OP_XOR_LONG:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_xmm;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_MUL_LONG: //used int
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = v1+1;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = v2;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_U;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = v2+1;
+        infoArray[3].refCount = 1;
+        infoArray[3].accessType = REGACCESS_U;
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = vA;
+        infoArray[4].refCount = 1;
+        infoArray[4].accessType = REGACCESS_D;
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[5].regNum = vA+1;
+        infoArray[5].refCount = 1;
+        infoArray[5].accessType = REGACCESS_D;
+        infoArray[5].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_ECX);
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+        }
+        num_regs_per_bytecode = 6;
+        break;
+    case OP_DIV_LONG: //v1: xmm v2,vA:
+    case OP_REM_LONG:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = v2+1;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_U;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = vA;
+        infoArray[3].refCount = 1;
+        infoArray[3].accessType = REGACCESS_D;
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = vA+1;
+        infoArray[4].refCount = 1;
+        infoArray[4].accessType = REGACCESS_D;
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 5;
+        break;
+    case OP_SHL_LONG: //v2: 32, move_ss; v1,vA: xmm CHECK
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_ss;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_SHR_LONG: //v2: 32, move_ss; v1,vA: xmm CHECK
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_ss;
+        infoArray[2].regNum = v1+1;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_U;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = vA;
+        infoArray[3].refCount = 1;
+        infoArray[3].accessType = REGACCESS_D;
+        infoArray[3].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 4;
+        break;
+    case OP_USHR_LONG: //v2: move_ss; v1,vA: move_sd
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm; //sd
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_ss; //ss
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_xmm; //sd
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_ADD_FLOAT: //move_ss
+    case OP_SUB_FLOAT:
+    case OP_MUL_FLOAT:
+    case OP_DIV_FLOAT:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_ss;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_ss;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_ss;
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_REM_FLOAT: //32 bit GPR, fp_stack for output
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_fs_s;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_ADD_DOUBLE: //move_sd
+    case OP_SUB_DOUBLE:
+    case OP_MUL_DOUBLE:
+    case OP_DIV_DOUBLE:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_xmm;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 3;
+        break;
+    case OP_REM_DOUBLE: //64 bit XMM, fp_stack for output
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        v2 = currentMIR->dalvikInsn.vC;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_D;
+        infoArray[2].physicalType = LowOpndRegType_fs;
+        infoArray[0].regNum = v1;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 3;
+        break;
+
+    case OP_ADD_INT_2ADDR:
+    case OP_SUB_INT_2ADDR:
+    case OP_MUL_INT_2ADDR:
+    case OP_AND_INT_2ADDR:
+    case OP_OR_INT_2ADDR:
+    case OP_XOR_INT_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD; //use then define
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_DIV_INT_2ADDR:
+    case OP_REM_INT_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 3;
+        infoArray[1].accessType = REGACCESS_UD; //use then define
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1; //for v1 is vA
+        if(inst_op == OP_REM_INT_2ADDR)
+            infoArray[1].allocConstraints[PhysicalReg_EDX].count = 1;//vA
+        else
+            infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;//vA
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_SHL_INT_2ADDR:
+    case OP_SHR_INT_2ADDR:
+    case OP_USHR_INT_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD; //use then define
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[0].allocConstraints[PhysicalReg_ECX].count = 1; //v2
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_ECX);
+        }
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_ADD_LONG_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = vA+1;
+        infoArray[3].refCount = 2;
+        infoArray[3].accessType = REGACCESS_UD;
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = v2+1;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_U;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 4;
+        break;
+    case OP_SUB_LONG_2ADDR:
+    case OP_AND_LONG_2ADDR:
+    case OP_OR_LONG_2ADDR:
+    case OP_XOR_LONG_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_MUL_LONG_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        num_regs_per_bytecode = 4;
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = v2+1;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 2;
+        infoArray[2].accessType = REGACCESS_UD;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = vA+1;
+        infoArray[3].refCount = 2;
+        infoArray[3].accessType = REGACCESS_UD;
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_ECX);
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+        }
+        break;
+    case OP_DIV_LONG_2ADDR: //vA used as xmm, then updated as gps
+    case OP_REM_LONG_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        num_regs_per_bytecode = 5;
+        infoArray[0].regNum = vA;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = v2;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = v2+1;
+        infoArray[2].refCount = 1;
+        infoArray[2].accessType = REGACCESS_U;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = vA;
+        infoArray[3].refCount = 1;
+        infoArray[3].accessType = REGACCESS_D;
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = vA+1;
+        infoArray[4].refCount = 1;
+        infoArray[4].accessType = REGACCESS_D;
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        break;
+    case OP_SHL_LONG_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        num_regs_per_bytecode = 2;
+        infoArray[0].regNum = v2; //ss
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_ss;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        break;
+    case OP_SHR_LONG_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        num_regs_per_bytecode = 3;
+        infoArray[0].regNum = v2; //ss
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_ss;
+        infoArray[1].regNum = vA+1;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_U;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = vA;
+        infoArray[2].refCount = 2;
+        infoArray[2].accessType = REGACCESS_UD;
+        infoArray[2].physicalType = LowOpndRegType_xmm;
+        break;
+    case OP_USHR_LONG_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        num_regs_per_bytecode = 2;
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_ss; //ss CHECK
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD;
+        infoArray[1].physicalType = LowOpndRegType_xmm; //sd
+        break;
+    case OP_ADD_FLOAT_2ADDR:
+    case OP_SUB_FLOAT_2ADDR:
+    case OP_MUL_FLOAT_2ADDR:
+    case OP_DIV_FLOAT_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD;
+        infoArray[1].physicalType = LowOpndRegType_ss;
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_ss;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_REM_FLOAT_2ADDR: //load vA as GPR, store from fs
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD;
+        infoArray[1].physicalType = LowOpndRegType_gp; //CHECK
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_ADD_DOUBLE_2ADDR:
+    case OP_SUB_DOUBLE_2ADDR:
+    case OP_MUL_DOUBLE_2ADDR:
+    case OP_DIV_DOUBLE_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_REM_DOUBLE_2ADDR: //load to xmm, store from fs
+        vA = currentMIR->dalvikInsn.vA;
+        v2 = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 2;
+        infoArray[1].accessType = REGACCESS_UD;
+        infoArray[1].physicalType = LowOpndRegType_xmm; //CHECK
+        infoArray[0].regNum = v2;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        num_regs_per_bytecode = 2;
+        break;
+
+    case OP_ADD_INT_LIT16:
+    case OP_RSUB_INT:
+    case OP_MUL_INT_LIT16:
+    case OP_AND_INT_LIT16:
+    case OP_OR_INT_LIT16:
+    case OP_XOR_INT_LIT16:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_DIV_INT_LIT16:
+    case OP_REM_INT_LIT16:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        tmp_s2 = currentMIR->dalvikInsn.vC;
+        if(tmp_s2 == 0) {
+            num_regs_per_bytecode = 0;
+            break;
+        }
+        infoArray[1].regNum = vA; //in edx for rem, in eax
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = vB; //in eax
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        if(inst_op == OP_DIV_INT_LIT16) {
+            int power = isPowerOfTwo(tmp_s2);
+            if(power >= 1) { /* divide by a power of 2 constant */
+                infoArray[1].refCount = 1;
+                break;
+            }
+        }
+        if(tmp_s2 == -1)
+            infoArray[1].refCount = 2;
+        else
+            infoArray[1].refCount = 1;
+        if(inst_op == OP_REM_INT_LIT16)
+            infoArray[1].allocConstraints[PhysicalReg_EDX].count = 1;
+        else
+            infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;
+        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        break;
+    case OP_ADD_INT_LIT8:
+    case OP_RSUB_INT_LIT8:
+    case OP_MUL_INT_LIT8:
+    case OP_AND_INT_LIT8:
+    case OP_OR_INT_LIT8:
+    case OP_XOR_INT_LIT8:
+    case OP_SHL_INT_LIT8:
+    case OP_SHR_INT_LIT8:
+    case OP_USHR_INT_LIT8:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[1].regNum = vA;
+        infoArray[1].refCount = 1;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        break;
+    case OP_DIV_INT_LIT8:
+    case OP_REM_INT_LIT8:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        tmp_s2 = currentMIR->dalvikInsn.vC;
+        if(tmp_s2 == 0) {
+            num_regs_per_bytecode = 0;
+            break;
+        }
+
+        infoArray[1].regNum = vA;
+        infoArray[1].accessType = REGACCESS_D;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[0].regNum = vB;
+        infoArray[0].refCount = 1;
+        infoArray[0].accessType = REGACCESS_U;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        num_regs_per_bytecode = 2;
+        if(inst_op == OP_DIV_INT_LIT8) {
+            int power = isPowerOfTwo(tmp_s2);
+            if(power >= 1) { /* divide by a power of 2 constant */
+                infoArray[1].refCount = 1;
+                break;
+            }
+        }
+
+        if(tmp_s2 == -1)
+            infoArray[1].refCount = 2;
+        else
+            infoArray[1].refCount = 1;
+        if(inst_op == OP_REM_INT_LIT8)
+            infoArray[1].allocConstraints[PhysicalReg_EDX].count = 1;
+        else
+            infoArray[1].allocConstraints[PhysicalReg_EAX].count = 1;
+        infoArray[0].allocConstraints[PhysicalReg_EAX].count = 1;
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        break;
+    case OP_EXECUTE_INLINE: //update glue->retval
+    case OP_EXECUTE_INLINE_RANGE:
+        u4 vC;
+        num = currentMIR->dalvikInsn.vA;
+
+        // get inline method id
+        u2 inlineMethodId;
+        inlineMethodId = currentMIR->dalvikInsn.vB;
+        if(inst_op == OP_EXECUTE_INLINE) {
+            // Note that vC, vD, vE, and vF might have bad values
+            // depending on count. The variable "num" should be
+            // checked before using any of these.
+            vC = currentMIR->dalvikInsn.arg[0];
+            vD = currentMIR->dalvikInsn.arg[1];
+            vE = currentMIR->dalvikInsn.arg[2];
+            vF = currentMIR->dalvikInsn.arg[3];
+        } else {
+            vC = currentMIR->dalvikInsn.vC;
+            vD = vC + 1;
+            vE = vC + 2;
+            vF = vC + 3;
+        }
+        if(num >= 1) {
+            infoArray[0].regNum = vC;
+            infoArray[0].refCount = 1;
+            infoArray[0].accessType = REGACCESS_U;
+            if (inlineMethodId == INLINE_MATH_ABS_DOUBLE) {
+                infoArray[0].physicalType = LowOpndRegType_xmm;
+            }
+            else {
+                infoArray[0].physicalType = LowOpndRegType_gp;
+            }
+        }
+        if(num >= 2) {
+            if (inlineMethodId != INLINE_MATH_ABS_DOUBLE) {
+                infoArray[1].regNum = vD;
+                infoArray[1].refCount = 1;
+                infoArray[1].accessType = REGACCESS_U;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+            }
+            else {
+                num_regs_per_bytecode = 1;
+                break;
+            }
+        }
+        if(num >= 3) {
+            infoArray[2].regNum = vE;
+            infoArray[2].refCount = 1;
+            infoArray[2].accessType = REGACCESS_U;
+            infoArray[2].physicalType = LowOpndRegType_gp;
+        }
+        if(num >= 4) {
+            infoArray[3].regNum = vF;
+            infoArray[3].refCount = 1;
+            infoArray[3].accessType = REGACCESS_U;
+            infoArray[3].physicalType = LowOpndRegType_gp;
+        }
+        if (updateBBConstraints == true)
+        {
+            updateCurrentBBWithConstraints (PhysicalReg_EAX);
+            updateCurrentBBWithConstraints (PhysicalReg_EDX);
+        }
+        num_regs_per_bytecode = num;
+        break;
+#if FIXME
+    case OP_INVOKE_OBJECT_INIT_RANGE:
+        codeSize = 3;
+        num_regs_per_bytecode = 0;
+        break;
+#endif
+    default:
+        ALOGI("JIT_INFO: JIT does not support bytecode 0x%hx when updating VR accesses", currentMIR->dalvikInsn.opcode);
+        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+        assert(false && "All opcodes should be supported.");
+        break;
+    }
+    return num_regs_per_bytecode;
+}
+
+/**
+ * @brief Updates infoArray(TempRegInfo) with temporaries accessed by INVOKE_NO_RANGE type
+ * @details Allocates both XMM and gp registers for INVOKE_(VIRTUAL,DIRECT,STATIC,INTERFACE,SUPER)
+ * @param infoArray the array to be filled
+ * @param startIndex the start index of infoArray
+ * @param currentMIR the instruction we are looking at
+ * @return j the new index of infoArray
+ */
+int updateInvokeNoRange(TempRegInfo* infoArray, int startIndex, const MIR * currentMIR) {
+    int j = startIndex;
+    int count = currentMIR->dalvikInsn.vA;// Max value is 5 (#ofArguments)
+
+    // Use XMM registers to read and store max of 5 arguments
+    infoArray[j].regNum = 22;
+    infoArray[j].refCount = 4; //DUDU Read & Store a pair of VRs. Max refCount is 4, since max #of VR pairs is 2
+    infoArray[j].physicalType = LowOpndRegType_xmm;
+    j++;
+
+    // Use gp registers when 64 bit move is not possible. Upto 5 gp reg may be needed.
+    if(count == 5) {
+        infoArray[j].regNum = 27;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 4) {
+        infoArray[j].regNum = 26;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 3) {
+        infoArray[j].regNum = 25;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 2) {
+        infoArray[j].regNum = 24;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 1) {
+        infoArray[j].regNum = 23;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    return j;
+}
+//! Updates infoArray(TempRegInfo) with temporaries accessed by INVOKE_RANGE
+
+//! LOOP_COUNT is used to indicate a variable is live through a loop
+int updateInvokeRange(TempRegInfo* infoArray, int startIndex, const MIR * currentMIR) {
+    int j = startIndex;
+    int count = currentMIR->dalvikInsn.vA;
+    infoArray[j].regNum = 21;
+    if(count <= 10) {
+        infoArray[j].refCount = 1+count; //DU
+    } else {
+        infoArray[j].refCount = 2+3*LOOP_COUNT;
+    }
+    infoArray[j].physicalType = LowOpndRegType_gp;
+    j++;
+    if(count >= 1 && count <= 10) {
+        infoArray[j].regNum = 22;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 2 && count <= 10) {
+        infoArray[j].regNum = 23;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 3 && count <= 10) {
+        infoArray[j].regNum = 24;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 4 && count <= 10) {
+        infoArray[j].regNum = 25;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 5 && count <= 10) {
+        infoArray[j].regNum = 26;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 6 && count <= 10) {
+        infoArray[j].regNum = 27;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 7 && count <= 10) {
+        infoArray[j].regNum = 28;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 8 && count <= 10) {
+        infoArray[j].regNum = 29;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count >= 9 && count <= 10) {
+        infoArray[j].regNum = 30;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count == 10) {
+        infoArray[j].regNum = 31;
+        infoArray[j].refCount = 2; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    if(count > 10) {
+        //NOTE: inside a loop, LOOP_COUNT can't be 1
+        //      if LOOP_COUNT is 1, it is likely that a logical register is freed inside the loop
+        //         and the next iteration will have incorrect result
+        infoArray[j].regNum = 12;
+        infoArray[j].refCount = 1+3*LOOP_COUNT; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+        infoArray[j].regNum = 13;
+        infoArray[j].refCount = 1+LOOP_COUNT; //DU
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+        infoArray[j].regNum = 14;
+        //MUST be 2, otherwise, transferToState will think its state was in memory
+        infoArray[j].refCount = 2; //DU local
+        infoArray[j].physicalType = LowOpndRegType_gp;
+        j++;
+    }
+    return j;
+}
+
+/* update temporaries used by predicted INVOKE_VIRTUAL & INVOKE_INTERFACE */
+int updateGenPrediction(TempRegInfo* infoArray, bool isInterface) {
+    infoArray[0].regNum = 40;
+    infoArray[0].physicalType = LowOpndRegType_gp;
+    infoArray[1].regNum = 41;
+    infoArray[1].physicalType = LowOpndRegType_gp;
+    infoArray[2].regNum = 32;
+    infoArray[2].refCount = 2;
+    infoArray[2].physicalType = LowOpndRegType_gp;
+
+    if(isInterface) {
+        infoArray[0].refCount = 2+2;
+        infoArray[1].refCount = 3+2-1; //for temp41, -1 for gingerbread
+        infoArray[3].regNum = 33;
+        infoArray[3].refCount = 4+1;
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = PhysicalReg_EAX;
+        infoArray[4].refCount = 5;
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[5].regNum = PhysicalReg_ECX;
+        infoArray[5].refCount = 1+1+2; //used in ArgsDone (twice)
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[6].regNum = 10;
+        infoArray[6].refCount = 2;
+        infoArray[6].physicalType = LowOpndRegType_scratch;
+        infoArray[7].regNum = 9;
+        infoArray[7].refCount = 2;
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        infoArray[8].regNum = 8;
+        infoArray[8].refCount = 2;
+        infoArray[8].physicalType = LowOpndRegType_scratch;
+        infoArray[9].regNum = PhysicalReg_EDX; //space holder
+        infoArray[9].refCount = 1;
+        infoArray[9].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[10].regNum = 43;
+        infoArray[10].refCount = 3;
+        infoArray[10].physicalType = LowOpndRegType_gp;
+        infoArray[11].regNum = 44;
+        infoArray[11].refCount = 3;
+        infoArray[11].physicalType = LowOpndRegType_gp;
+        infoArray[12].regNum = 45;
+        infoArray[12].refCount = 2;
+        infoArray[12].physicalType = LowOpndRegType_gp;
+        infoArray[13].regNum = 7;
+        infoArray[13].refCount = 4;
+        infoArray[13].physicalType = LowOpndRegType_scratch;
+        return 14;
+    } else { //virtual or virtual_quick
+        infoArray[0].refCount = 2+2;
+        infoArray[1].refCount = 3+2-2; //for temp41, -2 for gingerbread
+        infoArray[2].refCount++; //for temp32 gingerbread
+        infoArray[3].regNum = 33;
+        infoArray[3].refCount = 4+1;
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 34;
+        infoArray[4].refCount = 2;
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[5].regNum = PhysicalReg_EAX;
+        infoArray[5].refCount = 2;
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[6].regNum = PhysicalReg_ECX;
+        infoArray[6].refCount = 1+3+2;
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[7].regNum = 10;
+        infoArray[7].refCount = 2;
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        infoArray[8].regNum = PhysicalReg_EDX; //space holder
+        infoArray[8].refCount = 1;
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[9].regNum = 43;
+        infoArray[9].refCount = 3;
+        infoArray[9].physicalType = LowOpndRegType_gp;
+        infoArray[10].regNum = 44;
+        infoArray[10].refCount = 3;
+        infoArray[10].physicalType = LowOpndRegType_gp;
+        infoArray[11].regNum = 7;
+        infoArray[11].refCount = 4;
+        infoArray[11].physicalType = LowOpndRegType_scratch;
+        return 12;
+    }
+}
+
+int updateMarkCard(TempRegInfo* infoArray, int j1/*valReg*/,
+                    int j2/*tgtAddrReg*/, int j3/*scratchReg*/) {
+    infoArray[j3].regNum = 11;
+    infoArray[j3].physicalType = LowOpndRegType_gp;
+    infoArray[j3].refCount = 3;
+    infoArray[j3].is8Bit = true;
+    infoArray[j1].refCount++;
+    infoArray[j2].refCount += 2;
+    infoArray[j3+1].regNum = 6;
+    infoArray[j3+1].physicalType = LowOpndRegType_scratch;
+    infoArray[j3+1].refCount = 2;
+    return j3+2;
+}
+
+int updateMarkCard_notNull(TempRegInfo* infoArray,
+                           int j2/*tgtAddrReg*/, int j3/*scratchReg*/) {
+    infoArray[j3].regNum = 11;
+    infoArray[j3].physicalType = LowOpndRegType_gp;
+    infoArray[j3].refCount = 3;
+    infoArray[j3].is8Bit = true;
+    infoArray[j2].refCount += 2;
+    infoArray[j3+1].regNum = 2;
+    infoArray[j3+1].refCount = 2; //DU
+    infoArray[j3+1].physicalType = LowOpndRegType_scratch;
+    return j3+2;
+}
+
+int iget_obj_inst = -1;
+//! This function updates infoArray with temporaries accessed when lowering the bytecode
+
+//! returns the number of temporaries
+int getTempRegInfo(TempRegInfo* infoArray, const MIR * currentMIR) { //returns an array of TempRegInfo
+    int k;
+    int numTmps;
+    for(k = 0; k < MAX_TEMP_REG_PER_BYTECODE; k++) {
+        infoArray[k].linkageToVR = -1;
+        infoArray[k].versionNum = 0;
+        infoArray[k].shareWithVR = true;
+        infoArray[k].is8Bit = false;
+    }
+    u2 length, num, tmp;
+    int vA, vB, v1, v2;
+    Opcode inst_op = currentMIR->dalvikInsn.opcode;
+    s2 tmp_s2;
+    int tmpvalue, isConst;
+
+    /* A bytecode with the MIR_INLINED op will be treated as
+     * no-op during codegen */
+    if (currentMIR->OptimizationFlags & MIR_INLINED)
+        return 0; // No temporaries accessed
+
+    // Check if we need to handle an extended MIR
+    if (currentMIR->dalvikInsn.opcode >= static_cast<Opcode> (kMirOpFirst)) {
+        switch (static_cast<ExtendedMIROpcode>(currentMIR->dalvikInsn.opcode)) {
+            case kMirOpPhi:
+                return 0;
+            case kMirOpRegisterize:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 2; //UD
+
+                //Decide the type depending on the register class
+                switch (static_cast<RegisterClass> (currentMIR->dalvikInsn.vB))
+                {
+                    case kCoreReg:
+                        infoArray[0].physicalType = LowOpndRegType_gp;
+                        break;
+                    case kSFPReg:
+                    case kDFPReg:
+                        //Temps don't have concept of SS type so the physical type of both
+                        //Single FP and Double FP must be xmm.
+                        infoArray[0].physicalType = LowOpndRegType_xmm;
+                        break;
+                    default:
+                        ALOGI("JIT_INFO: kMirOpRegisterize does not support regClass %d", currentMIR->dalvikInsn.vB);
+                        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+                        break;
+                }
+                return 1;
+            default:
+                ALOGI("JIT_INFO: Extended MIR not supported in getTempRegInfo");
+                SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+                return -1;
+        }
+    }
+
+    switch (inst_op) {
+    case OP_NOP:
+        return 0;
+    case OP_MOVE:
+    case OP_MOVE_OBJECT:
+    case OP_MOVE_FROM16:
+    case OP_MOVE_OBJECT_FROM16:
+    case OP_MOVE_16:
+    case OP_MOVE_OBJECT_16:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        return 1;
+    case OP_MOVE_WIDE:
+    case OP_MOVE_WIDE_FROM16:
+    case OP_MOVE_WIDE_16:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        return 1;
+    case OP_MOVE_RESULT:
+    case OP_MOVE_RESULT_OBJECT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 1;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+        return 2;
+    case OP_MOVE_RESULT_WIDE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = 1;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+        return 2;
+    case OP_MOVE_EXCEPTION:
+        infoArray[0].regNum = 2;
+        infoArray[0].refCount = 3; //DUU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 3;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 1;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_scratch;
+        return 3;
+
+    case OP_CONST_4:
+    case OP_CONST_16:
+    case OP_CONST:
+    case OP_CONST_HIGH16:
+    case OP_CONST_WIDE_16:
+    case OP_CONST_WIDE_32:
+    case OP_CONST_WIDE:
+    case OP_CONST_WIDE_HIGH16:
+        return 0;
+    case OP_CONST_STRING: //hardcode %eax
+    case OP_CONST_STRING_JUMBO:
+        infoArray[0].regNum = 3;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 1;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+        infoArray[2].regNum = 2;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_scratch;
+        infoArray[3].regNum = PhysicalReg_EAX;
+        infoArray[3].refCount = 4;
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 4;
+    case OP_CONST_CLASS:
+        infoArray[0].regNum = 3;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 1;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+        infoArray[2].regNum = 2;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_scratch;
+        infoArray[3].regNum = PhysicalReg_EAX;
+        infoArray[3].refCount = 4;
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 4;
+
+    case OP_MONITOR_ENTER:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 5; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 3;
+        infoArray[1].refCount = 7; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 1;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_scratch;
+        infoArray[3].regNum = 2;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_scratch;
+        infoArray[4].regNum = PhysicalReg_EDX;
+        infoArray[4].refCount = 2;
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[5].regNum = 2;
+        infoArray[5].refCount = 4; //DU
+        infoArray[5].physicalType = LowOpndRegType_gp;
+        infoArray[6].regNum = 4;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp;
+        infoArray[7].regNum = 5;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_gp;
+        infoArray[8].regNum = PhysicalReg_EAX;
+        infoArray[8].refCount = 2; //DU
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+       return 9;
+
+    case OP_MONITOR_EXIT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = PhysicalReg_EAX;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[2].regNum = 1;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_scratch;
+        infoArray[3].regNum = PhysicalReg_EDX;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[4].regNum = 2;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        infoArray[5].regNum = 3;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+        infoArray[6].regNum = 3;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp;
+        infoArray[7].regNum = 4;
+        infoArray[7].refCount = 3; //DU
+        infoArray[7].physicalType = LowOpndRegType_gp;
+        infoArray[8].regNum = 5;
+        infoArray[8].refCount = 4; //DU
+        infoArray[8].physicalType = LowOpndRegType_gp;
+        infoArray[9].regNum = 6;
+        infoArray[9].refCount = 3; //DU
+        infoArray[9].physicalType = LowOpndRegType_gp;
+        infoArray[10].regNum = 7;
+        infoArray[10].refCount = 3; //DU
+        infoArray[10].physicalType = LowOpndRegType_gp;
+        return 11;
+
+    case OP_CHECK_CAST:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 4;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 6;
+        infoArray[2].refCount = 3; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+
+        infoArray[3].regNum = 1;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_scratch;
+        infoArray[4].regNum = 2;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+
+        infoArray[5].regNum = PhysicalReg_EAX;
+        /* %eax has 3 live ranges
+           1> 5 accesses: to resolve the class object
+           2> call dvmInstanceofNonTrivial to define %eax, then use it once
+           3> move exception object to %eax, then jump to throw_exception
+           if WITH_JIT is true, the first live range has 6 accesses
+        */
+        infoArray[5].refCount = 6;
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[6].regNum = PhysicalReg_EDX;
+        infoArray[6].refCount = 2; //export_pc
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[7].regNum = PhysicalReg_ECX;
+        infoArray[7].refCount = 1;
+        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[8].regNum = 3;
+        infoArray[8].refCount = 2; //DU
+        infoArray[8].physicalType = LowOpndRegType_scratch;
+        return 9;
+    case OP_INSTANCE_OF:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 3;
+        infoArray[1].refCount = 4; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 4;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 6;
+        infoArray[3].refCount = 3; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+
+        infoArray[4].regNum = 1;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        infoArray[5].regNum = 2;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+
+        infoArray[6].regNum = PhysicalReg_EAX;
+        infoArray[6].refCount = 6;
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[7].regNum = 3;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        infoArray[8].regNum = PhysicalReg_EDX;
+        infoArray[8].refCount = 2; //export_pc for class_resolve
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 9;
+
+    case OP_ARRAY_LENGTH:
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[1].linkageToVR = vA;
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 3;
+    case OP_NEW_INSTANCE:
+        infoArray[0].regNum = PhysicalReg_EAX;
+        //6: class object
+        //3: defined by C function, used twice
+        infoArray[0].refCount = 6; //next version has 3 references
+        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[1].regNum = PhysicalReg_ECX; //before common_throw_message
+        infoArray[1].refCount = 1;
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 5;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[3].is8Bit = true;
+        infoArray[4].regNum = 6;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+
+        infoArray[5].regNum = 1;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+        infoArray[6].regNum = 2;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_scratch;
+        infoArray[7].regNum = 3;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+
+        infoArray[8].regNum = PhysicalReg_EDX; //before common_throw_message
+        infoArray[8].refCount = 2;
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[9].regNum = 4;
+        infoArray[9].refCount = 2; //DU
+        infoArray[9].physicalType = LowOpndRegType_scratch;
+        return 10;
+
+    case OP_NEW_ARRAY:
+        infoArray[0].regNum = PhysicalReg_EAX;
+        //4: class object
+        //3: defined by C function, used twice
+        infoArray[0].refCount = 4; //next version has 3 references
+        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 5;
+        infoArray[3].refCount = 3; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+
+        infoArray[4].regNum = 1;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        infoArray[5].regNum = 2;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+        infoArray[6].regNum = 3;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_scratch;
+        infoArray[7].regNum = 4;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        return 8;
+
+    case OP_FILLED_NEW_ARRAY:
+        length = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = PhysicalReg_EAX;
+        //4: class object
+        //3: defined by C function, used twice (array object)
+        //length: access array object to update the content
+        infoArray[0].refCount = 4; //next version has 5+length references
+        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 5;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 6;
+        infoArray[4].refCount = 8; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[4].is8Bit = true;
+
+        if(length >= 1) {
+            infoArray[5].regNum = 7;
+            infoArray[5].refCount = 2; //DU
+            infoArray[5].physicalType = LowOpndRegType_gp;
+        }
+        if(length >= 2) {
+            infoArray[6].regNum = 8;
+            infoArray[6].refCount = 2; //DU
+            infoArray[6].physicalType = LowOpndRegType_gp;
+        }
+        if(length >= 3) {
+            infoArray[7].regNum = 9;
+            infoArray[7].refCount = 2; //DU
+            infoArray[7].physicalType = LowOpndRegType_gp;
+        }
+        if(length >= 4) {
+            infoArray[8].regNum = 10;
+            infoArray[8].refCount = 2; //DU
+            infoArray[8].physicalType = LowOpndRegType_gp;
+        }
+        if(length >= 5) {
+            infoArray[9].regNum = 11;
+            infoArray[9].refCount = 2; //DU
+            infoArray[9].physicalType = LowOpndRegType_gp;
+        }
+        infoArray[5+length].regNum = 1;
+        infoArray[5+length].refCount = 2; //DU
+        infoArray[5+length].physicalType = LowOpndRegType_scratch;
+        infoArray[6+length].regNum = 2;
+        infoArray[6+length].refCount = 4; //DU
+        infoArray[6+length].physicalType = LowOpndRegType_scratch;
+        infoArray[7+length].regNum = 3;
+        infoArray[7+length].refCount = 2; //DU
+        infoArray[7+length].physicalType = LowOpndRegType_scratch;
+        infoArray[8+length].regNum = 4;
+        infoArray[8+length].refCount = 5; //DU
+        infoArray[8+length].physicalType = LowOpndRegType_scratch;
+        return 9+length;
+
+    case OP_FILLED_NEW_ARRAY_RANGE:
+        length = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = PhysicalReg_EAX;
+        //4: class object
+        //3: defined by C function, used twice (array object)
+        //if length is 0, no access to array object
+        //else, used inside a loop
+        infoArray[0].refCount = 4; //next version: 5+(length >= 1 ? LOOP_COUNT : 0)
+        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 5;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 6;
+        infoArray[4].refCount = 8; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[4].is8Bit = true;
+
+        infoArray[5].regNum = 1;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+        infoArray[6].regNum = 2;
+        infoArray[6].refCount = 4; //DU
+        infoArray[6].physicalType = LowOpndRegType_scratch;
+        infoArray[7].regNum = 3;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+
+        infoArray[8].regNum = 7;
+        infoArray[8].refCount = 3*(length >= 1 ? LOOP_COUNT : 0);
+        infoArray[8].physicalType = LowOpndRegType_gp;
+        infoArray[9].regNum = 8;
+        infoArray[9].refCount = 3*(length >= 1 ? LOOP_COUNT : 0);
+        infoArray[9].physicalType = LowOpndRegType_gp;
+        infoArray[10].regNum = 9;
+        infoArray[10].refCount = 2*(length >= 1 ? LOOP_COUNT : 0);
+        infoArray[10].physicalType = LowOpndRegType_gp;
+        infoArray[11].regNum = 10;
+        infoArray[11].refCount = 2*(length >= 1 ? LOOP_COUNT : 0);
+        infoArray[11].physicalType = LowOpndRegType_gp;
+        infoArray[12].regNum = 4;
+        infoArray[12].refCount = 5; //DU
+        infoArray[12].physicalType = LowOpndRegType_scratch;
+        return 13;
+
+    case OP_FILL_ARRAY_DATA:
+        infoArray[0].regNum = PhysicalReg_EAX;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
+#if 0//def HARDREG_OPT
+        infoArray[1].refCount = 3; //next version has refCount of 2
+#else
+        infoArray[1].refCount = 5;
+#endif
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[2].regNum =1;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+
+        infoArray[3].regNum = 1;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_scratch;
+        infoArray[4].regNum = 2;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        return 5;
+
+    case OP_THROW:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = PhysicalReg_EDX; //before common_throw_message
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[2].regNum = 1;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_scratch;
+        infoArray[3].regNum = 2;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_scratch;
+        return 4;
+    case OP_THROW_VERIFICATION_ERROR:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = PhysicalReg_EDX; //export_pc
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[2].regNum = 1;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_scratch;
+        infoArray[3].regNum = 2;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_scratch;
+        return 4;
+
+    case OP_GOTO: //called function common_periodicChecks4
+#if defined(ENABLE_TRACING)
+        tt = INST_AA(inst);
+        tmp_s2 = (s2)((s2)tt << 8) >> 8;
+        if(tmp_s2 < 0) {
+            infoArray[0].regNum = PhysicalReg_EDX;
+            infoArray[0].refCount = 2;
+            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            return 1;
+        }
+#endif
+        return 0;
+    case OP_GOTO_16:
+#if defined(ENABLE_TRACING)
+        tmp_s2 = (s2)FETCH(1);
+        if(tmp_s2 < 0) {
+            infoArray[0].regNum = PhysicalReg_EDX;
+            infoArray[0].refCount = 2;
+            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            return 1;
+        }
+#endif
+        return 0;
+    case OP_GOTO_32:
+#if defined(ENABLE_TRACING)
+        tmp_u4 = (u4)FETCH(1);
+        tmp_u4 |= (u4)FETCH(2) << 16;
+        if(((s4)tmp_u4) < 0) {
+            infoArray[0].regNum = PhysicalReg_EDX;
+            infoArray[0].refCount = 2;
+            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            return 1;
+        }
+#endif
+        return 0;
+    case OP_IF_EQ:
+    case OP_IF_NE:
+    case OP_IF_LT:
+    case OP_IF_GE:
+    case OP_IF_GT:
+    case OP_IF_LE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+#if defined(ENABLE_TRACING)
+        tmp_s2 = (s2)FETCH(1);
+        if(tmp_s2 < 0) {
+            infoArray[1].regNum = PhysicalReg_EDX;
+            infoArray[1].refCount = 2;
+            infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            return 2;
+        }
+#endif
+        return 1;
+    case OP_IF_EQZ: //called function common_periodicChecks4
+    case OP_IF_NEZ:
+    case OP_IF_LTZ:
+    case OP_IF_GEZ:
+    case OP_IF_GTZ:
+    case OP_IF_LEZ:
+#if defined(ENABLE_TRACING)
+        tmp_s2 = (s2)FETCH(1);
+        if(tmp_s2 < 0) {
+            infoArray[0].regNum = PhysicalReg_EDX;
+            infoArray[0].refCount = 2;
+            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            return 1;
+        }
+#endif
+        return 0;
+    case OP_PACKED_SWITCH: //jump common_backwardBranch, which calls common_periodicChecks_entry, then jump_reg %eax
+    case OP_SPARSE_SWITCH: //%edx, %eax
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = PhysicalReg_EDX;
+        infoArray[1].refCount = 6;
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[2].regNum = PhysicalReg_EAX; //return by dvm helper
+        infoArray[2].refCount = 2+1; //2 uses
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[3].regNum = 1;
+        infoArray[3].refCount = 2;
+        infoArray[3].physicalType = LowOpndRegType_scratch;
+        infoArray[4].regNum = 2;
+        infoArray[4].refCount = 2;
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        return 5;
+
+    case OP_AGET:
+    case OP_AGET_OBJECT:
+    case OP_AGET_BOOLEAN:
+    case OP_AGET_BYTE:
+    case OP_AGET_CHAR:
+    case OP_AGET_SHORT:
+#ifdef INC_NCG_O0
+        if(gDvm.helper_switch[7]) {
+            infoArray[0].regNum = PhysicalReg_EBX;
+            infoArray[0].refCount = 2;
+            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            infoArray[1].regNum = PhysicalReg_ECX;
+            infoArray[1].refCount = 2;
+            infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            infoArray[2].regNum = PhysicalReg_EDX;
+            infoArray[2].refCount = 2;
+            infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            return 3;
+        }
+#endif
+        vA = currentMIR->dalvikInsn.vA;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 3; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 4;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[3].linkageToVR = vA;
+
+        infoArray[4].regNum = PhysicalReg_EDX;
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        if(inst_op == OP_AGET_BYTE || inst_op == OP_AGET_BOOLEAN)
+            infoArray[3].is8Bit = true;
+        infoArray[4].refCount = 2;
+        return 5;
+#else
+        infoArray[4].refCount = 4;
+        // Use temp 5 to store address of heap access
+        infoArray[5].regNum = 5;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_gp;
+        // Return value from calling loadFromShadowHeap will be in EAX
+        infoArray[6].regNum = PhysicalReg_EAX;
+        infoArray[6].refCount = 4;
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        // Scratch for calling loadFromShadowHeap
+        infoArray[7].regNum = 1;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        infoArray[8].regNum = PhysicalReg_ECX;
+        infoArray[8].refCount = 2;
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 9;
+#endif
+    case OP_AGET_WIDE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 3; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 1;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_xmm;
+
+        infoArray[4].regNum = PhysicalReg_EDX;
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[4].refCount = 2;
+        return 5;
+#else
+        infoArray[4].refCount = 4;
+        infoArray[5].regNum = PhysicalReg_XMM7;
+        infoArray[5].refCount = 1; //U
+        infoArray[5].physicalType = LowOpndRegType_xmm | LowOpndRegType_hard;
+        // Use temp 5 to store address of heap access
+        infoArray[6].regNum = 5;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp;
+        // Scratch for calling loadFromShadowHeap
+        infoArray[7].regNum = 1;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        infoArray[8].regNum = PhysicalReg_EAX;
+        infoArray[8].refCount = 2;
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[9].regNum = PhysicalReg_ECX;
+        infoArray[9].refCount = 2;
+        infoArray[9].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 10;
+#endif
+    case OP_APUT_BYTE:
+        for(k = 0; k < MAX_TEMP_REG_PER_BYTECODE; k++)
+            infoArray[k].shareWithVR = true; //false;
+        // Intentional fall through
+    case OP_APUT:
+    case OP_APUT_BOOLEAN:
+    case OP_APUT_CHAR:
+    case OP_APUT_SHORT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 3; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 4;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+
+        infoArray[4].regNum = PhysicalReg_EDX;
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        if(inst_op == OP_APUT_BYTE || inst_op == OP_APUT_BOOLEAN)
+            infoArray[3].is8Bit = true;
+        infoArray[4].refCount = 2;
+        return 5;
+#else
+        infoArray[4].refCount = 4;
+        // Use temp 5 to store address of heap access
+        infoArray[5].regNum = 5;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_gp;
+        // Scratch for calling storeToShadowHeap
+        infoArray[6].regNum = 1;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_scratch;
+        infoArray[7].regNum = PhysicalReg_ECX;
+        infoArray[7].refCount = 2;
+        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[8].regNum = PhysicalReg_EAX;
+        infoArray[8].refCount = 2;
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 9;
+#endif
+    case OP_APUT_WIDE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 3; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 1;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_xmm;
+
+        infoArray[4].regNum = PhysicalReg_EDX;
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[4].refCount = 2;
+        return 5;
+#else
+        infoArray[4].refCount = 4;
+        // Use temp 4 to store address of heap access
+        infoArray[5].regNum = 4;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_gp;
+        // Scratch for calling storeToShadowHeap
+        infoArray[6].regNum = 1;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_scratch;
+        infoArray[7].regNum = PhysicalReg_EAX;
+        infoArray[7].refCount = 2;
+        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[8].regNum = PhysicalReg_ECX;
+        infoArray[8].refCount = 2;
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 9;
+#endif
+    case OP_APUT_OBJECT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 5+1; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2; //live through function call dvmCanPut
+        infoArray[1].refCount = 3+1; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 4;
+        infoArray[3].refCount = 4+1; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 5;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[5].regNum = 6;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_gp;
+
+        infoArray[6].regNum = PhysicalReg_EDX;
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[7].regNum = PhysicalReg_EAX;
+        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[6].refCount = 2; //DU
+        infoArray[7].refCount = 2; //DU
+#else
+        infoArray[6].refCount = 4+2; //DU
+        infoArray[7].refCount = 4+2;
+#endif
+        infoArray[8].regNum = 1;
+        infoArray[8].refCount = 2; //DU
+        infoArray[8].physicalType = LowOpndRegType_scratch;
+        infoArray[0].shareWithVR = false;
+
+#ifndef WITH_SELF_VERIFICATION
+        return updateMarkCard_notNull(infoArray,
+                                      0/*index for tgtAddrReg*/, 9);
+#else
+        // Use temp 7 to store address of heap access
+        infoArray[9].regNum = 7;
+        infoArray[9].refCount = 4; //DU
+        infoArray[9].physicalType = LowOpndRegType_gp;
+        // Scratch for calling storeToShadowHeap
+        infoArray[10].regNum = 1;
+        infoArray[10].refCount = 6; //DU
+        infoArray[10].physicalType = LowOpndRegType_scratch;
+        infoArray[11].regNum = PhysicalReg_ECX;
+        infoArray[11].refCount = 2+2;
+        infoArray[11].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return updateMarkCard_notNull(infoArray,
+                                      0/*index for tgtAddrReg*/, 12);
+#endif
+
+    case OP_IGET:
+    case OP_IGET_OBJECT:
+    case OP_IGET_VOLATILE:
+    case OP_IGET_OBJECT_VOLATILE:
+    case OP_IGET_BOOLEAN:
+    case OP_IGET_BYTE:
+    case OP_IGET_CHAR:
+    case OP_IGET_SHORT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_scratch;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[3].regNum = PhysicalReg_EAX;
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[2].refCount = 2; //DU
+        infoArray[3].refCount = 3; //DU
+#else
+        infoArray[2].refCount = 4; //DU
+        // Return value from calling loadFromShadowHeap will be in EAX
+        infoArray[3].refCount = 6; //DU
+#endif
+        infoArray[4].regNum = 3;
+        infoArray[4].refCount = 3; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[5].regNum = 7;
+#ifdef DEBUG_IGET_OBJ
+        //add hack for a specific instance (iget_obj_inst) of IGET_OBJECT within a method
+        if(inst_op == OP_IGET_OBJECT && !strncmp(currentMethod->clazz->descriptor, "Lspec/benchmarks/_228_jack/Parse", 32) &&
+           !strncmp(currentMethod->name, "buildPhase3", 11))
+        {
+#if 0
+          if(iget_obj_inst == 12) {
+            ALOGD("increase count for instance %d of %s %s", iget_obj_inst, currentMethod->clazz->descriptor, currentMethod->name);
+            infoArray[5].refCount = 4; //DU
+          }
+          else
+#endif
+            infoArray[5].refCount = 3;
+          iget_obj_inst++;
+        }
+        else
+          infoArray[5].refCount = 3;
+#else
+        infoArray[5].refCount = 3; //DU
+#endif
+        infoArray[5].physicalType = LowOpndRegType_gp;
+        infoArray[6].regNum = 8;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp;
+        infoArray[7].regNum = 9;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_gp;
+#ifndef WITH_SELF_VERIFICATION
+        return 8;
+#else
+        // Use temp 10 to store address of heap access
+        infoArray[8].regNum = 10;
+        infoArray[8].refCount = 2; //DU
+        infoArray[8].physicalType = LowOpndRegType_gp;
+        infoArray[9].regNum = 5;
+        infoArray[9].refCount = 2; //DU
+        infoArray[9].physicalType = LowOpndRegType_scratch;
+        infoArray[10].regNum = PhysicalReg_ECX;
+        infoArray[10].refCount = 2;
+        infoArray[10].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 11;
+#endif
+    case OP_IPUT:
+    case OP_IPUT_OBJECT:
+    case OP_IPUT_VOLATILE:
+    case OP_IPUT_OBJECT_VOLATILE:
+    case OP_IPUT_BOOLEAN:
+    case OP_IPUT_BYTE:
+    case OP_IPUT_CHAR:
+    case OP_IPUT_SHORT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_scratch;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[3].regNum = PhysicalReg_EAX;
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[2].refCount = 2; //DU
+        infoArray[3].refCount = 3; //DU
+#else
+        infoArray[2].refCount = 4; //DU
+        infoArray[3].refCount = 5; //DU
+#endif
+        infoArray[4].regNum = 3;
+        infoArray[4].refCount = 3; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[5].regNum = 7;
+        infoArray[5].refCount = 3; //DU
+        infoArray[5].physicalType = LowOpndRegType_gp;
+        infoArray[6].regNum = 8;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp;
+        infoArray[7].regNum = 9;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_gp;
+#ifndef WITH_SELF_VERIFICATION
+        if(inst_op == OP_IPUT_OBJECT || inst_op == OP_IPUT_OBJECT_VOLATILE) {
+            infoArray[5].shareWithVR = false;
+            return updateMarkCard(infoArray, 7/*index for valReg*/,
+                                  5/*index for tgtAddrReg*/, 8);
+        }
+        return 8;
+#else
+        // Use temp 10 to store address of heap access
+        infoArray[8].regNum = 10;
+        infoArray[8].refCount = 2; //DU
+        infoArray[8].physicalType = LowOpndRegType_gp;
+        infoArray[9].regNum = 5;
+        infoArray[9].refCount = 2; //DU
+        infoArray[9].physicalType = LowOpndRegType_scratch;
+        infoArray[10].regNum = PhysicalReg_ECX;
+        infoArray[10].refCount = 2;
+        infoArray[10].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        if(inst_op == OP_IPUT_OBJECT || inst_op == OP_IPUT_OBJECT_VOLATILE) {
+            infoArray[5].shareWithVR = false;
+            return updateMarkCard(infoArray, 7/*index for valReg*/,
+                                  5/*index for tgtAddrReg*/, 11);
+        }
+        return 11;
+#endif
+
+    case OP_IGET_WIDE:
+    case OP_IGET_WIDE_VOLATILE:
+    case OP_IPUT_WIDE:
+    case OP_IPUT_WIDE_VOLATILE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_scratch;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[3].regNum = PhysicalReg_EAX;
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[2].refCount = 2; //DU
+        infoArray[3].refCount = 3; //DU
+#else
+        infoArray[2].refCount = 4;
+        infoArray[3].refCount = 5;
+#endif
+        infoArray[4].regNum = 3;
+        infoArray[4].refCount = 3; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[5].regNum = 7;
+        infoArray[5].refCount = 3; //DU
+        infoArray[5].physicalType = LowOpndRegType_gp;
+        infoArray[6].regNum = 8;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp;
+        infoArray[7].regNum = 1;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_xmm;
+#ifndef WITH_SELF_VERIFICATION
+        if(inst_op == OP_IPUT_WIDE_VOLATILE || inst_op == OP_IGET_WIDE_VOLATILE) {
+            infoArray[8].regNum = 3;
+            infoArray[8].refCount = 2; //DU
+            infoArray[8].physicalType = LowOpndRegType_scratch;
+            infoArray[9].regNum = 9;
+            infoArray[9].refCount = 2; //DU
+            infoArray[9].physicalType = LowOpndRegType_gp;
+            return 10;
+        }
+        return 8;
+
+#else
+        infoArray[8].regNum = PhysicalReg_XMM7;
+        infoArray[8].refCount = 1; //U
+        infoArray[8].physicalType = LowOpndRegType_xmm | LowOpndRegType_hard;
+        // Use temp 10 to store address of heap access
+        infoArray[9].regNum = 10;
+        infoArray[9].refCount = 4; //DU
+        infoArray[9].physicalType = LowOpndRegType_gp;
+        infoArray[10].regNum = 5;
+        infoArray[10].refCount = 4; //DU
+        infoArray[10].physicalType = LowOpndRegType_scratch;
+        infoArray[11].regNum = PhysicalReg_ECX;
+        infoArray[11].refCount = 2;
+        infoArray[11].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        if(inst_op == OP_IPUT_WIDE_VOLATILE || inst_op == OP_IGET_WIDE_VOLATILE) {
+            infoArray[12].regNum = 3;
+            infoArray[12].refCount = 4; //DU
+            infoArray[12].physicalType = LowOpndRegType_scratch;
+            infoArray[13].regNum = 9;
+            infoArray[13].refCount = 2; //DU
+            infoArray[13].physicalType = LowOpndRegType_gp;
+            return 14;
+        }
+        return 12;
+#endif
+    case OP_SGET:
+    case OP_SGET_OBJECT:
+    case OP_SGET_VOLATILE:
+    case OP_SGET_OBJECT_VOLATILE:
+    case OP_SGET_BOOLEAN:
+    case OP_SGET_BYTE:
+    case OP_SGET_CHAR:
+    case OP_SGET_SHORT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_scratch;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+
+        infoArray[2].regNum = PhysicalReg_EAX;
+#if defined(WITH_SELF_VERIFICATION)
+        // Return value from calling loadFromShadowHeap will be in EAX
+        infoArray[2].refCount = 6;
+#else
+        infoArray[2].refCount = 2;
+#endif
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[3].regNum = 3;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 7;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+
+        infoArray[5].regNum = PhysicalReg_EDX;
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[5].refCount = 2; //DU
+        return 6;
+#else
+        infoArray[5].refCount = 4; //DU
+        // Use temp 8 to store address of heap access
+        infoArray[6].regNum = 8;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp;
+        // Scratch for calling loadFromShadowHeap
+        infoArray[7].regNum = 5;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        infoArray[8].regNum = PhysicalReg_ECX;
+        infoArray[8].refCount = 2;
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 9;
+#endif
+
+    case OP_SPUT:
+    case OP_SPUT_OBJECT:
+    case OP_SPUT_VOLATILE:
+    case OP_SPUT_OBJECT_VOLATILE:
+    case OP_SPUT_BOOLEAN:
+    case OP_SPUT_BYTE:
+    case OP_SPUT_CHAR:
+    case OP_SPUT_SHORT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_scratch;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+
+        infoArray[2].regNum = PhysicalReg_EAX;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[2].refCount = 2+1; //access clazz of the field
+#else
+        infoArray[2].refCount = 4+2;
+#endif
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[3].regNum = 3;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 7;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+
+        infoArray[5].regNum = PhysicalReg_EDX;
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[5].refCount = 2; //DU
+        if(inst_op == OP_SPUT_OBJECT || inst_op == OP_SPUT_OBJECT_VOLATILE) {
+            infoArray[2].shareWithVR = false;
+            infoArray[6].regNum = 12;
+            infoArray[6].refCount = 1; //1 def, 2 uses in updateMarkCard
+            infoArray[6].physicalType = LowOpndRegType_gp;
+            return updateMarkCard(infoArray, 4/*index for valReg*/,
+                                  6/*index for tgtAddrReg */, 7);
+        }
+        return 6;
+#else
+        infoArray[5].refCount = 4; //DU
+        // Use temp 8 to store address of heap access
+        infoArray[6].regNum = 8;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp;
+        // Scratch for calling storeToShadowHeap
+        infoArray[7].regNum = 5;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        infoArray[8].regNum = PhysicalReg_ECX;
+        infoArray[8].refCount = 2;
+        infoArray[8].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        if(inst_op == OP_SPUT_OBJECT || inst_op == OP_SPUT_OBJECT_VOLATILE) {
+            infoArray[2].shareWithVR = false;
+            infoArray[9].regNum = 12;
+            infoArray[9].refCount = 3; //1 def, 2 uses in updateMarkCard
+            infoArray[9].physicalType = LowOpndRegType_gp;
+            return updateMarkCard(infoArray, 4/*index for valReg*/,
+                                  6/*index for tgtAddrReg */, 10);
+        }
+        return 9;
+#endif
+    case OP_SGET_WIDE:
+    case OP_SGET_WIDE_VOLATILE:
+    case OP_SPUT_WIDE:
+    case OP_SPUT_WIDE_VOLATILE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_scratch;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_scratch;
+
+        infoArray[2].regNum = PhysicalReg_EAX;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[2].refCount = 2;
+#else
+        infoArray[2].refCount = 4;
+#endif
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[3].regNum = 3;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 1;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_xmm;
+
+        infoArray[5].regNum = PhysicalReg_EDX;
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[5].refCount = 2; //DU
+        if(inst_op == OP_SPUT_WIDE_VOLATILE || inst_op == OP_SGET_WIDE_VOLATILE) {
+            infoArray[6].regNum = 3;
+            infoArray[6].refCount = 2; //DU
+            infoArray[6].physicalType = LowOpndRegType_scratch;
+            infoArray[7].regNum = 9;
+            infoArray[7].refCount = 2; //DU
+            infoArray[7].physicalType = LowOpndRegType_gp;
+            return 8;
+        }
+        return 6;
+#else
+        infoArray[5].refCount = 4; //DU
+        // use temp 4 to store address of shadow heap access
+        infoArray[6].regNum = 4;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp;
+        // Scratch for calling storeToShadowHeap
+        infoArray[7].regNum = 5;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        infoArray[8].regNum = PhysicalReg_XMM7;
+        infoArray[8].refCount = 1; //U
+        infoArray[8].physicalType = LowOpndRegType_xmm | LowOpndRegType_hard;
+        infoArray[9].regNum = PhysicalReg_ECX;
+        infoArray[9].refCount = 2;
+        infoArray[9].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        if(inst_op == OP_SPUT_WIDE_VOLATILE || inst_op == OP_SGET_WIDE_VOLATILE) {
+            infoArray[10].regNum = 3;
+            infoArray[10].refCount = 2; //DU
+            infoArray[10].physicalType = LowOpndRegType_scratch;
+            infoArray[11].regNum = 9;
+            infoArray[11].refCount = 2; //DU
+            infoArray[11].physicalType = LowOpndRegType_gp;
+            return 12;
+        }
+        return 10;
+#endif
+
+
+    case OP_IGET_QUICK:
+    case OP_IGET_OBJECT_QUICK:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[2].refCount = 2; //DU
+        return 3;
+#else
+        infoArray[2].refCount = 4; //DU
+        // Use temp 3 to store address of heap access
+        infoArray[3].regNum = 3;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        // Return value from calling loadFromShadowHeap will be in EAX
+        infoArray[4].regNum = PhysicalReg_EAX;
+        infoArray[4].refCount = 4;
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[5].regNum = PhysicalReg_ECX;
+        infoArray[5].refCount = 2;
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        // Scratch for calling loadFromShadowHeap
+        infoArray[6].regNum = 1;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_scratch;
+        return 7;
+#endif
+    case OP_IPUT_QUICK:
+    case OP_IPUT_OBJECT_QUICK:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[2].refCount = 2; //DU
+        if(inst_op == OP_IPUT_OBJECT_QUICK) {
+            infoArray[0].shareWithVR = false;
+            return updateMarkCard(infoArray, 1/*index for valReg*/,
+                                  0/*index for tgtAddrReg*/, 3);
+        }
+        return 3;
+#else
+        infoArray[2].refCount = 4; //DU
+        // Use temp 3 to store address of heap access
+        infoArray[3].regNum = 3;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        // Scratch for calling storeToShadowHeap
+        infoArray[4].regNum = 1;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        infoArray[5].regNum = PhysicalReg_EAX;
+        infoArray[5].refCount = 2;
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[6].regNum = PhysicalReg_ECX;
+        infoArray[6].refCount = 2;
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        if(inst_op == OP_IPUT_OBJECT_QUICK) {
+            infoArray[0].shareWithVR = false;
+            return updateMarkCard(infoArray, 1/*index for valReg*/,
+                                  0/*index for tgtAddrReg*/, 7/*ScratchReg*/);
+        }
+        return 7;
+#endif
+    case OP_IGET_WIDE_QUICK:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 1;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[2].refCount = 2; //DU
+        return 3;
+#else
+        infoArray[2].refCount = 4; //DU
+        // use temp 3 to store address of heap access
+        infoArray[3].regNum = 3;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        // Scratch for calling storeToShadowHeap
+        infoArray[4].regNum = 1;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        infoArray[5].regNum = PhysicalReg_XMM7;
+        infoArray[5].refCount = 1; //U
+        infoArray[5].physicalType = LowOpndRegType_xmm | LowOpndRegType_hard;
+        infoArray[6].regNum = PhysicalReg_EAX;
+        infoArray[6].refCount = 2;
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[7].regNum = PhysicalReg_ECX;
+        infoArray[7].refCount = 2;
+        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 8;
+#endif
+    case OP_IPUT_WIDE_QUICK:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 1;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+#ifndef WITH_SELF_VERIFICATION
+        infoArray[2].refCount = 2; //DU
+        return 3;
+#else
+        infoArray[2].refCount = 4; //DU
+        // use temp 3 to store address of heap access
+        infoArray[3].regNum = 3;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        // Scratch for calling storeToShadowHeap
+        infoArray[4].regNum = 1;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        infoArray[5].regNum = PhysicalReg_EAX;
+        infoArray[5].refCount = 2;
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[6].regNum = PhysicalReg_ECX;
+        infoArray[6].refCount = 2;
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 7;
+#endif
+
+    case OP_RETURN_VOID:
+    case OP_RETURN_VOID_BARRIER:
+        infoArray[0].regNum = PhysicalReg_ECX;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[1].regNum = PhysicalReg_EDX;
+        infoArray[1].refCount = 1; //D
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 2;
+    case OP_RETURN:
+    case OP_RETURN_OBJECT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = PhysicalReg_ECX;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].refCount = 1; //D
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 3;
+    case OP_RETURN_WIDE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = PhysicalReg_ECX;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].refCount = 1; //D
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 3;
+    case OP_INVOKE_VIRTUAL:
+    case OP_INVOKE_VIRTUAL_RANGE:
+#ifdef PREDICTED_CHAINING
+        numTmps = updateGenPrediction(infoArray, false /*not interface*/);
+        infoArray[numTmps].regNum = 5;
+        infoArray[numTmps].refCount = 3; //DU
+        infoArray[numTmps].physicalType = LowOpndRegType_gp;
+        numTmps++;
+        if(inst_op == OP_INVOKE_VIRTUAL)
+            k = updateInvokeNoRange(infoArray, numTmps, currentMIR);
+        else
+            k = updateInvokeRange(infoArray, numTmps, currentMIR);
+        return k;
+#else
+        infoArray[0].regNum = 3;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 7;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 8;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 6;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 5;
+        infoArray[4].refCount = 3; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[5].regNum = PhysicalReg_EDX;
+        infoArray[5].refCount = 2; //2 versions, first version DU is for exception, 2nd version: eip right before jumping to invokeArgsDone
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[6].regNum = PhysicalReg_ECX; //ecx is ued in invokeArgsDone
+        infoArray[6].refCount = 1+1; //used in .invokeArgsDone
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        //when WITH_JIT is true and PREDICTED_CHAINING is false
+        //  temp 8 and EAX are not used; but it is okay to keep it here
+        infoArray[7].regNum = PhysicalReg_EAX;
+        infoArray[7].refCount = 4; //DU
+        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[8].regNum = 1;
+        infoArray[8].refCount = 2; //DU
+        infoArray[8].physicalType = LowOpndRegType_scratch;
+        infoArray[9].regNum = 2;
+        infoArray[9].refCount = 2; //DU
+        infoArray[9].physicalType = LowOpndRegType_scratch;
+        if(inst_op == OP_INVOKE_VIRTUAL)
+            k = updateInvokeNoRange(infoArray, 10);
+        else
+            k = updateInvokeRange(infoArray, 10);
+        return k;
+#endif
+    case OP_INVOKE_SUPER:
+    case OP_INVOKE_SUPER_RANGE:
+        infoArray[0].regNum = 3;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 7;
+        infoArray[1].refCount = 3; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 8;
+        infoArray[2].refCount = 3; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 6;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 9;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+
+        infoArray[5].regNum = PhysicalReg_EDX;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[6].regNum = PhysicalReg_ECX;
+        infoArray[6].refCount = 1+1; //DU
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[7].regNum = PhysicalReg_EAX;
+        infoArray[7].refCount = 4; //DU
+        infoArray[7].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[8].regNum = 1;
+        infoArray[8].refCount = 2; //DU
+        infoArray[8].physicalType = LowOpndRegType_scratch;
+        infoArray[9].regNum = 2;
+        infoArray[9].refCount = 2; //DU
+        infoArray[9].physicalType = LowOpndRegType_scratch;
+        infoArray[10].regNum = 3;
+        infoArray[10].refCount = 2; //DU
+        infoArray[10].physicalType = LowOpndRegType_scratch;
+        infoArray[11].regNum = 4;
+        infoArray[11].refCount = 2; //DU
+        infoArray[11].physicalType = LowOpndRegType_scratch;
+        if(inst_op == OP_INVOKE_SUPER)
+            k = updateInvokeNoRange(infoArray, 12, currentMIR);
+        else
+            k = updateInvokeRange(infoArray, 12, currentMIR);
+        return k;
+    case OP_INVOKE_DIRECT:
+    case OP_INVOKE_DIRECT_RANGE:
+        infoArray[0].regNum = 3;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 5;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+
+        infoArray[2].regNum = PhysicalReg_EDX;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[3].regNum = PhysicalReg_ECX;
+        infoArray[3].refCount = 2;
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[4].regNum = PhysicalReg_EAX;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[5].regNum = 1;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+        infoArray[6].regNum = 2;
+        infoArray[6].refCount = 2; //DU
+        infoArray[6].physicalType = LowOpndRegType_scratch;
+        if(inst_op == OP_INVOKE_DIRECT)
+            k = updateInvokeNoRange(infoArray, 7, currentMIR);
+        else
+            k = updateInvokeRange(infoArray, 7, currentMIR);
+        return k;
+    case OP_INVOKE_STATIC:
+    case OP_INVOKE_STATIC_RANGE:
+        infoArray[0].regNum = 3;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+
+        infoArray[1].regNum = PhysicalReg_EDX;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[2].regNum = PhysicalReg_ECX;
+        infoArray[2].refCount = 2;
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[3].regNum = PhysicalReg_EAX;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[4].regNum = 1;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        infoArray[5].regNum = 2;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+        if(inst_op == OP_INVOKE_STATIC)
+            k = updateInvokeNoRange(infoArray, 6, currentMIR);
+        else
+            k = updateInvokeRange(infoArray, 6, currentMIR);
+        return k;
+    case OP_INVOKE_INTERFACE:
+    case OP_INVOKE_INTERFACE_RANGE:
+#ifdef PREDICTED_CHAINING
+        numTmps = updateGenPrediction(infoArray, true /*interface*/);
+        infoArray[numTmps].regNum = 1;
+        infoArray[numTmps].refCount = 3; //DU
+        infoArray[numTmps].physicalType = LowOpndRegType_gp;
+        numTmps++;
+        if(inst_op == OP_INVOKE_INTERFACE)
+            k = updateInvokeNoRange(infoArray, numTmps, currentMIR);
+        else
+            k = updateInvokeRange(infoArray, numTmps, currentMIR);
+        return k;
+#else
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 3;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 4;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 5;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+
+        infoArray[4].regNum = PhysicalReg_EDX;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[5].regNum = PhysicalReg_ECX;
+        infoArray[5].refCount = 1+1; //DU
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[6].regNum = PhysicalReg_EAX;
+        infoArray[6].refCount = 2+1; //2 uses
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[7].regNum = 1;
+        infoArray[7].refCount = 2; //DU
+        infoArray[7].physicalType = LowOpndRegType_scratch;
+        infoArray[8].regNum = 2;
+        infoArray[8].refCount = 2; //DU
+        infoArray[8].physicalType = LowOpndRegType_scratch;
+        infoArray[9].regNum = 3;
+        infoArray[9].refCount = 2; //DU
+        infoArray[9].physicalType = LowOpndRegType_scratch;
+        if(inst_op == OP_INVOKE_INTERFACE)
+            k = updateInvokeNoRange(infoArray, 10);
+        else
+            k = updateInvokeRange(infoArray, 10);
+        return k;
+#endif
+        ////////////////////////////////////////////// ALU
+    case OP_NEG_INT:
+    case OP_NOT_INT:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //define, update, use
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (vA != vB)
+            infoArray[0].shareWithVR = false;
+        return 1;
+    case OP_NEG_LONG:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //define, use
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 4; //define, update, use
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        return 2;
+    case OP_NOT_LONG:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //define, update, use
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        if (vA != vB)
+            infoArray[0].shareWithVR = false;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        return 2;
+    case OP_NEG_FLOAT:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //define, update, use
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (vA != vB)
+            infoArray[0].shareWithVR = false;
+        return 1;
+    case OP_NEG_DOUBLE:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //define, use
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //define, update, use
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        if (vA != vB)
+            infoArray[0].shareWithVR = false;
+        return 2;
+
+    case OP_INT_TO_LONG: //hard-code eax & edx
+        infoArray[0].regNum = PhysicalReg_EAX;
+        infoArray[0].refCount = 2+1;
+        infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[0].shareWithVR = false;
+        infoArray[1].regNum = PhysicalReg_EDX;
+        infoArray[1].refCount = 1+1; //cdq accesses edx & eax
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 2;
+    case OP_INT_TO_DOUBLE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        return 2;
+    case OP_INT_TO_FLOAT:
+    case OP_LONG_TO_FLOAT:
+    case OP_LONG_TO_DOUBLE:
+    case OP_FLOAT_TO_DOUBLE:
+    case OP_DOUBLE_TO_FLOAT:
+        return 0; //fp stack
+    case OP_LONG_TO_INT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        return 1;
+    case OP_FLOAT_TO_INT:
+    case OP_DOUBLE_TO_INT: //fp stack
+        return 0;
+    case OP_FLOAT_TO_LONG:
+    case OP_DOUBLE_TO_LONG:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //define, use
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //define, use
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //define, use
+        infoArray[2].physicalType = LowOpndRegType_xmm;
+        return 3;
+    case OP_INT_TO_BYTE:
+    case OP_INT_TO_CHAR:
+    case OP_INT_TO_SHORT:
+        vA = currentMIR->dalvikInsn.vA;
+        vB = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4; //define, update, update, use
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (vA != vB)
+            infoArray[0].shareWithVR = false;
+        if (inst_op == OP_INT_TO_BYTE)
+            infoArray[0].is8Bit = true;
+        return 1;
+
+    case OP_ADD_INT:
+    case OP_SUB_INT:
+    case OP_MUL_INT:
+    case OP_AND_INT:
+    case OP_OR_INT:
+    case OP_XOR_INT:
+    case OP_ADD_INT_2ADDR:
+    case OP_SUB_INT_2ADDR:
+    case OP_MUL_INT_2ADDR:
+    case OP_AND_INT_2ADDR:
+    case OP_OR_INT_2ADDR:
+    case OP_XOR_INT_2ADDR:
+        if(inst_op == OP_ADD_INT || inst_op == OP_SUB_INT || inst_op == OP_MUL_INT ||
+           inst_op == OP_AND_INT || inst_op == OP_OR_INT || inst_op == OP_XOR_INT) {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
+        } else {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = vA;
+        }
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //define, update, use
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if(vA != v1)
+            infoArray[0].shareWithVR = false;
+        return 1; //common_alu_int
+
+    case OP_SHL_INT:
+    case OP_SHR_INT:
+    case OP_USHR_INT:
+    case OP_SHL_INT_2ADDR:
+    case OP_SHR_INT_2ADDR:
+    case OP_USHR_INT_2ADDR: //use %cl or %ecx?
+        if(inst_op == OP_SHL_INT || inst_op == OP_SHR_INT || inst_op == OP_USHR_INT) {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
+        } else {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = vA;
+        }
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //define, update, use
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if(vA != v1)
+            infoArray[0].shareWithVR = false;
+        infoArray[1].regNum = PhysicalReg_ECX;
+        infoArray[1].refCount = 2; //define, use
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 2;//common_shift_int
+
+    case OP_DIV_INT:
+    case OP_REM_INT:
+    case OP_DIV_INT_2ADDR:
+    case OP_REM_INT_2ADDR: //hard-code %eax, %edx (dividend in edx:eax; quotient in eax; remainder in edx)
+        if (inst_op == OP_DIV_INT || inst_op == OP_REM_INT) {
+            v2 = currentMIR->dalvikInsn.vC;
+        }else {
+            v2 = currentMIR->dalvikInsn.vB;
+        }
+
+        //Check if the virtual register is a constant because if it is we can figure out result without division
+        isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, &tmpvalue, false);
+
+        //If we have a constant, we can use a multiplication approach instead.
+        //However, we currently do not handle case of -1 constant so we take the divide path.
+        //It also does not make sense to optimize division by zero.
+        if (isConst == VR_IS_CONSTANT && tmpvalue != -1)
+        {
+            if (tmpvalue == 0 || tmpvalue == 1) {
+                infoArray[0].regNum = 2;
+                infoArray[0].refCount = 2;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[1].regNum = PhysicalReg_EAX;
+                infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+                infoArray[1].shareWithVR = false;
+                infoArray[1].refCount = 2;
+                infoArray[2].regNum = PhysicalReg_EDX;
+                infoArray[2].refCount = 1;
+                infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+                if (tmpvalue == 1 && (inst_op == OP_REM_INT || inst_op == OP_REM_INT_2ADDR)) {
+                    infoArray[2].refCount++;
+                } else if (tmpvalue == 1) {
+                    infoArray[1].refCount++;
+                }
+                return 3;
+            } else {
+                int magic, shift;
+                calculateMagicAndShift(tmpvalue, &magic, &shift);
+
+                infoArray[0].regNum = 2;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[1].regNum = PhysicalReg_EAX;
+                infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+                infoArray[1].shareWithVR = false;
+                infoArray[2].regNum = PhysicalReg_EDX;
+                infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+                infoArray[3].regNum = 1;
+                infoArray[3].physicalType = LowOpndRegType_gp;
+                if (inst_op == OP_REM_INT || inst_op == OP_REM_INT_2ADDR) {
+                    infoArray[0].refCount = 8;
+                    infoArray[1].refCount = 7;
+                    infoArray[2].refCount = 9;
+                    infoArray[3].refCount = 3;
+                    if ((tmpvalue > 0 && magic < 0) || (tmpvalue < 0 && magic > 0)) {
+                        infoArray[3].refCount++;
+                        infoArray[2].refCount++;
+                    }
+                    if (shift != 0) {
+                        infoArray[2].refCount++;
+                    }
+                }else {
+                    infoArray[0].refCount = 6;
+                    infoArray[1].refCount = 4;
+                    infoArray[2].refCount = 6;
+                    infoArray[3].refCount = 1;
+                    if ((tmpvalue > 0 && magic < 0) || (tmpvalue < 0 && magic > 0)) {
+                        infoArray[3].refCount++;
+                        infoArray[2].refCount++;
+                    }
+                    if (shift != 0) {
+                        infoArray[2].refCount++;
+                    }
+                }
+               return 4;
+            }
+        }else {
+            infoArray[0].regNum = 2;
+            infoArray[0].refCount = 7; //define, update, use
+            infoArray[0].physicalType = LowOpndRegType_gp;
+            infoArray[1].regNum = PhysicalReg_EAX; //dividend, quotient
+            infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            infoArray[1].shareWithVR = false;
+            infoArray[2].regNum = PhysicalReg_EDX; //export_pc, output for REM
+            infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            infoArray[3].regNum = 1;
+            infoArray[3].refCount = 2; //define, use
+            infoArray[3].physicalType = LowOpndRegType_scratch;
+            infoArray[4].regNum = 3;
+            infoArray[4].refCount = 4; //define, use
+            infoArray[4].physicalType = LowOpndRegType_gp;
+            infoArray[5].regNum = 4;
+            infoArray[5].refCount = 2; //define, use
+            infoArray[5].physicalType = LowOpndRegType_gp;
+            infoArray[5].is8Bit = true;
+            if(inst_op == OP_DIV_INT || inst_op == OP_DIV_INT_2ADDR) {
+                infoArray[1].refCount = 11;
+                infoArray[2].refCount = 9;
+            } else {
+                infoArray[1].refCount = 10;
+                infoArray[2].refCount = 12;
+            }
+            return 6;
+         }
+    case OP_ADD_INT_LIT16:
+    case OP_MUL_INT_LIT16:
+    case OP_AND_INT_LIT16:
+    case OP_OR_INT_LIT16:
+    case OP_XOR_INT_LIT16:
+    case OP_ADD_INT_LIT8:
+    case OP_MUL_INT_LIT8:
+    case OP_AND_INT_LIT8:
+    case OP_OR_INT_LIT8:
+    case OP_XOR_INT_LIT8:
+    case OP_SHL_INT_LIT8:
+    case OP_SHR_INT_LIT8:
+    case OP_USHR_INT_LIT8:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //define, update, use
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if(vA != v1)
+            infoArray[0].shareWithVR = false;
+        return 1;
+
+    case OP_RSUB_INT_LIT8:
+    case OP_RSUB_INT:
+        vA = currentMIR->dalvikInsn.vA;
+        v1 = currentMIR->dalvikInsn.vB;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if(vA != v1)
+            infoArray[0].shareWithVR = false;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 3;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        if(vA != v1)
+            infoArray[1].shareWithVR = false;
+        return 2;
+
+    case OP_DIV_INT_LIT16:
+    case OP_REM_INT_LIT16:
+    case OP_DIV_INT_LIT8:
+    case OP_REM_INT_LIT8:
+        tmp_s2 = currentMIR->dalvikInsn.vC;
+        if((inst_op == OP_DIV_INT_LIT8 || inst_op == OP_DIV_INT_LIT16)) {
+            int power = isPowerOfTwo(tmp_s2);
+            if(power >= 1) { /* divide by a power of 2 constant */
+                infoArray[0].regNum = 2;
+                infoArray[0].refCount = 3; //define, use, use
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[1].regNum = 1;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                if(power == 1) infoArray[1].refCount = 5;
+                else infoArray[1].refCount = 6;
+                return 2;
+            }
+        }
+        if(tmp_s2 == 0) {
+            //export_pc
+            infoArray[0].regNum = PhysicalReg_EDX; //export_pc, output for REM
+            infoArray[0].refCount = 2;
+            infoArray[0].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+            return 1;
+        }
+        if(inst_op == OP_DIV_INT_LIT16 || inst_op == OP_DIV_INT_LIT8) {
+            if(tmp_s2 == -1)
+                infoArray[1].refCount = 4+1;
+            else
+                infoArray[1].refCount = 4;
+            infoArray[2].refCount = 2; //edx
+        } else {
+            if(tmp_s2 == -1)
+                infoArray[1].refCount = 3+1;
+            else
+                infoArray[1].refCount = 3;
+            infoArray[2].refCount = 3; //edx
+        }
+        infoArray[0].regNum = 2;
+        infoArray[0].refCount = 2; //define, use
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = PhysicalReg_EAX; //dividend, quotient
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[1].shareWithVR = false;
+        infoArray[2].regNum = PhysicalReg_EDX; //export_pc, output for REM
+        infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 3;
+
+    case OP_ADD_LONG:
+    case OP_ADD_LONG_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+
+        //In the case of OP_ADD_LONG, we use vB otherwise we use vA
+        if (inst_op == OP_ADD_LONG)
+        {
+            v1 = currentMIR->dalvikInsn.vB;
+        }
+        else
+        {
+            v1 = vA;
+        }
+
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //define, update, use
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if (vA != v1)
+        {
+            infoArray[0].shareWithVR = false;
+        }
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 3; //define, use
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        if (vA != v1)
+        {
+            infoArray[1].shareWithVR = false;
+        }
+        return 2;
+    case OP_SUB_LONG:
+    case OP_AND_LONG:
+    case OP_OR_LONG:
+    case OP_XOR_LONG:
+    case OP_SUB_LONG_2ADDR:
+    case OP_AND_LONG_2ADDR:
+    case OP_OR_LONG_2ADDR:
+    case OP_XOR_LONG_2ADDR:
+        //In the case of non 2ADDR, we use vB otherwise we use vA
+        if(inst_op == OP_ADD_LONG || inst_op == OP_SUB_LONG || inst_op == OP_AND_LONG ||
+           inst_op == OP_OR_LONG || inst_op == OP_XOR_LONG) {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
+        } else {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = vA;
+        }
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //define, update, use
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        if(vA != v1)
+            infoArray[0].shareWithVR = false;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //define, use
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        return 2;
+
+    case OP_SHL_LONG:
+    case OP_SHL_LONG_2ADDR:
+        //In the case of non 2ADDR, we use vB and vC otherwise we use vA
+        if(inst_op == OP_SHL_LONG) {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
+            v2 = currentMIR->dalvikInsn.vC;
+            isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, &tmpvalue, false); //do not update refCount
+        } else {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = vA;
+            isConst = isVirtualRegConstant(v1, LowOpndRegType_gp, &tmpvalue, false); //do not update refCount
+        }
+        if(isConst == 3) {  // case where VR contents is a constant, shift amount is available
+           infoArray[0].regNum = 1;
+           infoArray[0].refCount = 3; //define, update, use
+           infoArray[0].physicalType = LowOpndRegType_xmm;
+           if(vA != v1)
+               infoArray[0].shareWithVR = false;
+           infoArray[1].regNum = 2;
+           infoArray[1].refCount = 1; //define, update, use
+           infoArray[1].physicalType = LowOpndRegType_xmm;
+           infoArray[1].shareWithVR = false;
+           return 2;
+        } else {      // case where VR content is not a constant, shift amount has to be read from VR.
+           infoArray[0].regNum = 1;
+           infoArray[0].refCount = 3; //define, update, use
+           infoArray[0].physicalType = LowOpndRegType_xmm;
+           if(vA != v1)
+               infoArray[0].shareWithVR = false;
+           infoArray[1].regNum = 2;
+           infoArray[1].refCount = 3; //define, update, use
+           infoArray[1].physicalType = LowOpndRegType_xmm;
+           infoArray[1].shareWithVR = false;
+           infoArray[2].regNum = 3;
+           infoArray[2].refCount = 2; //define, use
+           infoArray[2].physicalType = LowOpndRegType_xmm;
+           return 3;
+        }
+    case OP_SHR_LONG:
+    case OP_SHR_LONG_2ADDR:
+        if(inst_op == OP_SHR_LONG) {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
+        } else {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = vA;
+        }
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4; //define, update, use
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        if(vA != v1)
+            infoArray[0].shareWithVR = false;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 4; //define, update, use
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        infoArray[1].shareWithVR = false;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //define, use
+        infoArray[2].physicalType = LowOpndRegType_xmm;
+        infoArray[3].regNum = 4;
+        infoArray[3].refCount = 3;
+        infoArray[3].physicalType = LowOpndRegType_xmm;
+        infoArray[4].regNum = 5;
+        infoArray[4].refCount = 3;
+        infoArray[4].physicalType = LowOpndRegType_xmm;
+        return 5;
+
+    case OP_USHR_LONG:
+    case OP_USHR_LONG_2ADDR:
+        if(inst_op == OP_USHR_LONG) {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = currentMIR->dalvikInsn.vB;
+            v2 = currentMIR->dalvikInsn.vC;
+            isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, &tmpvalue, false);
+        } else {
+            vA = currentMIR->dalvikInsn.vA;
+            v1 = vA;
+            isConst = isVirtualRegConstant(v1, LowOpndRegType_gp, &tmpvalue, false);
+        }
+        if (isConst == 3) { // case where VR contents is a constant, shift amount is available
+            infoArray[0].regNum = 1;
+            infoArray[0].refCount = 3; //define, update, use
+            infoArray[0].physicalType = LowOpndRegType_xmm;
+            if(vA != v1)
+                infoArray[0].shareWithVR = false;
+            infoArray[1].regNum = 2;
+            infoArray[1].refCount = 1; //define, update, use
+            infoArray[1].physicalType = LowOpndRegType_xmm;
+            infoArray[1].shareWithVR = false;
+            return 2;
+         } else { // case where VR content is not a constant, shift amount has to be read from VR.
+            infoArray[0].regNum = 1;
+            infoArray[0].refCount = 3; //define, update, use
+            infoArray[0].physicalType = LowOpndRegType_xmm;
+            if(vA != v1)
+                infoArray[0].shareWithVR = false;
+            infoArray[1].regNum = 2;
+            infoArray[1].refCount = 3; //define, update, use
+            infoArray[1].physicalType = LowOpndRegType_xmm;
+            infoArray[1].shareWithVR = false;
+            infoArray[2].regNum = 3;
+            infoArray[2].refCount = 2; //define, use
+            infoArray[2].physicalType = LowOpndRegType_xmm;
+            return 3;
+         }
+
+    case OP_MUL_LONG:
+    case OP_MUL_LONG_2ADDR:
+        if(inst_op == OP_MUL_LONG)
+        {
+            v1 = currentMIR->dalvikInsn.vB;
+        }
+        else
+        {
+            //For 2addr form, the destination is also first operand
+            v1 = currentMIR->dalvikInsn.vA;
+        }
+        v2 = currentMIR->dalvikInsn.vC;
+
+        if (v1 != v2) // when the multiplicands are not the same
+        {
+           infoArray[0].regNum = 1;
+           infoArray[0].refCount = 6;
+           infoArray[0].physicalType = LowOpndRegType_gp;
+           infoArray[0].shareWithVR = false;
+           infoArray[1].regNum = 2;
+           infoArray[1].refCount = 3;
+           infoArray[1].physicalType = LowOpndRegType_gp;
+           infoArray[2].regNum = 3;
+           infoArray[2].refCount = 3;
+           infoArray[2].physicalType = LowOpndRegType_gp;
+           infoArray[3].regNum = PhysicalReg_EAX;
+           infoArray[3].refCount = 2+1; //for mul_opc
+           infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+           infoArray[4].regNum = PhysicalReg_EDX;
+           infoArray[4].refCount = 2; //for mul_opc
+           infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+           return 5;
+        }
+        else // when square of a number is to be computed
+        {
+           infoArray[0].regNum = 1;
+           infoArray[0].refCount = 8;
+           infoArray[0].physicalType = LowOpndRegType_gp;
+           infoArray[0].shareWithVR = false;
+           infoArray[1].regNum = PhysicalReg_EAX;
+           infoArray[1].refCount = 2+1; //for mul_opc
+           infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+           infoArray[2].regNum = PhysicalReg_EDX;
+           infoArray[2].refCount = 3+1; //for mul_opc
+           infoArray[2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+           return 3;
+         }
+    case OP_DIV_LONG:
+    case OP_REM_LONG:
+    case OP_DIV_LONG_2ADDR:
+    case OP_REM_LONG_2ADDR:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[0].shareWithVR = false;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 3;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 1;
+        infoArray[2].refCount = 2;
+        infoArray[2].physicalType = LowOpndRegType_xmm;
+        infoArray[3].regNum = PhysicalReg_EAX;
+        infoArray[3].refCount = 2; //defined by function call
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[4].regNum = PhysicalReg_EDX;
+        infoArray[4].refCount = 2; //next version has 2 references
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[5].regNum = 1;
+        infoArray[5].refCount = 2;
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+        return 6;
+
+    case OP_ADD_FLOAT:
+    case OP_SUB_FLOAT:
+    case OP_MUL_FLOAT:
+    case OP_ADD_FLOAT_2ADDR:
+    case OP_SUB_FLOAT_2ADDR:
+    case OP_MUL_FLOAT_2ADDR:
+    case OP_ADD_DOUBLE: //PhysicalReg_FP TODO
+    case OP_SUB_DOUBLE:
+    case OP_MUL_DOUBLE:
+    case OP_ADD_DOUBLE_2ADDR:
+    case OP_SUB_DOUBLE_2ADDR:
+    case OP_MUL_DOUBLE_2ADDR:
+    case OP_DIV_FLOAT:
+    case OP_DIV_FLOAT_2ADDR:
+    case OP_DIV_DOUBLE:
+    case OP_DIV_DOUBLE_2ADDR:
+        vA = currentMIR->dalvikInsn.vA;
+        //In the case of non 2ADDR, we use vB and vC otherwise we use vA
+        if (inst_op == OP_ADD_FLOAT || inst_op == OP_SUB_FLOAT
+                || inst_op == OP_MUL_FLOAT || inst_op == OP_ADD_DOUBLE
+                || inst_op == OP_SUB_DOUBLE || inst_op == OP_MUL_DOUBLE
+                || inst_op == OP_DIV_FLOAT || inst_op == OP_DIV_DOUBLE)
+        {
+            v1 = currentMIR->dalvikInsn.vB;
+        }
+        else
+        {
+            v1 = vA;
+        }
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        if(vA != v1)
+        {
+            infoArray[0].shareWithVR = false;
+        }
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        return 2;
+
+    case OP_REM_FLOAT:
+    case OP_REM_FLOAT_2ADDR:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 1;
+        infoArray[2].refCount = 2;
+        infoArray[2].physicalType = LowOpndRegType_scratch;
+        return 3;
+
+    case OP_REM_DOUBLE:
+    case OP_REM_DOUBLE_2ADDR:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        infoArray[2].regNum = 1;
+        infoArray[2].refCount = 2;
+        infoArray[2].physicalType = LowOpndRegType_scratch;
+        return 3;
+
+    case OP_CMPL_FLOAT:
+    case OP_CMPL_DOUBLE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        infoArray[1].regNum = 1;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 2;
+        infoArray[2].refCount = 2;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 3;
+        infoArray[3].refCount = 2;
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 4; //return
+        infoArray[4].refCount = 5;
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        return 5;
+
+    case OP_CMPG_FLOAT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        return 1;
+
+    case OP_CMPG_DOUBLE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_xmm;
+        return 1;
+
+    case OP_CMP_LONG:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        return 2;
+
+    case OP_EXECUTE_INLINE:
+    case OP_EXECUTE_INLINE_RANGE:
+        num = currentMIR->dalvikInsn.vA;
+#if defined(WITH_JIT)
+        tmp = currentMIR->dalvikInsn.vB;
+        switch (tmp) {
+            case INLINE_STRING_LENGTH:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 3;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 2;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[2].regNum = 3;
+                infoArray[2].refCount = 2;
+                infoArray[2].physicalType = LowOpndRegType_gp;
+                infoArray[3].regNum = 1;
+                infoArray[3].refCount = 2;
+                infoArray[3].physicalType = LowOpndRegType_scratch;
+                return 4;
+            case INLINE_STRING_IS_EMPTY:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 3;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 4;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[2].regNum = 1;
+                infoArray[2].refCount = 2;
+                infoArray[2].physicalType = LowOpndRegType_scratch;
+                return 3;
+            case INLINE_STRING_CHARAT:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 7;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[0].shareWithVR = false;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 7;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[1].shareWithVR = false;
+                infoArray[2].regNum = 3;
+                infoArray[2].refCount = 2;
+                infoArray[2].physicalType = LowOpndRegType_gp;
+                return 3;
+            case INLINE_STRING_FASTINDEXOF_II:
+#if defined(USE_GLOBAL_STRING_DEFS)
+                break;
+#else
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 14 * LOOP_COUNT;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[0].shareWithVR = false;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 3 * LOOP_COUNT;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[2].regNum = 3;
+                infoArray[2].refCount = 11 * LOOP_COUNT;
+                infoArray[2].physicalType = LowOpndRegType_gp;
+                infoArray[2].shareWithVR = false;
+                infoArray[3].regNum = 4;
+                infoArray[3].refCount = 3 * LOOP_COUNT;
+                infoArray[3].physicalType = LowOpndRegType_gp;
+                infoArray[4].regNum = 5;
+                infoArray[4].refCount = 9 * LOOP_COUNT;
+                infoArray[4].physicalType = LowOpndRegType_gp;
+                infoArray[5].regNum = 6;
+                infoArray[5].refCount = 4 * LOOP_COUNT;
+                infoArray[5].physicalType = LowOpndRegType_gp;
+                infoArray[6].regNum = 7;
+                infoArray[6].refCount = 2;
+                infoArray[6].physicalType = LowOpndRegType_gp;
+                infoArray[7].regNum = 1;
+                infoArray[7].refCount = 2;
+                infoArray[7].physicalType = LowOpndRegType_scratch;
+                return 8;
+#endif
+            case INLINE_MATH_ABS_LONG:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 7;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[0].shareWithVR = false;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 2;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[2].regNum = 3;
+                infoArray[2].refCount = 3;
+                infoArray[2].physicalType = LowOpndRegType_gp;
+                infoArray[3].regNum = 4;
+                infoArray[3].refCount = 3;
+                infoArray[3].physicalType = LowOpndRegType_gp;
+                infoArray[4].regNum = 5;
+                infoArray[4].refCount = 2;
+                infoArray[4].physicalType = LowOpndRegType_gp;
+                infoArray[5].regNum = 6;
+                infoArray[5].refCount = 5;
+                infoArray[5].physicalType = LowOpndRegType_gp;
+                return 6;
+            case INLINE_MATH_ABS_INT:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 5;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[0].shareWithVR = false;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 4;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[2].regNum = 3;
+                infoArray[2].refCount = 2;
+                infoArray[2].physicalType = LowOpndRegType_gp;
+                return 3;
+            case INLINE_MATH_MAX_INT:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 4;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[0].shareWithVR = false;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 3;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[2].regNum = 3;
+                infoArray[2].refCount = 2;
+                infoArray[2].physicalType = LowOpndRegType_gp;
+                return 3;
+            case INLINE_MATH_MIN_INT:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 4;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[0].shareWithVR = false;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 3;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[2].regNum = 3;
+                infoArray[2].refCount = 2;
+                infoArray[2].physicalType = LowOpndRegType_gp;
+                return 3;
+            case INLINE_MATH_ABS_FLOAT:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 3;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[0].shareWithVR = false;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 2;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                return 2;
+            case INLINE_MATH_ABS_DOUBLE:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 3;
+                infoArray[0].physicalType = LowOpndRegType_xmm;
+                infoArray[0].shareWithVR = false;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 2;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                return 2;
+            case INLINE_FLOAT_TO_RAW_INT_BITS:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 2;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 2;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                return 2;
+            case INLINE_INT_BITS_TO_FLOAT:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 2;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 2;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                return 2;
+            case INLINE_DOUBLE_TO_RAW_LONG_BITS:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 2;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 2;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[2].regNum = 3;
+                infoArray[2].refCount = 3;
+                infoArray[2].physicalType = LowOpndRegType_gp;
+                return 3;
+            case INLINE_LONG_BITS_TO_DOUBLE:
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 2;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 2;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+                infoArray[2].regNum = 3;
+                infoArray[2].refCount = 3;
+                infoArray[2].physicalType = LowOpndRegType_gp;
+                return 3;
+            default:
+                break;
+        }
+#endif
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 4;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        if(num >= 1) {
+            infoArray[1].regNum = 2;
+            infoArray[1].refCount = 2;
+            infoArray[1].physicalType = LowOpndRegType_gp;
+        }
+        if(num >= 2) {
+            infoArray[2].regNum = 3;
+            infoArray[2].refCount = 2;
+            infoArray[2].physicalType = LowOpndRegType_gp;
+        }
+        if(num >= 3) {
+            infoArray[3].regNum = 4;
+            infoArray[3].refCount = 2;
+            infoArray[3].physicalType = LowOpndRegType_gp;
+        }
+        if(num >= 4) {
+            infoArray[4].regNum = 5;
+            infoArray[4].refCount = 2;
+            infoArray[4].physicalType = LowOpndRegType_gp;
+        }
+        infoArray[num+1].regNum = 6;
+        infoArray[num+1].refCount = 2;
+        infoArray[num+1].physicalType = LowOpndRegType_gp;
+        infoArray[num+2].regNum = PhysicalReg_EAX;
+        infoArray[num+2].refCount = 2;
+        infoArray[num+2].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[num+3].regNum = PhysicalReg_EDX;
+        infoArray[num+3].refCount = 2;
+        infoArray[num+3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[num+4].regNum = 1;
+        infoArray[num+4].refCount = 4;
+        infoArray[num+4].physicalType = LowOpndRegType_scratch;
+        return num+5;
+#if FIXME
+    case OP_INVOKE_OBJECT_INIT_RANGE:
+        return 0;
+#endif
+    case OP_INVOKE_VIRTUAL_QUICK:
+    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
+#ifdef PREDICTED_CHAINING
+        numTmps = updateGenPrediction(infoArray, false /*not interface*/);
+        infoArray[numTmps].regNum = 1;
+        infoArray[numTmps].refCount = 3; //DU
+        infoArray[numTmps].physicalType = LowOpndRegType_gp;
+        numTmps++;
+        if(inst_op == OP_INVOKE_VIRTUAL_QUICK)
+            k = updateInvokeNoRange(infoArray, numTmps, currentMIR);
+        else
+            k = updateInvokeRange(infoArray, numTmps, currentMIR);
+        return k;
+#else
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+
+        infoArray[3].regNum = PhysicalReg_ECX;
+        infoArray[3].refCount = 1+1;
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[4].regNum = PhysicalReg_EDX;
+        infoArray[4].refCount = 2;
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        if(inst_op == OP_INVOKE_VIRTUAL_QUICK_RANGE)
+            k = updateInvokeRange(infoArray, 5);
+        else
+            k = updateInvokeNoRange(infoArray, 5);
+        return k;
+#endif
+    case OP_INVOKE_SUPER_QUICK:
+    case OP_INVOKE_SUPER_QUICK_RANGE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2;
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 4;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 5;
+        infoArray[2].refCount = 2;
+        infoArray[2].physicalType = LowOpndRegType_gp;
+
+        infoArray[3].regNum = PhysicalReg_ECX;
+        infoArray[3].refCount = 1+1;
+        infoArray[3].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        infoArray[4].regNum = PhysicalReg_EDX;
+        infoArray[4].refCount = 2;
+        infoArray[4].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+
+        infoArray[5].regNum = 1;
+        infoArray[5].refCount = 2;
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+        infoArray[6].regNum = 2;
+        infoArray[6].refCount = 2;
+        infoArray[6].physicalType = LowOpndRegType_scratch;
+        if(inst_op == OP_INVOKE_SUPER_QUICK_RANGE)
+            k = updateInvokeRange(infoArray, 7, currentMIR);
+        else
+            k = updateInvokeNoRange(infoArray, 7, currentMIR);
+        return k;
+#ifdef SUPPORT_HLO
+    case kExtInstruction:
+        switch(inst) {
+    case OP_X_AGET_QUICK:
+    case OP_X_AGET_OBJECT_QUICK:
+    case OP_X_AGET_BOOLEAN_QUICK:
+    case OP_X_AGET_BYTE_QUICK:
+    case OP_X_AGET_CHAR_QUICK:
+    case OP_X_AGET_SHORT_QUICK:
+        vA = FETCH(1) & 0xff;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 4;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[3].linkageToVR = vA;
+        if(inst == OP_X_AGET_BYTE_QUICK || inst == OP_X_AGET_BOOLEAN_QUICK)
+            infoArray[3].is8Bit = true;
+        return 4;
+    case OP_X_AGET_WIDE_QUICK:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 1;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_xmm;
+        return 4;
+    case OP_X_APUT_QUICK:
+    case OP_X_APUT_OBJECT_QUICK:
+    case OP_X_APUT_BOOLEAN_QUICK:
+    case OP_X_APUT_BYTE_QUICK:
+    case OP_X_APUT_CHAR_QUICK:
+    case OP_X_APUT_SHORT_QUICK:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 4;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        if(inst == OP_X_APUT_BYTE_QUICK || inst == OP_X_APUT_BOOLEAN_QUICK)
+            infoArray[3].is8Bit = true;
+        return 4;
+    case OP_X_APUT_WIDE_QUICK:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 1;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_xmm;
+        return 4;
+    case OP_X_DEREF_GET:
+    case OP_X_DEREF_GET_OBJECT:
+    case OP_X_DEREF_GET_BOOLEAN:
+    case OP_X_DEREF_GET_BYTE:
+    case OP_X_DEREF_GET_CHAR:
+    case OP_X_DEREF_GET_SHORT:
+        vA = FETCH(1) & 0xff;
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[1].linkageToVR = vA;
+        if(inst == OP_X_DEREF_GET_BYTE || inst == OP_X_DEREF_GET_BOOLEAN)
+            infoArray[1].is8Bit = true;
+        return 2;
+    case OP_X_DEREF_GET_WIDE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 1;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        return 2;
+    case OP_X_DEREF_PUT:
+    case OP_X_DEREF_PUT_OBJECT:
+    case OP_X_DEREF_PUT_BOOLEAN:
+    case OP_X_DEREF_PUT_BYTE:
+    case OP_X_DEREF_PUT_CHAR:
+    case OP_X_DEREF_PUT_SHORT:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        if(inst == OP_X_DEREF_PUT_BYTE || inst == OP_X_DEREF_PUT_BOOLEAN)
+            infoArray[1].is8Bit = true;
+        return 2;
+    case OP_X_DEREF_PUT_WIDE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 1;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_xmm;
+        return 2;
+    case OP_X_ARRAY_CHECKS:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        return 2;
+    case OP_X_CHECK_BOUNDS:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 2; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        return 2;
+    case OP_X_CHECK_NULL:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 2; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = PhysicalReg_EDX;
+        infoArray[1].refCount = 2;
+        infoArray[1].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 2;
+    case OP_X_CHECK_TYPE:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 3; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 5;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 6;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 1;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_scratch;
+        infoArray[5].regNum = PhysicalReg_EAX;
+        infoArray[5].refCount = 2;
+        infoArray[5].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 6;
+    case OP_X_ARRAY_OBJECT_CHECKS:
+        infoArray[0].regNum = 1;
+        infoArray[0].refCount = 3; //DU
+        infoArray[0].physicalType = LowOpndRegType_gp;
+        infoArray[1].regNum = 2;
+        infoArray[1].refCount = 4; //DU
+        infoArray[1].physicalType = LowOpndRegType_gp;
+        infoArray[2].regNum = 3;
+        infoArray[2].refCount = 2; //DU
+        infoArray[2].physicalType = LowOpndRegType_gp;
+        infoArray[3].regNum = 5;
+        infoArray[3].refCount = 2; //DU
+        infoArray[3].physicalType = LowOpndRegType_gp;
+        infoArray[4].regNum = 6;
+        infoArray[4].refCount = 2; //DU
+        infoArray[4].physicalType = LowOpndRegType_gp;
+        infoArray[5].regNum = 1;
+        infoArray[5].refCount = 2; //DU
+        infoArray[5].physicalType = LowOpndRegType_scratch;
+        infoArray[6].regNum = PhysicalReg_EAX;
+        infoArray[6].refCount = 2;
+        infoArray[6].physicalType = LowOpndRegType_gp | LowOpndRegType_hard;
+        return 7;
+    }
+#endif
+    default:
+        ALOGI("JIT_INFO: JIT does not support bytecode 0x%hx when updating temp accesses",
+                currentMIR->dalvikInsn.opcode);
+        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+        assert(false && "All opcodes should be supported.");
+        break;
+    }
+    return -1;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/CodegenErrors.cpp b/vm/compiler/codegen/x86/lightcg/CodegenErrors.cpp
new file mode 100644
index 0000000..0f82fbc
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/CodegenErrors.cpp
@@ -0,0 +1,259 @@
+/*
+ * Copyright (C) 2012-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "CodegenErrors.h"
+
+/**
+ * @class ErrorInformation Used to keep track of information associated with an error
+ */
+
+struct ErrorInformation
+{
+    /** @brief Type of error */
+    JitCompilationErrors type;
+
+    /** @brief Error message */
+    const char * errorMessage;
+
+    /** @brief Whether we can possibly fix the error */
+    bool canResolve;
+
+    /** @brief Whether error is fatal */
+    bool isFatal;
+};
+
+/**
+ * @brief Three macros to help error definitions
+ */
+
+
+#define START_ERRORS \
+    static ErrorInformation gErrorInformation[] = {
+
+#define NEW_ERROR(TYPE, MESSAGE, CANRESOLVE, ISFATAL) \
+        { \
+            TYPE, MESSAGE, CANRESOLVE, ISFATAL \
+        }
+
+#define END_ERRORS \
+    };
+
+/**
+ * @brief Table that stores information about errors defined in
+ * JitCompilationErrors
+ */
+START_ERRORS
+    NEW_ERROR (kJitErrorMaxVR,                "Exceeded maximum allowed VRs in a basic block.",               false, false),
+    NEW_ERROR (kJitErrorShortJumpOffset,      "Jump offset greater than 8-bits.",                              true, false),
+    NEW_ERROR (kJitErrorUnsupportedBytecode,  "Trace contains bytecode with no implementation.",              false, false),
+    NEW_ERROR (kJitErrorUnresolvedField,      "Trace contains SGET / SPUT bytecode with unresolved field.",   false, false),
+    NEW_ERROR (kJitErrorInvalidBBId,          "Cannot find BasicBlock_O1 corresponding to a BasicBlock.",     false, false),
+    NEW_ERROR (kJitErrorCodeCacheFull,        "Jit code cache is full.",                                       true, false),
+    NEW_ERROR (kJitErrorRegAllocFailed,       "Failure in register allocator or register tables.",            false, false),
+    NEW_ERROR (kJitErrorMallocFailed,         "Malloc failure during trace compilation.",                      false, true),
+    NEW_ERROR (kJitErrorMaxXferPoints,        "Exceeded maximum number of transfer points per BB.",           false, false),
+    NEW_ERROR (kJitErrorMaxDestRegPerSource,  "Exceeded number of destination regs for a source reg.",        false, false),
+    NEW_ERROR (kJitErrorStateTransfer,        "Problem with state transfer in JIT.",                          false, false),
+    NEW_ERROR (kJitErrorTraceFormation,       "Problem with trace formation.",                                false, false),
+    NEW_ERROR (kJitErrorNullBoundCheckFailed, "Problem while performing null or bound check.",                false, false),
+    NEW_ERROR (kJitErrorMergeLiveRange,       "Problem while merging live ranges (mergeLiveRange).",          false, false),
+    NEW_ERROR (kJitErrorGlobalData,           "Global data not defined.",                                     false, false),
+    NEW_ERROR (kJitErrorInsScheduling,        "Problem during instruction scheduling.",                       false, false),
+    NEW_ERROR (kJitErrorBERegisterization,    "Issue registerizing the trace in the backend.",                 true, false),
+    NEW_ERROR (kJitErrorSpill,                "The trace provoked a spill.",                                   true, false),
+    NEW_ERROR (kJitErrorBBCannotBeHandled,    "The backend decided it cannot safely handle the Basic Block.", false, false),
+    NEW_ERROR (kJitErrorConstInitFail,        "Patching of Double/Long constants failed.",                     true, false),
+    NEW_ERROR (kJitErrorChainingCell,         "An issue was encountered while generating chaining cell.",     false, false),
+    NEW_ERROR (kJitErrorInvalidOperandSize,   "Invalid Operand Size was encountered.",                        false, false),
+    NEW_ERROR (kJitErrorPlugin,               "Problem with the plugin system.",                              false, false),
+    NEW_ERROR (kJitErrorConstantFolding,      "Constant folding failed due to unhandled case.",               false, false),
+    NEW_ERROR (kJitErrorCodegen,              "Undefined issues in trace formation.",                         false, false),
+END_ERRORS
+
+
+/**
+ * @brief Tries to resolve errors from which we can recover.
+ * @param cUnit Compilation unit
+ * @param isLastAttempt whether the next compilation attempt will be the last one
+ */
+inline void resolveErrors(CompilationUnit * cUnit, bool isLastAttempt) {
+    /* Handle any errors which can be handled */
+
+    //Handle error due to large jump offset
+    if ( IS_JIT_ERROR_SET(kJitErrorShortJumpOffset)) {
+        gDvmJit.disableOpt |= (1 << kShortJumpOffset);
+        ALOGI("JIT_INFO: Successfully resolved short jump offset issue");
+        //Clear the error:
+        CLEAR_JIT_ERROR(kJitErrorShortJumpOffset);
+    }
+
+    //Handle error due to spilling
+    if (IS_JIT_ERROR_SET(kJitErrorSpill)) {
+        //Clear the error:
+        CLEAR_JIT_ERROR(kJitErrorSpill);
+
+        //Ok we are going to see if we are registerizing something
+        int max = cUnit->maximumRegisterization;
+
+        // We should only get this error if maximum registerization is > 0
+        assert (max > 0);
+
+        // Divide it by 2, fastest way to get to 0 if we have issues across the board
+        // If it is a last attempt then force setting to 0
+        // It is better to compile instead of give a last try for registerization
+        int newMax = (isLastAttempt == true) ? 0 : max / 2;
+        cUnit->maximumRegisterization = newMax;
+        ALOGI("Trying less registerization from %d to %d", max, newMax);
+    }
+
+    //Handle error due to backend
+    if (IS_JIT_ERROR_SET(kJitErrorBERegisterization)) {
+        //If registerization in the Backend is on
+        if (gDvmJit.backEndRegisterization == true) {
+
+            //Turn off registerization
+            gDvmJit.backEndRegisterization = false;
+
+            //Set maximum registerization for this cUnit to 0
+            //since we disabled registerization
+            cUnit->maximumRegisterization = 0;
+
+            //Clear the error.
+            //Registerization can cause other errors
+            //Let's clear all errors for now, and see if they
+            //re-occur without registerization.
+            CLEAR_ALL_JIT_ERRORS();
+
+            //Notify about this special action
+            ALOGI("Ignoring other issues and retrying without backend registerization");
+        }
+    }
+
+    //Handle constant initialization failure
+    if (IS_JIT_ERROR_SET(kJitErrorConstInitFail) == true) {
+        gDvmJit.disableOpt |= (1 << kElimConstInitOpt);
+        ALOGI("Resolved error due to constant initialization failure");
+        //Clear the error:
+        CLEAR_JIT_ERROR(kJitErrorConstInitFail);
+    }
+}
+
+/**
+ * @brief Checks whether a compilation should be re-attempted
+ * Fixes anything which can be fixed. At the end we
+ * either don't have an error, have fixed the error, or
+ * cannot recover from the error. For the second case, we
+ * retry the compilation.
+ * @param cUnit Compilation Unit context
+ * @param isLastAttempt whether the next compilation attempt will be the last one
+ * @return whether we can retry the trace.
+ */
+bool dvmCanFixErrorsAndRetry(CompilationUnit *cUnit, bool isLastAttempt){
+    // Make sure that the error information table is synced up with the
+    // error enum JitCompilationErrors
+    assert (NELEM(gErrorInformation) == static_cast<int>(kJitErrorMaxDefined));
+
+    // Checks if any error has occurred. If not, we do not need to retry.
+    if (IS_ANY_JIT_ERROR_SET() == false) {
+        return false;
+    }
+
+    bool firstError = true;
+
+    //Get the maximum error number the system is aware of so far:
+    int maxError = static_cast<int>(kJitErrorCodegen);
+
+    // Check which errors have been raised
+    for (int errorIndex = 0; errorIndex <= maxError; errorIndex++) {
+
+        //Get the error from the table
+        JitCompilationErrors error = gErrorInformation[errorIndex].type;
+
+        if (IS_JIT_ERROR_SET(error) == true) {
+
+            // If this is the first error we are seeing, print some information
+            // to be noticeable in the logs
+            if (firstError == true) {
+                ALOGD("++++++++++++++++++++++++++++++++++++++++++++");
+                ALOGD("JIT_INFO: Issues while compiling trace  %s%s, offset %d",
+                        cUnit->method->clazz->descriptor, cUnit->method->name,
+                        cUnit->traceDesc->trace[0].info.frag.startOffset);
+
+                // If kJitErrorCodegen is the first error we encounter,
+                // somebody forgot to raise an error flag somewhere.
+                // Otherwise, we should clear the flag because another
+                // non-generic message will be printed out.
+                if (error != kJitErrorCodegen) {
+                    CLEAR_JIT_ERROR(kJitErrorCodegen);
+                }
+
+                firstError = false;
+            }
+
+            // Paranoid. Make sure we actually found an entry
+            assert (errorIndex < NELEM(gErrorInformation));
+
+            // Find out if error is fatal
+            bool fatalError = gErrorInformation[errorIndex].isFatal;
+
+            // If we are set to abort on error and error cannot be resolved, then
+            // the error is fatal.
+            fatalError = (fatalError == true) ||
+                            (gDvmJit.abortOnCompilerError == true
+                                && gErrorInformation[errorIndex].canResolve == false);
+
+            // Print error message
+            if (fatalError == true) {
+                ALOGE("\t%s",
+                        gErrorInformation[errorIndex].errorMessage);
+                ALOGE("FATAL_ERRORS in JIT. Aborting compilation.");
+                dvmCompilerAbort(cUnit);
+            }
+            else {
+                ALOGD("\t%s",
+                        gErrorInformation[errorIndex].errorMessage);
+            }
+        }
+    }
+
+    // Handle any errors which can be resolved
+    resolveErrors(cUnit, isLastAttempt);
+
+    // If we have no errors set at this point, we have successfully resolved
+    // them and thus we can retry the trace now.
+    if (IS_ANY_JIT_ERROR_SET() == false){
+        ALOGD("JIT_INFO: Retrying trace %s%s, offset %d", cUnit->method->clazz->descriptor,
+                cUnit->method->name, cUnit->traceDesc->trace[0].info.frag.startOffset);
+        ALOGD("++++++++++++++++++++++++++++++++++++++++++++");
+        return true;
+    }
+
+    // Otherwise, error cannot be handled or does not have a handler
+    ALOGD("JIT_INFO: Terminating trace due to unresolved issues");
+    ALOGD("++++++++++++++++++++++++++++++++++++++++++++");
+
+    return false;
+}
+
+void dvmSaveOptimizationState(SErrorCompilationState &info) {
+    info.disableOpt = gDvmJit.disableOpt;
+    info.backEndRegisterization = gDvmJit.backEndRegisterization;
+}
+
+void dvmRestoreCompilationState(SErrorCompilationState &info) {
+    gDvmJit.disableOpt = info.disableOpt;
+    gDvmJit.backEndRegisterization = info.backEndRegisterization;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/CodegenErrors.h b/vm/compiler/codegen/x86/lightcg/CodegenErrors.h
new file mode 100644
index 0000000..4301da0
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/CodegenErrors.h
@@ -0,0 +1,161 @@
+/*
+ * Copyright (C) 2012-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @enum JitCompilationErrors
+ * @brief Possible errors which can happen during compilation
+ * Values indicate bit position in DvmJitGlobals.jitErrorFlags + 1
+ * IMPORTANT: Update jitErrorMessages when making changes to this list
+ */
+
+#ifndef CODEGEN_X86_CODEGEN_ERRORS_H_
+#define CODEGEN_X86_CODEGEN_ERRORS_H_
+
+#include "Dalvik.h"
+#include "../../CompilerIR.h"
+
+/**
+ * @brief Keep any information that can be changed by the error framework
+ */
+typedef struct sErrorCompilationState {
+    int disableOpt;                 /**< @brief Disable the optimizations */
+    bool backEndRegisterization;    /**< @brief Backend registerization */
+}SErrorCompilationState;
+
+/**
+ * @class ErrorFlags
+ */
+enum JitCompilationErrors {
+    /** @brief Exceeded maximum allowed VRs in a basic block */
+    kJitErrorMaxVR = 0,
+    /** @brief 8-bit jump offset not enough to reach label */
+    kJitErrorShortJumpOffset,
+    /** @brief Trace contains a bytecode with no JIT implementation */
+    kJitErrorUnsupportedBytecode,
+    /** @brief Field ptr unresolved for SGET/SPUT bytecodes */
+    kJitErrorUnresolvedField,
+    /** @brief Cannot find BasicBlock_O1 corresponding to a BasicBlock */
+    kJitErrorInvalidBBId,
+    /** @brief JIT code cache is full */
+    kJitErrorCodeCacheFull,
+    /** @brief Failures while allocating registers or error
+     *  in locating / putting registers in register tables
+     */
+    kJitErrorRegAllocFailed,
+    /** @brief Malloc failed. */
+    kJitErrorMallocFailed,
+    /** @brief Exceeded maximum number of transfer points per BB */
+    kJitErrorMaxXferPoints,
+    /** @brief Exceeded number of destination regs for a source reg */
+    kJitErrorMaxDestRegPerSource,
+    /** @brief Problem with state transfer in JIT */
+    kJitErrorStateTransfer,
+    /** @brief General trace formation issues */
+    kJitErrorTraceFormation,
+    /** @brief Errors while performing Null and Bound checks */
+    kJitErrorNullBoundCheckFailed,
+    /** @brief Errors while merging LiveRanges */
+    kJitErrorMergeLiveRange,
+    /** @brief Errors while accessing global data */
+    kJitErrorGlobalData,
+    /** @brief Errors while scheduling instructions */
+    kJitErrorInsScheduling,
+    /** @brief Errors due to backend registerization */
+    kJitErrorBERegisterization,
+    /** @brief Errors due to spilling logical registers */
+    kJitErrorSpill,
+    /** @brief Set when a basic block is reject by backend */
+    kJitErrorBBCannotBeHandled,
+    /** @brief Errors while performing double/long constant initialization */
+    kJitErrorConstInitFail,
+    /** @brief Error while generating chaining cell */
+    kJitErrorChainingCell,
+    /** @brief Invalid operand size */
+    kJitErrorInvalidOperandSize,
+    /** @brief Problem with the plugin system */
+    kJitErrorPlugin,
+    /** @brief Unhandled case during constant folding */
+    kJitErrorConstantFolding,
+
+    /* ----- Add more errors above ---------------------------
+     * ----- Don't add new errors beyond this point ----------
+     * When adding more errors, update error information table
+     * in CodegenErrors.cpp
+     */
+
+    /** @brief Indicates "some" error happened
+     * Specifically, the purpose is that if someone forgets
+     * to use SET_JIT_ERROR at the specific error location,
+     * but does throw a return, the function handling that
+     * return can set this generic error. Also useful if a
+     * function can set multiple errors, the calling function
+     * won't have to worry about which one to set. Hopefully
+     * all of the errors have been individually set too.
+     * THIS NEEDS TO BE THE SECOND LAST VALUE
+     */
+    kJitErrorCodegen,
+    /** @brief Guarding value
+     * THIS NEEDS TO BE THE LAST VALUE
+     */
+    kJitErrorMaxDefined
+};
+
+/*
+ * Getter / Setter for  JitCompilationErrors
+ */
+/* Sets a particular error */
+#define SET_JIT_ERROR(jit_error)  gDvmJit.jitErrorFlags |= (1 << (int) jit_error);
+
+/* Clears only a particular error */
+#define CLEAR_JIT_ERROR(jit_error) gDvmJit.jitErrorFlags &= ~(1 << (int) jit_error);                                                                                    \
+
+/* Checks if a particular error is set */
+#define IS_JIT_ERROR_SET(jit_error) ( (gDvmJit.jitErrorFlags & (1 << (int) jit_error)) != 0 )
+
+/* Clears all errors */
+#define CLEAR_ALL_JIT_ERRORS() gDvmJit.jitErrorFlags = 0;                                                                                  \
+
+/* Checks if ANY error is set */
+#define IS_ANY_JIT_ERROR_SET() (gDvmJit.jitErrorFlags != 0)
+
+/* Max retries limits the number of retries for a given trace, used in CodegenInterface.cpp */
+#define MAX_RETRIES 2
+
+/**
+ * @brief Checks whether a compilation should be re-attempted
+ * Fixes anything which can be fixed. At the end we
+ * either don't have an error, have fixed the error, or
+ * cannot recover from the error. For the second case, we
+ * retry the compilation.
+ * @param cUnit Compilation Unit context
+ * @param isLastAttempt whether the next compilation attempt will be the last one
+ * @return whether we can retry the trace.
+ */
+bool dvmCanFixErrorsAndRetry(CompilationUnit *cUnit, bool isLastAttempt);
+
+/**
+ * @brief Save the error flags that can be changed by dvmCanFixErrorsAndRetry
+ * @param info the information relating to flags that can be changed by the framework (updated by the function)
+ */
+void dvmSaveOptimizationState(SErrorCompilationState &info);
+
+/**
+ * @brief Restore compilation state
+ * @param info the state we want to reset to
+ */
+void dvmRestoreCompilationState(SErrorCompilationState &info);
+
+#endif
diff --git a/vm/compiler/codegen/x86/lightcg/CodegenInterface.cpp b/vm/compiler/codegen/x86/lightcg/CodegenInterface.cpp
new file mode 100644
index 0000000..36db7e6
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/CodegenInterface.cpp
@@ -0,0 +1,2580 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#include <sys/mman.h>
+#include "CompilationUnit.h"
+#include "Dalvik.h"
+#include "libdex/DexOpcodes.h"
+#include "compiler/Compiler.h"
+#include "compiler/CompilerIR.h"
+#include "interp/Jit.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "NcgAot.h"
+#include "compiler/codegen/CompilerCodegen.h"
+#include "InstructionGeneration.h"
+#include "Singleton.h"
+#include "ExceptionHandling.h"
+#include "Scheduler.h"
+#include "CodegenErrors.h"
+#include "Profile.h"
+
+#ifdef HAVE_ANDROID_OS
+#include <cutils/properties.h>
+#endif
+
+#if !defined(VTUNE_DALVIK)
+/* Handy function for VTune updates of PredictedChainingCells */
+static void updateCodeCache(PredictedChainingCell& dst, const PredictedChainingCell& src)
+{
+    dst = src;
+}
+
+/* Handy function for VTune updates of ints */
+static void updateCodeCache(int &dst, int src)
+{
+    dst = src;
+}
+
+/* Send updated code cache content to VTune */
+static void SendUpdateToVTune(void * address, unsigned size, unsigned method_id = 0)
+{
+    (void) address; (void) size; (void) method_id;
+}
+
+#else
+
+#include "compiler/JitProfiling.h"
+
+/* Send updated code cache content to VTune */
+static void SendUpdateToVTune(void * address, unsigned size, unsigned method_id = 0)
+{
+    if (gDvmJit.vtuneInfo == kVTuneInfoDisabled || gDvmJit.vtuneVersion < 279867) {
+        return;
+    }
+
+    iJIT_Method_Load jitMethod;
+    memset (&jitMethod, 0, sizeof (jitMethod));
+
+    jitMethod.method_id = method_id;
+    jitMethod.method_load_address = address;
+    jitMethod.method_size = size;
+
+    // Send the trace update event to the VTune analyzer
+    int res = iJIT_NotifyEvent(iJVM_EVENT_TYPE_METHOD_UPDATE, (void*)&jitMethod);
+    if (gDvmJit.printMe == true) {
+        if (res != 0) {
+            if (jitMethod.method_id == 0) {
+                ALOGD("JIT API: a trace update with address=%p size=%u was not written.",
+                        jitMethod.method_load_address, jitMethod.method_size);
+            } else {
+                ALOGD("JIT API: a trace update with method_id=%u address=%p size=%u was written successfully.",
+                        jitMethod.method_id, jitMethod.method_load_address, jitMethod.method_size);
+            }
+        } else {
+            ALOGD("JIT API: failed to write a trace update with method_id=%u address=%p size=%u.",
+                    jitMethod.method_id, jitMethod.method_load_address, jitMethod.method_size);
+        }
+    }
+}
+
+/* Handy function for VTune updates of changed PredictedChainingCells */
+static void updateCodeCache(PredictedChainingCell& dst, const PredictedChainingCell& src)
+{
+    bool isDiff = dst.branch != src.branch || dst.branch2 != src.branch2;
+    dst = src;
+    if (isDiff == true) {
+        SendUpdateToVTune(&dst, sizeof(PredictedChainingCell));
+    }
+}
+
+/* Handy function for VTune updates of changed ints */
+static void updateCodeCache(int &dst, int src)
+{
+    bool isDiff = dst != src;
+    dst = src;
+    if (isDiff == true) {
+        SendUpdateToVTune(&dst, sizeof(int));
+    }
+}
+#endif // !defined(VTUNE_DALVIK)
+
+/* JIT opcode filtering */
+bool jitOpcodeTable[kNumPackedOpcodes];
+Opcode jitNotSupportedOpcode[] = {
+    OP_INVOKE_OBJECT_INIT_RANGE,
+#if defined (WITH_SELF_VERIFICATION)
+    OP_MONITOR_ENTER,
+    OP_MONITOR_EXIT,
+    OP_NEW_INSTANCE,
+    OP_NEW_ARRAY,
+    OP_CHECK_CAST,
+    OP_MOVE_EXCEPTION,
+    OP_FILL_ARRAY_DATA,
+    OP_EXECUTE_INLINE,
+    OP_EXECUTE_INLINE_RANGE,
+
+    //TODO: fix for the test case
+    /* const does not generate assembly instructions
+     * so a divergence will falsely occur when interp executes and sets
+     * the virtual registers (in memory ).
+     *
+     * const*
+     * return
+     *
+     * const*
+     * invoke_*
+     */
+    OP_CONST_4,
+    OP_CONST_16,
+    OP_CONST,
+    OP_CONST_HIGH16,
+    OP_CONST_WIDE_16,
+    OP_CONST_WIDE_32,
+    OP_CONST_WIDE,
+    OP_CONST_WIDE_HIGH16,
+    OP_CONST_STRING,
+    OP_CONST_STRING_JUMBO,
+
+    OP_RETURN,
+    OP_RETURN_VOID, //const and return
+    OP_RETURN_OBJECT,
+    OP_INVOKE_VIRTUAL_QUICK_RANGE,
+    OP_INVOKE_VIRTUAL_QUICK,
+    OP_INVOKE_INTERFACE,
+    OP_INVOKE_STATIC,
+
+    //occurs with threaded apps
+    OP_APUT_CHAR,
+    OP_APUT_BOOLEAN,
+    OP_APUT_BYTE,
+
+#endif
+};
+
+/* Init values when a predicted chain is initially assembled */
+/* E7FE is branch to self */
+#define PREDICTED_CHAIN_BX_PAIR_INIT     0xe7fe
+
+#if defined(WITH_JIT)
+/* Target-specific save/restore */
+extern "C" void dvmJitCalleeSave(double *saveArea);
+extern "C" void dvmJitCalleeRestore(double *saveArea);
+#endif
+
+/*
+ * Determine the initial instruction set to be used for this trace.
+ * Later components may decide to change this.
+ */
+//JitInstructionSetType dvmCompilerInstructionSet(CompilationUnit *cUnit)
+JitInstructionSetType dvmCompilerInstructionSet(void)
+{
+    return DALVIK_JIT_IA32;
+}
+
+JitInstructionSetType dvmCompilerGetInterpretTemplateSet()
+{
+    return DALVIK_JIT_IA32;
+}
+
+/* we don't use template for IA32 */
+void *dvmCompilerGetInterpretTemplate()
+{
+      return NULL;//(void*) ((int)gDvmJit.codeCache);
+}
+
+/* Initialize the jitOpcodeTable which records what opcodes are supported
+ *  by the JIT compiler.
+ */
+void dvmInitJitOpcodeTable() {
+    unsigned int i;
+    memset(jitOpcodeTable, 1, sizeof(jitOpcodeTable));
+    for (i = 0; i < sizeof(jitNotSupportedOpcode)/sizeof(Opcode); i++) {
+        jitOpcodeTable[((unsigned int)jitNotSupportedOpcode[i])] = false;
+    }
+    for (i = 0; i < sizeof(jitOpcodeTable)/sizeof(bool); i++) {
+        if (jitOpcodeTable[i] == false)
+            ALOGV("opcode 0x%x not supported by JIT", i);
+    }
+}
+
+/* Return true if the opcode is supported by the JIT compiler. */
+bool dvmIsOpcodeSupportedByJit(const DecodedInstruction & insn)
+{
+     /* reject traces containing bytecodes requesting virtual registers exceeding allowed limit */
+     if ((insn.opcode == OP_INVOKE_VIRTUAL_RANGE) || (insn.opcode == OP_INVOKE_VIRTUAL_QUICK_RANGE) ||
+         (insn.opcode == OP_INVOKE_SUPER_RANGE) || (insn.opcode == OP_INVOKE_SUPER_QUICK_RANGE) ||
+         (insn.opcode == OP_INVOKE_DIRECT_RANGE) || (insn.opcode == OP_INVOKE_STATIC_RANGE) ||
+         (insn.opcode == OP_INVOKE_INTERFACE_RANGE)){
+        int opcodeArgs = (int) (insn.vA);
+        if (opcodeArgs > MAX_REG_PER_BYTECODE)
+           return false;
+     }
+    return jitOpcodeTable[((int) insn.opcode)];
+}
+
+/* Track the number of times that the code cache is patched */
+#if defined(WITH_JIT_TUNING)
+#define UPDATE_CODE_CACHE_PATCHES()    (gDvmJit.codeCachePatches++)
+#else
+#define UPDATE_CODE_CACHE_PATCHES()
+#endif
+
+//! default JIT table size used by x86 JIT
+#define DEFAULT_X86_ATOM_DALVIK_JIT_TABLE_SIZE 1<<12
+//! default JIT threshold used by x86 JIT
+#define DEFAULT_X86_ATOM_DALVIK_JIT_THRESHOLD 50
+//! default JIT code cache size used by x86 JIT
+#define DEFAULT_X86_ATOM_DALVIK_JIT_CODE_CACHE_SIZE 512*1024
+
+//! Initializes target-specific configuration
+
+//! Configures the jit table size, jit threshold, and jit code cache size
+//! Initializes status of all threads and the table of supported bytecodes
+//! @return true when initialization is successful (NOTE: current
+//! implementation always returns true)
+bool dvmCompilerArchInit() {
+#ifdef HAVE_ANDROID_OS
+    // Used to get global properties
+    char propertyBuffer[PROPERTY_VALUE_MAX];
+#endif
+    unsigned long propertyValue;
+
+    // Used to identify cpu
+    int familyAndModelInformation;
+    const int familyIdMask = 0xF00;
+    const int familyIdShift = 8;
+    const int modelMask = 0XF0;
+    const int modelShift = 4;
+    const int modelWidth = 4;
+    const int extendedModelIdMask = 0xF0000;
+    const int extendedModelShift = 16;
+
+    // Initialize JIT table size
+    if(gDvmJit.jitTableSize == 0 || (gDvmJit.jitTableSize & (gDvmJit.jitTableSize - 1))) {
+        // JIT table size has not been initialized yet or is not a power of two
+#ifdef HAVE_ANDROID_OS
+        memset(propertyBuffer, 0, PROPERTY_VALUE_MAX); // zero out buffer so we don't use junk
+        property_get("dalvik.jit.table_size", propertyBuffer, NULL);
+        propertyValue = strtoul(propertyBuffer, NULL, 10 /*base*/);
+#else
+        propertyValue = 0ul;
+#endif
+        if (errno == ERANGE || propertyValue == 0ul || (propertyValue & (propertyValue - 1ul)))
+            /* out of range, conversion failed, trying to use invalid value of 0, or using non-power of two */
+            gDvmJit.jitTableSize = DEFAULT_X86_ATOM_DALVIK_JIT_TABLE_SIZE;
+        else // property is valid, but we still need to cast from unsigned long to unsigned int
+            gDvmJit.jitTableSize = static_cast<unsigned int>(propertyValue);
+    }
+
+    // Initialize JIT table mask
+    gDvmJit.jitTableMask = gDvmJit.jitTableSize - 1;
+    gDvmJit.optLevel = kJitOptLevelO1;
+
+    // Initialize JIT threshold
+    if(gDvmJit.threshold == 0) { // JIT threshold has not been initialized yet
+#ifdef HAVE_ANDROID_OS
+        memset(propertyBuffer, 0, PROPERTY_VALUE_MAX); // zero out buffer so we don't use junk
+        property_get("dalvik.jit.threshold", propertyBuffer, NULL);
+        propertyValue = strtoul(propertyBuffer, NULL, 10 /*base*/);
+#else
+        propertyValue = 0ul;
+#endif
+        if (errno == ERANGE || propertyValue == 0ul)
+            /* out of range, conversion failed, or trying to use invalid value of 0 */
+            gDvmJit.threshold = DEFAULT_X86_ATOM_DALVIK_JIT_THRESHOLD;
+        else // property is valid, but we still need to cast from unsigned long to unsigned short
+            gDvmJit.threshold = static_cast<unsigned short>(propertyValue);
+    }
+
+    // Initialize JIT code cache size
+    if(gDvmJit.codeCacheSize == 0) { // JIT code cache size has not been initialized yet
+#ifdef HAVE_ANDROID_OS
+        memset(propertyBuffer, 0, PROPERTY_VALUE_MAX); // zero out buffer so we don't use junk
+        property_get("dalvik.jit.code_cache_size", propertyBuffer, NULL);
+        propertyValue = strtoul(propertyBuffer, NULL, 10 /*base*/);
+#else
+        propertyValue = 0ul;
+#endif
+        if (errno == ERANGE || propertyValue == 0ul)
+            /* out of range, conversion failed, or trying to use invalid value of 0 */
+            gDvmJit.codeCacheSize = DEFAULT_X86_ATOM_DALVIK_JIT_CODE_CACHE_SIZE;
+        else // property is valid, but we still need to cast from unsigned long to unsigned int
+            gDvmJit.codeCacheSize = static_cast<unsigned int>(propertyValue);
+    }
+
+    // Print out values used
+    ALOGV("JIT threshold set to %hu",gDvmJit.threshold);
+    ALOGV("JIT table size set to %u",gDvmJit.jitTableSize);
+    ALOGV("JIT code cache size set to %u",gDvmJit.codeCacheSize);
+
+    //Disable Method-JIT
+    gDvmJit.disableOpt |= (1 << kMethodJit);
+
+#ifdef HAVE_ANDROID_OS
+    // If JIT verbose has not been enabled, check the global property dalvik.jit.verbose
+    if (!gDvmJit.printMe) {
+        memset(propertyBuffer, 0, PROPERTY_VALUE_MAX); // zero out buffer so we don't use junk
+        property_get("dalvik.jit.verbose", propertyBuffer, NULL);
+        // Look for text ". We could enable finer control by checking application
+        // name, but the VM would need to know which application it is running
+        if (strncmp("true", propertyBuffer, PROPERTY_VALUE_MAX) == 0) {
+            gDvmJit.printMe = true;
+        }
+    }
+#endif
+
+    // Now determine machine model
+    asm volatile (
+            "movl $1, %%eax\n\t"
+            "pushl %%ebx\n\t"
+            "cpuid\n\t"
+            "popl %%ebx\n\t"
+            "movl %%eax, %0"
+            : "=r" (familyAndModelInformation)
+            :
+            : "eax", "ecx", "edx");
+    gDvmJit.cpuFamily = (familyAndModelInformation & familyIdMask) >> familyIdShift;
+    gDvmJit.cpuModel = (((familyAndModelInformation & extendedModelIdMask)
+            >> extendedModelShift) << modelWidth)
+            + ((familyAndModelInformation & modelMask) >> modelShift);
+
+#if defined(WITH_SELF_VERIFICATION)
+    /* Force into blocking mode */
+    gDvmJit.blockingMode = true;
+    gDvm.nativeDebuggerActive = true;
+#endif
+
+    // Make sure all threads have current values
+    dvmJitUpdateThreadStateAll();
+
+    /* Initialize jitOpcodeTable for JIT supported opcode */
+    dvmInitJitOpcodeTable();
+
+    return true;
+}
+
+void dvmCompilerPatchInlineCache(void)
+{
+    int i;
+    PredictedChainingCell *minAddr, *maxAddr;
+
+    /* Nothing to be done */
+    if (gDvmJit.compilerICPatchIndex == 0) return;
+
+    /*
+     * Since all threads are already stopped we don't really need to acquire
+     * the lock. But race condition can be easily introduced in the future w/o
+     * paying attention so we still acquire the lock here.
+     */
+    dvmLockMutex(&gDvmJit.compilerICPatchLock);
+
+    UNPROTECT_CODE_CACHE(gDvmJit.codeCache, gDvmJit.codeCacheByteUsed);
+
+    //ALOGD("Number of IC patch work orders: %d", gDvmJit.compilerICPatchIndex);
+
+    /* Initialize the min/max address range */
+    minAddr = (PredictedChainingCell *)
+        ((char *) gDvmJit.codeCache + gDvmJit.codeCacheSize);
+    maxAddr = (PredictedChainingCell *) gDvmJit.codeCache;
+
+    for (i = 0; i < gDvmJit.compilerICPatchIndex; i++) {
+        ICPatchWorkOrder *workOrder = &gDvmJit.compilerICPatchQueue[i];
+        PredictedChainingCell *cellAddr = workOrder->cellAddr;
+        PredictedChainingCell *cellContent = &workOrder->cellContent;
+        ClassObject *clazz = dvmFindClassNoInit(workOrder->classDescriptor,
+                                                workOrder->classLoader);
+
+        assert(clazz->serialNumber == workOrder->serialNumber);
+
+        /* Use the newly resolved clazz pointer */
+        cellContent->clazz = clazz;
+
+        if (cellAddr->clazz == NULL) {
+            COMPILER_TRACE_CHAINING(
+                ALOGI("Jit Runtime: predicted chain %p to %s (%s) initialized",
+                      cellAddr,
+                      cellContent->clazz->descriptor,
+                      cellContent->method->name));
+        } else {
+            COMPILER_TRACE_CHAINING(
+                ALOGI("Jit Runtime: predicted chain %p from %s to %s (%s) "
+                      "patched",
+                      cellAddr,
+                      cellAddr->clazz->descriptor,
+                      cellContent->clazz->descriptor,
+                      cellContent->method->name));
+        }
+
+        /* Patch the chaining cell */
+        updateCodeCache(*cellAddr, *cellContent);
+
+        minAddr = (cellAddr < minAddr) ? cellAddr : minAddr;
+        maxAddr = (cellAddr > maxAddr) ? cellAddr : maxAddr;
+    }
+
+    PROTECT_CODE_CACHE(gDvmJit.codeCache, gDvmJit.codeCacheByteUsed);
+
+    gDvmJit.compilerICPatchIndex = 0;
+    dvmUnlockMutex(&gDvmJit.compilerICPatchLock);
+}
+
+/* Target-specific cache clearing */
+void dvmCompilerCacheClear(char *start, size_t size)
+{
+    /* "0xFF 0xFF" is an invalid opcode for x86. */
+    memset(start, 0xFF, size);
+}
+
+/* for JIT debugging, to be implemented */
+void dvmJitCalleeSave(double *saveArea) {
+}
+
+void dvmJitCalleeRestore(double *saveArea) {
+}
+
+void dvmJitToInterpSingleStep() {
+}
+
+JitTraceDescription *dvmCopyTraceDescriptor(const u2 *pc,
+                                            const JitEntry *knownEntry) {
+    return NULL;
+}
+
+void dvmCompilerCodegenDump(CompilationUnit *cUnit) //in ArchUtility.c
+{
+}
+
+void dvmCompilerArchDump(void)
+{
+}
+
+void dvmCompilerAssembleLIR(CompilationUnit *cUnit, JitTranslationInfo* info)
+{
+}
+
+void dvmJitInstallClassObjectPointers(CompilationUnit *cUnit, char *codeAddress)
+{
+}
+
+void dvmCompilerMethodMIR2LIR(CompilationUnit *cUnit)
+{
+    // Method-based JIT not supported for x86.
+}
+
+void dvmJitScanAllClassPointers(void (*callback)(void *))
+{
+}
+
+/**
+ * @brief Generates a jump with 32-bit relative immediate that jumps
+ * to the target.
+ * @details Updates the instruction stream with the jump.
+ * @param target absolute address of target.
+ */
+void unconditional_jump_rel32(void * target) {
+    // We will need to figure out the immediate to use for the relative
+    // jump, so we need to flush scheduler so that stream is updated.
+    // In most cases this won't affect the schedule since the jump would've
+    // ended the native BB anyway and would've been scheduled last.
+    if(gDvmJit.scheduling)
+        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+
+    // Calculate the address offset between the destination of jump and the
+    // function we are jumping to.
+    int relOffset = reinterpret_cast<int>(target)
+            - reinterpret_cast<int>(stream);
+
+    // Since instruction pointer will already be updated when executing this,
+    // subtract size of jump instruction
+    relOffset -= getJmpCallInstSize(OpndSize_32, JmpCall_uncond);
+
+    // Generate the unconditional jump now
+    unconditional_jump_int(relOffset, OpndSize_32);
+}
+
+// works whether instructions for target basic block are generated or not
+LowOp* jumpToBasicBlock(char* instAddr, int targetId,
+        bool immediateNeedsAligned) {
+    stream = instAddr;
+    bool unknown;
+    OpndSize size;
+    if(gDvmJit.scheduling) {
+        // If target is chaining cell, we must align the immediate
+        unconditional_jump_block(targetId, immediateNeedsAligned);
+    } else {
+        if (immediateNeedsAligned == true) {
+            alignOffset(1);
+        }
+        int relativeNCG = getRelativeNCG(targetId, JmpCall_uncond, &unknown, &size);
+        unconditional_jump_int(relativeNCG, size);
+    }
+    return NULL;
+}
+
+LowOp* condJumpToBasicBlock(char* instAddr, ConditionCode cc, int targetId,
+        bool immediateNeedsAligned) {
+    stream = instAddr;
+    bool unknown;
+    OpndSize size;
+    if(gDvmJit.scheduling) {
+        // If target is chaining cell, we must align the immediate
+        conditional_jump_block(cc, targetId, immediateNeedsAligned);
+    } else {
+        int relativeNCG = getRelativeNCG(targetId, JmpCall_cond, &unknown, &size);
+        conditional_jump_int(cc, relativeNCG, size);
+    }
+    return NULL;
+}
+
+/*
+ * Attempt to enqueue a work order to patch an inline cache for a predicted
+ * chaining cell for virtual/interface calls.
+ */
+static bool inlineCachePatchEnqueue(PredictedChainingCell *cellAddr,
+                                    PredictedChainingCell *newContent)
+{
+    bool result = true;
+
+    /*
+     * Make sure only one thread gets here since updating the cell (ie fast
+     * path and queueing the request (ie the queued path) have to be done
+     * in an atomic fashion.
+     */
+    dvmLockMutex(&gDvmJit.compilerICPatchLock);
+
+    /* Fast path for uninitialized chaining cell */
+    if (cellAddr->clazz == NULL &&
+        cellAddr->branch == PREDICTED_CHAIN_BX_PAIR_INIT) {
+        UNPROTECT_CODE_CACHE(cellAddr, sizeof(*cellAddr));
+
+        cellAddr->method = newContent->method;
+
+        /* remember if the branch address has changed, other fields do not matter.
+         * If changed then send new values to VTune a bit later */
+        bool isBranchUpdated = cellAddr->branch != newContent->branch || cellAddr->branch2 != newContent->branch2;
+
+        cellAddr->branch = newContent->branch;
+        cellAddr->branch2 = newContent->branch2;
+
+        /*
+         * The update order matters - make sure clazz is updated last since it
+         * will bring the uninitialized chaining cell to life.
+         */
+        android_atomic_release_store((int32_t)newContent->clazz,
+            (volatile int32_t *)(void*) &cellAddr->clazz);
+        //cacheflush((intptr_t) cellAddr, (intptr_t) (cellAddr+1), 0);
+        UPDATE_CODE_CACHE_PATCHES();
+        if (isBranchUpdated == true) {
+            SendUpdateToVTune(cellAddr, sizeof(*cellAddr));
+        }
+
+        PROTECT_CODE_CACHE(cellAddr, sizeof(*cellAddr));
+
+#if 0
+        MEM_BARRIER();
+        cellAddr->clazz = newContent->clazz;
+        //cacheflush((intptr_t) cellAddr, (intptr_t) (cellAddr+1), 0);
+#endif
+#if defined(WITH_JIT_TUNING)
+        gDvmJit.icPatchInit++;
+#endif
+        COMPILER_TRACE_CHAINING(
+            ALOGI("Jit Runtime: FAST predicted chain %p to method %s%s %p",
+                  cellAddr, newContent->clazz->descriptor, newContent->method->name, newContent->method));
+    /* Check if this is a frequently missed clazz */
+    } else if (cellAddr->stagedClazz != newContent->clazz) {
+        /* Not proven to be frequent yet - build up the filter cache */
+        UNPROTECT_CODE_CACHE(cellAddr, sizeof(*cellAddr));
+
+        cellAddr->stagedClazz = newContent->clazz;
+
+        UPDATE_CODE_CACHE_PATCHES();
+        PROTECT_CODE_CACHE(cellAddr, sizeof(*cellAddr));
+
+#if defined(WITH_JIT_TUNING)
+        gDvmJit.icPatchRejected++;
+#endif
+    /*
+     * Different classes but same method implementation - it is safe to just
+     * patch the class value without the need to stop the world.
+     */
+    } else if (cellAddr->method == newContent->method) {
+        UNPROTECT_CODE_CACHE(cellAddr, sizeof(*cellAddr));
+
+        cellAddr->clazz = newContent->clazz;
+        /* No need to flush the cache here since the branch is not patched */
+        UPDATE_CODE_CACHE_PATCHES();
+
+        PROTECT_CODE_CACHE(cellAddr, sizeof(*cellAddr));
+
+#if defined(WITH_JIT_TUNING)
+        gDvmJit.icPatchLockFree++;
+#endif
+    /*
+     * Cannot patch the chaining cell inline - queue it until the next safe
+     * point.
+     */
+    } else if (gDvmJit.compilerICPatchIndex < COMPILER_IC_PATCH_QUEUE_SIZE)  {
+        int index = gDvmJit.compilerICPatchIndex++;
+        const ClassObject *clazz = newContent->clazz;
+
+        gDvmJit.compilerICPatchQueue[index].cellAddr = cellAddr;
+        gDvmJit.compilerICPatchQueue[index].cellContent = *newContent;
+        gDvmJit.compilerICPatchQueue[index].classDescriptor = clazz->descriptor;
+        gDvmJit.compilerICPatchQueue[index].classLoader = clazz->classLoader;
+        /* For verification purpose only */
+        gDvmJit.compilerICPatchQueue[index].serialNumber = clazz->serialNumber;
+
+#if defined(WITH_JIT_TUNING)
+        gDvmJit.icPatchQueued++;
+#endif
+        COMPILER_TRACE_CHAINING(
+            ALOGI("Jit Runtime: QUEUE predicted chain %p to method %s%s",
+                  cellAddr, newContent->clazz->descriptor, newContent->method->name));
+    } else {
+    /* Queue is full - just drop this patch request */
+#if defined(WITH_JIT_TUNING)
+        gDvmJit.icPatchDropped++;
+#endif
+
+        COMPILER_TRACE_CHAINING(
+            ALOGI("Jit Runtime: DROP predicted chain %p to method %s%s",
+                  cellAddr, newContent->clazz->descriptor, newContent->method->name));
+    }
+
+    dvmUnlockMutex(&gDvmJit.compilerICPatchLock);
+    return result;
+}
+
+/*
+ * This method is called from the invoke templates for virtual and interface
+ * methods to speculatively setup a chain to the callee. The templates are
+ * written in assembly and have setup method, cell, and clazz at r0, r2, and
+ * r3 respectively, so there is a unused argument in the list. Upon return one
+ * of the following three results may happen:
+ *   1) Chain is not setup because the callee is native. Reset the rechain
+ *      count to a big number so that it will take a long time before the next
+ *      rechain attempt to happen.
+ *   2) Chain is not setup because the callee has not been created yet. Reset
+ *      the rechain count to a small number and retry in the near future.
+ *   3) Ask all other threads to stop before patching this chaining cell.
+ *      This is required because another thread may have passed the class check
+ *      but hasn't reached the chaining cell yet to follow the chain. If we
+ *      patch the content before halting the other thread, there could be a
+ *      small window for race conditions to happen that it may follow the new
+ *      but wrong chain to invoke a different method.
+ */
+extern "C" const Method *dvmJitToPatchPredictedChain(const Method *method,
+                                          Thread *self,
+                                          PredictedChainingCell *cell,
+                                          const ClassObject *clazz)
+{
+    int newRechainCount = PREDICTED_CHAIN_COUNTER_RECHAIN;
+    /* Don't come back here for a long time if the method is native */
+    if (dvmIsNativeMethod(method)) {
+        UNPROTECT_CODE_CACHE(cell, sizeof(*cell));
+
+        /*
+         * Put a non-zero/bogus value in the clazz field so that it won't
+         * trigger immediate patching and will continue to fail to match with
+         * a real clazz pointer.
+         */
+        cell->clazz = (ClassObject *) PREDICTED_CHAIN_FAKE_CLAZZ;
+
+        UPDATE_CODE_CACHE_PATCHES();
+        PROTECT_CODE_CACHE(cell, sizeof(*cell));
+        COMPILER_TRACE_CHAINING(
+            ALOGI("Jit Runtime: predicted chain %p to native method %s ignored",
+                  cell, method->name));
+        goto done;
+    }
+    {
+    int tgtAddr = (int) dvmJitGetTraceAddr(method->insns);
+
+    /*
+     * Compilation not made yet for the callee. Reset the counter to a small
+     * value and come back to check soon.
+     */
+    if ((tgtAddr == 0) ||
+        ((void*)tgtAddr == dvmCompilerGetInterpretTemplate())) {
+        COMPILER_TRACE_CHAINING(
+            ALOGI("Jit Runtime: predicted chain %p to method %s%s delayed",
+                  cell, method->clazz->descriptor, method->name));
+        goto done;
+    }
+
+    PredictedChainingCell newCell;
+
+    if (cell->clazz == NULL) {
+        newRechainCount = self->icRechainCount;
+    }
+
+    int relOffset = (int) tgtAddr - (int)cell;
+    OpndSize immSize = estOpndSizeFromImm(relOffset);
+    int jumpSize = getJmpCallInstSize(immSize, JmpCall_uncond);
+    relOffset -= jumpSize;
+    COMPILER_TRACE_CHAINING(
+            ALOGI("inlineCachePatchEnqueue chain %p to method %s%s inst size %d",
+                  cell, method->clazz->descriptor, method->name, jumpSize));
+
+    // This does not need to go through lowering interface and can encode directly
+    // at address because it does not actually update code stream until safe point.
+    // Can't use stream here since it is used by the compilation thread.
+    encoder_imm(Mnemonic_JMP, immSize, relOffset, (char*) (&newCell)); //update newCell.branch
+
+    newCell.clazz = clazz;
+    newCell.method = method;
+
+    /*
+     * Enter the work order to the queue and the chaining cell will be patched
+     * the next time a safe point is entered.
+     *
+     * If the enqueuing fails reset the rechain count to a normal value so that
+     * it won't get indefinitely delayed.
+     */
+    inlineCachePatchEnqueue(cell, &newCell);
+    }
+done:
+    self->icRechainCount = newRechainCount;
+    return method;
+}
+
+/**
+ * @class BackwardBranchChainingCellContents
+ * @brief Defines the data structure of a Backward Branch Chaining Cell.
+ */
+struct __attribute__ ((packed)) BackwardBranchChainingCellContents
+{
+    /**
+     * @brief Used to hold the "call rel32" to dvmJitToInterpBackwardBranch
+     */
+    char instructionHolder[5];
+
+    unsigned int nextPC;    //!< Next bytecode PC
+
+    /**
+     * @brief Holds address of operand of jump instruction that is to be patched.
+     * After chaining, the jump is  filled with relative offset to loop header.
+     * After unchaining it is filled with relative offset to the VR write-back.
+     */
+    char * codePtr;
+
+    char * loopHeaderAddr;    //!< Address of loop header block.
+    char * vrWriteBackAddr;   //!< Address of VR write-back block.
+    char * loopPreHeaderAddr; //!< Address of loop pre-header block.
+
+    /**
+     * Doxygen does not like documentation of functions here, so let's just document it but not expose it to doxygen
+     * brief Used for unchaining backward branch chaining cells.
+     * param location This is location where unchaining method can assume that a Backward Branch CC exists.
+     * return Returns size of unchained cell.
+     */
+    static size_t unchain (u1 * location)
+    {
+        //First we reinterpret the location to be a chaining cell
+        BackwardBranchChainingCellContents * contents =
+                reinterpret_cast<BackwardBranchChainingCellContents *> (location);
+
+        //We want to jump to the VR write back address and we know that the code
+        //pointer points to operand of jump. Thus we also subtract our assumed
+        //operand size of 32-bits.
+        int relativeOffset = (contents->vrWriteBackAddr - contents->codePtr)
+                - OpndSize_32;
+
+        //We want to patch with an int value so we reinterpret the address here
+        int * addressOfJumpOperand = reinterpret_cast<int *> (contents->codePtr);
+
+        //This does the actual patching with the offset we calculated
+        updateCodeCache(*addressOfJumpOperand, relativeOffset);
+
+        //We return size of our chaining cell
+        return sizeof(*contents);
+    }
+};
+
+#define BYTES_OF_NORMAL_CHAINING 13
+#define BYTES_OF_HOT_CHAINING 17
+#define BYTES_OF_SINGLETON_CHAINING 13
+#define BYTES_OF_PREDICTED_CHAINING 20
+#define OFFSET_OF_PATCHADDR 9 // offset in chaining cell to the field for the location to be patched
+#define OFFSET_OF_ISMOVEFLAG 13  // offset in hot chaining cell to the ismove_flag field
+#define BYTES_OF_32BITS 4
+/*
+ * Unchain a trace given the starting address of the translation
+ * in the code cache.  Refer to the diagram in dvmCompilerAssembleLIR.
+ * For ARM, it returns the address following the last cell unchained.
+ * For IA, it returns NULL since cacheflush is not required for IA.
+ */
+u4* dvmJitUnchain(void* codeAddr)
+{
+    /* codeAddr is 4-byte aligned, so is chain cell count offset */
+    u2* pChainCellCountOffset = (u2*)((char*)codeAddr - 4);
+    u2 chainCellCountOffset = *pChainCellCountOffset;
+    /* chain cell counts information is 4-byte aligned */
+    ChainCellCounts *pChainCellCounts =
+          (ChainCellCounts*)((char*)codeAddr + chainCellCountOffset);
+    u2* pChainCellOffset = (u2*)((char*)codeAddr - 2);
+    u2 chainCellOffset = *pChainCellOffset;
+    u1* pChainCells;
+    int i,j;
+    PredictedChainingCell *predChainCell;
+    int padding;
+    u1* patchAddr;
+    int relativeNCG;
+    int ismove_flag = 0;
+
+    /* Locate the beginning of the chain cell region */
+    pChainCells = (u1 *)((char*)codeAddr + chainCellOffset);
+
+    /* The cells are sorted in order - walk through them and reset */
+    for (i = 0; i < kChainingCellGap; i++) {
+        /* for normal chaining:
+               call imm32
+               rPC
+               codePtr (offset address of jmp/jcc)
+           after chaining:
+               codePtr is filled with a relative offset to the target
+           after unchaining:
+              codePtr is filled with original relative offset to the chaining cell
+
+           for backward chaining:
+               call imm32
+               rPC
+               codePtr (offset address of jmp/jcc)
+               loop header address
+               vrStoreCodePtr (code address of deferred VR store)
+           after chaining:
+               codePtr is filled with a relative offset to the loop header
+           after unchaining:
+               if (vrStoreCodePtr)
+                   codePtr is filled with relative offset to the deferred vr store
+               else
+                   codePtr is filled with relative offset to the chaining cell
+
+          for singleton chaining:
+               call imm32
+               rPC
+               codePtr (offset address of movl)
+           after chaining:
+               codePtr is filled with absolute address to the target
+           after unchaining:
+               codePtr is filled with absolute adress of the chaining cell
+
+           for hot chaining:
+               call imm32
+               rPC
+               codePtr (offset address of jmp or movl)
+               ismove_flag
+           after chaining:
+               if (ismove_flag)
+                 codePtr is filled with a relative offset to the target
+               else
+                 codePtr is filled with absolute address to the target
+           after unchaining:
+               if (ismove_flag)
+                 codePtr is filled with original relative offset to the chaining cell
+               else
+                 codePtr is filled with absolute adress of the chaining cell
+
+           Space occupied by the chaining cell in bytes:
+                normal, singleton: 5+4+4
+                backward: 5+4+4+4+4
+                hot: 5+4+4+4
+                codePtr should be within 16B line.
+
+           Space for predicted chaining: 5 words = 20 bytes + padding to make it 4-byte aligned
+        */
+        int elemSize = 0;
+
+        COMPILER_TRACE_CHAINING(
+            ALOGI("Jit Runtime: unchaining type %d count %d", i, pChainCellCounts->u.count[i]));
+
+        for (j = 0; j < pChainCellCounts->u.count[i]; j++) {
+            switch(i) {
+                case kChainingCellNormal:
+                    COMPILER_TRACE_CHAINING(
+                        ALOGI("Jit Runtime: unchaining of normal"));
+                    elemSize = BYTES_OF_NORMAL_CHAINING;
+                    patchAddr = (u1 *)(*(int *)((char*)pChainCells + OFFSET_OF_PATCHADDR));
+                    relativeNCG = (pChainCells - patchAddr) - BYTES_OF_32BITS;
+                    if (patchAddr)
+                        updateCodeCache(*(int*)patchAddr, relativeNCG);
+                    break;
+                case kChainingCellHot:
+                    COMPILER_TRACE_CHAINING(
+                        ALOGI("Jit Runtime: unchaining of hot"));
+                    elemSize = BYTES_OF_HOT_CHAINING;
+                    patchAddr = (u1 *)(*(int *)((char*)pChainCells + OFFSET_OF_PATCHADDR));
+                    ismove_flag = *(int *)((char*)pChainCells + OFFSET_OF_ISMOVEFLAG);
+                    if (patchAddr) {
+                        if (ismove_flag) {
+                            relativeNCG = (pChainCells - patchAddr) - BYTES_OF_32BITS;
+                            updateCodeCache(*(int*)patchAddr, relativeNCG);
+                        } else
+                            updateCodeCache(*(int*)patchAddr, (int)pChainCells);
+                    }
+                    break;
+                case kChainingCellInvokeSingleton:
+                    COMPILER_TRACE_CHAINING(
+                        ALOGI("Jit Runtime: unchaining of singleton"));
+                    elemSize = BYTES_OF_SINGLETON_CHAINING;
+                    patchAddr = (u1 *)(*(int *)((char*)pChainCells + OFFSET_OF_PATCHADDR));
+                    if (patchAddr)
+                        updateCodeCache(*(int*)patchAddr, (int)pChainCells);
+                    break;
+                case kChainingCellBackwardBranch:
+                    COMPILER_TRACE_CHAINING(
+                        ALOGI("Jit Runtime: unchaining of backward"));
+                    elemSize = BackwardBranchChainingCellContents::unchain (pChainCells);
+                    break;
+                case kChainingCellInvokePredicted:
+                    COMPILER_TRACE_CHAINING(
+                        ALOGI("Jit Runtime: unchaining of predicted"));
+                    //The cell is always 4-byte aligned so we need to take that
+                    //into account first
+                    padding = (4 - ((u4) pChainCells & 3)) & 3;
+                    pChainCells += padding;
+
+                    predChainCell = reinterpret_cast<PredictedChainingCell *> (
+                            pChainCells);
+                    /*
+                     * There could be a race on another mutator thread to use
+                     * this particular predicted cell and the check has passed
+                     * the clazz comparison. So we cannot safely wipe the
+                     * method and branch but it is safe to clear the clazz,
+                     * which serves as the key.
+                     */
+                    predChainCell->clazz = PREDICTED_CHAIN_CLAZZ_INIT;
+
+                    elemSize = sizeof(*predChainCell);
+                    break;
+                default:
+                    ALOGI("JIT_INFO: Unexpected chaining type: %d", i);
+                    //Error is beyond the scope of the x86 JIT back-end
+                    ALOGE("\t FATAL ERROR. ABORTING!");
+                    dvmAbort();  // dvmAbort OK here - can't safely recover
+            }
+            COMPILER_TRACE_CHAINING(
+                ALOGI("Jit Runtime: unchaining 0x%x", (int)pChainCells));
+            pChainCells += elemSize;  /* Advance by a fixed number of bytes */
+        }
+    }
+    return NULL;
+}
+
+/* Unchain all translation in the cache. */
+void dvmJitUnchainAll()
+{
+    ALOGV("Jit Runtime: unchaining all");
+    if (gDvmJit.pJitEntryTable != NULL) {
+        COMPILER_TRACE_CHAINING(ALOGI("Jit Runtime: unchaining all"));
+        dvmLockMutex(&gDvmJit.tableLock);
+
+        UNPROTECT_CODE_CACHE(gDvmJit.codeCache, gDvmJit.codeCacheByteUsed);
+
+        for (size_t i = 0; i < gDvmJit.jitTableSize; i++) {
+            if (gDvmJit.pJitEntryTable[i].dPC &&
+                !gDvmJit.pJitEntryTable[i].u.info.isMethodEntry &&
+                gDvmJit.pJitEntryTable[i].codeAddress) {
+                      dvmJitUnchain(gDvmJit.pJitEntryTable[i].codeAddress);
+            }
+        }
+
+        PROTECT_CODE_CACHE(gDvmJit.codeCache, gDvmJit.codeCacheByteUsed);
+
+        dvmUnlockMutex(&gDvmJit.tableLock);
+        gDvmJit.translationChains = 0;
+    }
+    gDvmJit.hasNewChain = false;
+}
+
+/* Chaining cell for code that may need warmup. */
+/* ARM assembly: ldr r0, [r6, #76] (why a single instruction to access member of glue structure?)
+                 blx r0
+                 data 0xb23a //bytecode address: 0x5115b23a
+                 data 0x5115
+   IA32 assembly:
+                  call imm32 //relative offset to dvmJitToInterpNormal
+                  rPC
+                  codePtr
+*/
+static int handleNormalChainingCell(CompilationUnit *cUnit, unsigned int offset, int blockId)
+{
+    ALOGV("In handleNormalChainingCell for method %s block %d BC offset %x NCG offset %x",
+          cUnit->method->name, blockId, offset, stream - streamMethodStart);
+    if(dump_x86_inst)
+        ALOGI("LOWER NormalChainingCell at offsetPC %x offsetNCG %x @%p",
+              offset, stream - streamMethodStart, stream);
+#ifndef WITH_SELF_VERIFICATION
+    call_dvmJitToInterpNormal();
+#else
+    call_dvmJitToInterpBackwardBranch();
+#endif
+    unsigned int *ptr = (unsigned int*)stream;
+    *ptr++ = (unsigned int)(cUnit->method->insns + offset);
+    char* codePtr = searchNCGWorklist(blockId);
+    *ptr++ = (unsigned int)codePtr;
+    stream = (char*)ptr;
+    return 0;
+}
+
+/*
+ * Chaining cell for instructions that immediately following already translated
+ * code.
+   IA32 assembly:
+                  call imm32 // relative offset to dvmJitToInterpNormal or dvmJitToInterpTraceSelect
+                  rPC
+                  codePtr
+                  ismove_flag
+ */
+static int handleHotChainingCell(CompilationUnit *cUnit, unsigned int offset, int blockId)
+{
+    ALOGV("In handleHotChainingCell for method %s block %d BC offset %x NCG offset %x",
+          cUnit->method->name, blockId, offset, stream - streamMethodStart);
+    if(dump_x86_inst)
+        ALOGI("LOWER HotChainingCell at offsetPC %x offsetNCG %x @%p",
+              offset, stream - streamMethodStart, stream);
+
+    int ismove_flag = 0;
+    char* codePtr = searchChainingWorklist(blockId);
+    if (codePtr == NULL) {
+        codePtr = searchNCGWorklist(blockId);
+        if (codePtr) {
+            call_dvmJitToInterpNormal();
+            ismove_flag = 1;
+        }
+        else call_dvmJitToInterpTraceSelect();
+    }
+    else
+        call_dvmJitToInterpTraceSelect();
+    unsigned int *ptr = (unsigned int*)stream;
+    *ptr++ = (unsigned int)(cUnit->method->insns + offset);
+    *ptr++ = (unsigned int)codePtr;
+    *ptr++ = (unsigned int)ismove_flag;
+    stream = (char*)ptr;
+    return 0;
+}
+
+/**
+ * @brief Generates code for backward branch chaining cell.
+ * @param cUnit the compilation unit
+ * @param chainingCell the chaining cell we are generating code for
+ * @return true if chaining cell was successfully generated
+ */
+static bool handleBackwardBranchChainingCell (CompilationUnit *cUnit,
+        BasicBlock_O1 *chainingCell)
+{
+    assert(chainingCell != 0);
+    assert(chainingCell->blockType == kChainingCellBackwardBranch);
+
+    //Get the loop entry
+    BasicBlock *loopEntry = chainingCell->fallThrough;
+
+    //Paranoid
+    assert(cUnit->loopInformation != 0);
+
+    //We want the loop header and preloop header
+    char *loopHeaderAddr = 0;
+    char *preLoopHeaderAddr = 0;
+
+    BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (loopEntry);
+    assert(bbO1 != 0);
+
+    //Set the loop header address
+    loopHeaderAddr = bbO1->streamStart;
+
+    //Get the associated loop information
+    LoopInformation *info = cUnit->loopInformation;
+
+    //But if info is 0, we might not have that and should just use the fallThrough's information
+    //This can happen if the user has used the old loop system, and should only happen then
+    if (info == 0)
+    {
+        //Then request the interpreter jump back to where the loop is
+        preLoopHeaderAddr = loopHeaderAddr;
+    }
+    else
+    {
+        //Get the right loop
+        info = info->getLoopInformationByEntry (loopEntry);
+
+        //Paranoid
+        if (info != 0)
+        {
+            //We have a preLoop
+            BasicBlock *preLoop = info->getPreHeader ();
+
+            //Paranoid
+            if (preLoop != 0)
+            {
+                bbO1 = reinterpret_cast<BasicBlock_O1 *> (preLoop);
+
+                //Paranoid
+                if (bbO1 != 0)
+                {
+                    preLoopHeaderAddr = bbO1->streamStart;
+                }
+            }
+        }
+    }
+
+    //If we cannot find these, then we have a problem
+    if (loopHeaderAddr == 0 || preLoopHeaderAddr == 0)
+    {
+        return false;
+    }
+
+    //Every backward branch chaining cell must have a prebackward
+    //predecessor. So we look for it.
+    if (chainingCell->predecessors == 0)
+    {
+        return false;
+    }
+
+    //Initialize iterator
+    BitVectorIterator bvIterator;
+    dvmBitVectorIteratorInit (chainingCell->predecessors, &bvIterator);
+
+    //Get the block index of predecessor
+    int blockIdx = dvmBitVectorIteratorNext (&bvIterator);
+
+    //Return false if we did not find predecessor
+    if (blockIdx == -1)
+    {
+        return false;
+    }
+
+    //Get the predecessor block
+    BasicBlock_O1 *preBackward =
+            reinterpret_cast<BasicBlock_O1 *> (dvmGrowableListGetElement (
+                    &cUnit->blockList, blockIdx));
+
+    //If it is not the right type then we return false
+    if (preBackward == 0 || preBackward->blockType != kPreBackwardBlock)
+    {
+        return false;
+    }
+
+    char *vrStoreCodePtr = preBackward->streamStart;
+
+    //We should have already generated code for the prebackward block
+    if (vrStoreCodePtr == 0)
+    {
+        return false;
+    }
+
+    //If scheduling is enabled, lets assert that queue is empty. Otherwise,
+    //it is not safe to use the stream pointer.
+    if (gDvmJit.scheduling)
+    {
+        //Using stream pointer is not safe unless scheduler queue is empty.
+        //We should never get here with anything in queue.
+        if (singletonPtr<Scheduler>()->isQueueEmpty() == false)
+        {
+            return false;
+        }
+    }
+
+    //At this point we have tried gathering all information we could so we
+    //ready to generate the chaining cell
+    if (cUnit->printMe)
+    {
+        ALOGI("LOWER BackwardBranchChainingCell with offsetPC %x @%p",
+                chainingCell->startOffset, stream);
+    }
+
+    BackwardBranchChainingCellContents *backwardContents =
+            reinterpret_cast<BackwardBranchChainingCellContents *> (stream);
+
+    //Generate the call to interpreter
+    call_dvmJitToInterpBackwardBranch ();
+
+    //Paranoid, we want to make sure that chaining cell has enough room
+    //for the call instruction
+    assert((reinterpret_cast<int>(stream) - reinterpret_cast<int>(backwardContents))
+            == sizeof(backwardContents->instructionHolder));
+
+    //Find the jump that goes to the prebackward block
+    char *codePtr = searchNCGWorklist (preBackward->id);
+
+    //If we cannot find this jump, something went wrong
+    if (codePtr == 0)
+    {
+        return false;
+    }
+
+    //Now write the data into the chaining cell
+    backwardContents->nextPC =
+            reinterpret_cast<unsigned int> (cUnit->method->insns
+                    + chainingCell->startOffset);
+    backwardContents->codePtr = codePtr;
+    backwardContents->loopHeaderAddr = loopHeaderAddr;
+    backwardContents->vrWriteBackAddr = vrStoreCodePtr;
+    backwardContents->loopPreHeaderAddr = preLoopHeaderAddr;
+
+    //Update stream pointer
+    stream = reinterpret_cast<char *> (backwardContents)
+            + sizeof(*backwardContents);
+
+    //We have successfully generated the chaining cell
+    return true;
+}
+
+/* Chaining cell for monomorphic method invocations.
+   IA32 assembly:
+                  call imm32 // relative offset to dvmJitToInterpTraceSelect
+                  rPC
+                  codePtr
+*/
+static int handleInvokeSingletonChainingCell(CompilationUnit *cUnit,
+                                              const Method *callee, int blockId)
+{
+    ALOGV("In handleInvokeSingletonChainingCell for method %s block %d callee %s NCG offset %x",
+          cUnit->method->name, blockId, callee->name, stream - streamMethodStart);
+    if(dump_x86_inst)
+        ALOGI("LOWER InvokeSingletonChainingCell at block %d offsetNCG %x @%p",
+              blockId, stream - streamMethodStart, stream);
+
+    call_dvmJitToInterpTraceSelect();
+    unsigned int *ptr = (unsigned int*)stream;
+    *ptr++ = (unsigned int)(callee->insns);
+    char* codePtr = searchChainingWorklist(blockId);
+    *ptr++ = (unsigned int)codePtr;
+    stream = (char*)ptr;
+    return 0;
+}
+
+/**
+ * @brief Generates code for predicted chaining cell.
+ * @details This chaining cell is used for polymorphic invocations.
+ * @param cUnit the compilation unit
+ * @param chainingCell the chaining cell we are generating code for
+ * @return true if chaining cell was successfully generated
+ */
+static bool handleInvokePredictedChainingCell (CompilationUnit *cUnit,
+        BasicBlock_O1 *chainingCell)
+{
+    if(cUnit->printMe)
+    {
+        ALOGI("LOWER InvokePredictedChainingCell (block %d) @%p",
+                chainingCell->id, stream);
+    }
+
+#ifdef PREDICTED_CHAINING
+
+    //Because we will be patching this at runtime, we want to make sure that
+    //the chaining cell is 4 byte aligned. Since every field of the chaining
+    //cell is 4 byte wide, this will ensure atomic updates since the cell
+    //won't be split across cache line.
+    int padding = (4 - ((u4) stream & 3)) & 3;
+    stream += padding;
+
+    //Since we are aligning, we should also update the offset so anyone using
+    //it accesses the correct data.
+    chainingCell->label->lop.generic.offset += padding;
+
+    PredictedChainingCell *predictedContents =
+            reinterpret_cast<PredictedChainingCell *> (stream);
+
+    //Now initialize the data using the predefined macros for initialization
+    predictedContents->branch = PREDICTED_CHAIN_BX_PAIR_INIT;
+    predictedContents->branch2 = 0;
+    predictedContents->clazz = PREDICTED_CHAIN_CLAZZ_INIT;
+    predictedContents->method = PREDICTED_CHAIN_METHOD_INIT;
+    predictedContents->stagedClazz = PREDICTED_CHAIN_COUNTER_INIT;
+
+    //Update stream pointer
+    stream = reinterpret_cast<char *> (predictedContents)
+            + sizeof(*predictedContents);
+
+#else
+    //assume rPC for callee->insns in %ebx
+    scratchRegs[0] = PhysicalReg_EAX;
+#if defined(WITH_JIT_TUNING)
+    /* Predicted chaining is not enabled. Fall back to interpreter and
+     * indicate that predicted chaining was not done.
+     */
+    move_imm_to_reg(OpndSize_32, kInlineCacheMiss, PhysicalReg_EDX, true);
+#endif
+    call_dvmJitToInterpTraceSelectNoChain();
+#endif
+
+    //We have successfully generated the chaining cell
+    return true;
+}
+
+/**
+ * @brief Used to handle semantics of extended MIRs, including possibly generating native code.
+ * @param cUnit The compilation unit
+ * @param bb The basic block containing the MIR
+ * @param mir The extended instruction
+ * @return Returns whether or not it successfully handled the extended MIR
+ */
+bool handleExtendedMIR (CompilationUnit *cUnit, BasicBlock_O1 *bb, MIR *mir)
+{
+    if (cUnit->printMe == true)
+    {
+        char * decodedString = dvmCompilerGetDalvikDisassembly(&mir->dalvikInsn,
+                NULL);
+        ALOGI("LOWER %s @%p\n", decodedString, stream);
+    }
+
+    //Eagerly assume that we will be able to handle it
+    bool result = true;
+
+    ExecutionMode origMode = gDvm.executionMode;
+    gDvm.executionMode = kExecutionModeNcgO0;
+    switch ((ExtendedMIROpcode)mir->dalvikInsn.opcode) {
+        case kMirOpPhi: {
+            break;
+        }
+        case kMirOpNullCheck: {
+            genHoistedNullCheck (cUnit, mir);
+            break;
+        }
+        case kMirOpBoundCheck: {
+            genHoistedBoundCheck (cUnit, mir);
+            break;
+        }
+        case kMirOpNullNRangeUpCheck: {
+            genHoistedChecksForCountUpLoop(cUnit, mir);
+            break;
+        }
+        case kMirOpNullNRangeDownCheck: {
+            genHoistedChecksForCountDownLoop(cUnit, mir);
+            break;
+        }
+        case kMirOpLowerBound: {
+            genHoistedLowerBoundCheck(cUnit, mir);
+            break;
+        }
+        case kMirOpPunt: {
+            break;
+        }
+        case kMirOpRegisterize: {
+            gDvm.executionMode = origMode;
+            result = genRegisterize (cUnit, bb, mir);
+            break;
+        }
+#ifdef WITH_JIT_INLINING_PHASE2
+        case kMirOpCheckInlinePrediction: { //handled in ncg_o1_data.c
+            genValidationForPredictedInline(cUnit, mir);
+            break;
+        }
+#endif
+        default:
+        {
+            char * decodedString = dvmCompilerGetDalvikDisassembly(&mir->dalvikInsn, NULL);
+            ALOGD ("JIT_INFO: No logic to handle extended MIR %s", decodedString);
+            result = false;
+            break;
+        }
+    }
+    gDvm.executionMode = origMode;
+
+    return result;
+}
+
+#define PRINT_BUFFER_LEN 1024
+/* Print the code block in code cache in the range of [startAddr, endAddr)
+ * in readable format.
+ */
+void printEmittedCodeBlock(unsigned char *startAddr, unsigned char *endAddr)
+{
+    char strbuf[PRINT_BUFFER_LEN];
+    unsigned char *addr;
+    unsigned char *next_addr;
+    int n;
+
+    static const unsigned char nops[10][9] = {
+        { 0, },                                                     // 0, this line is dummy and not used in the loop below
+        { 0x90, },                                                  // 1-byte NOP
+        { 0x66, 0x90, },                                            // 2
+        { 0x0F, 0x1F, 0x00, },                                      // 3
+        { 0x0F, 0x1F, 0x40, 0x00, },                                // 4
+        { 0x0F, 0x1F, 0x44, 0x00, 0x00, },                          // 5
+        { 0x66, 0x0F, 0x1F, 0x44, 0x00, 0x00, },                    // 6
+        { 0x0F, 0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, },              // 7
+        { 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, },        // 8
+        { 0x66, 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00 },   // 9-byte NOP
+    };
+    int nopSize;
+    int pos;
+
+    if (gDvmJit.printBinary) {
+        // print binary in bytes
+        n = 0;
+        for (addr = startAddr; addr < endAddr; addr++) {
+            n += snprintf(&strbuf[n], PRINT_BUFFER_LEN-n, "0x%x, ", *addr);
+            if (n > PRINT_BUFFER_LEN - 10) {
+                ALOGD("## %s", strbuf);
+                n = 0;
+            }
+        }
+        if (n > 0)
+            ALOGD("## %s", strbuf);
+    }
+
+    // print disassembled instructions
+    addr = startAddr;
+    while (addr < endAddr) {
+        next_addr = reinterpret_cast<unsigned char*>
+            (decoder_disassemble_instr(reinterpret_cast<char*>(addr),
+                                       strbuf, PRINT_BUFFER_LEN));
+        if (addr != next_addr) {
+            ALOGD("**  %p: %s", addr, strbuf);
+        } else {
+            for (nopSize = 1; nopSize < 10; nopSize++) {
+                for (pos = 0; pos < nopSize; pos++) {
+                    if (addr[pos] != nops[nopSize][pos])
+                        break;
+                }
+                if (pos == nopSize) {
+                    ALOGD("**  %p: NOP (%d byte)", addr, nopSize);
+                    next_addr += nopSize;
+                    break;
+                }
+            }
+            if (nopSize == 10) {
+                ALOGD("** unable to decode binary at %p", addr);
+                break;
+            }
+        }
+        addr = next_addr;
+    }
+}
+
+/**
+ * @brief Print the content of chaining cell block in code cache to LOG.
+ * @param startAddr - starting address of the chaining cell block in code cache
+ * @param blockType - chaining cell block type
+ */
+static void printChainingCellBlocks(char *startAddr, BBType blockType)
+{
+    unsigned int *ui_ptr;
+
+    if (startAddr == 0 || blockType >= kChainingCellGap) {
+        return;
+    }
+
+    // Chaining cell block starts with a 5B "call rel32" at [startAddr, ui_ptr).
+    ui_ptr = (unsigned int *) ((unsigned char *)(startAddr+5));
+
+    switch (blockType) {
+        case kChainingCellNormal:
+            ALOGD("** // Normal Chaining Cell");
+            printEmittedCodeBlock((unsigned char *) startAddr, (unsigned char *) ui_ptr);
+            ALOGD("**  %p: %#x \t// next bytecode PC", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// code address to be patched", (void*)ui_ptr, *ui_ptr);
+            break;
+
+        case kChainingCellInvokeSingleton:
+            ALOGD("** // InvokeSingleton Chaining Cell");
+            printEmittedCodeBlock((unsigned char *) startAddr, (unsigned char *) ui_ptr);
+            ALOGD("**  %p: %#x \t// next bytecode PC", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// code address to be patched", (void*)ui_ptr, *ui_ptr);
+            break;
+
+        case kChainingCellHot:
+            ALOGD("** // Hot Chaining Cell");
+            printEmittedCodeBlock((unsigned char *) startAddr, (unsigned char *) ui_ptr);
+            ALOGD("**  %p: %#x \t// next bytecode PC", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// code address to be patched", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %d   \t// above needs an IP-relative offset", (void*)ui_ptr, *ui_ptr);
+            break;
+
+        case kChainingCellBackwardBranch:
+            ALOGD("** // BackwardBranch Chaining Cell");
+            printEmittedCodeBlock((unsigned char *) startAddr, (unsigned char *) ui_ptr);
+            ALOGD("**  %p: %#x \t// next bytecode PC", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// code address to be patched", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// address of loop header block", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// address of VR write-back block", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// address of loop pre-header block", (void*)ui_ptr, *ui_ptr);
+            break;
+
+        case kChainingCellInvokePredicted:
+            ui_ptr = (unsigned int *) startAddr;
+            ALOGD("** // InvokePredicted Chaining Cell: %p",
+                  (void*) startAddr);
+            ALOGD("**  %p: %#x \t// to be patched", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// to be patched", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// class", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// method", (void*)ui_ptr, *ui_ptr);
+            ui_ptr++;
+            ALOGD("**  %p: %#x \t// rechain count", (void*)ui_ptr, *ui_ptr);
+            break;
+
+        default:
+            ALOGD("printChainingCellBlocks: Unknown chaining cell type %u!",
+                  blockType);
+            break;      // Not yet supported! Do nothing.
+    }
+}
+
+/**
+ * @brief Print the content of the content of a trace to LOG.
+ * @param cUnit - pointer to the CompilationUnit
+ * @param code_block_table - reference to the code block table for code/data section addresses
+ * @param chainCellCounts - reference to the ChainCellCounts table
+ * @param wide_const_count - number of long/double constants in the constant section
+ * @param pCCOffsetSection - pointer to the chaining cell offset header
+ */
+typedef std::pair<BBType, char*> CodeBlockElem;
+static void printTrace(CompilationUnit *cUnit, std::vector<CodeBlockElem> &code_block_table,
+                       ChainCellCounts &chainCellCounts, int wide_const_count, u2* pCCOffsetSection)
+{
+    char *code_ptr, *next_code_ptr = 0;
+    BBType blk_type;
+    int k;
+
+    ALOGD("-------- Emit trace for [%s%s@%#x] binary code starts at %p (cache start %p)",
+          cUnit->method->clazz->descriptor, cUnit->method->name,
+          cUnit->traceDesc->trace[0].info.frag.startOffset,
+          cUnit->baseAddr, gDvmJit.codeCache);
+    ALOGD("** %s%s@%#x:", cUnit->method->clazz->descriptor,
+          cUnit->method->name, cUnit->traceDesc->trace[0].info.frag.startOffset);
+
+    code_ptr = NULL;
+    next_code_ptr = NULL;
+    for (k = 0; k < (code_block_table.size() - 1); k++) {
+        blk_type = code_block_table[k].first;
+        code_ptr = code_block_table[k].second;
+        next_code_ptr = code_block_table[k+1].second;
+
+        switch (blk_type) {
+        case kExceptionHandling:
+            if (code_ptr < next_code_ptr) {
+                ALOGD("** // exception handling VR restores");
+                // print like a normal code block
+                printEmittedCodeBlock((unsigned char *) code_ptr,
+                                      (unsigned char *) next_code_ptr);
+            }
+            break;
+
+        case kDalvikByteCode:
+            if (code_ptr < next_code_ptr) {
+                printEmittedCodeBlock((unsigned char *) code_ptr,
+                                      (unsigned char *) next_code_ptr);
+            }
+            break;
+
+        case kChainingCellNormal:
+        case kChainingCellHot:
+        case kChainingCellInvokeSingleton:
+        case kChainingCellInvokePredicted:
+        case kChainingCellBackwardBranch:
+            printChainingCellBlocks(code_ptr, blk_type);
+            break;
+
+        default:          // no print for other block types
+            break;
+        }
+    }
+
+    if (next_code_ptr == NULL) {
+        // simply return if there is no entry in code block
+        return;
+    }
+
+    // next_code_ptr should hold the pre-padded address of the chaining cell count section
+    // print the chaining cell count section
+    next_code_ptr = reinterpret_cast<char*>((reinterpret_cast<unsigned int>(next_code_ptr) + 3) & ~0x3);
+    ALOGD("** // chaining cell counts section (4B aligned)");
+    for (k=0; k< kChainingCellGap; k++) {
+        ALOGD("**  %p: %u", (void*) next_code_ptr, chainCellCounts.u.count[k]);
+        next_code_ptr += sizeof(chainCellCounts.u.count[k]);
+    }
+
+    // print the long/double constant section if any
+    if (wide_const_count > 0) {
+        long long *llptr;
+        double *dblptr;
+        ALOGD("** // long/double constant section (16B aligned)");
+        next_code_ptr = reinterpret_cast<char*>((reinterpret_cast<unsigned int>(next_code_ptr) + 0xF) & ~0xF);
+        llptr = (long long *) next_code_ptr;
+        for (k=0; k<wide_const_count; k++) {
+            dblptr = (double *) llptr;
+            ALOGD("**  %p: %lld (%g)", llptr, *llptr, *dblptr);
+            llptr++;        // increases pointer by 8B
+        }
+    }
+
+    // print the chaining cell offset header content
+    ALOGD("** // Patched (offset to chaining cell counts)@%p = %#x",
+          (void*)pCCOffsetSection, *pCCOffsetSection);
+    ALOGD("** // Patched (offset to chaining cell blocks)@%p = %#x",
+          (void*)&pCCOffsetSection[1], pCCOffsetSection[1]);
+}
+
+/**
+ * @brief Handle fallthrough branch: determine whether we need one or not
+ * @param cUnit the CompilationUnit
+ * @param bb the BasicBlock
+ * @param ptrNextFallThrough pointer to the nextFallThrough if requested (can be 0)
+ */
+static void handleFallThroughBranch (CompilationUnit *cUnit, BasicBlock *bb, BasicBlock **ptrNextFallThrough)
+{
+    //Get next fall through
+    BasicBlock *nextFallThrough = *ptrNextFallThrough;
+
+    //Check if the jump needs alignment. If it needs alignment it means it will be patched at runtime
+    //and thus we cannot skip generating this jump
+    bool jumpNeedsAlignment = false;
+    if (nextFallThrough != 0)
+    {
+        jumpNeedsAlignment = doesJumpToBBNeedAlignment (nextFallThrough);
+    }
+
+    //We need a fallthrough branch if we had a next and it isn't the current BasicBlock or jump is needed
+    bool needFallThroughBranch = (nextFallThrough != 0 && (jumpNeedsAlignment == true || bb != nextFallThrough));
+
+    if (needFallThroughBranch == true)
+    {
+        //Generate the jump now
+        jumpToBasicBlock (stream, nextFallThrough->id, jumpNeedsAlignment);
+    }
+
+    //Clear it
+    *ptrNextFallThrough = 0;
+}
+
+/**
+ * @brief Create a new record of 64bit constant in use
+ * @details Allocates memory to store a 64bit constant and its details. All address fields
+ *          are initialized to NULL.
+ * @param listPtr address of the constList
+ * @param constL the lower 32bits
+ * @param constH the higher 32bits
+ * @param reg Virtual Register number
+ * @param align Align to 16 bytes
+ */
+void addNewToConstList(struct ConstInfo** listPtr, int constL, int constH, int reg, bool align) {
+    struct ConstInfo* tmpPtr =static_cast<ConstInfo *>(dvmCompilerNew(sizeof(ConstInfo), false));
+    tmpPtr->valueL = constL;
+    tmpPtr->valueH = constH;
+    tmpPtr->regNum = reg;
+    tmpPtr->offsetAddr = 0;
+    tmpPtr->streamAddr = NULL;
+    tmpPtr->constAddr = NULL;
+    tmpPtr->constAlign = align;
+    tmpPtr->next = *listPtr;
+    *listPtr = tmpPtr;
+    assert(*listPtr != NULL);
+}
+
+/**
+ * @brief Save address of memory access into constList
+ * @details Populates stream information
+ * @param listPtr address of the constList
+ * @param constL the lower 32bits
+ * @param constH the higher 32bits
+ * @param reg Virtual Register number
+ * @param patchAddr The address of memory location to be patched currently
+ * @param offset the offset where to save the constant
+ * @return true when it succeeds, false when it fails
+ */
+bool saveAddrToConstList(struct ConstInfo** listPtr, int constL, int constH, int reg, char* patchAddr, int offset) {
+    struct ConstInfo* tmpPtr = *listPtr;
+    while (tmpPtr != NULL) {                // check all elements of the structure
+        if (tmpPtr->valueL == constL && tmpPtr->valueH == constH && tmpPtr->regNum == reg && tmpPtr->streamAddr==NULL) {
+            tmpPtr->streamAddr = patchAddr; // save address of instruction in jit stream
+            tmpPtr->offsetAddr = offset;    // save offset to memory location to patch for the instruction
+#ifdef DEBUG_CONST
+            ALOGD("**Save constants for VR# %d containing constant (%x):(%x) streamAddr is (%d)%x, offset %d",
+                             tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueH,
+                             tmpPtr->streamAddr, tmpPtr->streamAddr, tmpPtr->offsetAddr);
+#endif
+            return true;
+        }
+        tmpPtr = tmpPtr->next; // check next element
+    }
+    return false;
+}
+
+/**
+ * @brief insert 64bit constants in a Constant Data Section at end of a trace
+ * @details Populates stream information
+ * @param constListTemp pointer to list of 64 bit constants
+ * @param stream pointer to jit code
+ * @return the updated value of stream
+ */
+char* insertConstDataSection(struct ConstInfo *constListTemp,  char * stream) {
+    unsigned int *intaddr = reinterpret_cast<unsigned int *>(stream);
+    while(constListTemp != NULL){
+
+        /* Align trace to 16-bytes before Constant Data Section */
+        if (constListTemp->constAlign == true) {
+            stream = (char*)(((unsigned int)stream + 0xF) & ~0xF);
+        }
+        constListTemp->constAddr = stream;
+        intaddr = reinterpret_cast<unsigned int *>(stream);
+        *intaddr = constListTemp->valueL;    // store lower 32 bit of a constant
+#ifdef DEBUG_CONST
+        ALOGI("**Lower constants at  %p: %d(%x), VR# %d containing constant (%x):(%x) constAddr is %p",
+                             intaddr, *intaddr, *intaddr, constListTemp->regNum, constListTemp->valueL, constListTemp->valueH,
+                             *intaddr, constListTemp->constAddr);
+#endif
+        intaddr++;
+        *intaddr = constListTemp->valueH;    // store higher 32 bits of a constant
+        intaddr++;
+        stream = reinterpret_cast<char *>(intaddr);
+        constListTemp = constListTemp->next; // move to next constant in list
+    }
+    return stream;
+}
+
+/**
+ * @brief patch stream with address of constants in Constant Data Section
+ * @details lowers address of constant if placeholder data is found
+ * @param constListTemp pointer to list of 64 bit constants
+ * @param cUnit the compilation unit
+ * @return returns -1 if error, else reports number of patches
+ */
+int patchConstToStream(struct ConstInfo *constListTemp, CompilationUnit *cUnit) {
+    unsigned int *writeval;
+    char *iaddr;
+    int pResult = 0;
+
+    while (constListTemp != NULL){
+        /* iterate through the generated code to patch constants */
+        iaddr = static_cast<char*>(constListTemp->streamAddr+constListTemp->offsetAddr);
+
+        writeval = reinterpret_cast<unsigned int*>(iaddr);
+        unsigned int dispAddr =  getGlobalDataAddr("64bits");
+
+        if (*writeval == dispAddr){ /* verify that place holder data inserted is present */
+            *writeval = reinterpret_cast<unsigned int>(constListTemp->constAddr);
+#ifdef DEBUG_CONST
+            ALOGI("Patched location of VR# %d with constant (%x):(%x)",
+                            constListTemp->regNum, constListTemp->valueL, constListTemp->valueH);
+            ALOGI("Address is streamAddr %p,  offset %d with constAddr %p",
+                       constListTemp->streamAddr, constListTemp->offsetAddr, constListTemp->constAddr);
+#endif
+            pResult++;              /* keep count of successful patches in stream */
+        } else {
+            ALOGI("JIT_INFO: Error Wrong value found at streamAddr");
+#ifdef DEBUG_CONST
+            ALOGI("Tried patching VR# %d with constant (%x):(%x)",
+                            constListTemp->regNum, constListTemp->valueL, constListTemp->valueH);
+            ALOGI("Address is streamAddr %p, offset %d with constAddr %p",
+                    constListTemp->streamAddr, constListTemp->offsetAddr, constListTemp->constAddr);
+#endif
+            ALOGI("JIT_INFO: Constant init opt could not patch all required locations");
+            SET_JIT_ERROR(kJitErrorConstInitFail);
+            cUnit->constListHead = NULL;
+            return -1;              /* incorrect data found at patch location, reject trace */
+        }
+        constListTemp = constListTemp->next;
+    }
+    return pResult;
+}
+
+/**
+ * @brief Generate the code for the BasicBlock
+ * @param cUnit the CompilationUnit
+ * @param bb the BasicBlock
+ * @param nextFallThrough a pointer to the next fall through BasicBlock
+ * @return whether the generation went well
+ */
+static bool generateCode (CompilationUnit *cUnit, BasicBlock *bb, BasicBlock **nextFallThrough)
+{
+    ALOGV("Get ready to handle JIT bb %d type %d hidden %d @%p",
+            bb->id, bb->blockType, bb->hidden, stream);
+
+    /* We want to update the stream start to remember it for future backward chaining cells */
+    BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (bb);
+    assert (bbO1 != 0);
+    bbO1->streamStart = stream;
+
+#ifdef WITH_JIT_TPROFILE
+    //Generate the loop counter profile code for loop
+    genLoopCounterProfileCode(cUnit, bbO1);
+#endif
+    //Generate the code
+    startOfBasicBlock(bb);
+    int cg_ret = codeGenBasicBlockJit(cUnit->method, bb);
+    endOfBasicBlock(bb);
+
+    //Error handling, we return false
+    if(cg_ret < 0 || IS_ANY_JIT_ERROR_SET()) {
+        ALOGI("Could not compile trace for %s%s, offset %d",
+                cUnit->method->clazz->descriptor, cUnit->method->name,
+                cUnit->traceDesc->trace[0].info.frag.startOffset);
+        SET_JIT_ERROR(kJitErrorCodegen);
+        endOfTrace (cUnit);
+        PROTECT_CODE_CACHE(stream, unprotected_code_cache_bytes);
+        return false;
+    }
+
+    //Register next fall through
+    *nextFallThrough = bb->fallThrough;
+
+    //Everything went fine
+    return true;
+}
+
+//! \brief Lower middle-level IR ro low-level IR
+//!
+//! \details Entry function to invoke the backend of the JIT compiler
+//!
+//! \param cUnit: The current compilation unit
+//! \param info: JitTranslationInfo. Holds generated code address on success
+static void compilerMIR2LIRJit(CompilationUnit_O1 *cUnit, JitTranslationInfo *info)
+{
+    //Used to determine whether we need a fallthrough jump
+    BasicBlock *nextFallThrough = 0;
+    // Define the code_block_table for tracking various type of code blocks
+    //  for printing.
+    // CodeBlockElem has a block type and the starting address of the block.
+    std::vector<CodeBlockElem> code_block_table;
+    char *print_stream_ptr = 0; // current block stream pointer
+
+    dump_x86_inst = cUnit->printMe;
+
+    GrowableList chainingListByType[kChainingCellLast];
+
+    unsigned int i, padding;
+
+    traceMode = cUnit->jitMode;
+
+    //Initialize the base address to null
+    cUnit->baseAddr = NULL;
+
+    /*
+     * Initialize various types chaining lists.
+     */
+    for (i = 0; i < kChainingCellLast; i++) {
+        dvmInitGrowableList(&chainingListByType[i], 2);
+    }
+
+    GrowableListIterator iterator;
+
+    //BasicBlock **blockList = cUnit->blockList;
+    GrowableList *blockList = &cUnit->blockList;
+    BasicBlock *bb;
+
+    info->codeAddress = NULL;
+    stream = (char*)gDvmJit.codeCache + gDvmJit.codeCacheByteUsed;
+
+    // TODO: compile into a temporary buffer and then copy into the code cache.
+    // That would let us leave the code cache unprotected for a shorter time.
+    size_t unprotected_code_cache_bytes =
+            gDvmJit.codeCacheSize - gDvmJit.codeCacheByteUsed - CODE_CACHE_PADDING;
+    UNPROTECT_CODE_CACHE(stream, unprotected_code_cache_bytes);
+    //Use the variable otherwise compiler warns about it
+    (void) unprotected_code_cache_bytes;
+
+    streamStart = stream; /* trace start before alignment */
+
+#if defined(WITH_JIT_TPROFILE)
+    /* Align stream's address end with 0100, this is to make sure the code start address align to 16-bytes after add the extra bytes */
+    stream = ((u4)stream & 0x7) < 4 ? (char*)(((unsigned int)stream + 0x4) & ~0x3) : (char*)(((unsigned int)stream + 0x8) & ~0x3);
+    stream += EXTRA_BYTES_FOR_LOOP_COUNT_ADDR; /*This is for the loop count's addr*/
+    stream += EXTRA_BYTES_FOR_PROF_ADDR; /* This is for the execution count's addr */
+
+    //zero the loop count address, so we can check if the trace is a loop
+    memset(stream - EXTRA_BYTES_FOR_LOOP_COUNT_ADDR - EXTRA_BYTES_FOR_PROF_ADDR, 0, EXTRA_BYTES_FOR_LOOP_COUNT_ADDR + EXTRA_BYTES_FOR_PROF_ADDR);
+#endif
+
+    stream += EXTRA_BYTES_FOR_CHAINING; /* This is needed for chaining. */
+    stream = (char*)(((unsigned int)stream + 0xF) & ~0xF); /* Align trace to 16-bytes */
+    streamMethodStart = stream; /* code start */
+
+    cUnit->exceptionBlockId = -1;
+    for (i = 0; i < blockList->numUsed; i++) {
+        bb = (BasicBlock *) blockList->elemList[i];
+        if(bb->blockType == kExceptionHandling)
+            cUnit->exceptionBlockId = i;
+    }
+    startOfTrace(cUnit->method, cUnit->exceptionBlockId, cUnit);
+
+    /* Traces start with a profiling entry point.  Generate it here */
+    cUnit->profileCodeSize = genTraceProfileEntry(cUnit);
+
+    cUnit->constListHead = NULL; // Initialize constant list
+
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+
+        //Go over the basic blocks of the compilation unit
+        dvmGrowableListIteratorInit(&cUnit->blockList, &iterator);
+        for (bb = (BasicBlock *) (dvmGrowableListIteratorNext(&iterator));
+                bb != NULL;
+                bb = (BasicBlock *) (dvmGrowableListIteratorNext(&iterator))) {
+
+            int retCode = preprocessingBB (cUnit, bb);
+
+            if (retCode < 0) {
+                endOfTrace (cUnit);
+                SET_JIT_ERROR(kJitErrorCodegen);
+                return;
+            }
+        }
+    }
+
+    dvmGrowableListIteratorInit(&cUnit->blockList, &iterator);
+
+    /* Handle the content in each basic block */
+    for (bb = (BasicBlock *) (dvmGrowableListIteratorNext (&iterator)),
+         i = 0;
+         //We stop when bb is 0
+         bb != 0;
+         //Induction variables: bb goes to next iterator, i is incremented
+         bb = (BasicBlock *) (dvmGrowableListIteratorNext (&iterator)),
+         i++) {
+
+        //Get O1 version
+        BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (bb);
+
+        //Paranoid
+        if (bbO1 == 0) {
+            continue;
+        }
+
+        //Switch depending on the BasicBlock type
+        switch (bbO1->blockType)
+        {
+            case kEntryBlock:
+                //The entry block should always be processed first because it is entry to trace
+                assert (i == 0);
+
+                //Intentional fallthrough because we handle it same way as an exit block
+            case kExitBlock:
+                //Only handle the fallthrough if there is an instruction
+                if (bbO1->firstMIRInsn != 0)
+                {
+                    //First handle fallthrough branch
+                    handleFallThroughBranch (cUnit, bbO1, &nextFallThrough);
+                }
+
+                //Set label offset
+                bbO1->label->lop.generic.offset = (stream - streamMethodStart);
+
+                if (generateCode (cUnit, bbO1, &nextFallThrough) == false)
+                {
+                    //Generate code set an error for the jit, we can just return
+                    return;
+                }
+                break;
+            case kDalvikByteCode:
+            case kPreBackwardBlock:
+                //If hidden, we don't generate code
+                if (bbO1->hidden == false)
+                {
+                    //First handle fallthrough branch
+                    handleFallThroughBranch (cUnit, bbO1, &nextFallThrough);
+
+                    //Set label offset
+                    bbO1->label->lop.generic.offset = (stream - streamMethodStart);
+
+                    if (generateCode (cUnit, bbO1, &nextFallThrough) == false)
+                    {
+                        //Generate code set an error for the jit, we can just return
+                        return;
+                    }
+                }
+                break;
+            case kChainingCellNormal:
+                /* Handle the codegen later */
+                dvmInsertGrowableList(&chainingListByType[kChainingCellNormal], i);
+                break;
+            case kChainingCellInvokeSingleton:
+                /* Handle the codegen later */
+                dvmInsertGrowableList (&chainingListByType[kChainingCellInvokeSingleton], i);
+                break;
+            case kChainingCellInvokePredicted:
+                /* Handle the codegen later */
+                dvmInsertGrowableList(&chainingListByType[kChainingCellInvokePredicted], i);
+                break;
+            case kChainingCellHot:
+                /* Handle the codegen later */
+                dvmInsertGrowableList(&chainingListByType[kChainingCellHot], i);
+                break;
+            case kExceptionHandling:
+                //First handle fallthrough branch
+                handleFallThroughBranch (cUnit, bbO1, &nextFallThrough);
+                bbO1->label->lop.generic.offset = (stream - streamMethodStart);
+                scratchRegs[0] = PhysicalReg_EAX;
+                jumpToInterpPunt();
+                break;
+            case kChainingCellBackwardBranch:
+                /* Handle the codegen later */
+                dvmInsertGrowableList(&chainingListByType[kChainingCellBackwardBranch], i);
+                break;
+            default:
+                break;
+            }
+        }
+
+    if (cUnit->printMe) {
+        // record all assmebly code before chaining cells as a block
+        CodeBlockElem code_blk_elem(kDalvikByteCode, streamMethodStart);
+        code_block_table.push_back(code_blk_elem);
+        print_stream_ptr = stream;
+    }
+
+    char* streamChainingStart = 0;
+    /* Handle the chaining cells in predefined order */
+
+    for (i = 0; i < kChainingCellGap; i++) {
+        size_t j;
+        cUnit->numChainingCells[i] = chainingListByType[i].numUsed;
+
+        /* No chaining cells of this type */
+        if (cUnit->numChainingCells[i] == 0)
+            continue;
+
+        //First handle fallthrough branch
+        handleFallThroughBranch (cUnit, 0, &nextFallThrough);
+
+        //If we haven't initialized the start of the chaining cells we do it now
+        if (streamChainingStart == 0)
+        {
+            //Stream has been updated because handleFallThroughBranch always generates jumps which
+            //forces scheduler to update the stream pointer. Thus we can use it here.
+            assert (singletonPtr<Scheduler>()->isQueueEmpty() == true);
+
+            //Initialize the beginning of the chaining cells
+            streamChainingStart = stream;
+        }
+
+        if (cUnit->printMe && print_stream_ptr < stream) {
+            // If there is any code before the chaining cell block and the
+            // last recorded block, make it a separate code block.
+            CodeBlockElem code_blk_elem(kDalvikByteCode, print_stream_ptr);
+            code_block_table.push_back(code_blk_elem);
+            print_stream_ptr = stream;
+        }
+
+        /* Record the first LIR for a new type of chaining cell */
+        for (j = 0; j < chainingListByType[i].numUsed; j++) {
+            int blockId = (int) dvmGrowableListGetElement (& (chainingListByType[i]), j);
+
+            BasicBlock *chainingBlock =
+                (BasicBlock *) dvmGrowableListGetElement(&cUnit->blockList,
+                                                         blockId);
+
+            //Get O1 version
+            BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (chainingBlock);
+
+            //Paranoid
+            if (bbO1 == 0) {
+                continue;
+            }
+
+            //Set offset
+            bbO1->label->lop.generic.offset = (stream - streamMethodStart);
+
+            //Eagerly assume we successfully generated chaining cell
+            bool success = true;
+
+            int nop_size;
+            switch (chainingBlock->blockType) {
+                case kChainingCellNormal:
+                    nop_size = handleNormalChainingCell(cUnit,
+                     chainingBlock->startOffset, blockId);
+                    bbO1->label->lop.generic.offset += nop_size; //skip over nop
+                    break;
+                case kChainingCellInvokeSingleton:
+                    nop_size = handleInvokeSingletonChainingCell(cUnit,
+                        chainingBlock->containingMethod, blockId);
+                    bbO1->label->lop.generic.offset += nop_size; //skip over nop
+                    break;
+                case kChainingCellInvokePredicted:
+                    success = handleInvokePredictedChainingCell (cUnit, bbO1);
+                    break;
+                case kChainingCellHot:
+                    nop_size = handleHotChainingCell(cUnit,
+                        chainingBlock->startOffset, blockId);
+                    bbO1->label->lop.generic.offset += nop_size; //skip over nop
+                    break;
+                case kChainingCellBackwardBranch:
+                    success = handleBackwardBranchChainingCell (cUnit, bbO1);
+                    break;
+                default:
+                    ALOGI("JIT_INFO: Bad blocktype %d", chainingBlock->blockType);
+                    SET_JIT_ERROR(kJitErrorTraceFormation);
+                    endOfTrace (cUnit);
+                    code_block_table.clear();
+                    return;
+            }
+
+            if (success == false)
+            {
+                SET_JIT_ERROR(kJitErrorChainingCell);
+                endOfTrace (cUnit);
+                return;
+            }
+
+            if (cUnit->printMe) {
+                // record the chaining cell block
+                CodeBlockElem code_blk_elem(chainingBlock->blockType, print_stream_ptr);
+                code_block_table.push_back(code_blk_elem);
+                print_stream_ptr = stream;
+            }
+
+            if (gDvmJit.codeCacheByteUsed + (stream - streamStart) + CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
+                ALOGI("JIT_INFO: Code cache full after ChainingCell (trace uses %uB)", (stream - streamStart));
+                SET_JIT_ERROR(kJitErrorCodeCacheFull);
+                gDvmJit.codeCacheFull = true;
+                endOfTrace (cUnit);
+                PROTECT_CODE_CACHE(stream, unprotected_code_cache_bytes);
+                code_block_table.clear();
+                return;
+            }
+        }
+    }
+
+    // Now that we finished handling all of the MIR BBs, we can dump all exception handling
+    // restore state to the code stream
+    singletonPtr<ExceptionHandlingRestoreState>()->dumpAllExceptionHandlingRestoreState();
+
+    //In case, handle fallthrough branch
+    handleFallThroughBranch (cUnit, 0, &nextFallThrough);
+
+    //Since we are at end of trace, we need to finish all work in the worklists
+    performWorklistWork ();
+
+    //We finished generating code for trace so we can signal end of trace now
+    endOfTrace (cUnit);
+
+    if (cUnit->printMe) {
+        // record exception VR restores as block type kExceptionHandling
+        CodeBlockElem code_blk_elem(kExceptionHandling, print_stream_ptr);
+        code_block_table.push_back(code_blk_elem);
+        print_stream_ptr = stream;
+    }
+
+    if (gDvmJit.codeCacheFull) {
+        // We hit code cache size limit either after dumping exception handling
+        // state or after calling endOfTrace. Bail out for this trace!
+        ALOGI("JIT_INFO: Code cache full after endOfTrace (trace uses %uB)", (stream - streamStart));
+        SET_JIT_ERROR(kJitErrorCodeCacheFull);
+        PROTECT_CODE_CACHE(stream, unprotected_code_cache_bytes);
+        code_block_table.clear();
+        return;
+    }
+
+    /* dump section for chaining cell counts, make sure it is 4-byte aligned */
+    padding = (4 - ((u4)stream & 3)) & 3;
+    stream += padding;
+    ChainCellCounts chainCellCounts;
+    /* Install the chaining cell counts */
+    for (i=0; i< kChainingCellGap; i++) {
+        chainCellCounts.u.count[i] = cUnit->numChainingCells[i];
+    }
+    char* streamCountStart = (char*)stream;
+    memcpy((char*)stream, &chainCellCounts, sizeof(chainCellCounts));
+    stream += sizeof(chainCellCounts);
+
+    cUnit->totalSize = (stream - streamStart);
+    if(gDvmJit.codeCacheByteUsed + cUnit->totalSize + CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
+        ALOGI("JIT_INFO: Code cache full after ChainingCellCounts (trace uses %uB)", (stream - streamStart));
+        SET_JIT_ERROR(kJitErrorCodeCacheFull);
+        gDvmJit.codeCacheFull = true;
+        PROTECT_CODE_CACHE(stream, unprotected_code_cache_bytes);
+        code_block_table.clear();
+        return;
+    }
+
+    /* write chaining cell count offset & chaining cell offset */
+    u2* pOffset = (u2*)(streamMethodStart - EXTRA_BYTES_FOR_CHAINING); /* space was already allocated for this purpose */
+    *pOffset = streamCountStart - streamMethodStart; /* from codeAddr */
+    pOffset[1] = streamChainingStart - streamMethodStart;
+
+#if defined(WITH_JIT_TPROFILE)
+    /* Install the trace description, so that we can retrieve the trace info from trace code addr later */
+    int descSize = (cUnit->jitMode == kJitMethod) ?
+        0 : getTraceDescriptionSize(cUnit->traceDesc);
+    memcpy((char*) stream, cUnit->traceDesc, descSize);
+    stream += descSize;
+    cUnit->totalSize = (stream - streamStart);
+
+    /* Check if the trace installation will cause the code cache full */
+    if(gDvmJit.codeCacheByteUsed + cUnit->totalSize + CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
+        ALOGI("JIT_INFO: Code cache full after Trace Description (trace uses %uB)", (stream - streamStart));
+        SET_JIT_ERROR(kJitErrorCodeCacheFull);
+        gDvmJit.codeCacheFull = true;
+        cUnit->baseAddr = NULL;
+        PROTECT_CODE_CACHE(stream, unprotected_code_cache_bytes);
+        return;
+    }
+#endif
+
+    PROTECT_CODE_CACHE(stream, unprotected_code_cache_bytes);
+
+    /* Align trace to 16-bytes before Constant Data Section */
+    stream = reinterpret_cast<char*>((reinterpret_cast<unsigned int>(stream) + 0xF) & ~0xF);
+    char * streamEnd = stream; // To store end of stream including constant data section
+
+    int patchCount = 0;       // Store number of constants initialized in a trace
+    ConstInfo *constListTemp; // Temp ptr for constant initialization
+
+    if(((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false) && cUnit->constListHead != NULL ) {
+
+        constListTemp = cUnit->constListHead;
+        while(constListTemp != NULL){ // Find projected end of trace
+            streamEnd = (char*)(((unsigned int)streamEnd + 0xF));
+            constListTemp = constListTemp->next;
+        }
+
+        //  Sum of bytes used in code cache, constant data section should be lower than code cache size
+        if((gDvmJit.codeCacheByteUsed + (streamEnd - streamStart)) > gDvmJit.codeCacheSize) {
+            ALOGI("JIT_INFO: Code cache full after ChainingCellCounts and constant data section");
+            SET_JIT_ERROR(kJitErrorCodeCacheFull);
+            gDvmJit.codeCacheFull = true;
+            cUnit->baseAddr = NULL;
+            PROTECT_CODE_CACHE(stream, unprotected_code_cache_bytes);
+            return;
+        }
+
+        /* Insert constant data section at the end of a trace */
+        streamEnd = insertConstDataSection(cUnit->constListHead, stream);
+
+        /* Patch address of constants into stream */
+        constListTemp = cUnit->constListHead;
+        patchCount = patchConstToStream(constListTemp, cUnit);
+
+        if (patchCount < 0) {// if patchCount is less than 0, trigger error recovery
+            ALOGI("JIT_INFO: Constant init opt could not patch all required locations");
+            SET_JIT_ERROR(kJitErrorConstInitFail);
+            cUnit->baseAddr = NULL;
+            cUnit->constListHead = NULL;
+            return;
+        }
+    }
+
+    cUnit->constListHead = NULL;
+    cUnit->totalSize = (streamEnd - streamStart);  // store size of trace in cUnit->totalSize
+    gDvmJit.codeCacheByteUsed += cUnit->totalSize; // store code cache byte used to include the current trace
+
+    // Now print out the trace in code cache based on code_block_table
+    if (cUnit->printMe) {
+        // Push an kExitBlock block as an end marker of the trace.
+        // The chaining cell count and the long/double constants are
+        //  emit after the end marker.
+        CodeBlockElem code_blk_elem(kExitBlock, print_stream_ptr);
+        code_block_table.push_back(code_blk_elem);
+        printTrace(cUnit, code_block_table, chainCellCounts, patchCount, pOffset);
+    }
+    code_block_table.clear();
+    ALOGV("JIT CODE after trace %p to %p size %x START %p", streamMethodStart,
+          (char *) gDvmJit.codeCache + gDvmJit.codeCacheByteUsed,
+          cUnit->totalSize, gDvmJit.codeCache);
+
+    gDvmJit.numCompilations++;
+
+    //Update the base addr
+    cUnit->baseAddr = streamMethodStart;
+
+    info->codeAddress = (char*)cUnit->baseAddr;// + cUnit->headerSize;
+#if defined(WITH_JIT_TPROFILE)
+    info->profileCodeSize = cUnit->profileCodeSize;
+#endif
+}
+
+//! \brief Helper function to call compilerMIR2LIRJit
+//!
+//! \details Calls dvmCompilerMIR2LIRJit, checks for errors
+//! and retries if possible.
+//!
+//! \param cUnit: The current compilation unit
+//! \param info: JitTranslationInfo.
+void dvmCompilerMIR2LIR(CompilationUnit *cUnit, JitTranslationInfo *info) {
+
+    //Make a x86 version of our CompilationUnit
+    CompilationUnit_O1 &x86CUnit = *static_cast<CompilationUnit_O1*>(cUnit);
+
+   //Save the optimization state to restore it at the end of this compilation
+   SErrorCompilationState compilationState;
+   dvmSaveOptimizationState (compilationState);
+
+   //Start the counter
+   int numTries = 0;
+   bool isLastAttempt = false;
+
+   //Try to lower MIR
+   do {
+        //See if we have been here too many times:
+        if (numTries > MAX_RETRIES) {
+
+            //Abort if the flag is set.
+            if (gDvmJit.abortOnCompilerError == true) {
+                ALOGE("Too many retries for trace  %s%s, offset %d", cUnit->method->clazz->descriptor,
+                        cUnit->method->name, cUnit->traceDesc->trace[0].info.frag.startOffset);
+
+                //This will cause a full abort due to the flag
+                dvmCompilerAbort(cUnit);
+            }
+
+            ALOGD("Too many retries while compiling trace  %s%s, offset %d", cUnit->method->clazz->descriptor,
+                    cUnit->method->name, cUnit->traceDesc->trace[0].info.frag.startOffset);
+            ALOGD("Rejecting Trace");
+
+            //Restore the compilation state
+            dvmRestoreCompilationState (compilationState);
+            //Make sure 'NULL' will be returned as compilation result
+            cUnit->baseAddr = NULL;
+            info->codeAddress = NULL;
+            return;
+        }
+
+        //Ignore errors in previous compilations
+        CLEAR_ALL_JIT_ERRORS();
+
+        //Do the trace compilation
+        numTries++;
+        compilerMIR2LIRJit(&x86CUnit, info);
+
+        //Once done, see if errors happened, and if so
+        //see if we can retry and come back
+        isLastAttempt = numTries == MAX_RETRIES;
+    } while (IS_ANY_JIT_ERROR_SET() && dvmCanFixErrorsAndRetry(&x86CUnit, isLastAttempt));
+
+   //Restore the compilation state
+   dvmRestoreCompilationState (compilationState);
+}
+
+
+/*
+ * Perform translation chain operation.
+ */
+void* dvmJitChain(void* tgtAddr, u4* branchAddr)
+{
+#ifdef JIT_CHAIN
+    int relOffset;
+
+    if ((gDvmJit.pProfTable != NULL) && (gDvm.sumThreadSuspendCount == 0) &&
+        (gDvmJit.codeCacheFull == false)) {
+
+        UNPROTECT_CODE_CACHE(branchAddr, sizeof(int));
+        gDvmJit.translationChains++;
+        UPDATE_CODE_CACHE_PATCHES();
+
+        relOffset = (int) tgtAddr - (int)branchAddr - 4; // 32bit offset
+
+        updateCodeCache(*(int*)branchAddr, relOffset);
+
+        gDvmJit.hasNewChain = true;
+
+        PROTECT_CODE_CACHE(branchAddr, sizeof(int));
+        COMPILER_TRACE_CHAINING(
+            ALOGI("Jit Runtime: chaining 0x%x to %p with relOffset %x",
+                  (int) branchAddr, tgtAddr, relOffset));
+    }
+#endif
+    return tgtAddr;
+}
+
+/*
+ * Perform chaining operation. Patched branchAddr using static address tgtAddr
+ */
+void* dvmJitChain_staticAddr(void* tgtAddr, u4* branchAddr)
+{
+#ifdef JIT_CHAIN
+    if ((gDvmJit.pProfTable != NULL) && (gDvm.sumThreadSuspendCount == 0) &&
+        (gDvmJit.codeCacheFull == false)) {
+
+        UNPROTECT_CODE_CACHE(branchAddr, sizeof(int));
+        gDvmJit.translationChains++;
+        UPDATE_CODE_CACHE_PATCHES();
+
+        updateCodeCache(*(int*)branchAddr, (int)tgtAddr);
+
+        gDvmJit.hasNewChain = true;
+
+        PROTECT_CODE_CACHE(branchAddr, sizeof(int));
+        COMPILER_TRACE_CHAINING(
+            ALOGI("Jit Runtime: chaining 0x%x to %p\n",
+                 (int) branchAddr, tgtAddr));
+    }
+#endif
+    return tgtAddr;
+}
+
+/**
+ * @brief Send off the work
+ * @param work the CompilerWorkOrder
+ * @return if the compilation succeeded
+ */
+static bool sendOffWork (CompilerWorkOrder *work)
+{
+    //Get trace description
+    JitTraceDescription *desc = static_cast<JitTraceDescription *> (work->info);
+    bool success = true;
+
+    //Will we compile it?
+    bool (*middleEndGate) (JitTraceDescription *, int, JitTranslationInfo *, jmp_buf *, int ) = gDvmJit.jitFramework.middleEndGate;
+
+    //Compilation function
+    bool (*middleEndFunction) (JitTraceDescription *, int, JitTranslationInfo *, jmp_buf *, int ) = gDvmJit.jitFramework.middleEndFunction;
+
+    //If we have a middle-end function, we have work
+    if (middleEndFunction != 0)
+    {
+        //Suppose we will compile it
+        bool willCompile = true;
+
+        //If we have a gate
+        if (middleEndGate != 0)
+        {
+            willCompile = middleEndGate (desc, JIT_MAX_TRACE_LEN, &work->result, work->bailPtr, 0);
+        }
+
+        if (willCompile == true)
+        {
+            //Get middle end function
+
+            success = middleEndFunction (desc, JIT_MAX_TRACE_LEN, &work->result, work->bailPtr, 0 /* no hints */);
+        }
+    }
+
+    return success;
+}
+
+/*
+ * Accept the work and start compiling.  Returns true if compilation
+ * is attempted.
+ */
+bool dvmCompilerDoWork(CompilerWorkOrder *work)
+{
+    bool isCompile = true;
+    bool success = true;
+
+    if (gDvmJit.codeCacheFull == true) {
+        return false;
+    }
+
+    switch (work->kind) {
+        case kWorkOrderTrace:
+            sendOffWork (work);
+            break;
+        case kWorkOrderTraceDebug:
+            {
+                bool oldPrintMe = gDvmJit.printMe;
+                gDvmJit.printMe = true;
+                sendOffWork (work);
+                gDvmJit.printMe = oldPrintMe;
+                break;
+            }
+        case kWorkOrderProfileMode:
+            dvmJitChangeProfileMode ( (TraceProfilingModes) (int) work->info);
+            isCompile = false;
+            break;
+        default:
+            isCompile = false;
+            ALOGI ("JIT_INFO: Unknown work order type");
+            assert (0);  // Bail if debug build, discard otherwise
+            ALOGI ("\tError ignored");
+            break;
+    }
+
+    if (success == false) {
+        work->result.codeAddress = NULL;
+    }
+
+    return isCompile;
+}
+
+void dvmCompilerCacheFlush(long start, long end, long flags) {
+  /* cacheflush is needed for ARM, but not for IA32 (coherent icache) */
+}
+
+bool dvmCompilerFindRegClass (MIR *mir, int vR, RegisterClass &regClass)
+{
+    //Get information about the VRs in current bytecode
+    VirtualRegInfo infoByteCode[MAX_REG_PER_BYTECODE];
+    int numVRs = getVirtualRegInfo (infoByteCode, mir);
+
+    //If we get a negative return value, there was an error.
+    if (numVRs < 0)
+    {
+        return false;
+    }
+
+    int entry;
+    for (entry = 0; entry < numVRs; entry++) {
+        if (infoByteCode[entry].regNum == vR) {
+            break;
+        }
+    }
+
+    // If we cannot find this VR, we failed
+    if (entry == numVRs)
+    {
+        return false;
+    }
+
+    switch (infoByteCode[entry].physicalType)
+    {
+        case LowOpndRegType_gp:
+            regClass = kCoreReg;
+            break;
+        case LowOpndRegType_fs_s:
+        case LowOpndRegType_fs:
+            regClass = kX87Reg;
+            break;
+        case LowOpndRegType_ss:
+            regClass = kSFPReg;
+            break;
+        case LowOpndRegType_xmm:
+            regClass = kDFPReg;
+            break;
+        default:
+            ALOGD ("JIT_INFO: dvmCompilerFindClass: Type not found %d\n",
+                    infoByteCode[entry].physicalType);
+            return false;
+    }
+
+    //Success, signal it
+    return true;
+}
+
+BasicBlock *x86StandAloneArchSpecificNewBB (void)
+{
+    // Make space on arena for this BB
+    void * space = dvmCompilerNew(sizeof(BasicBlock_O1), true);
+
+    // Ensure that constructor is called
+    BasicBlock_O1 * newBB = new (space) BasicBlock_O1;
+
+    // Paranoid because dvmCompilerNew should never return NULL
+    assert(newBB != 0);
+
+    return newBB;
+}
+
+void x86StandAloneArchSpecificDumpBB (CompilationUnit *cUnit, BasicBlock *bb, FILE *file, bool beforeMIRs)
+{
+    // We have already created the x86 specific BB so cast is okay
+    BasicBlock_O1 * curBB = reinterpret_cast<BasicBlock_O1 *>(bb);
+
+    if (beforeMIRs == true)
+    {
+        curBB->associationTable.printToDot(file);
+    }
+}
diff --git a/vm/compiler/codegen/x86/lightcg/CompilationUnit.cpp b/vm/compiler/codegen/x86/lightcg/CompilationUnit.cpp
new file mode 100644
index 0000000..5faea03
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/CompilationUnit.cpp
@@ -0,0 +1,45 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "CompilationUnit.h"
+
+bool CompilationUnit_O1::getCanSpillRegister (int reg)
+{
+    //Check overflow first
+    if (reg < 0 || reg >= PhysicalReg_Null)
+    {
+        return false;
+    }
+
+    //Otherwise, use what is in the array
+    return canSpillRegister[reg];
+}
+
+bool CompilationUnit_O1::setCanSpillRegister (int reg, bool value)
+{
+    //Check overflow first
+    if (reg < 0 || reg >= PhysicalReg_Null)
+    {
+        //Cannot update it
+        return false;
+    }
+
+    //Otherwise, use what is in the array
+    canSpillRegister[reg] = value;
+
+    //Update succeeded
+    return true;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/CompilationUnit.h b/vm/compiler/codegen/x86/lightcg/CompilationUnit.h
new file mode 100644
index 0000000..6e93dc0
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/CompilationUnit.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef H_COMPILATIONUNIT
+#define H_COMPILATIONUNIT
+
+#include "Lower.h"
+#include "compiler/CompilerIR.h"
+
+class CompilationUnit_O1: public CompilationUnit
+{
+    protected:
+        /** @brief Physical registers that should not be spilled */
+        bool canSpillRegister[PhysicalReg_Null];
+
+    public:
+        /**
+         * @brief Can we spill a register?
+         * @param reg the register we care about
+         * @return true if reg can be spilled, false if outside of the range of the array or should not spill
+         */
+        bool getCanSpillRegister (int reg);
+
+        /**
+         * @brief Set whether we can spill a register? Does nothing if reg would overflow the array
+         * @param reg the register we care about
+         * @param value if we should spill or not
+         * @return whether the update was successful
+         */
+        bool setCanSpillRegister (int reg, bool value);
+
+        void resetCanSpillRegisters (void)
+        {
+            for(int k = 0; k < PhysicalReg_Null; k++) {
+                canSpillRegister[k] = true;
+            }
+        }
+};
+
+#endif
diff --git a/vm/compiler/codegen/x86/lightcg/CompileTable.cpp b/vm/compiler/codegen/x86/lightcg/CompileTable.cpp
new file mode 100644
index 0000000..341f670
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/CompileTable.cpp
@@ -0,0 +1,99 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "CompileTable.h"
+#include <algorithm>
+
+void CompileTableEntry::reset (void)
+{
+    //We do not reset regNum and physicalType because those uniquely represent an entry.
+    //If we reset those we would be creating an invalid CompileTableEntry so we do not.
+
+    //Initialize size based on physical type
+    size = getRegSize (physicalType);
+
+    //Reset physical register to null
+    physicalReg = PhysicalReg_Null;
+
+    //Unknown number of references
+    refCount = 0;
+
+    //If temporary, we don't know the VR it represents
+    linkageToVR = -1;
+
+    //We have not spilled this entry so no spill index
+    spill_loc_index = -1;
+
+    //We have not written to this
+    isWritten = false;
+}
+
+bool CompileTableEntry::rememberState (int stateNum)
+{
+    RegisterState newState;
+
+    newState.physicalReg = physicalReg;
+    newState.spill_loc_index = spill_loc_index;
+
+    state[stateNum] = newState;
+    return true;
+}
+
+bool CompileTableEntry::goToState (int stateNum)
+{
+    //Look to see if we have the state requested
+    std::map<int, RegisterState>::const_iterator stateIter = state.find (stateNum);
+
+    if (stateIter == state.end ())
+    {
+        //We do not have the state and therefore we cannot go to it. Fail now.
+        return false;
+    }
+
+    //Now load data from state
+    physicalReg = state[stateNum].physicalReg;
+    spill_loc_index = state[stateNum].spill_loc_index;
+
+    return true;
+}
+
+CompileTable::iterator CompileTable::find (int regNum, int physicalType)
+{
+    CompileTableEntry lookupEntry (regNum, physicalType);
+
+    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
+}
+
+CompileTable::const_iterator CompileTable::find (int regNum, int physicalType) const
+{
+    CompileTableEntry lookupEntry (regNum, physicalType);
+
+    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
+}
+
+CompileTable::iterator CompileTable::findVirtualRegister (int regNum, LowOpndRegType physicalType)
+{
+    CompileTableEntry lookupEntry (regNum, LowOpndRegType_virtual | physicalType);
+
+    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
+}
+
+CompileTable::const_iterator CompileTable::findVirtualRegister (int regNum, LowOpndRegType physicalType) const
+{
+    CompileTableEntry lookupEntry (regNum, LowOpndRegType_virtual | physicalType);
+
+    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
+}
diff --git a/vm/compiler/codegen/x86/lightcg/CompileTable.h b/vm/compiler/codegen/x86/lightcg/CompileTable.h
new file mode 100644
index 0000000..2465b8e
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/CompileTable.h
@@ -0,0 +1,502 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef COMPILETABLE_H_
+#define COMPILETABLE_H_
+
+#include <vector>
+#include <map>
+#include "AnalysisO1.h"
+
+/**
+ * @brief Represents an entry to the compilation table, helping the compiler follow what register is where.
+ * @details The pair <regNum, physicalType> uniquely determines a variable
+ */
+class CompileTableEntry {
+public:
+    /**
+     * @brief Constructor which initializes an entry with its register number and type.
+     * @param regNum The register number: vr number, temp number, or hardcoded register number.
+     * @param physicalType the LowOpndRegType for this register. Should reflect both its physical
+     * type and its logical type.
+     */
+    CompileTableEntry (int regNum, int physicalType) :
+            regNum (regNum), physicalType (physicalType), physicalReg (PhysicalReg_Null),
+            refCount(0), spill_loc_index (-1), isWritten (false), linkageToVR (0)
+    {
+        logicalType = static_cast<LogicalRegType> (physicalType & ~MASK_FOR_TYPE);
+        size = getRegSize (physicalType);
+    }
+
+    /**
+     * @brief Constructor which initializes an entry with its register number, its logical type, and
+     * its physical type.
+     * @param regNum The register number: vr number, temp number, or hardcoded register number.
+     * @param physicalType The physical type for this register.
+     * @param logicalType The logical type for this register.
+     */
+    CompileTableEntry (int regNum, LowOpndRegType physicalType, LogicalRegType logicalType) :
+            regNum (regNum), physicalReg (PhysicalReg_Null), refCount(0), spill_loc_index (-1),
+            isWritten (false), logicalType (logicalType), linkageToVR (0)
+    {
+        this->physicalType = logicalType | physicalType;
+        size = getRegSize (physicalType);
+    }
+
+    /**
+     * @brief Constructs a compile table entry which represents a virtual register.
+     * @param vrInfo The virtual register info to use for initialization.
+     */
+    CompileTableEntry (const VirtualRegInfo &vrInfo) :
+            regNum (vrInfo.regNum), physicalType (LowOpndRegType_virtual | vrInfo.physicalType),
+            physicalReg (PhysicalReg_Null), refCount(vrInfo.refCount), spill_loc_index (-1),
+            isWritten (false), logicalType (LowOpndRegType_virtual), linkageToVR (0)
+    {
+        size = getRegSize (vrInfo.physicalType);
+    }
+
+    /**
+     * @brief Constructs a compile table entry which represents a temporary.
+     * @param tempInfo The temporary info to use for initialization.
+     */
+    CompileTableEntry (const TempRegInfo &tempInfo) :
+            regNum (tempInfo.regNum), physicalType (tempInfo.physicalType),
+            physicalReg (PhysicalReg_Null), refCount(tempInfo.refCount),
+            spill_loc_index (-1), isWritten (false), linkageToVR (tempInfo.linkageToVR)
+    {
+        logicalType = static_cast<LogicalRegType> (physicalType & ~MASK_FOR_TYPE);
+        size = getRegSize (tempInfo.physicalType);
+    }
+
+    /**
+     * @brief Destructor.
+     */
+    ~CompileTableEntry (void)
+    {
+        reset ();
+    }
+
+    /**
+     * @brief Get the register number.
+     * @return Returns the register number for this entry.
+     */
+    int getRegisterNumber (void) const
+    {
+        return regNum;
+    }
+
+    /**
+     * @brief Used to get the physical type for this entry.
+     * @details This returns only type of physical register usable for this entry.
+     * @return Returns the physical type for this entry
+     */
+    LowOpndRegType getPhysicalType (void) const
+    {
+        return static_cast<LowOpndRegType> (physicalType & MASK_FOR_TYPE);
+    }
+
+    /**
+     * @brief Used to get the logical type.
+     * @return Returns the logical type for the entry.
+     */
+    LogicalRegType getLogicalType (void) const
+    {
+        return logicalType;
+    }
+
+    /**
+     * @brief Used to get an integer whose low 3 bits represent the physical type and the high bits
+     * represent the logical type.
+     * @return Returns the representation of logical and physical types.
+     */
+    int getLogicalAndPhysicalTypes (void) const
+    {
+        //For now the physical type field holds both the logical and physical types so we return that
+        return physicalType;
+    }
+
+    /**
+     * @brief Used to get the physical register.
+     * @return Returns the physical register used for this entry. If no register is used, it returns
+     * PhysicalReg_Null.
+     */
+    PhysicalReg getPhysicalReg (void) const
+    {
+        return static_cast<PhysicalReg> (physicalReg);
+    }
+
+    /**
+     * @brief Used to get the size of this entry which depends on the physical type.
+     * @return Returns the size of the physical type for this entry.
+     */
+    OpndSize getSize (void) const
+    {
+        return size;
+    }
+
+    /**
+     * @brief Sets a new physical register for this entry.
+     * @param newReg The new physical register.
+     */
+    void setPhysicalReg (PhysicalReg newReg)
+    {
+        setPhysicalReg (static_cast<int> (newReg));
+    }
+
+    /**
+     * @brief Sets a new physical register for this entry.
+     * @param newReg The new physical register.
+     */
+    void setPhysicalReg (int newReg)
+    {
+        // It doesn't make sense to set physical register to a non-existent register.
+        // Thus we have this check here for sanity.
+        assert (newReg <= PhysicalReg_Null);
+        physicalReg = newReg;
+    }
+
+    /**
+     * @brief Updates the reference count for this entry.
+     * @param newCount The reference count to set.
+     */
+    void updateRefCount (int newCount)
+    {
+        refCount = newCount;
+    }
+
+    /**
+     * @brief Used to reset the spilled location of temporary thus marking it as non-spilled.
+     */
+    void resetSpillLocation ()
+    {
+        spill_loc_index = -1;
+    }
+
+    /**
+     * @brief Checks if entry is in a physical register.
+     * @return Returns whether this entry is in a physical register.
+     */
+    bool inPhysicalRegister (void) const
+    {
+        return physicalReg != PhysicalReg_Null;
+    }
+
+    /**
+     * @brief Checks if entry is in a general purpose register.
+     * @return Returns whether this entry is in a general purpose register.
+     */
+    bool inGeneralPurposeRegister (void) const
+    {
+        return (physicalReg >= PhysicalReg_StartOfGPMarker && physicalReg <= PhysicalReg_EndOfGPMarker);
+    }
+
+    /**
+     * @brief Checks if entry is in an xmm register.
+     * @return Returns whether this entry is in an xmm register.
+     */
+    bool inXMMRegister (void) const
+    {
+        return (physicalReg >= PhysicalReg_StartOfXmmMarker && physicalReg <= PhysicalReg_EndOfXmmMarker);
+    }
+
+    /**
+     * @brief Checks if entry is in an X87 register.
+     * @return Returns whether this entry is in an X87 register.
+     */
+    bool inX87Register (void) const
+    {
+        return (physicalReg >= PhysicalReg_StartOfX87Marker && physicalReg <= PhysicalReg_EndOfX87Marker);
+    }
+
+    /**
+     * @brief Checks whether logical type represents a virtual register.
+     * @return Returns whether this entry represent a virtual register.
+     */
+    bool isVirtualReg (void) const
+    {
+        return ((physicalType & LowOpndRegType_virtual) != 0);
+    }
+
+    /**
+     * @brief Checks if this is a backend temporary used during bytecode generation.
+     * @return Returns whether this entry represent a backend temporary.
+     */
+    bool isTemporary (void) const
+    {
+        return (isVirtualReg () == false);
+    }
+
+    /**
+     * @brief Links a temporary to a corresponding virtual register.
+     * @param vR The virtual register number.
+     */
+    void linkToVR (int vR)
+    {
+        assert (isTemporary () == true);
+        linkageToVR = vR;
+    }
+
+    /**
+     * @brief Given that the entry is a temporary, it returns the virtual register it is linked to.
+     * @return Returns corresponding virtual register.
+     */
+    int getLinkedVR (void) const
+    {
+        assert (isTemporary () == true);
+        return linkageToVR;
+    }
+
+    /**
+     * @brief Resets properties of compile entry to default values. Does not reset the type and register represented
+     * by this compile entry.
+     */
+    void reset (void);
+
+    /**
+     * @brief Equality operator for checking equivalence.
+     * @details The pair <regNum, physicalType> uniquely determines a variable.
+     * @param other The compile table entry to compare to
+     * @return Returns true if the two entries are equivalent.
+     */
+    bool operator== (const CompileTableEntry& other) const
+    {
+        if (regNum == other.regNum && physicalType == other.physicalType)
+        {
+            return true;
+        }
+        else
+        {
+            return false;
+        }
+    }
+
+    /**
+     * @brief For a given state number it remembers some properties about the compile entry.
+     * @param stateNum The state number to associate current state with.
+     * @return Returns true if it successfully remembered state.
+     */
+    bool rememberState (int stateNum);
+
+    /**
+     * @brief Updates the current state of the compile entry to match the state we are interested in.
+     * @param stateNum The state number to look at for updating self state.
+     * @return Returns true if it successfully changed to the other state.
+     */
+    bool goToState (int stateNum);
+
+    /**
+     * @brief Provides physical register for an entry for a specific state.
+     * @param stateNum The state to look at.
+     * @return Returns the physical register for that state.
+     */
+    int getStatePhysicalRegister (int stateNum)
+    {
+        return state[stateNum].physicalReg;
+    }
+
+    /**
+     * @brief Provides spill location for an entry for a specific state.
+     * @param stateNum The state to look at.
+     * @return Returns the spill location for that state.
+     */
+    int getStateSpillLocation (int stateNum)
+    {
+        return state[stateNum].spill_loc_index;
+    }
+
+    int regNum;               /**< @brief The register number */
+
+    /**
+     * @brief This field holds BOTH physical type (like XMM register) and the logical type (like virtual register)
+     * @details The low 7 bits hold LowOpndRegType and the rest of bits hold LogicalRegType
+     */
+    int physicalType;
+
+    int physicalReg;          /**< @brief Which physical register was chosen */
+
+    int refCount;             /**< @brief Number of reference counts for the entry */
+
+    int spill_loc_index;      /**< @brief what is the spill location index (for temporary registers only) */
+    bool isWritten;           /**< @brief is the entry written */
+
+private:
+    /**
+     * @brief Used to save the sate of register allocator.
+     */
+    struct RegisterState
+    {
+        int spill_loc_index;
+        int physicalReg;
+    };
+
+    /**
+     * @brief Keeps track of the register state for state number.
+     */
+    std::map<int, RegisterState> state;
+
+    LogicalRegType logicalType;  /**< @brief The logical type for this entry */
+    OpndSize size;               /**< @brief Used to keep track of size of entry */
+    int linkageToVR;             /**< @brief Linked to which VR, for temporary registers only */
+};
+
+class CompileTable
+{
+public:
+    /**
+     * @brief Used to access an element of the compile table.
+     * @details If index matches the key of an element in the container, the function returns a reference to its mapped
+     * value. If index does not match the key of any element in the container, the function inserts a new element with
+     * that key and returns a reference to its mapped value.
+     * @param index The index of the entry we want to access.
+     * @return Returns a reference to the mapped value of the element with a key value equivalent to index.
+     */
+    CompileTableEntry& operator[] (size_t index)
+    {
+        return compileTable[index];
+    }
+
+    /**
+     * @brief Used to access an element of the compile table in a constant fashion.
+     * @details If index matches the key of an element in the container, the function returns a reference to its mapped
+     * value. If index does not match the key of any element in the container, the function inserts a new element with
+     * that key and returns a reference to its mapped value.
+     * @param index The index of the entry we want to access.
+     * @return Returns a reference to the mapped value of the element with a key value equivalent to index.
+     */
+    const CompileTableEntry& operator[] (size_t index) const
+    {
+        return compileTable[index];
+    }
+
+    /**
+     * @brief Random access const iterator. This does not modify structure it is iterating.
+     */
+    typedef std::vector<CompileTableEntry>::const_iterator const_iterator;
+
+    /**
+     * @brief Random access iterator. This may modify structure it is iterating.
+     */
+    typedef std::vector<CompileTableEntry>::iterator iterator;
+
+    /**
+     * @brief Returns an iterator pointing to the first compile entry.
+     * @return iterator to beginning
+     */
+    iterator begin (void)
+    {
+        return compileTable.begin ();
+    }
+
+    /**
+     * @brief Returns a const iterator pointing to the first compile entry.
+     * @return iterator to beginning
+     */
+    const_iterator begin (void) const
+    {
+        return compileTable.begin ();
+    }
+
+    /**
+     * @brief Returns an iterator referring to the past-the-end compile entry.
+     * @return iterator past end
+     */
+    iterator end (void)
+    {
+        return compileTable.end ();
+    }
+
+    /**
+     * @brief Returns a const iterator referring to the past-the-end compile entry.
+     * @return iterator past end
+     */
+    const_iterator end (void) const
+    {
+        return compileTable.end ();
+    }
+
+    /**
+     * @brief Used to get an iterator pointing to the entry matching number and type.
+     * @param regNum The register number (can be temp, virtual, or hardcoded)
+     * @param physicalType The physical type and logical type representing the entry.
+     * @return Returns iterator pointing to the desired entry. If one is not found, it
+     * returns the past-the-end iterator.
+     */
+    iterator find (int regNum, int physicalType);
+
+    /**
+     * @brief Used to get a const iterator pointing to the entry matching number and type.
+     * @param regNum The register number (can be temp, virtual, or hardcoded)
+     * @param physicalType The physical type and logical type representing the entry.
+     * @return Returns const iterator pointing to the desired entry. If one is not found, it
+     * returns the past-the-end const iterator.
+     */
+    const_iterator find (int regNum, int physicalType) const;
+
+    /**
+     * @brief Used to get an iterator pointing to the virtual register whose physical type matches.
+     * @param regNum The virtual register number.
+     * @param physicalType The physical type of the virtual register.
+     * @return Returns iterator pointing to the desired entry. If one is not found, it
+     * returns the past-the-end iterator.
+     */
+    iterator findVirtualRegister (int regNum, LowOpndRegType physicalType);
+
+    /**
+     * @brief Used to get a const iterator pointing to the virtual register whose physical type matches.
+     * @param regNum The virtual register number.
+     * @param physicalType The physical type of the virtual register.
+     * @return Returns const iterator pointing to the desired entry. If one is not found, it
+     * returns the past-the-end const iterator.
+     */
+    const_iterator findVirtualRegister (int regNum, LowOpndRegType physicalType) const;
+
+    /**
+     * @brief Used to get size of compile table.
+     * @return Returns the size of the compile table.
+     */
+    int size (void) const
+    {
+        return compileTable.size ();
+    }
+
+    /**
+     * @brief Used to insert a new entry into the compile table.
+     * @param newEntry The compile table entry to insert into the table.
+     */
+    void insert (const CompileTableEntry &newEntry)
+    {
+        compileTable.push_back (newEntry);
+    }
+
+    /**
+     * @brief Used to clear the compile table.
+     */
+    void clear (void)
+    {
+        compileTable.clear ();
+    }
+
+private:
+    /**
+     * @brief Used to keep track of the entries in the compile table.
+     * @todo Ideally this should be a set or a map so that lookup is fast.
+     */
+    std::vector<CompileTableEntry> compileTable;
+};
+
+extern CompileTable compileTable;
+
+#endif /* COMPILETABLE_H_ */
diff --git a/vm/compiler/codegen/x86/lightcg/ExceptionHandling.cpp b/vm/compiler/codegen/x86/lightcg/ExceptionHandling.cpp
new file mode 100644
index 0000000..4c7ca3c
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/ExceptionHandling.cpp
@@ -0,0 +1,149 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @file ExceptionHandling.cpp
+ * @brief Implements interfaces and utilities used for managed exception handling.
+ */
+
+#include "Lower.h"
+#include "ExceptionHandling.h"
+#include "NcgAot.h"
+#include "Scheduler.h"
+#include "Singleton.h"
+
+ExceptionHandlingRestoreState::ExceptionHandlingRestoreState(void) :
+        uniqueStreamId(0), lastLabelGenerated(NULL) {
+    // For now, there's nothing else we need to do in constructor
+}
+
+ExceptionHandlingRestoreState::~ExceptionHandlingRestoreState(void) {
+    this->reset();
+}
+
+void ExceptionHandlingRestoreState::reset(void) {
+    this->streams.clear();
+    this->targets.clear();
+    this->lastLabelGenerated = NULL;
+    this->uniqueStreamId = 0;
+
+    // We must free the labels we inserted
+    freeShortMap();
+}
+
+char * ExceptionHandlingRestoreState::getUniqueLabel(void) {
+    // Allocate a label
+    char * label = static_cast<char *>(dvmCompilerNew(LABEL_SIZE, false));
+
+    // Give it a unique name
+    snprintf(label, LABEL_SIZE, "exception_restore_state_%d",
+            this->uniqueStreamId);
+
+    // Ensure future ids will be unique
+    this->uniqueStreamId++;
+
+    // Save label generated
+    this->lastLabelGenerated = label;
+
+    return label;
+}
+
+void ExceptionHandlingRestoreState::createExceptionHandlingStream(
+        char * beginningOfStream, char * endOfStream,
+        const char * targetLabel) {
+    // Just converting some pointers to unsigned ints to do some math.
+    unsigned int beginning = reinterpret_cast<unsigned int>(beginningOfStream);
+    unsigned int end = reinterpret_cast<unsigned int>(endOfStream);
+    size_t lengthOfTargetLabel = strlen(targetLabel);
+
+    // Developer needs to ensure that this doesn't happen
+    assert(end >= beginning);
+
+    // Calculate size of exception handling instructions
+    size_t sizeOfStream = end - beginning;
+
+    // Create the new stream using compiler arena
+    char * newStream = static_cast<char *>(dvmCompilerNew(sizeOfStream, false));
+
+    // Copy instructions to the new stream
+    memcpy(newStream, beginningOfStream, sizeOfStream);
+
+    // Reset stream pointer now
+    stream = beginningOfStream;
+
+    // Add new stream to list of exception handling stream
+    this->streams.push_back(std::make_pair(newStream, sizeOfStream));
+
+    // Create a copy of the targetLabel because we cannot assume it won't be destroyed
+    // before we use it. Ensure that it can fit the entire old label, that is all zeros
+    // on allocation, and that it has room for the terminating null.
+    char * targetLabelCopy = static_cast<char *>(dvmCompilerNew(
+            lengthOfTargetLabel + 1, true));
+
+    // Copy string to our label copy. Allocation already ensures null termination
+    strncpy(targetLabelCopy, targetLabel, lengthOfTargetLabel);
+
+    // Save the name of own label and name of target label so we know
+    // where to generate jump to
+    this->targets.push_back(std::make_pair(this->lastLabelGenerated, targetLabelCopy));
+}
+
+void ExceptionHandlingRestoreState::dumpAllExceptionHandlingRestoreState(void) {
+    // Flush scheduler queue before copying to stream
+    if (gDvmJit.scheduling)
+        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+
+    // Go through each saved restore state
+    for (unsigned int i = 0; i < this->streams.size(); ++i) {
+        size_t sizeOfExceptionRestore = this->streams[i].second;
+
+        // Ensure that we won't overfill the code cache
+        if (gDvmJit.codeCacheByteUsed + (stream - streamStart)
+                + sizeOfExceptionRestore + CODE_CACHE_PADDING
+                > gDvmJit.codeCacheSize) {
+            gDvmJit.codeCacheFull = true;
+            ALOGI("JIT_INFO: Code cache full while dumping exception handling restore state");
+            SET_JIT_ERROR(kJitErrorCodeCacheFull);
+            this->reset();
+            return;
+        }
+
+        char * label = this->targets[i].first;
+        char * targetLabel = this->targets[i].second;
+
+        // JIT verbosity
+        if (dump_x86_inst)
+            ALOGD("LOWER %s @%p", label, stream);
+
+        // Insert label exception_restore_state_# where # is the unique identifier
+        if (insertLabel(label, true) == -1) {
+            this->reset();
+            return;
+        }
+
+        // Copy to instruction stream
+        memcpy(stream, this->streams[i].first, sizeOfExceptionRestore);
+
+        // After the copy, we still need to update stream pointer
+        stream = stream + sizeOfExceptionRestore;
+
+        // Jump to the target error label
+        unconditional_jump_global_API(targetLabel, false);
+    }
+
+    // Since we dumped to code stream, we can clear out the data structures
+    this->reset();
+}
diff --git a/vm/compiler/codegen/x86/lightcg/ExceptionHandling.h b/vm/compiler/codegen/x86/lightcg/ExceptionHandling.h
new file mode 100644
index 0000000..dafc750
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/ExceptionHandling.h
@@ -0,0 +1,107 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @file ExceptionHandling.h
+ * @brief Defines interfaces and implements utilities used for managed exception handling.
+ */
+
+#include <vector>
+#include <utility> // for std::pair
+
+/**
+ * @brief Used to defer committing instructions for exception handling
+ * restore state before punting to interpreter or common exception handler
+ */
+class ExceptionHandlingRestoreState {
+private:
+    /**
+     * @brief List of streams created for exception handling restore state
+     * along with their sizes.
+     * @details This is a list of stream pointers and their corresponding sizes.
+     */
+    std::vector<std::pair<char *, size_t> > streams;
+
+    /**
+     * @brief For each exception handling stream, contains a pair of stream's
+     * name and its target.
+     */
+    std::vector<std::pair<char *, char *> > targets;
+
+    /**
+     * @brief Counter to ensure some uniqueness for labels generated.
+     */
+    unsigned int uniqueStreamId;
+
+    /**
+     * @brief Keeps track of last label generated.
+     */
+    char * lastLabelGenerated;
+
+    // Declare the copy constructor and the equal operator as private to
+    // prevent copying
+    ExceptionHandlingRestoreState(ExceptionHandlingRestoreState const&);
+    void operator=(ExceptionHandlingRestoreState const&);
+
+public:
+    /**
+     * @brief Default constructor
+     */
+    ExceptionHandlingRestoreState(void);
+
+    /**
+     * @brief Default destructor
+     */
+    ~ExceptionHandlingRestoreState(void);
+
+    /**
+     * @brief Generates a label which will be used to tag
+     * exception handling restore state.
+     * @details Does not guarantee uniqueness across instances of this
+     * class. Does not guarantee uniqueness after
+     * dumpAllExceptionHandlingRestoreState is called.
+     * @return label for exception handling restore state
+     */
+    char * getUniqueLabel(void);
+
+    /**
+     * @brief Creates stream for exception handling and copies all
+     * instructions for the restore state to this stream.
+     * @details It uses the last label generated as the label for this
+     * stream. It resets stream pointer to be beginningOfStream.
+     * @param beginningOfStream Pointer to stream before exception handling
+     * restore state was dumped.
+     * @param endOfStream Pointer to stream after exception handling
+     * restore state was dumped.
+     * @param targetLabel label of target error
+     */
+    void createExceptionHandlingStream(char * beginningOfStream,
+            char * endOfStream, const char * targetLabel);
+
+    /**
+     * @brief Copies all of the buffered exception handling restore states
+     * to the instruction stream.
+     * @details After dumping each of the exception handling restore states to the
+     * stream, it generates a jump to the error name label (which ends up punting
+     * to interpreter).
+     */
+    void dumpAllExceptionHandlingRestoreState(void);
+
+    /**
+     * @brief Resets state of instance.
+     */
+    void reset(void);
+};
diff --git a/vm/compiler/codegen/x86/lightcg/InstructionGeneration.cpp b/vm/compiler/codegen/x86/lightcg/InstructionGeneration.cpp
new file mode 100644
index 0000000..c6edaad
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/InstructionGeneration.cpp
@@ -0,0 +1,344 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "Dalvik.h"
+#include "InstructionGeneration.h"
+#include "libdex/DexOpcodes.h"
+#include "compiler/Compiler.h"
+#include "compiler/CompilerIR.h"
+#include "interp/Jit.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "NcgAot.h"
+#include "compiler/codegen/CompilerCodegen.h"
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+/**
+ * @brief Generate a Null check
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR instruction
+ */
+void genHoistedNullCheck (CompilationUnit *cUnit, MIR *mir)
+{
+    /*
+     * NOTE: these synthesized blocks don't have ssa names assigned
+     * for Dalvik registers.  However, because they dominate the following
+     * blocks we can simply use the Dalvik name w/ subscript 0 as the
+     * ssa name.
+     */
+    /* Assign array in virtual register to P_GPR_1 */
+    get_virtual_reg(mir->dalvikInsn.vA, OpndSize_32, P_GPR_1, true);
+    export_pc();
+    compare_imm_reg(OpndSize_32, 0, P_GPR_1, true);
+    condJumpToBasicBlock(stream, Condition_E, cUnit->exceptionBlockId);
+}
+
+/**
+ * @brief Generate a Bound check:
+      vA arrayReg
+      arg[0] -> determines whether it is a constant or a register
+      arg[1] -> register or constant
+
+      is idx < 0 || idx >= array.length ?
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR instruction
+ */
+void genHoistedBoundCheck (CompilationUnit *cUnit, MIR *mir)
+{
+    /* Assign array in virtual register to P_GPR_1 */
+    get_virtual_reg(mir->dalvikInsn.vA, OpndSize_32, P_GPR_1, true);
+
+    if (mir->dalvikInsn.arg[0] == MIR_BOUND_CHECK_REG)
+    {
+        //Ok let us get the index in P_GPR_2
+        get_virtual_reg(mir->dalvikInsn.arg[1], OpndSize_32, P_GPR_2, true);
+    }
+    else
+    {
+        //Move the constant to P_GPR_2
+        move_imm_to_reg(OpndSize_32, mir->dalvikInsn.arg[1], P_GPR_2, true);
+    }
+    export_pc();
+
+    //Compare array length with index value
+    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), P_GPR_1, true, P_GPR_2, true);
+    //Jump to exception block if array.length <= index
+    condJumpToBasicBlock(stream, Condition_LE, cUnit->exceptionBlockId);
+
+    //Now, compare to 0
+    compare_imm_reg(OpndSize_32, 0, P_GPR_2, true);
+    //Jump to exception if index < 0
+    condJumpToBasicBlock(stream, Condition_L, cUnit->exceptionBlockId);
+}
+
+//use O0 code generator for hoisted checks outside of the loop
+/*
+ * vA = arrayReg;
+ * vB = idxReg;
+ * vC = endConditionReg;
+ * arg[0] = maxC
+ * arg[1] = minC
+ * arg[2] = loopBranchConditionCode
+ */
+void genHoistedChecksForCountUpLoop(CompilationUnit *cUnit, MIR *mir)
+{
+    /*
+     * NOTE: these synthesized blocks don't have ssa names assigned
+     * for Dalvik registers.  However, because they dominate the following
+     * blocks we can simply use the Dalvik name w/ subscript 0 as the
+     * ssa name.
+     */
+    DecodedInstruction *dInsn = &mir->dalvikInsn;
+    const int maxC = dInsn->arg[0];
+
+    //First do the null check
+    genHoistedNullCheck (cUnit, mir);
+
+    /* assign index in virtual register to P_GPR_2 */
+    get_virtual_reg(mir->dalvikInsn.vC, OpndSize_32, P_GPR_2, true);
+
+    int delta = maxC;
+    /*
+     * If the loop end condition is ">=" instead of ">", then the largest value
+     * of the index is "endCondition - 1".
+     */
+    if (dInsn->arg[2] == OP_IF_GE) {
+        delta--;
+    }
+
+    if (delta < 0) { //+delta
+        //if P_GPR_2 is mapped to a VR, we can't do this
+        alu_binary_imm_reg(OpndSize_32, sub_opc, -delta, P_GPR_2, true);
+    } else if(delta > 0) {
+        alu_binary_imm_reg(OpndSize_32, add_opc, delta, P_GPR_2, true);
+    }
+    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), P_GPR_1, true, P_GPR_2, true);
+    condJumpToBasicBlock(stream, Condition_NC, cUnit->exceptionBlockId);
+}
+
+/*
+ * vA = arrayReg;
+ * vB = idxReg;
+ * vC = endConditionReg;
+ * arg[0] = maxC
+ * arg[1] = minC
+ * arg[2] = loopBranchConditionCode
+ */
+void genHoistedChecksForCountDownLoop(CompilationUnit *cUnit, MIR *mir)
+{
+    DecodedInstruction *dInsn = &mir->dalvikInsn;
+    const int maxC = dInsn->arg[0];
+
+    //First do the null check
+    genHoistedNullCheck (cUnit, mir);
+
+    /* assign index in virtual register to P_GPR_2 */
+    get_virtual_reg(mir->dalvikInsn.vB, OpndSize_32, P_GPR_2, true);
+
+    if (maxC < 0) {
+        //if P_GPR_2 is mapped to a VR, we can't do this
+        alu_binary_imm_reg(OpndSize_32, sub_opc, -maxC, P_GPR_2, true);
+    } else if(maxC > 0) {
+        alu_binary_imm_reg(OpndSize_32, add_opc, maxC, P_GPR_2, true);
+    }
+    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), P_GPR_1, true, P_GPR_2, true);
+    condJumpToBasicBlock(stream, Condition_NC, cUnit->exceptionBlockId);
+
+}
+
+#undef P_GPR_1
+#undef P_GPR_2
+
+/*
+ * vA = idxReg;
+ * vB = minC;
+ */
+#define P_GPR_1 PhysicalReg_ECX
+void genHoistedLowerBoundCheck(CompilationUnit *cUnit, MIR *mir)
+{
+    DecodedInstruction *dInsn = &mir->dalvikInsn;
+    const int minC = dInsn->vB;
+    get_virtual_reg(mir->dalvikInsn.vA, OpndSize_32, P_GPR_1, true); //array
+    export_pc();
+    compare_imm_reg(OpndSize_32, -minC, P_GPR_1, true);
+    condJumpToBasicBlock(stream, Condition_C, cUnit->exceptionBlockId);
+}
+#undef P_GPR_1
+
+#ifdef WITH_JIT_INLINING_PHASE2
+void genValidationForPredictedInline(CompilationUnit *cUnit, MIR *mir)
+{
+    CallsiteInfo *callsiteInfo = mir->meta.callsiteInfo;
+    if(gDvm.executionMode == kExecutionModeNcgO0) {
+        get_virtual_reg(mir->dalvikInsn.vC, OpndSize_32, PhysicalReg_EBX, true);
+        move_imm_to_reg(OpndSize_32, (int) callsiteInfo->clazz, PhysicalReg_ECX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EBX, true);
+        export_pc(); //use %edx
+        conditional_jump_global_API(, Condition_E, "common_errNullObject", false);
+        move_mem_to_reg(OpndSize_32, offObject_clazz, PhysicalReg_EBX, true, PhysicalReg_EAX, true);
+        compare_reg_reg(PhysicalReg_ECX, true, PhysicalReg_EAX, true);
+    } else {
+        get_virtual_reg(mir->dalvikInsn.vC, OpndSize_32, 5, false);
+        move_imm_to_reg(OpndSize_32, (int) callsiteInfo->clazz, 4, false);
+        nullCheck(5, false, 1, mir->dalvikInsn.vC);
+        move_mem_to_reg(OpndSize_32, offObject_clazz, 5, false, 6, false);
+        compare_reg_reg(4, false, 6, false);
+    }
+
+    //immdiate will be updated later in genLandingPadForMispredictedCallee
+    streamMisPred = stream;
+    callsiteInfo->misPredBranchOver = (LIR*)conditional_jump_int(Condition_NE, 0, OpndSize_8);
+}
+#endif
+
+/**
+ * @brief Uses heuristics to determine whether a registerize request should be satisfied.
+ * @param physicalType The backend physical type for the registerize request
+ * @return Returns true if the registerize request should be satisfied.
+ */
+static bool shouldGenerateRegisterize (LowOpndRegType physicalType)
+{
+    std::set<PhysicalReg> freeGPs, freeXMMs;
+
+    //Get the free registers available
+    findFreeRegisters (freeGPs, true, false);
+    findFreeRegisters (freeXMMs, false, true);
+
+    //If we want to registerize into a GP and we have no more, then reject this request
+    if (freeGPs.size () == 0 && physicalType == LowOpndRegType_gp)
+    {
+        return false;
+    }
+
+    //If we want to registerize into an XMM and we have no more, then reject this request
+    if (freeXMMs.size () == 0 && (physicalType == LowOpndRegType_ss || physicalType == LowOpndRegType_xmm))
+    {
+        return false;
+    }
+
+    //We accept the registerize request if we get here
+    return true;
+}
+
+bool genRegisterize (CompilationUnit *cUnit, BasicBlock_O1 *bb, MIR *mir)
+{
+    //Get the virtual register which is vA
+    int vR = mir->dalvikInsn.vA;
+
+    //Get the class from vB, it determines which instruction to use for the move
+    RegisterClass regClass = static_cast<RegisterClass> (mir->dalvikInsn.vB);
+
+    LowOpndRegType physicalType = LowOpndRegType_invalid;
+
+    // We want to figure out the mapping between the register class and the backend physical type
+    if (regClass == kCoreReg)
+    {
+        physicalType = LowOpndRegType_gp;
+    }
+    else if (regClass == kSFPReg)
+    {
+        physicalType = LowOpndRegType_ss;
+    }
+    else if (regClass == kDFPReg)
+    {
+        physicalType = LowOpndRegType_xmm;
+    }
+
+    //If we haven't determined a proper backend type, we reject this case
+    if (physicalType == LowOpndRegType_invalid)
+    {
+        ALOGI ("JIT_INFO: genRegisterize is requesting an unsupported regClass %d", regClass);
+        SET_JIT_ERROR (kJitErrorUnsupportedBytecode);
+        return false;
+    }
+
+    //We haven't registerized yet so we mark it as false for now until we actually do it
+    bool registerized = false;
+
+    //Look for this virtual register in the compile table
+    CompileTable::const_iterator vrPtr = compileTable.findVirtualRegister (vR, physicalType);
+
+    //We should already have this virtual register in the compile table because it is part of
+    //uses of this extended MIR. However, if we don't, then simply ignore the registerize request.
+    if (vrPtr != compileTable.end())
+    {
+        //Get the compile entry reference
+        const CompileTableEntry &compileEntry = *vrPtr;
+
+        //We check if it is already in physical register so we don't reload if not needed.
+        if (compileEntry.inPhysicalRegister () == false)
+        {
+            //We might want to load it in physical register so check the heuristics
+            if (shouldGenerateRegisterize (physicalType) == true)
+            {
+                //What is the size of this virtual register?
+                OpndSize size = compileEntry.getSize ();
+
+                //Define the temporary we will load into
+                const int temp = 1;
+
+                //Now we want to do the actual loading of this virtual register. We do this by using a trick
+                //to load the virtual register into a temp. And then to make sure the load happens we alias
+                //the virtual register to that temp
+                if (physicalType == LowOpndRegType_gp)
+                {
+                    get_virtual_reg (vR, size, temp, false);
+                    set_virtual_reg (vR, size, temp, false);
+                    registerized = true;
+                }
+                else if (physicalType == LowOpndRegType_ss)
+                {
+                    get_VR_ss (vR, temp, false);
+                    set_VR_ss (vR, temp, false);
+                    registerized = true;
+                }
+                else if (physicalType == LowOpndRegType_xmm)
+                {
+                    get_VR_sd (vR, temp, false);
+                    set_VR_sd (vR, temp, false);
+                    registerized = true;
+                }
+            }
+        }
+        else
+        {
+            //This is already in physical register so we mark it as having been registerized
+            registerized = true;
+        }
+    }
+
+    //If we don't satisfy this registerize request then we should make this part of the writeback requests
+    if (registerized == false)
+    {
+        //We have a wide virtual register if its backend type is xmm
+        bool isWideVr = (physicalType == LowOpndRegType_xmm);
+
+        BitVector *writebacks = bb->requestWriteBack;
+
+        //Put this VR in this block's writeback requests
+        dvmSetBit (writebacks, vR);
+
+        //If it is wide, we make sure the high VR also makes it in the writeback requests
+        if (isWideVr == true)
+        {
+            dvmSetBit (writebacks, vR + 1);
+        }
+    }
+
+    //If we get here, everything was handled
+    return true;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/InstructionGeneration.h b/vm/compiler/codegen/x86/lightcg/InstructionGeneration.h
new file mode 100644
index 0000000..da71a61
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/InstructionGeneration.h
@@ -0,0 +1,97 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef H_DALVIK_INSTRUCTIONGENERATION
+#define H_DALVIK_INSTRUCTIONGENERATION
+
+#include "Dalvik.h"
+
+//Forward declarations
+class BasicBlock_O1;
+
+/**
+ * @brief Generate a Null check
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR instruction
+ */
+void genHoistedNullCheck (CompilationUnit *cUnit, MIR *mir);
+
+/**
+ * @brief Generate a Bound check:
+      vA arrayReg
+      arg[0] -> determines whether it is a constant or a register
+      arg[1] -> register or constant
+
+      is idx < 0 || idx >= array.length ?
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR instruction
+ */
+void genHoistedBoundCheck (CompilationUnit *cUnit, MIR *mir);
+
+//use O0 code generator for hoisted checks outside of the loop
+/**
+ * @brief Generate the null and upper bound check for a count up loop
+ * vA = arrayReg;
+ * vB = idxReg;
+ * vC = endConditionReg;
+ * arg[0] = maxC
+ * arg[1] = minC
+ * arg[2] = loopBranchConditionCode
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR instruction
+ */
+void genHoistedChecksForCountUpLoop(CompilationUnit *cUnit, MIR *mir);
+
+/**
+ * @brief Generate the null and upper bound check for a count down loop
+ * vA = arrayReg;
+ * vB = idxReg;
+ * vC = endConditionReg;
+ * arg[0] = maxC
+ * arg[1] = minC
+ * arg[2] = loopBranchConditionCode
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR instruction
+ */
+void genHoistedChecksForCountDownLoop(CompilationUnit *cUnit, MIR *mir);
+
+/**
+ * @brief Generate the lower bound check
+ * vA = arrayReg;
+ * vB = minimum constant used for the array;
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR instruction
+ */
+void genHoistedLowerBoundCheck(CompilationUnit *cUnit, MIR *mir);
+
+/**
+ * @brief Generate the validation for a predicted inline
+ * vC actual class
+ * @param cUnit the CompilationUnit
+ * @param mir the MIR instruction
+ */
+void genValidationForPredictedInline(CompilationUnit *cUnit, MIR *mir);
+
+/**
+ * @brief Generate native code for the registerize extended instruction
+ * @details vA of the mir has the register to set in a physical register
+ * @param cUnit The Compilation Unit
+ * @param bb The basic block that contains the request
+ * @param mir the MIR instruction representing the registerization request
+ * @return Returns whether or not it successfully handled the request
+ */
+bool genRegisterize (CompilationUnit *cUnit, BasicBlock_O1 *bb, MIR *mir);
+#endif
diff --git a/vm/compiler/codegen/x86/lightcg/Lower.cpp b/vm/compiler/codegen/x86/lightcg/Lower.cpp
new file mode 100644
index 0000000..73d22f0
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/Lower.cpp
@@ -0,0 +1,1255 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file Lower.cpp
+    \brief This file implements the high-level wrapper for lowering
+
+*/
+
+#include "CompilationUnit.h"
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include <math.h>
+#include <sys/mman.h>
+#include "Translator.h"
+#include "Lower.h"
+#include "enc_wrapper.h"
+#include "vm/mterp/Mterp.h"
+#include "NcgHelper.h"
+#include "libdex/DexCatch.h"
+#include "compiler/CompilerIR.h"
+#if defined VTUNE_DALVIK
+#include "compiler/JitProfiling.h"
+#endif
+#include "Singleton.h"
+#include "ExceptionHandling.h"
+#include "compiler/Dataflow.h"
+
+//statistics for optimization
+int num_removed_nullCheck;
+
+PhysicalReg scratchRegs[4];
+
+LowOp* ops[BUFFER_SIZE];
+LowOp* op;
+u2* rPC; //PC pointer to bytecode
+int offsetPC/*offset in bytecode*/, offsetNCG/*byte offset in native code*/;
+int ncg_rPC;
+//! map from PC in bytecode to PC in native code
+int mapFromBCtoNCG[BYTECODE_SIZE_PER_METHOD]; //initially mapped to -1
+char* streamStart = NULL; //start of the Pure CodeItem?, not include the global symbols
+char* streamCode = NULL; //start of the Pure CodeItem?, not include the global symbols
+char* streamMethodStart; //start of the method
+char* stream; //current stream pointer
+Method* currentMethod = NULL;
+int currentExceptionBlockIdx = -1;
+BasicBlock* traceCurrentBB = NULL;
+JitMode traceMode = kJitTrace;
+CompilationUnit_O1 *gCompilationUnit;
+
+int common_invokeArgsDone(ArgsDoneType);
+
+//data section of .ia32:
+char globalData[128];
+
+char strClassCastException[] = "Ljava/lang/ClassCastException;";
+char strInstantiationError[] = "Ljava/lang/InstantiationError;";
+char strInternalError[] = "Ljava/lang/InternalError;";
+char strFilledNewArrayNotImpl[] = "filled-new-array only implemented for 'int'";
+char strArithmeticException[] = "Ljava/lang/ArithmeticException;";
+char strArrayIndexException[] = "Ljava/lang/ArrayIndexOutOfBoundsException;";
+char strArrayStoreException[] = "Ljava/lang/ArrayStoreException;";
+char strDivideByZero[] = "divide by zero";
+char strNegativeArraySizeException[] = "Ljava/lang/NegativeArraySizeException;";
+char strNoSuchMethodError[] = "Ljava/lang/NoSuchMethodError;";
+char strNullPointerException[] = "Ljava/lang/NullPointerException;";
+char strStringIndexOutOfBoundsException[] = "Ljava/lang/StringIndexOutOfBoundsException;";
+
+int LstrClassCastExceptionPtr, LstrInstantiationErrorPtr, LstrInternalError, LstrFilledNewArrayNotImpl;
+int LstrArithmeticException, LstrArrayIndexException, LstrArrayStoreException, LstrStringIndexOutOfBoundsException;
+int LstrDivideByZero, LstrNegativeArraySizeException, LstrNoSuchMethodError, LstrNullPointerException;
+int LdoubNeg, LvaluePosInfLong, LvalueNegInfLong, LvalueNanLong, LshiftMask, Lvalue64, L64bits, LintMax, LintMin;
+
+void initConstDataSec() {
+    char* tmpPtr = globalData;
+
+    LdoubNeg = (int)tmpPtr;
+    *((u4*)tmpPtr) = 0x00000000;
+    tmpPtr += sizeof(u4);
+    *((u4*)tmpPtr) = 0x80000000;
+    tmpPtr += sizeof(u4);
+
+    // 16 byte aligned
+    tmpPtr = align(tmpPtr, 16);
+    LvaluePosInfLong = (int)tmpPtr;
+    *((u4*)tmpPtr) = 0xFFFFFFFF;
+    tmpPtr += sizeof(u4);
+    *((u4*)tmpPtr) = 0x7FFFFFFF;
+    tmpPtr += sizeof(u4);
+
+    LvalueNegInfLong = (int)tmpPtr;
+    *((u4*)tmpPtr) = 0x00000000;
+    tmpPtr += sizeof(u4);
+    *((u4*)tmpPtr) = 0x80000000;
+    tmpPtr += sizeof(u4);
+
+    LvalueNanLong = (int)tmpPtr;
+    *((u4*)tmpPtr) = 0;
+    tmpPtr += sizeof(u4);
+    *((u4*)tmpPtr) = 0;
+    tmpPtr += sizeof(u4);
+
+    LshiftMask = (int)tmpPtr;
+    *((u4*)tmpPtr) = 0x3f;
+    tmpPtr += sizeof(u4);
+    *((u4*)tmpPtr) = 0;
+    tmpPtr += sizeof(u4);
+
+    Lvalue64 = (int)tmpPtr;
+    *((u4*)tmpPtr) = 0x40;
+    tmpPtr += sizeof(u4);
+    *((u4*)tmpPtr) = 0;
+    tmpPtr += sizeof(u4);
+
+    L64bits = (int)tmpPtr;
+    *((u4*)tmpPtr) = 0xFFFFFFFF;
+    tmpPtr += sizeof(u4);
+    *((u4*)tmpPtr) = 0xFFFFFFFF;
+    tmpPtr += sizeof(u4);
+
+    LintMin = (int)tmpPtr;
+    *((u4*)tmpPtr) = 0x80000000;
+    tmpPtr += sizeof(u4);
+
+    LintMax = (int)tmpPtr;
+    *((u4*)tmpPtr) = 0x7FFFFFFF;
+    tmpPtr += sizeof(u4);
+
+    LstrClassCastExceptionPtr = (int)strClassCastException;
+    LstrInstantiationErrorPtr = (int)strInstantiationError;
+    LstrInternalError = (int)strInternalError;
+    LstrFilledNewArrayNotImpl = (int)strFilledNewArrayNotImpl;
+    LstrArithmeticException = (int)strArithmeticException;
+    LstrArrayIndexException = (int)strArrayIndexException;
+    LstrArrayStoreException = (int)strArrayStoreException;
+    LstrDivideByZero = (int)strDivideByZero;
+    LstrNegativeArraySizeException = (int)strNegativeArraySizeException;
+    LstrNoSuchMethodError = (int)strNoSuchMethodError;
+    LstrNullPointerException = (int)strNullPointerException;
+    LstrStringIndexOutOfBoundsException = (int)strStringIndexOutOfBoundsException;
+}
+
+//declarations of functions used in this file
+int spill_reg(int reg, bool isPhysical);
+int unspill_reg(int reg, bool isPhysical);
+
+int const_string_resolve();
+int sget_sput_resolve();
+int new_instance_needinit();
+int new_instance_abstract();
+int invoke_virtual_resolve();
+int invoke_direct_resolve();
+int invoke_static_resolve();
+int filled_new_array_notimpl();
+int resolve_class2(
+                   int startLR/*logical register index*/, bool isPhysical, int indexReg/*const pool index*/,
+                   bool indexPhysical,
+                   int thirdArg);
+int resolve_method2(
+                    int startLR/*logical register index*/, bool isPhysical, int indexReg/*const pool index*/,
+                    bool indexPhysical,
+                    int thirdArg/*VIRTUAL*/);
+int resolve_inst_field2(
+                        int startLR/*logical register index*/, bool isPhysical,
+                        int indexReg/*const pool index*/,
+                        bool indexPhysical);
+int resolve_static_field2(
+                          int startLR/*logical register index*/, bool isPhysical,
+                          int indexReg/*const pool index*/,
+                          bool indexPhysical);
+
+int invokeMethodNoRange_1_helper();
+int invokeMethodNoRange_2_helper();
+int invokeMethodNoRange_3_helper();
+int invokeMethodNoRange_4_helper();
+int invokeMethodNoRange_5_helper();
+int invokeMethodRange_helper();
+
+int invoke_virtual_helper();
+int invoke_virtual_quick_helper();
+int invoke_static_helper();
+int invoke_direct_helper();
+int new_instance_helper();
+int sget_sput_helper(int flag);
+int aput_obj_helper();
+int aget_helper(int flag);
+int aput_helper(int flag);
+int monitor_enter_helper();
+int monitor_exit_helper();
+int throw_helper();
+int const_string_helper();
+int array_length_helper();
+int invoke_super_helper();
+int invoke_interface_helper();
+int iget_iput_helper(int flag);
+int check_cast_helper(bool instance);
+int new_array_helper();
+
+/*!
+\brief dump helper functions
+
+*/
+int performCGWorklist() {
+    int retCode = 0;
+    filled_new_array_notimpl();
+    freeShortMap();
+    const_string_resolve();
+    freeShortMap();
+
+    resolve_class2(PhysicalReg_EAX, true, PhysicalReg_EAX, true, 0);
+    freeShortMap();
+    resolve_method2(PhysicalReg_EAX, true, PhysicalReg_EAX, true, METHOD_VIRTUAL);
+    freeShortMap();
+    resolve_method2(PhysicalReg_EAX, true, PhysicalReg_EAX, true, METHOD_DIRECT);
+    freeShortMap();
+    resolve_method2(PhysicalReg_EAX, true, PhysicalReg_EAX, true, METHOD_STATIC);
+    freeShortMap();
+    resolve_inst_field2(PhysicalReg_EAX, true, PhysicalReg_EAX, true);
+    freeShortMap();
+    resolve_static_field2(PhysicalReg_EAX, true, PhysicalReg_EAX, true);
+    freeShortMap();
+    throw_exception_message(PhysicalReg_ECX, PhysicalReg_EAX, true, PhysicalReg_Null, true);
+    freeShortMap();
+    throw_exception(PhysicalReg_ECX, PhysicalReg_EAX, PhysicalReg_Null, true);
+    freeShortMap();
+    retCode = new_instance_needinit();
+    freeShortMap();
+    return retCode;
+}
+
+int aput_object_count;
+int common_periodicChecks_entry();
+int common_periodicChecks4();
+
+#if 0 /* Commented out because it is dead code. If re-enabling, this needs to be updated
+         to take MIR as parameter */
+/*!
+\brief for debugging purpose, dump the sequence of native code for each bytecode
+
+*/
+int ncgMethodFake(Method* method) {
+    //to measure code size expansion, no need to patch up labels
+    methodDataWorklist = NULL;
+    globalShortWorklist = NULL;
+    globalNCGWorklist = NULL;
+    streamMethodStart = stream;
+
+    //initialize mapFromBCtoNCG
+    memset(&mapFromBCtoNCG[0], -1, BYTECODE_SIZE_PER_METHOD * sizeof(mapFromBCtoNCG[0]));
+    unsigned int i;
+    u2* rStart = (u2*)malloc(5*sizeof(u2));
+    if(rStart == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at ncgMethodFake");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    rPC = rStart;
+    method->insns = rStart;
+    for(i = 0; i < 5; i++) *rPC++ = 0;
+    for(i = 0; i < 256; i++) {
+        rPC = rStart;
+        //modify the opcode
+        char* tmp = (char*)rStart;
+        *tmp++ = i;
+        *tmp = i;
+        inst = FETCH(0);
+        char* tmpStart = stream;
+        lowerByteCodeCanThrowCheck(method); //use inst, rPC, method, modify rPC
+        int size_in_u2 = rPC - rStart;
+        if(stream - tmpStart  > 0)
+            ALOGI("LOWER bytecode %x size in u2: %d ncg size in byte: %d", i, size_in_u2, stream - tmpStart);
+    }
+    exit(0);
+}
+#endif
+
+bool existATryBlock(Method* method, int startPC, int endPC) {
+    const DexCode* pCode = dvmGetMethodCode(method);
+    u4 triesSize = pCode->triesSize;
+    const DexTry* pTries = dexGetTries(pCode);
+    unsigned int i;
+    for (i = 0; i < triesSize; i++) {
+        const DexTry* pTry = &pTries[i];
+        u4 start = pTry->startAddr; //offsetPC
+        u4 end = start + pTry->insnCount;
+        //if [start, end] overlaps with [startPC, endPC] returns true
+        if((int)end < startPC || (int)start > endPC) { //no overlap
+        } else {
+            return true;
+        }
+    }
+    return false;
+}
+
+int mm_bytecode_size = 0;
+int mm_ncg_size = 0;
+int mm_relocation_size = 0;
+int mm_map_size = 0;
+void resetCodeSize() {
+    mm_bytecode_size = 0;
+    mm_ncg_size = 0;
+    mm_relocation_size = 0;
+    mm_map_size = 0;
+}
+
+bool bytecodeIsRemoved(const Method* method, u4 bytecodeOffset) {
+    if(gDvm.executionMode == kExecutionModeNcgO0) return false;
+    u4 ncgOff = mapFromBCtoNCG[bytecodeOffset];
+    int k = bytecodeOffset+1;
+    u2 insnsSize = dvmGetMethodInsnsSize(method);
+    while(k < insnsSize) {
+        if(mapFromBCtoNCG[k] < 0) {
+            k++;
+            continue;
+        }
+        if(mapFromBCtoNCG[k] == (int)ncgOff) return true;
+        return false;
+    }
+    return false;
+}
+
+int invoke_super_nsm();
+void init_common(const char* curFileName, DvmDex *pDvmDex, bool forNCG); //forward declaration
+void initGlobalMethods(); //forward declaration
+
+//called once when compiler thread starts up
+void initJIT(const char* curFileName, DvmDex *pDvmDex) {
+    init_common(curFileName, pDvmDex, false);
+}
+
+void init_common(const char* curFileName, DvmDex *pDvmDex, bool forNCG) {
+    if(!gDvm.constInit) {
+        globalMapNum = 0;
+        globalMap = NULL;
+        initConstDataSec();
+        gDvm.constInit = true;
+    }
+
+    //for initJIT: stream is already set
+    if(!gDvm.commonInit) {
+        initGlobalMethods();
+        gDvm.commonInit = true;
+    }
+}
+
+void initGlobalMethods() {
+    bool old_dump_x86_inst = dump_x86_inst;
+    bool old_scheduling = gDvmJit.scheduling;
+    dump_x86_inst = false; // Enable this to debug common section
+
+    //! \warning Scheduling should be turned off when creating common section
+    //! because it relies on the fact the register allocation has already been
+    //! done (either via register allocator or via hardcoded registers). But,
+    //! when we get to this point, the execution mode is Jit instead of either
+    //! NcgO1 or NcgO0, which leads to the unintended consequence that NcgO0
+    //! path is taken, but logical registers are used instead of physical
+    //! registers and thus relies on encoder to do the mapping, which the
+    //! scheduler cannot predict for dependency graph creation.
+    //! \todo The reason "logical" registers are used is because variable
+    //! isScratchPhysical is set to false even when a physical register is
+    //! used. This issue should be addressed at some point.
+    gDvmJit.scheduling = false;
+
+    // generate native code for function ncgGetEIP
+    if (insertLabel("ncgGetEIP", false) == -1)
+        return;
+    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EDX, true);
+    x86_return();
+
+    //generate code for common labels
+    //jumps within a helper function is treated as short labels
+    globalShortMap = NULL;
+    common_periodicChecks_entry();
+    freeShortMap();
+    common_periodicChecks4();
+    freeShortMap();
+
+    if(dump_x86_inst)
+        ALOGI("ArgsDone_Normal start");
+    common_invokeArgsDone(ArgsDone_Normal);
+    freeShortMap();
+    if(dump_x86_inst)
+        ALOGI("ArgsDone_Native start");
+    common_invokeArgsDone(ArgsDone_Native);
+    freeShortMap();
+    if(dump_x86_inst)
+        ALOGI("ArgsDone_Full start");
+    common_invokeArgsDone(ArgsDone_Full);
+    if(dump_x86_inst)
+        ALOGI("ArgsDone_Full end");
+    freeShortMap();
+
+    common_backwardBranch();
+    freeShortMap();
+    common_exceptionThrown();
+    freeShortMap();
+    common_errNullObject();
+    freeShortMap();
+    common_errArrayIndex();
+    freeShortMap();
+    common_errArrayStore();
+    freeShortMap();
+    common_errNegArraySize();
+    freeShortMap();
+    common_errNoSuchMethod();
+    freeShortMap();
+    common_errDivideByZero();
+    freeShortMap();
+    common_gotoBail();
+    freeShortMap();
+    common_gotoBail_0();
+    freeShortMap();
+    invoke_super_nsm();
+    freeShortMap();
+
+    performCGWorklist(); //generate code for helper functions
+    performLabelWorklist(); //it is likely that the common labels will jump to other common labels
+
+    gDvmJit.scheduling = old_scheduling;
+    dump_x86_inst = old_dump_x86_inst;
+}
+
+ExecutionMode origMode;
+
+/**
+ * @brief Lowers bytecode to native code
+ * @param method parent method of trace
+ * @param mir bytecode representation
+ * @param dalvikPC the program counter of the instruction
+ * @return true when NOT handled and false when it IS handled
+ */
+bool lowerByteCodeJit(const Method* method, const MIR * mir, const u2 * dalvikPC) {
+    int retCode = lowerByteCodeCanThrowCheck(method, mir, dalvikPC);
+    freeShortMap();
+    if(retCode >= 0) return false; //handled
+    return true; //not handled
+}
+
+void startOfBasicBlock(BasicBlock* bb) {
+    traceCurrentBB = bb;
+    if(gDvm.executionMode == kExecutionModeNcgO0) {
+        isScratchPhysical = true;
+    } else {
+        isScratchPhysical = false;
+    }
+}
+
+void startOfTrace(const Method* method, int exceptionBlockId,
+                  CompilationUnit_O1 *cUnit) {
+
+    //Set the global compilation
+    gCompilationUnit = cUnit;
+
+    origMode = gDvm.executionMode;
+    gDvm.executionMode = kExecutionModeNcgO1;
+    if(gDvm.executionMode == kExecutionModeNcgO0) {
+        isScratchPhysical = true;
+    } else {
+        isScratchPhysical = false;
+    }
+    currentMethod = (Method*)method;
+    currentExceptionBlockIdx = exceptionBlockId;
+    methodDataWorklist = NULL;
+    globalShortWorklist = NULL;
+    globalNCGWorklist = NULL;
+    singletonPtr<ExceptionHandlingRestoreState>()->reset();
+
+    streamMethodStart = stream;
+    //initialize mapFromBCtoNCG
+    memset(&mapFromBCtoNCG[0], -1, BYTECODE_SIZE_PER_METHOD * sizeof(mapFromBCtoNCG[0]));
+    if(gDvm.executionMode == kExecutionModeNcgO1)
+        startOfTraceO1(method, exceptionBlockId, cUnit);
+}
+
+/**
+ * @brief Used to free the data structures in basic blocks that were used by backend
+ * @param basicBlocks The list of all basic blocks in current cUnit
+ */
+static void freeCFG (GrowableList &basicBlocks)
+{
+    //Create and initialize the basic block iterator
+    GrowableListIterator iter;
+    dvmGrowableListIteratorInit (&basicBlocks, &iter);
+
+    //Get the first basic block provided by iterator
+    BasicBlock_O1 *bb = reinterpret_cast<BasicBlock_O1 *> (dvmGrowableListIteratorNext (&iter));
+
+    while (bb != 0)
+    {
+        //Call the BasicBlock_O1 clear function
+        bb->freeIt ();
+
+        //We want to move on to next basic block
+        bb = reinterpret_cast<BasicBlock_O1 *> (dvmGrowableListIteratorNext (&iter));
+    }
+}
+
+void performWorklistWork (void)
+{
+    performLabelWorklist ();
+    performNCGWorklist (); //handle forward jump (GOTO, IF)
+    performDataWorklist (); //handle SWITCH & FILL_ARRAY_DATA
+    performChainingWorklist ();
+}
+
+void endOfTrace (CompilationUnit *cUnit) {
+    freeLabelWorklist ();
+    freeNCGWorklist ();
+    freeDataWorklist ();
+    freeChainingWorklist ();
+
+    //Now we want to free anything in BasicBlock that we used during backend but was not
+    //allocated using the arena
+    freeCFG (cUnit->blockList);
+
+    //Restore the execution mode as the ME expects it
+    gDvm.executionMode = origMode;
+
+    //Reset the global compilation unit
+    gCompilationUnit = 0;
+}
+
+int lowerByteCodeCanThrowCheck(const Method* method, const MIR * mir, const u2 * dalvikPC) {
+    bool delay_requested = false;
+
+    int flags = dvmCompilerGetOpcodeFlags (mir->dalvikInsn.opcode);
+
+    // Delay free VRs if we potentially can exit to interpreter
+    // We do not activate delay if VRs state is not changed
+    if ((flags & kInstrCanThrow) != 0)
+    {
+        int dfAttributes = dvmCompilerDataFlowAttributes[mir->dalvikInsn.opcode];
+
+        if ( (dfAttributes & DF_IS_CALL) == 0) { // Not applicable to calls
+            int mirOptFlags = mir->OptimizationFlags;
+
+            // Avoid delay if we null/range check optimized
+            if ( (dfAttributes & DF_HAS_NR_CHECKS) != 0 ) {
+                // Both null check and range check applicable
+
+                if( (mirOptFlags & MIR_IGNORE_NULL_CHECK) == 0 ) {
+                    // Null check is not optimized, request delay
+                    if(requestVRFreeDelayAll(VRDELAY_CAN_THROW) == true) {
+                        delay_requested = true;
+                    }
+                }
+
+                if( (mirOptFlags & MIR_IGNORE_RANGE_CHECK) == 0 ) {
+                    // Range check is not optimized, put additional request delay
+                    if(requestVRFreeDelayAll(VRDELAY_CAN_THROW) == true) {
+                        delay_requested = true;
+                    }
+                }
+            } else if ( (dfAttributes & DF_HAS_OBJECT_CHECKS) != 0 ) {
+                // Only null check applicable to opcode
+
+                if( (mirOptFlags & MIR_IGNORE_NULL_CHECK) == 0 ) {
+                    // Null check is not optimized, request delay
+                    if(requestVRFreeDelayAll(VRDELAY_CAN_THROW) == true) {
+                        delay_requested = true;
+                    }
+                }
+            } else {
+                // Can exit to interpreter but have no null/range checks
+                if(requestVRFreeDelayAll(VRDELAY_CAN_THROW) == true) {
+                    delay_requested = true;
+                }
+            }
+        }
+    }
+
+    int retCode = lowerByteCode(method, mir, dalvikPC);
+
+    if(delay_requested == true) {
+        bool state_changed = cancelVRFreeDelayRequestAll(VRDELAY_CAN_THROW);
+        if(state_changed==true) {
+            // Not optimized case (delay was not canceled inside bytecode generation)
+            ALOGI("JIT_INFO: VRDELAY_CAN_THROW cancel was not optimized for bytecode=0x%x",
+                  mir->dalvikInsn.opcode);
+            // Release all remaining VRDELAY_CAN_THROW requests
+            do {
+                state_changed = cancelVRFreeDelayRequestAll(VRDELAY_CAN_THROW);
+            } while (state_changed == true);
+        }
+    }
+    return retCode;
+}
+
+/**
+ * @brief Generates native code for the bytecode.
+ * @details May update code stream.
+ * @param method parent method of trace
+ * @param mir bytecode representation
+ * @param dalvikPC the program counter of the instruction
+ * @return 0 or greater when handled
+ */
+int lowerByteCode(const Method* method, const MIR * mir, const u2 * dalvikPC) {
+    /* offsetPC is used in O1 code generator, where it is defined as the sequence number
+       use a local version to avoid overwriting */
+    int offsetPC = mir->offset; //! \warning When doing method inlining, offsetPC
+                                //! will be the same for the invoke and the inlined
+                                //! bytecode. This WILL break mapping from BC to NCG
+                                //! if more than one bytecode is inlined.
+
+    if (dump_x86_inst == true)
+    {
+        const int maxDecodedLen = 256;
+        char decodedString[maxDecodedLen];
+
+        //We want to decode the current instruction but we pass a null cUnit because we don't
+        //care to have any ssa information printed.
+        dvmCompilerExtendedDisassembler (0, mir, &(mir->dalvikInsn), decodedString, maxDecodedLen);
+
+        ALOGI ("LOWER %s with offsetPC %x offsetNCG %x @%p\n", decodedString, offsetPC,
+                stream - streamMethodStart, stream);
+    }
+
+    //update mapFromBCtoNCG
+    offsetNCG = stream - streamMethodStart;
+    if(offsetPC >= BYTECODE_SIZE_PER_METHOD) {
+        ALOGI("JIT_INFO: offsetPC %d exceeds BYTECODE_SIZE_PER_METHOD", offsetPC);
+        SET_JIT_ERROR(kJitErrorTraceFormation);
+        return -1;
+    }
+    mapFromBCtoNCG[offsetPC] = offsetNCG;
+#if defined(ENABLE_TRACING) && defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC, mapFromBCtoNCG[offsetPC], 1);
+#endif
+    //return number of LowOps generated
+    switch (mir->dalvikInsn.opcode) {
+    case OP_NOP:
+        return op_nop(mir);
+    case OP_MOVE:
+    case OP_MOVE_OBJECT:
+        return op_move(mir);
+    case OP_MOVE_FROM16:
+    case OP_MOVE_OBJECT_FROM16:
+        return op_move_from16(mir);
+    case OP_MOVE_16:
+    case OP_MOVE_OBJECT_16:
+        return op_move_16(mir);
+    case OP_MOVE_WIDE:
+        return op_move_wide(mir);
+    case OP_MOVE_WIDE_FROM16:
+        return op_move_wide_from16(mir);
+    case OP_MOVE_WIDE_16:
+        return op_move_wide_16(mir);
+    case OP_MOVE_RESULT:
+    case OP_MOVE_RESULT_OBJECT:
+        return op_move_result(mir);
+    case OP_MOVE_RESULT_WIDE:
+        return op_move_result_wide(mir);
+    case OP_MOVE_EXCEPTION:
+        return op_move_exception(mir);
+    case OP_RETURN_VOID:
+    case OP_RETURN_VOID_BARRIER:
+        return op_return_void(mir);
+    case OP_RETURN:
+    case OP_RETURN_OBJECT:
+        return op_return(mir);
+    case OP_RETURN_WIDE:
+        return op_return_wide(mir);
+    case OP_CONST_4:
+        return op_const_4(mir);
+    case OP_CONST_16:
+        return op_const_16(mir);
+    case OP_CONST:
+        return op_const(mir);
+    case OP_CONST_HIGH16:
+        return op_const_high16(mir);
+    case OP_CONST_WIDE_16:
+        return op_const_wide_16(mir);
+    case OP_CONST_WIDE_32:
+        return op_const_wide_32(mir);
+    case OP_CONST_WIDE:
+        return op_const_wide(mir);
+    case OP_CONST_WIDE_HIGH16:
+        return op_const_wide_high16(mir);
+    case OP_CONST_STRING:
+        return op_const_string(mir);
+    case OP_CONST_STRING_JUMBO:
+        return op_const_string_jumbo(mir);
+    case OP_CONST_CLASS:
+        return op_const_class(mir);
+    case OP_MONITOR_ENTER:
+        return op_monitor_enter(mir);
+    case OP_MONITOR_EXIT:
+        return op_monitor_exit(mir);
+    case OP_CHECK_CAST:
+        return op_check_cast(mir);
+    case OP_INSTANCE_OF:
+        return op_instance_of(mir);
+    case OP_ARRAY_LENGTH:
+        return op_array_length(mir);
+    case OP_NEW_INSTANCE:
+        return op_new_instance(mir);
+    case OP_NEW_ARRAY:
+        return op_new_array(mir);
+    case OP_FILLED_NEW_ARRAY:
+        return op_filled_new_array(mir);
+    case OP_FILLED_NEW_ARRAY_RANGE:
+        return op_filled_new_array_range(mir);
+    case OP_FILL_ARRAY_DATA:
+        return op_fill_array_data(mir, dalvikPC);
+    case OP_THROW:
+        return op_throw(mir);
+    case OP_THROW_VERIFICATION_ERROR:
+        return op_throw_verification_error(mir);
+    case OP_GOTO:
+        return op_goto(mir);
+    case OP_GOTO_16:
+        return op_goto_16(mir);
+    case OP_GOTO_32:
+        return op_goto_32(mir);
+    case OP_PACKED_SWITCH:
+        return op_packed_switch(mir, dalvikPC);
+    case OP_SPARSE_SWITCH:
+        return op_sparse_switch(mir, dalvikPC);
+    case OP_CMPL_FLOAT:
+        return op_cmpl_float(mir);
+    case OP_CMPG_FLOAT:
+        return op_cmpg_float(mir);
+    case OP_CMPL_DOUBLE:
+        return op_cmpl_double(mir);
+    case OP_CMPG_DOUBLE:
+        return op_cmpg_double(mir);
+    case OP_CMP_LONG:
+        return op_cmp_long(mir);
+    case OP_IF_EQ:
+        return op_if_eq(mir);
+    case OP_IF_NE:
+        return op_if_ne(mir);
+    case OP_IF_LT:
+        return op_if_lt(mir);
+    case OP_IF_GE:
+        return op_if_ge(mir);
+    case OP_IF_GT:
+        return op_if_gt(mir);
+    case OP_IF_LE:
+        return op_if_le(mir);
+    case OP_IF_EQZ:
+        return op_if_eqz(mir);
+    case OP_IF_NEZ:
+        return op_if_nez(mir);
+    case OP_IF_LTZ:
+        return op_if_ltz(mir);
+    case OP_IF_GEZ:
+        return op_if_gez(mir);
+    case OP_IF_GTZ:
+        return op_if_gtz(mir);
+    case OP_IF_LEZ:
+        return op_if_lez(mir);
+    case OP_AGET:
+        return op_aget(mir);
+    case OP_AGET_WIDE:
+        return op_aget_wide(mir);
+    case OP_AGET_OBJECT:
+        return op_aget_object(mir);
+    case OP_AGET_BOOLEAN:
+        return op_aget_boolean(mir);
+    case OP_AGET_BYTE:
+        return op_aget_byte(mir);
+    case OP_AGET_CHAR:
+        return op_aget_char(mir);
+    case OP_AGET_SHORT:
+        return op_aget_short(mir);
+    case OP_APUT:
+        return op_aput(mir);
+    case OP_APUT_WIDE:
+        return op_aput_wide(mir);
+    case OP_APUT_OBJECT:
+        return op_aput_object(mir);
+    case OP_APUT_BOOLEAN:
+        return op_aput_boolean(mir);
+    case OP_APUT_BYTE:
+        return op_aput_byte(mir);
+    case OP_APUT_CHAR:
+        return op_aput_char(mir);
+    case OP_APUT_SHORT:
+        return op_aput_short(mir);
+    case OP_IGET:
+    case OP_IGET_VOLATILE:
+        return op_iget(mir);
+    case OP_IGET_WIDE:
+        return op_iget_wide(mir, false /*isVolatile*/);
+    case OP_IGET_WIDE_VOLATILE:
+        return op_iget_wide(mir, true /*isVolatile*/);
+    case OP_IGET_OBJECT:
+    case OP_IGET_OBJECT_VOLATILE:
+        return op_iget_object(mir);
+    case OP_IGET_BOOLEAN:
+        return op_iget_boolean(mir);
+    case OP_IGET_BYTE:
+        return op_iget_byte(mir);
+    case OP_IGET_CHAR:
+        return op_iget_char(mir);
+    case OP_IGET_SHORT:
+        return op_iget_short(mir);
+    case OP_IPUT:
+    case OP_IPUT_VOLATILE:
+        return op_iput(mir);
+    case OP_IPUT_WIDE:
+        return op_iput_wide(mir, false /*isVolatile*/);
+    case OP_IPUT_WIDE_VOLATILE:
+        return op_iput_wide(mir, true /*isVolatile*/);
+    case OP_IPUT_OBJECT:
+    case OP_IPUT_OBJECT_VOLATILE:
+        return op_iput_object(mir);
+    case OP_IPUT_BOOLEAN:
+        return op_iput_boolean(mir);
+    case OP_IPUT_BYTE:
+        return op_iput_byte(mir);
+    case OP_IPUT_CHAR:
+        return op_iput_char(mir);
+    case OP_IPUT_SHORT:
+        return op_iput_short(mir);
+    case OP_SGET:
+    case OP_SGET_VOLATILE:
+        return op_sget(mir);
+    case OP_SGET_WIDE:
+        return op_sget_wide(mir, false /*isVolatile*/);
+    case OP_SGET_WIDE_VOLATILE:
+        return op_sget_wide(mir, true /*isVolatile*/);
+    case OP_SGET_OBJECT:
+    case OP_SGET_OBJECT_VOLATILE:
+        return op_sget_object(mir);
+    case OP_SGET_BOOLEAN:
+        return op_sget_boolean(mir);
+    case OP_SGET_BYTE:
+        return op_sget_byte(mir);
+    case OP_SGET_CHAR:
+        return op_sget_char(mir);
+    case OP_SGET_SHORT:
+        return op_sget_short(mir);
+    case OP_SPUT:
+    case OP_SPUT_VOLATILE:
+        return op_sput(mir, false /*isObj*/);
+    case OP_SPUT_WIDE:
+        return op_sput_wide(mir, false /*isVolatile*/);
+    case OP_SPUT_WIDE_VOLATILE:
+        return op_sput_wide(mir, true /*isVolatile*/);
+    case OP_SPUT_OBJECT:
+    case OP_SPUT_OBJECT_VOLATILE:
+        return op_sput_object(mir);
+    case OP_SPUT_BOOLEAN:
+        return op_sput_boolean(mir);
+    case OP_SPUT_BYTE:
+        return op_sput_byte(mir);
+    case OP_SPUT_CHAR:
+        return op_sput_char(mir);
+    case OP_SPUT_SHORT:
+        return op_sput_short(mir);
+    case OP_INVOKE_VIRTUAL:
+        return op_invoke_virtual(mir);
+    case OP_INVOKE_SUPER:
+        return op_invoke_super(mir);
+    case OP_INVOKE_DIRECT:
+        return op_invoke_direct(mir);
+    case OP_INVOKE_STATIC:
+        return op_invoke_static(mir);
+    case OP_INVOKE_INTERFACE:
+        return op_invoke_interface(mir);
+    case OP_INVOKE_VIRTUAL_RANGE:
+        return op_invoke_virtual_range(mir);
+    case OP_INVOKE_SUPER_RANGE:
+        return op_invoke_super_range(mir);
+    case OP_INVOKE_DIRECT_RANGE:
+        return op_invoke_direct_range(mir);
+    case OP_INVOKE_STATIC_RANGE:
+        return op_invoke_static_range(mir);
+    case OP_INVOKE_INTERFACE_RANGE:
+        return op_invoke_interface_range(mir);
+    case OP_NEG_INT:
+        return op_neg_int(mir);
+    case OP_NOT_INT:
+        return op_not_int(mir);
+    case OP_NEG_LONG:
+        return op_neg_long(mir);
+    case OP_NOT_LONG:
+        return op_not_long(mir);
+    case OP_NEG_FLOAT:
+        return op_neg_float(mir);
+    case OP_NEG_DOUBLE:
+        return op_neg_double(mir);
+    case OP_INT_TO_LONG:
+        return op_int_to_long(mir);
+    case OP_INT_TO_FLOAT:
+        return op_int_to_float(mir);
+    case OP_INT_TO_DOUBLE:
+        return op_int_to_double(mir);
+    case OP_LONG_TO_INT:
+        return op_long_to_int(mir);
+    case OP_LONG_TO_FLOAT:
+        return op_long_to_float(mir);
+    case OP_LONG_TO_DOUBLE:
+        return op_long_to_double(mir);
+    case OP_FLOAT_TO_INT:
+        return op_float_to_int(mir);
+    case OP_FLOAT_TO_LONG:
+        return op_float_to_long(mir);
+    case OP_FLOAT_TO_DOUBLE:
+        return op_float_to_double(mir);
+    case OP_DOUBLE_TO_INT:
+        return op_double_to_int(mir);
+    case OP_DOUBLE_TO_LONG:
+        return op_double_to_long(mir);
+    case OP_DOUBLE_TO_FLOAT:
+        return op_double_to_float(mir);
+    case OP_INT_TO_BYTE:
+        return op_int_to_byte(mir);
+    case OP_INT_TO_CHAR:
+        return op_int_to_char(mir);
+    case OP_INT_TO_SHORT:
+        return op_int_to_short(mir);
+    case OP_ADD_INT:
+        return op_add_int(mir);
+    case OP_SUB_INT:
+        return op_sub_int(mir);
+    case OP_MUL_INT:
+        return op_mul_int(mir);
+    case OP_DIV_INT:
+        return op_div_int(mir);
+    case OP_REM_INT:
+        return op_rem_int(mir);
+    case OP_AND_INT:
+        return op_and_int(mir);
+    case OP_OR_INT:
+        return op_or_int(mir);
+    case OP_XOR_INT:
+        return op_xor_int(mir);
+    case OP_SHL_INT:
+        return op_shl_int(mir);
+    case OP_SHR_INT:
+        return op_shr_int(mir);
+    case OP_USHR_INT:
+        return op_ushr_int(mir);
+    case OP_ADD_LONG:
+        return op_add_long(mir);
+    case OP_SUB_LONG:
+        return op_sub_long(mir);
+    case OP_MUL_LONG:
+        return op_mul_long(mir);
+    case OP_DIV_LONG:
+        return op_div_long(mir);
+    case OP_REM_LONG:
+        return op_rem_long(mir);
+    case OP_AND_LONG:
+        return op_and_long(mir);
+    case OP_OR_LONG:
+        return op_or_long(mir);
+    case OP_XOR_LONG:
+        return op_xor_long(mir);
+    case OP_SHL_LONG:
+        return op_shl_long(mir);
+    case OP_SHR_LONG:
+        return op_shr_long(mir);
+    case OP_USHR_LONG:
+        return op_ushr_long(mir);
+    case OP_ADD_FLOAT:
+        return op_add_float(mir);
+    case OP_SUB_FLOAT:
+        return op_sub_float(mir);
+    case OP_MUL_FLOAT:
+        return op_mul_float(mir);
+    case OP_DIV_FLOAT:
+        return op_div_float(mir);
+    case OP_REM_FLOAT:
+        return op_rem_float(mir);
+    case OP_ADD_DOUBLE:
+        return op_add_double(mir);
+    case OP_SUB_DOUBLE:
+        return op_sub_double(mir);
+    case OP_MUL_DOUBLE:
+        return op_mul_double(mir);
+    case OP_DIV_DOUBLE:
+        return op_div_double(mir);
+    case OP_REM_DOUBLE:
+        return op_rem_double(mir);
+    case OP_ADD_INT_2ADDR:
+        return op_add_int_2addr(mir);
+    case OP_SUB_INT_2ADDR:
+        return op_sub_int_2addr(mir);
+    case OP_MUL_INT_2ADDR:
+        return op_mul_int_2addr(mir);
+    case OP_DIV_INT_2ADDR:
+        return op_div_int_2addr(mir);
+    case OP_REM_INT_2ADDR:
+        return op_rem_int_2addr(mir);
+    case OP_AND_INT_2ADDR:
+        return op_and_int_2addr(mir);
+    case OP_OR_INT_2ADDR:
+        return op_or_int_2addr(mir);
+    case OP_XOR_INT_2ADDR:
+        return op_xor_int_2addr(mir);
+    case OP_SHL_INT_2ADDR:
+        return op_shl_int_2addr(mir);
+    case OP_SHR_INT_2ADDR:
+        return op_shr_int_2addr(mir);
+    case OP_USHR_INT_2ADDR:
+        return op_ushr_int_2addr(mir);
+    case OP_ADD_LONG_2ADDR:
+        return op_add_long_2addr(mir);
+    case OP_SUB_LONG_2ADDR:
+        return op_sub_long_2addr(mir);
+    case OP_MUL_LONG_2ADDR:
+        return op_mul_long_2addr(mir);
+    case OP_DIV_LONG_2ADDR:
+        return op_div_long_2addr(mir);
+    case OP_REM_LONG_2ADDR:
+        return op_rem_long_2addr(mir);
+    case OP_AND_LONG_2ADDR:
+        return op_and_long_2addr(mir);
+    case OP_OR_LONG_2ADDR:
+        return op_or_long_2addr(mir);
+    case OP_XOR_LONG_2ADDR:
+        return op_xor_long_2addr(mir);
+    case OP_SHL_LONG_2ADDR:
+        return op_shl_long_2addr(mir);
+    case OP_SHR_LONG_2ADDR:
+        return op_shr_long_2addr(mir);
+    case OP_USHR_LONG_2ADDR:
+        return op_ushr_long_2addr(mir);
+    case OP_ADD_FLOAT_2ADDR:
+        return op_add_float_2addr(mir);
+    case OP_SUB_FLOAT_2ADDR:
+        return op_sub_float_2addr(mir);
+    case OP_MUL_FLOAT_2ADDR:
+        return op_mul_float_2addr(mir);
+    case OP_DIV_FLOAT_2ADDR:
+        return op_div_float_2addr(mir);
+    case OP_REM_FLOAT_2ADDR:
+        return op_rem_float_2addr(mir);
+    case OP_ADD_DOUBLE_2ADDR:
+        return op_add_double_2addr(mir);
+    case OP_SUB_DOUBLE_2ADDR:
+        return op_sub_double_2addr(mir);
+    case OP_MUL_DOUBLE_2ADDR:
+        return op_mul_double_2addr(mir);
+    case OP_DIV_DOUBLE_2ADDR:
+        return op_div_double_2addr(mir);
+    case OP_REM_DOUBLE_2ADDR:
+        return op_rem_double_2addr(mir);
+    case OP_ADD_INT_LIT16:
+        return op_add_int_lit16(mir);
+    case OP_RSUB_INT:
+        return op_rsub_int(mir);
+    case OP_MUL_INT_LIT16:
+        return op_mul_int_lit16(mir);
+    case OP_DIV_INT_LIT16:
+        return op_div_int_lit16(mir);
+    case OP_REM_INT_LIT16:
+        return op_rem_int_lit16(mir);
+    case OP_AND_INT_LIT16:
+        return op_and_int_lit16(mir);
+    case OP_OR_INT_LIT16:
+        return op_or_int_lit16(mir);
+    case OP_XOR_INT_LIT16:
+        return op_xor_int_lit16(mir);
+    case OP_ADD_INT_LIT8:
+        return op_add_int_lit8(mir);
+    case OP_RSUB_INT_LIT8:
+        return op_rsub_int_lit8(mir);
+    case OP_MUL_INT_LIT8:
+        return op_mul_int_lit8(mir);
+    case OP_DIV_INT_LIT8:
+        return op_div_int_lit8(mir);
+    case OP_REM_INT_LIT8:
+        return op_rem_int_lit8(mir);
+    case OP_AND_INT_LIT8:
+        return op_and_int_lit8(mir);
+    case OP_OR_INT_LIT8:
+        return op_or_int_lit8(mir);
+    case OP_XOR_INT_LIT8:
+        return op_xor_int_lit8(mir);
+    case OP_SHL_INT_LIT8:
+        return op_shl_int_lit8(mir);
+    case OP_SHR_INT_LIT8:
+        return op_shr_int_lit8(mir);
+    case OP_USHR_INT_LIT8:
+        return op_ushr_int_lit8(mir);
+    case OP_EXECUTE_INLINE:
+        return op_execute_inline(mir, false /*isRange*/);
+    case OP_EXECUTE_INLINE_RANGE:
+        return op_execute_inline(mir, true /*isRange*/);
+//  case OP_INVOKE_OBJECT_INIT_RANGE:
+//      return op_invoke_object_init_range();
+    case OP_IGET_QUICK:
+        return op_iget_quick(mir);
+    case OP_IGET_WIDE_QUICK:
+        return op_iget_wide_quick(mir);
+    case OP_IGET_OBJECT_QUICK:
+        return op_iget_object_quick(mir);
+    case OP_IPUT_QUICK:
+        return op_iput_quick(mir);
+    case OP_IPUT_WIDE_QUICK:
+        return op_iput_wide_quick(mir);
+    case OP_IPUT_OBJECT_QUICK:
+        return op_iput_object_quick(mir);
+    case OP_INVOKE_VIRTUAL_QUICK:
+        return op_invoke_virtual_quick(mir);
+    case OP_INVOKE_VIRTUAL_QUICK_RANGE:
+        return op_invoke_virtual_quick_range(mir);
+    case OP_INVOKE_SUPER_QUICK:
+        return op_invoke_super_quick(mir);
+    case OP_INVOKE_SUPER_QUICK_RANGE:
+        return op_invoke_super_quick_range(mir);
+    default:
+        ALOGI("JIT_INFO: JIT does not support bytecode %s\n",
+                dexGetOpcodeName(mir->dalvikInsn.opcode));
+        SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
+        assert(false && "All opcodes should be supported.");
+        break;
+    }
+    return -1;
+}
+
+int op_nop(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NOP);
+    return 0;
+}
+
+#if defined VTUNE_DALVIK
+/**
+ * Send the label information (size, start_address and name) to VTune
+ */
+void sendLabelInfoToVTune(int startStreamPtr, int endStreamPtr, const char* labelName) {
+    if (endStreamPtr == startStreamPtr) {
+        return;
+    }
+
+    iJIT_Method_Load jitMethod;
+    memset(&jitMethod, 0, sizeof(iJIT_Method_Load));
+    jitMethod.method_id = iJIT_GetNewMethodID();
+    jitMethod.method_name = const_cast<char *>(labelName);
+    jitMethod.method_load_address = (void *)startStreamPtr;
+    jitMethod.method_size = endStreamPtr-startStreamPtr;
+    int res = iJIT_NotifyEvent(iJVM_EVENT_TYPE_METHOD_LOAD_FINISHED, (void*)&jitMethod);
+    if (gDvmJit.printMe == true) {
+        if (res != 0) {
+            ALOGD("JIT API: a trace of %s method was written successfully address: id=%u, address=%p, size=%d."
+                    , labelName, jitMethod.method_id, jitMethod.method_load_address, jitMethod.method_size);
+        } else {
+            ALOGD("JIT API: failed to write a trace of %s method address: id=%u, address=%p, size=%d."
+                    , labelName, jitMethod.method_id, jitMethod.method_load_address, jitMethod.method_size);
+        }
+    }
+}
+#endif
+
+int getLabelOffset (unsigned int blockId) {
+    //Paranoid
+    if (gCompilationUnit == 0) {
+        //We can't do much except reporting an error
+        ALOGI("JIT_INFO: getLabelOffset has  null gCompilationUnit");
+        SET_JIT_ERROR(kJitErrorTraceFormation);
+        return -1;
+    }
+
+    //Get the BasicBlock
+    BasicBlock *bb = (BasicBlock *) dvmGrowableListGetElement(&gCompilationUnit->blockList, blockId);
+
+    //Transform into a BasicBlock_O1
+    BasicBlock_O1 *bbO1 = reinterpret_cast<BasicBlock_O1 *> (bb);
+
+    //Paranoid
+    if (bbO1 == 0 || bbO1->label == 0) {
+        //We can't do much except reporting an error
+        ALOGI("JIT_INFO: getLabelOffset has invalid basic block");
+        SET_JIT_ERROR(kJitErrorInvalidBBId);
+        return -1;
+    }
+
+    //Now return the label
+    return bbO1->label->lop.generic.offset;
+}
+
+
+/**
+ * @brief Calculate magic number and shift for a given divisor
+ * @param divisor divisor number for calculation
+ * @param magic hold calculated magic number
+ * @param shift hold calculated shift
+ * @return void
+ */
+void calculateMagicAndShift(int divisor, int* magic, int* shift) {
+    //It does not make sense to calculate magic and shift for zero divisor
+    assert (divisor != 0);
+
+    int p = 31;
+    unsigned abs_d, abs_nc, delta, quotient1, remainder1, quotient2, remainder2, tmp;
+    const unsigned two31 = 1 << p;
+
+    /* According to H.S.Warren's Hacker's Delight Chapter 10 and
+       T,Grablund, P.L.Montogomery's Division by invariant integers using multiplication
+       The magic number M and shift S can be calculated in the following way:
+       Let nc be the most positive value of numerator(n) such that nc = kd - 1, where divisor(d) >=2
+       Let nc be the most negative value of numerator(n) such that nc = kd + 1, where divisor(d) <= -2
+       Thus nc can be calculated like:
+       nc = 2^31 + 2^31 % d - 1, where d >= 2
+       nc = -2^31 + (2^31 + 1) % d, where d >= 2.
+
+       So the shift p is the smallest p satisfying
+       2^p > nc * (d - 2^p % d), where d >= 2
+       2^p > nc * (d + 2^p % d), where d <= -2.
+
+       the magic number M is calcuated by
+       M = (2^p + d - 2^p % d) / d, where d >= 2
+       M = (2^p - d - 2^p % d) / d, where d <= -2.
+
+       Notice that p is always bigger than or equal to 32, so we just return 32-p as the shift number S. */
+
+    // Initialize
+    abs_d = abs(divisor);
+    tmp = two31 + ((unsigned)divisor >> 31);
+    abs_nc = tmp - 1 - tmp % abs_d;
+    quotient1 = two31 / abs_nc;
+    remainder1 = two31 % abs_nc;
+    quotient2 = two31 / abs_d;
+    remainder2 = two31 % abs_d;
+
+    // To avoid handling both positive and negative divisor, Hacker's Delight introduces a method to handle
+    // these 2 cases together to avoid duplication.
+    do {
+        p++;
+        quotient1 = 2 * quotient1;
+        remainder1 = 2 * remainder1;
+        if (remainder1 >= abs_nc){
+            quotient1++;
+            remainder1 = remainder1 - abs_nc;
+        }
+        quotient2 = 2 * quotient2;
+        remainder2 = 2 * remainder2;
+        if (remainder2 >= abs_d){
+            quotient2++;
+            remainder2 = remainder2 - abs_d;
+        }
+        delta = abs_d - remainder2;
+    }while (quotient1 < delta || (quotient1 == delta && remainder1 == 0));
+
+    *magic = (divisor > 0) ? (quotient2 + 1) : (-quotient2 - 1);
+    *shift = p - 32;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/Lower.h b/vm/compiler/codegen/x86/lightcg/Lower.h
new file mode 100644
index 0000000..44c8923
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/Lower.h
@@ -0,0 +1,1453 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file Lower.h
+    \brief A header file to define interface between lowering, register allocator, and scheduling
+*/
+
+#ifndef _DALVIK_LOWER
+#define _DALVIK_LOWER
+
+#define CODE_CACHE_PADDING 1024 //code space for a single bytecode
+// comment out for phase 1 porting
+#define PREDICTED_CHAINING
+#define JIT_CHAIN
+
+#define NCG_O1
+//compilaton flags used by NCG O1
+#define DUMP_EXCEPTION //to measure performance, required to have correct exception handling
+/*! multiple versions for hardcoded registers */
+#define HARDREG_OPT
+#define CFG_OPT
+/*! remove redundant move ops when accessing virtual registers */
+#define MOVE_OPT
+/*! remove redundant spill of virtual registers */
+#define SPILL_OPT
+#define XFER_OPT
+/*! use live range analysis to allocate registers */
+#define LIVERANGE_OPT
+/*! remove redundant null check */
+#define NULLCHECK_OPT
+//#define BOUNDCHECK_OPT
+#define CALL_FIX
+#define NATIVE_FIX
+#define INVOKE_FIX //optimization
+#define GETVR_FIX //optimization
+
+#include "CodegenErrors.h"
+#include "Dalvik.h"
+#include "enc_wrapper.h"
+#include "AnalysisO1.h"
+#include "CompileTable.h"
+#include "compiler/CompilerIR.h"
+
+//compilation flags for debugging
+//#define DEBUG_INFO
+//#define DEBUG_CALL_STACK
+//#define DEBUG_IGET_OBJ
+//#define DEBUG_NCG_CODE_SIZE
+//#define DEBUG_NCG
+//#define DEBUG_NCG_1
+//#define DEBUG_LOADING
+//#define USE_INTERPRETER
+//#define DEBUG_EACH_BYTECODE
+
+/*! registers for functions are hardcoded */
+#define HARDCODE_REG_CALL
+#define HARDCODE_REG_SHARE
+#define HARDCODE_REG_HELPER
+
+#define PhysicalReg_FP PhysicalReg_EDI
+#define PhysicalReg_Glue PhysicalReg_EBP
+
+//COPIED from interp/InterpDefs.h
+#define FETCH(_offset) (rPC[(_offset)])
+#define INST_INST(_inst) ((_inst) & 0xff)
+#define INST_A(_inst)       (((_inst) >> 8) & 0x0f)
+#define INST_B(_inst)       ((_inst) >> 12)
+#define INST_AA(_inst)      ((_inst) >> 8)
+
+#define offEBP_self 8
+#define offEBP_spill -56
+#define offThread_jniLocal_nextEntry 168
+
+// Definitions must be consistent with vm/mterp/x86/header.S
+#define FRAME_SIZE     124
+
+typedef enum ArgsDoneType {
+    ArgsDone_Normal = 0,
+    ArgsDone_Native,
+    ArgsDone_Full
+} ArgsDoneType;
+
+/*! An enum type
+    to list bytecodes for AGET, APUT
+*/
+typedef enum ArrayAccess {
+    AGET, AGET_WIDE, AGET_CHAR, AGET_SHORT, AGET_BOOLEAN, AGET_BYTE,
+    APUT, APUT_WIDE, APUT_CHAR, APUT_SHORT, APUT_BOOLEAN, APUT_BYTE
+} ArrayAccess;
+/*! An enum type
+    to list bytecodes for IGET, IPUT
+*/
+typedef enum InstanceAccess {
+    IGET, IGET_WIDE, IPUT, IPUT_WIDE
+} InstanceAccess;
+/*! An enum type
+    to list bytecodes for SGET, SPUT
+*/
+typedef enum StaticAccess {
+    SGET, SGET_WIDE, SPUT, SPUT_WIDE
+} StaticAccess;
+
+typedef enum JmpCall_type {
+    JmpCall_uncond = 1,
+    JmpCall_cond,
+    JmpCall_reg, //jump reg32
+    JmpCall_call
+} JmpCall_type;
+
+//! \enum AtomOpCode
+//! \brief Pseudo-mnemonics for Atom
+//! \details Initially included to be in sync with ArmOpCode which specifies
+//! additional pseudo mnemonics for use during codegen, but it has
+//! diverted. Although there are references to this everywhere,
+//! very little of this is actually used for functionality.
+//! \todo Either refactor to match ArmOpCode or remove dependency on this.
+enum AtomOpCode {
+    ATOM_PSEUDO_CHAINING_CELL_BACKWARD_BRANCH = -15,
+    ATOM_NORMAL_ALU = -14,
+    ATOM_PSEUDO_ENTRY_BLOCK = -13,
+    ATOM_PSEUDO_EXIT_BLOCK = -12,
+    ATOM_PSEUDO_TARGET_LABEL = -11,
+    ATOM_PSEUDO_CHAINING_CELL_HOT = -10,
+    ATOM_PSEUDO_CHAINING_CELL_INVOKE_PREDICTED = -9,
+    ATOM_PSEUDO_CHAINING_CELL_INVOKE_SINGLETON = -8,
+    ATOM_PSEUDO_CHAINING_CELL_NORMAL = -7,
+    ATOM_PSEUDO_DALVIK_BYTECODE_BOUNDARY = -6,
+    ATOM_PSEUDO_ALIGN4 = -5,
+    ATOM_PSEUDO_PC_RECONSTRUCTION_CELL = -4,
+    ATOM_PSEUDO_PC_RECONSTRUCTION_BLOCK_LABEL = -3,
+    ATOM_PSEUDO_EH_BLOCK_LABEL = -2,
+    ATOM_PSEUDO_NORMAL_BLOCK_LABEL = -1,
+    ATOM_NORMAL,
+};
+
+//! \enum LowOpndType
+//! \brief Defines types of operands that a LowOp can have.
+//! \details The Imm, Mem, and Reg variants correspond literally to what
+//! the final encoded x86 instruction will have. The others are used for
+//! additional behavior needed before the x86 encoding.
+//! \see LowOp
+enum LowOpndType {
+    //! \brief Immediate
+    LowOpndType_Imm,
+    //! \brief Register
+    LowOpndType_Reg,
+    //! \brief Memory access
+    LowOpndType_Mem,
+    //! \brief Used for jumps to labels
+    LowOpndType_Label,
+    //! \brief Used for jumps to other blocks
+    LowOpndType_BlockId,
+    //! \brief Used for chaining
+    LowOpndType_Chain
+};
+
+//! \enum LowOpndDefUse
+//! \brief Defines type of usage that a LowOpnd can have.
+//! \see LowOpnd
+enum LowOpndDefUse {
+    //! \brief Definition
+    LowOpndDefUse_Def,
+    //! \brief Usage
+    LowOpndDefUse_Use,
+    //! \brief Usage and Definition
+    LowOpndDefUse_UseDef
+};
+
+//! \enum MemoryAccessType
+//! \brief Classifies type of memory access.
+enum MemoryAccessType {
+    //! \brief access Dalvik virtual register
+    MemoryAccess_VR,
+    //! \brief access spill region
+    MemoryAccess_SPILL,
+    //! \brief unclassified memory access
+    MemoryAccess_Unknown,
+    //! \brief access to read-only constant section
+    MemoryAccess_Constants,
+};
+
+//! \enum UseDefEntryType
+//! \brief Defines types of resources on which there can be a dependency.
+enum UseDefEntryType {
+    //! \brief Control flags, EFLAGS register
+    UseDefType_Ctrl,
+    //! \brief Floating-point stack
+    //! \details This is a very generic resource for x87 operations and
+    //! doesn't break down different possible resources like control word,
+    //! status word, FPU flags, etc. All of x87 resources fall into this
+    //! type of resource.
+    UseDefType_Float,
+    //! \brief Dalvik virtual register. Corresponds to MemoryAccess_VR
+    UseDefType_MemVR,
+    //! \brief Spill region. Corresponds to MemoryAccess_SPILL
+    UseDefType_MemSpill,
+    //! \brief Unclassified memory access. Corresponds to MemoryAccess_Unknown
+    //! \details No memory disambiguation will be done with unknown accesses
+    UseDefType_MemUnknown,
+    //! \brief Register
+    UseDefType_Reg
+};
+
+//! \enum DependencyType
+//! \brief Defines types of dependencies on a resource.
+enum DependencyType {
+    //! \brief Read after Write
+    Dependency_RAW,
+    //! \brief Write after Write
+    Dependency_WAW,
+    //! \brief Write after Read
+    Dependency_WAR,
+};
+
+//! \enum LatencyBetweenNativeInstructions
+//! \brief Defines reasons for what causes pipeline stalls
+//! between two instructions.
+//! \warning Make sure that if adding new reasons here,
+//! the scheduler needs updated with the actual latency value.
+//! \see mapLatencyReasonToValue
+enum LatencyBetweenNativeInstructions {
+    //! \brief No latency between the two instructions
+    Latency_None = 0,
+    //! \brief Stall in address generation phase of pipeline
+    //! when register is not available.
+    Latency_Agen_stall,
+    //! \brief Stall when a memory load is blocked by a store
+    //! and there is no store forwarding.
+    Latency_Load_blocked_by_store,
+    //! \brief Stall due to cache miss during load from memory
+    Latency_Memory_Load,
+};
+
+//! \brief Defines a relationship between a resource and its producer.
+struct UseDefProducerEntry {
+    //! \brief Resource type on which there is a dependency.
+    UseDefEntryType type;
+    //! \brief Virtual or physical register this resource is
+    //! associated with.
+    //! \details When physical, this is of enum type PhysicalReg.
+    //! When VR, this is the virtual register number.
+    //! When there is no register related dependency, this is
+    //! negative.
+    int regNum;
+    //! \brief Corresponds to LowOp::slotId to keep track of producer.
+    unsigned int producerSlot;
+};
+
+//! \brief Defines a relationship between a resource and its users.
+struct UseDefUserEntry {
+    //! \brief Resource type on which there is a dependency.
+    UseDefEntryType type;
+    //! \brief Virtual or physical register this resource is
+    //! associated with.
+    //! \details When physical, this is of enum type PhysicalReg.
+    //! When VR, this is the virtual register number.
+    //! When there is no register related dependency, this is
+    //! negative.
+    int regNum;
+    //! \brief A list of LowOp::slotId to keep track of all users
+    //! of this resource.
+    std::vector<unsigned int> useSlotsList;
+};
+
+//! \brief Holds information on the data dependencies
+struct DependencyInformation {
+    //! \brief Type of data hazard
+    DependencyType dataHazard;
+    //! \brief Holds the LowOp::slotId of the LIR that causes this
+    //! data dependence.
+    unsigned int lowopSlotId;
+    //! \brief Description for what causes the edge latency
+    //! \see LatencyBetweenNativeInstructions
+    LatencyBetweenNativeInstructions causeOfEdgeLatency;
+    //! \brief Holds latency information for edges in the
+    //! dependency graph, not execute to execute latency for the
+    //! instructions.
+    int edgeLatency;
+};
+
+//! \brief Holds general information about an operand.
+struct LowOpnd {
+    //! \brief Classification of operand.
+    LowOpndType type;
+    //! \brief Size of operand.
+    OpndSize size;
+    //! \brief Usage, definition, or both of operand.
+    LowOpndDefUse defuse;
+};
+
+//! \brief Holds information about a register operand.
+struct LowOpndReg {
+    //! \brief Classification on type of register.
+    LowOpndRegType regType;
+    //! \brief Register number, either logical or physical.
+    int regNum;
+    //! \brief When false, register is logical.
+    bool isPhysical;
+};
+
+//! \brief Holds information about an immediate operand.
+struct LowOpndImm {
+    //! \brief Value of the immediate.
+    s4 value;
+};
+
+//! \brief Holds information about an immediate operand where the immediate
+//! has not been generated yet.
+struct LowOpndBlock {
+    //! \brief Holds id of MIR level basic block.
+    s4 value;
+    //! \brief Whether the immediate needs to be aligned within 16-bytes
+    bool immediateNeedsAligned;
+};
+
+//! \brief Defines maximum length of string holding label name.
+#define LABEL_SIZE 256
+
+//! \brief Holds information about an immediate operand where the immediate
+//! has not been generated yet from label.
+struct LowOpndLabel {
+    //! \brief Name of the label for which to generate immediate.
+    char label[LABEL_SIZE];
+    //! \brief This is true when label is short term distance from caller
+    //! and an 8-bit operand is sufficient.
+    bool isLocal;
+};
+
+//! \brief Holds information about a memory operand.
+struct LowOpndMem {
+    //! \brief Displacement
+    LowOpndImm m_disp;
+    //! \brief Scaling
+    LowOpndImm m_scale;
+    //! \brief Index Register
+    LowOpndReg m_index;
+    //! \brief Base Register
+    LowOpndReg m_base;
+    //! \brief If true, must use the scaling value.
+    bool hasScale;
+    //! \brief Defines type of memory access.
+    MemoryAccessType mType;
+    //! \brief If positive, this represents the VR number
+    int index;
+};
+
+//! \brief Data structure for an x86 LIR.
+//! \todo Decouple fields used for scheduling from this struct.
+//! is a good idea if using it throughout the trace JIT and never
+//! actually passing it for scheduling.
+struct LowOp {
+    //! \brief Holds general LIR information (Google's implementation)
+    //! \warning Only offset information is used for x86 and the other
+    //! fields are not valid except in LowOpBlockLabel.
+    LIR generic;
+    //! \brief x86 mnemonic for instruction
+    Mnemonic opCode;
+    //! \brief x86 pseudo-mnemonic
+    AtomOpCode opCode2;
+    //! \brief Destination operand
+    //! \details This is not used when there are only 0 or 1 operands.
+    LowOpnd opndDest;
+    //! \brief Source operand
+    //! \details This is used when there is a single operand.
+    LowOpnd opndSrc;
+    //! \brief Holds number of operands for this LIR (0, 1, or 2)
+    unsigned short numOperands;
+    //! \brief Logical timestamp for ordering.
+    //! \details This value should uniquely identify an LIR and also
+    //! provide natural ordering depending on when it was requested.
+    //! This is used during scheduling to hold original order for the
+    //! native basic block.
+    unsigned int slotId;
+    //! \brief Logical time for when the LIR is ready.
+    //! \details This field is used only for scheduling.
+    int readyTime;
+    //! \brief Cycle in which LIR is scheduled for issue.
+    //! \details This field is used only for scheduling.
+    int scheduledTime;
+    //! \brief Execute to execute time for this instruction.
+    //! \details This field is used only for scheduling.
+    //! \see MachineModelEntry::executeToExecuteLatency
+    int instructionLatency;
+    //! \brief Issue port for this instruction.
+    //! \details This field is used only for scheduling.
+    //! \see MachineModelEntry::issuePortType
+    int portType;
+    //! \brief Weight of longest path in dependency graph from
+    //! current instruction to end of the basic block.
+    //! \details This field is used only for scheduling.
+    int longestPath;
+};
+
+//! \brief Specialized LowOp with known label operand but
+//! whose offset immediate is not known yet.
+struct LowOpLabel : LowOp {
+    //! \brief Label operand whose immediate has not yet been
+    //! generated.
+    LowOpndLabel labelOpnd;
+};
+
+//! \brief Specialized LowOp for use with block operand whose id
+//! is known but the offset immediate has not been generated yet.
+struct LowOpBlock : LowOp {
+    //! \brief Non-generated immediate operand
+    LowOpndBlock blockIdOpnd;
+};
+
+//! \brief Specialized LowOp which is only used with
+//! pseudo-mnemonic.
+//! \see AtomOpCode
+struct LowOpBlockLabel {
+    //! \todo Does not use inheritance like the other LowOp
+    //! data structures because of a git merge issue. In future,
+    //! this can be safely updated.
+    LowOp lop;
+};
+
+//! \brief Specialized LowOp with an immediate operand.
+struct LowOpImm : LowOp {
+    //! \brief Immediate
+    LowOpndImm immOpnd;
+};
+
+//! \brief Specialized LowOp with a memory operand.
+struct LowOpMem : LowOp {
+    //! \brief Memory Operand
+    LowOpndMem memOpnd;
+};
+
+//! \brief Specialized LowOp with register operand.
+struct LowOpReg : LowOp {
+    //! \brief Register
+    LowOpndReg regOpnd;
+};
+
+//! \brief Specialized LowOp for immediate to register.
+struct LowOpImmReg : LowOp {
+    //! \brief Immediate as source.
+    LowOpndImm immSrc;
+    //! \brief Register as destination.
+    LowOpndReg regDest;
+};
+
+//! \brief Specialized LowOp for register to register.
+struct LowOpRegReg : LowOp {
+    //! \brief Register as source.
+    LowOpndReg regSrc;
+    //! \brief Register as destination.
+    LowOpndReg regDest;
+};
+
+//! \brief Specialized LowOp for memory to register.
+struct LowOpMemReg : LowOp {
+    //! \brief Memory as source.
+    LowOpndMem memSrc;
+    //! \brief Register as destination.
+    LowOpndReg regDest;
+   //! \brief ptr to data structure containing 64 bit constants
+    ConstInfo *constLink;
+};
+
+//! \brief Specialized LowOp for immediate to memory.
+struct LowOpImmMem : LowOp {
+    //! \brief Immediate as source.
+    LowOpndImm immSrc;
+    //! \brief Memory as destination.
+    LowOpndMem memDest;
+};
+
+//! \brief Specialized LowOp for register to memory.
+struct LowOpRegMem : LowOp {
+    //! \brief Register as source.
+    LowOpndReg regSrc;
+    //! \brief Memory as destination.
+    LowOpndMem memDest;
+};
+
+/*!
+\brief data structure for labels used when lowering a method
+
+four label maps are defined: globalMap globalShortMap globalWorklist globalShortWorklist
+globalMap: global labels where codePtr points to the label
+           freeLabelMap called in clearNCG
+globalWorklist: global labels where codePtr points to an instruciton using the label
+  standalone NCG -------
+                accessed by insertLabelWorklist & performLabelWorklist
+  code cache ------
+                inserted by performLabelWorklist(false),
+                handled & cleared by generateRelocation in NcgFile.c
+globalShortMap: local labels where codePtr points to the label
+                freeShortMap called after generation of one bytecode
+globalShortWorklist: local labels where codePtr points to an instruction using the label
+                accessed by insertShortWorklist & insertLabel
+definition of local label: life time of the label is within a bytecode or within a helper function
+extra label maps are used by code cache:
+  globalDataWorklist VMAPIWorklist
+*/
+typedef struct LabelMap {
+  char label[LABEL_SIZE];
+  char* codePtr; //code corresponding to the label or code that uses the label
+  struct LabelMap* nextItem;
+  OpndSize size;
+  uint  addend;
+} LabelMap;
+/*!
+\brief data structure to handle forward jump (GOTO, IF)
+
+accessed by insertNCGWorklist & performNCGWorklist
+*/
+typedef struct NCGWorklist {
+  //when WITH_JIT, relativePC stores the target basic block id
+  s4 relativePC; //relative offset in bytecode
+  int offsetPC;  //PC in bytecode
+  int offsetNCG; //PC in native code
+  char* codePtr; //code for native jump instruction
+  struct NCGWorklist* nextItem;
+  OpndSize size;
+}NCGWorklist;
+/*!
+\brief data structure to handle SWITCH & FILL_ARRAY_DATA
+
+two data worklist are defined: globalDataWorklist (used by code cache) & methodDataWorklist
+methodDataWorklist is accessed by insertDataWorklist & performDataWorklist
+*/
+typedef struct DataWorklist {
+  s4 relativePC; //relative offset in bytecode to access the data
+  int offsetPC;  //PC in bytecode
+  int offsetNCG; //PC in native code
+  char* codePtr; //code for native instruction add_imm_reg imm, %edx
+  char* codePtr2;//code for native instruction add_reg_reg %eax, %edx for SWITCH
+                 //                            add_imm_reg imm, %edx for FILL_ARRAY_DATA
+  struct DataWorklist* nextItem;
+}DataWorklist;
+#ifdef ENABLE_TRACING
+typedef struct MapWorklist {
+  u4 offsetPC;
+  u4 offsetNCG;
+  int isStartOfPC; //1 --> true 0 --> false
+  struct MapWorklist* nextItem;
+} MapWorklist;
+#endif
+
+#define BUFFER_SIZE 1024 //# of Low Ops buffered
+//the following three numbers are hardcoded, please CHECK
+#define BYTECODE_SIZE_PER_METHOD 81920
+#define NATIVE_SIZE_PER_DEX 19000000 //FIXME for core.jar: 16M --> 18M for O1
+#define NATIVE_SIZE_FOR_VM_STUBS 100000
+#define MAX_HANDLER_OFFSET 1024 //maximal number of handler offsets
+
+extern int LstrClassCastExceptionPtr, LstrInstantiationErrorPtr, LstrInternalError, LstrFilledNewArrayNotImpl;
+extern int LstrArithmeticException, LstrArrayIndexException, LstrArrayStoreException, LstrStringIndexOutOfBoundsException;
+extern int LstrDivideByZero, LstrNegativeArraySizeException, LstrNoSuchMethodError, LstrNullPointerException;
+extern int LdoubNeg, LvaluePosInfLong, LvalueNegInfLong, LvalueNanLong, LshiftMask, Lvalue64, L64bits, LintMax, LintMin;
+
+extern LabelMap* globalMap;
+extern LabelMap* globalShortMap;
+extern LabelMap* globalWorklist;
+extern LabelMap* globalShortWorklist;
+extern NCGWorklist* globalNCGWorklist;
+extern DataWorklist* methodDataWorklist;
+#ifdef ENABLE_TRACING
+extern MapWorklist* methodMapWorklist;
+#endif
+extern PhysicalReg scratchRegs[4];
+
+#define C_SCRATCH_1 scratchRegs[0]
+#define C_SCRATCH_2 scratchRegs[1]
+#define C_SCRATCH_3 scratchRegs[2] //scratch reg inside callee
+
+extern LowOp* ops[BUFFER_SIZE];
+extern bool isScratchPhysical;
+extern u2* rPC;
+extern int offsetPC;
+extern int offsetNCG;
+extern int mapFromBCtoNCG[BYTECODE_SIZE_PER_METHOD];
+extern char* streamStart;
+
+extern char* streamCode;
+
+extern char* streamMethodStart; //start of the method
+extern char* stream; //current stream pointer
+
+extern Method* currentMethod;
+extern int currentExceptionBlockIdx;
+
+extern int globalMapNum;
+extern int globalWorklistNum;
+extern int globalDataWorklistNum;
+extern int globalPCWorklistNum;
+extern int chainingWorklistNum;
+extern int VMAPIWorklistNum;
+
+extern LabelMap* globalDataWorklist;
+extern LabelMap* globalPCWorklist;
+extern LabelMap* chainingWorklist;
+extern LabelMap* VMAPIWorklist;
+
+extern int ncgClassNum;
+extern int ncgMethodNum;
+
+// Global pointer to the current CompilationUnit
+class CompilationUnit_O1;
+extern CompilationUnit_O1 *gCompilationUnit;
+
+bool existATryBlock(Method* method, int startPC, int endPC);
+// interface between register allocator & lowering
+extern int num_removed_nullCheck;
+
+//Allocate a register
+int registerAlloc(int type, int reg, bool isPhysical, bool updateRef, bool isDest = false);
+//Allocate a register trying to alias a virtual register with a temporary
+int registerAllocMove(int reg, int type, bool isPhysical, int srcReg, bool isDest = false);
+
+int checkVirtualReg(int reg, LowOpndRegType type, int updateRef); //returns the physical register
+int updateRefCount(int reg, LowOpndRegType type);
+int updateRefCount2(int reg, int type, bool isPhysical);
+int spillVirtualReg(int vrNum, LowOpndRegType type, bool updateTable);
+int checkTempReg(int reg, int type, bool isPhysical, int vA);
+bool checkTempReg2(int reg, int type, bool isPhysical, int physicalRegForVR, int vB);
+int freeReg(bool writeBackAllVRs);
+int nextVersionOfHardReg(PhysicalReg pReg, int refCount);
+int updateVirtualReg(int reg, LowOpndRegType type);
+int setVRNullCheck(int regNum, OpndSize size);
+bool isVRNullCheck(int regNum, OpndSize size);
+void setVRBoundCheck(int vr_array, int vr_index);
+bool isVRBoundCheck(int vr_array, int vr_index);
+int requestVRFreeDelay(int regNum, u4 reason);
+int cancelVRFreeDelayRequest(int regNum, u4 reason);
+
+// Update delay flag for all VRs, stored in physical registers
+bool requestVRFreeDelayAll(u4 reason);
+bool cancelVRFreeDelayRequestAll(u4 reason);
+
+bool getVRFreeDelayRequested(int regNum);
+
+//Update the virtual register use information
+void updateVRAtUse(int reg, LowOpndRegType pType, int regAll);
+int touchEcx();
+int touchEax();
+int touchEdx();
+int beforeCall(const char* target);
+int afterCall(const char* target);
+void startBranch();
+void endBranch();
+void rememberState(int);
+void goToState(int);
+void transferToState(int);
+
+//Handle virtual register writebacks
+int handleRegistersEndOfBB(bool syncChildren);
+
+//Call to reset certain flags before generating native code
+void startNativeCode(int num, int type);
+//Call to reset certain flags after generating native code
+void endNativeCode(void);
+
+#define XMM_1 PhysicalReg_XMM0
+#define XMM_2 PhysicalReg_XMM1
+#define XMM_3 PhysicalReg_XMM2
+#define XMM_4 PhysicalReg_XMM3
+
+/////////////////////////////////////////////////////////////////////////////////
+//LR[reg] = disp + PR[base_reg] or disp + LR[base_reg]
+void load_effective_addr(int disp, int base_reg, bool isBasePhysical,
+                          int reg, bool isPhysical);
+void load_effective_addr_scale(int base_reg, bool isBasePhysical,
+                                int index_reg, bool isIndexPhysical, int scale,
+                                int reg, bool isPhysical);
+//! lea reg, [base_reg + index_reg*scale + disp]
+void load_effective_addr_scale_disp(int base_reg, bool isBasePhysical, int disp,
+                int index_reg, bool isIndexPhysical, int scale,
+                int reg, bool isPhysical);
+//! Loads a 16-bit value into the x87 FPU control word. Typically used to
+//! establish or change the FPU's operational mode. Can cause exceptions to
+//! be thrown if not cleared beforehand.
+void load_fpu_cw(int disp, int base_reg, bool isBasePhysical);
+void store_fpu_cw(bool checkException, int disp, int base_reg, bool isBasePhysical);
+void convert_integer(OpndSize srcSize, OpndSize dstSize);
+void convert_int_to_fp(int srcReg, bool isSrcPhysical, int destReg, bool isDestPhysical, bool isDouble);
+void load_fp_stack(LowOp* op, OpndSize size, int disp, int base_reg, bool isBasePhysical);
+void load_int_fp_stack(OpndSize size, int disp, int base_reg, bool isBasePhysical);
+void load_int_fp_stack_imm(OpndSize size, int imm);
+void store_fp_stack(LowOp* op, bool pop, OpndSize size, int disp, int base_reg, bool isBasePhysical);
+void store_int_fp_stack(LowOp* op, bool pop, OpndSize size, int disp, int base_reg, bool isBasePhysical);
+
+void load_fp_stack_VR(OpndSize size, int vA);
+void load_int_fp_stack_VR(OpndSize size, int vA);
+void store_fp_stack_VR(bool pop, OpndSize size, int vA);
+void store_int_fp_stack_VR(bool pop, OpndSize size, int vA);
+void compare_VR_ss_reg(int vA, int reg, bool isPhysical);
+void compare_VR_sd_reg(int vA, int reg, bool isPhysical);
+void fpu_VR(ALU_Opcode opc, OpndSize size, int vA);
+void compare_reg_mem(LowOp* op, OpndSize size, int reg, bool isPhysical,
+                           int disp, int base_reg, bool isBasePhysical);
+void compare_mem_reg(OpndSize size,
+                           int disp, int base_reg, bool isBasePhysical,
+                           int reg, bool isPhysical);
+void compare_VR_reg(OpndSize size,
+                           int vA,
+                           int reg, bool isPhysical);
+void compare_imm_reg(OpndSize size, int imm,
+                           int reg, bool isPhysical);
+void compare_imm_mem(OpndSize size, int imm,
+                           int disp, int base_reg, bool isBasePhysical);
+void compare_imm_VR(OpndSize size, int imm,
+                           int vA);
+void compare_reg_reg(int reg1, bool isPhysical1,
+                           int reg2, bool isPhysical2);
+void compare_reg_reg_16(int reg1, bool isPhysical1,
+                         int reg2, bool isPhysical2);
+void compare_ss_mem_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
+                              int reg, bool isPhysical);
+void compare_ss_reg_with_reg(LowOp* op, int reg1, bool isPhysical1,
+                              int reg2, bool isPhysical2);
+void compare_sd_mem_with_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
+                              int reg, bool isPhysical);
+void compare_sd_reg_with_reg(LowOp* op, int reg1, bool isPhysical1,
+                              int reg2, bool isPhysical2);
+void compare_fp_stack(bool pop, int reg, bool isDouble);
+void test_imm_reg(OpndSize size, int imm, int reg, bool isPhysical);
+void test_imm_mem(OpndSize size, int imm, int disp, int reg, bool isPhysical);
+
+void conditional_move_reg_to_reg(OpndSize size, ConditionCode cc, int reg1, bool isPhysical1, int reg, bool isPhysical);
+void move_ss_mem_to_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
+                        int reg, bool isPhysical);
+void move_ss_reg_to_mem(LowOp* op, int reg, bool isPhysical,
+                         int disp, int base_reg, bool isBasePhysical);
+LowOpMemReg* move_ss_mem_to_reg_noalloc(int disp, int base_reg, bool isBasePhysical,
+                         MemoryAccessType mType, int mIndex,
+                         int reg, bool isPhysical);
+LowOpRegMem* move_ss_reg_to_mem_noalloc(int reg, bool isPhysical,
+                         int disp, int base_reg, bool isBasePhysical,
+                         MemoryAccessType mType, int mIndex);
+void move_sd_mem_to_reg(int disp, int base_reg, bool isBasePhysical,
+                         int reg, bool isPhysical);
+void move_sd_reg_to_mem(LowOp* op, int reg, bool isPhysical,
+                         int disp, int base_reg, bool isBasePhysical);
+
+void conditional_jump(ConditionCode cc, const char* target, bool isShortTerm);
+void unconditional_jump(const char* target, bool isShortTerm);
+void conditional_jump_int(ConditionCode cc, int target, OpndSize size);
+void unconditional_jump_int(int target, OpndSize size);
+void conditional_jump_block(ConditionCode cc, int targetBlockId, bool immediateNeedsAligned = false);
+void unconditional_jump_block(int targetBlockId, bool immediateNeedsAligned = false);
+void unconditional_jump_reg(int reg, bool isPhysical);
+void unconditional_jump_rel32(void * target);
+void call(const char* target);
+void call_reg(int reg, bool isPhysical);
+void call_reg_noalloc(int reg, bool isPhysical);
+void call_mem(int disp, int reg, bool isPhysical);
+void x86_return();
+
+void alu_unary_reg(OpndSize size, ALU_Opcode opc, int reg, bool isPhysical);
+void alu_unary_mem(LowOp* op, OpndSize size, ALU_Opcode opc, int disp, int base_reg, bool isBasePhysical);
+
+void alu_binary_imm_mem(OpndSize size, ALU_Opcode opc,
+                         int imm, int disp, int base_reg, bool isBasePhysical);
+void alu_binary_imm_reg(OpndSize size, ALU_Opcode opc, int imm, int reg, bool isPhysical);
+//Operate on a VR with another VR and an immediate
+bool alu_imm_to_VR(OpndSize size, ALU_Opcode opc,
+                         int srcVR, int destVR, int imm, int tempReg, bool isTempPhysical, const MIR * mir);
+void alu_binary_mem_reg(OpndSize size, ALU_Opcode opc,
+                         int disp, int base_reg, bool isBasePhysical,
+                         int reg, bool isPhysical);
+void alu_binary_VR_reg(OpndSize size, ALU_Opcode opc, int vA, int reg, bool isPhysical);
+void alu_sd_binary_VR_reg(ALU_Opcode opc, int vA, int reg, bool isPhysical, bool isSD);
+void alu_binary_reg_reg(OpndSize size, ALU_Opcode opc,
+                         int reg1, bool isPhysical1,
+                         int reg2, bool isPhysical2);
+void alu_binary_reg_mem(OpndSize size, ALU_Opcode opc,
+                         int reg, bool isPhysical,
+                         int disp, int base_reg, bool isBasePhysical);
+
+void fpu_mem(LowOp* op, ALU_Opcode opc, OpndSize size, int disp, int base_reg, bool isBasePhysical);
+void alu_ss_binary_reg_reg(ALU_Opcode opc, int reg, bool isPhysical,
+                            int reg2, bool isPhysical2);
+void alu_sd_binary_reg_reg(ALU_Opcode opc, int reg, bool isPhysical,
+                            int reg2, bool isPhysical2);
+
+void push_mem_to_stack(OpndSize size, int disp, int base_reg, bool isBasePhysical);
+void push_reg_to_stack(OpndSize size, int reg, bool isPhysical);
+
+// create a new record for a 64 bit constant
+void addNewToConstList(struct ConstInfo** listPtr, int constL, int constH, int reg, bool align);
+// save address of memory location to be patched
+bool saveAddrToConstList(struct ConstInfo** listPtr, int constL, int constH, int reg, char* patchAddr, int offset);
+// access address of global constants
+int getGlobalDataAddr(const char* dataName);
+
+//returns the pointer to end of the native code
+void move_reg_to_mem(OpndSize size,
+                      int reg, bool isPhysical,
+                      int disp, int base_reg, bool isBasePhysical);
+LowOpMemReg* move_mem_to_reg(OpndSize size,
+                      int disp, int base_reg, bool isBasePhysical,
+                      int reg, bool isPhysical);
+void movez_mem_to_reg(OpndSize size,
+                      int disp, int base_reg, bool isBasePhysical,
+                      int reg, bool isPhysical);
+void movez_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical,
+                      int reg2, bool isPhysical2);
+void moves_mem_to_reg(LowOp* op, OpndSize size,
+                      int disp, int base_reg, bool isBasePhysical,
+                      int reg, bool isPhysical);
+void movez_mem_disp_scale_to_reg(OpndSize size,
+                      int base_reg, bool isBasePhysical,
+                      int disp, int index_reg, bool isIndexPhysical, int scale,
+                      int reg, bool isPhysical);
+void moves_mem_disp_scale_to_reg(OpndSize size,
+                      int base_reg, bool isBasePhysical,
+                      int disp, int index_reg, bool isIndexPhysical, int scale,
+                      int reg, bool isPhysical);
+
+//! \brief Performs MOVSX reg, reg2
+//!
+//! \details Sign extends reg and moves to reg2
+//! Size of destination register is fixed at 32-bits
+//! \param size of the source operand
+//! \param reg source operand
+//! \param isPhysical if reg is a physical register
+//! \param reg2 destination register
+//! \param isPhysical2 if reg2 is a physical register
+void moves_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical,
+                      int reg2, bool isPhysical2);
+void move_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical,
+                      int reg2, bool isPhysical2);
+void move_reg_to_reg_noalloc(OpndSize size,
+                      int reg, bool isPhysical,
+                      int reg2, bool isPhysical2);
+void move_mem_scale_to_reg(OpndSize size,
+                            int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
+                            int reg, bool isPhysical);
+void move_mem_disp_scale_to_reg(OpndSize size,
+                int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                int reg, bool isPhysical);
+void move_reg_to_mem_scale(OpndSize size,
+                            int reg, bool isPhysical,
+                            int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale);
+void move_reg_to_mem_disp_scale(OpndSize size,
+                            int reg, bool isPhysical,
+                            int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale);
+void move_imm_to_mem(OpndSize size, int imm,
+                      int disp, int base_reg, bool isBasePhysical);
+void set_VR_to_imm(int vA, OpndSize size, int imm);
+void set_VR_to_imm_noalloc(int vA, OpndSize size, int imm);
+void set_VR_to_imm_noupdateref(LowOp* op, int vA, OpndSize size, int imm);
+void move_imm_to_reg(OpndSize size, int imm, int reg, bool isPhysical);
+void move_imm_to_reg_noalloc(OpndSize size, int imm, int reg, bool isPhysical);
+void compareAndExchange(OpndSize size,
+             int reg, bool isPhysical,
+             int disp, int base_reg, bool isBasePhysical);
+
+//LR[reg] = VR[vB]
+//or
+//PR[reg] = VR[vB]
+void get_virtual_reg(int vB, OpndSize size, int reg, bool isPhysical);
+void get_virtual_reg_noalloc(int vB, OpndSize size, int reg, bool isPhysical);
+//VR[v] = LR[reg]
+//or
+//VR[v] = PR[reg]
+void set_virtual_reg(int vA, OpndSize size, int reg, bool isPhysical);
+void set_virtual_reg_noalloc(int vA, OpndSize size, int reg, bool isPhysical);
+void get_VR_ss(int vB, int reg, bool isPhysical);
+void set_VR_ss(int vA, int reg, bool isPhysical);
+void get_VR_sd(int vB, int reg, bool isPhysical);
+void set_VR_sd(int vA, int reg, bool isPhysical);
+
+int spill_reg(int reg, bool isPhysical);
+int unspill_reg(int reg, bool isPhysical);
+
+void move_reg_to_mem_noalloc(OpndSize size,
+                      int reg, bool isPhysical,
+                      int disp, int base_reg, bool isBasePhysical,
+                      MemoryAccessType mType, int mIndex);
+LowOpMemReg* move_mem_to_reg_noalloc(OpndSize size,
+                      int disp, int base_reg, bool isBasePhysical,
+                      MemoryAccessType mType, int mIndex,
+                      int reg, bool isPhysical);
+
+//////////////////////////////////////////////////////////////
+int insertLabel(const char* label, bool checkDup);
+int export_pc();
+int simpleNullCheck(int reg, bool isPhysical, int vr);
+int nullCheck(int reg, bool isPhysical, int exceptionNum, int vr);
+int handlePotentialException(
+                             ConditionCode code_excep, ConditionCode code_okay,
+                             int exceptionNum, const char* errName);
+int get_currentpc(int reg, bool isPhysical);
+int get_self_pointer(int reg, bool isPhysical);
+int get_res_strings(int reg, bool isPhysical);
+int get_res_classes(int reg, bool isPhysical);
+int get_res_fields(int reg, bool isPhysical);
+int get_res_methods(int reg, bool isPhysical);
+int get_glue_method_class(int reg, bool isPhysical);
+int get_glue_method(int reg, bool isPhysical);
+int get_suspendCount(int reg, bool isPhysical);
+int get_return_value(OpndSize size, int reg, bool isPhysical);
+int set_return_value(OpndSize size, int reg, bool isPhysical);
+void set_return_value(OpndSize size, int sourceReg, bool isSourcePhysical,
+        int scratchRegForSelfThread, int isScratchPhysical);
+int clear_exception();
+int get_exception(int reg, bool isPhysical);
+int set_exception(int reg, bool isPhysical);
+int savearea_from_fp(int reg, bool isPhysical);
+
+int call_moddi3();
+int call_divdi3();
+int call_fmod();
+int call_fmodf();
+int call_dvmFindCatchBlock();
+int call_dvmThrowVerificationError();
+int call_dvmAllocObject();
+int call_dvmAllocArrayByClass();
+int call_dvmResolveMethod();
+int call_dvmResolveClass();
+int call_dvmInstanceofNonTrivial();
+int call_dvmThrow();
+int call_dvmThrowWithMessage();
+int call_dvmCheckSuspendPending();
+int call_dvmLockObject();
+int call_dvmUnlockObject();
+int call_dvmInitClass();
+int call_dvmAllocPrimitiveArray();
+int call_dvmInterpHandleFillArrayData();
+int call_dvmNcgHandlePackedSwitch();
+int call_dvmNcgHandleSparseSwitch();
+int call_dvmJitHandlePackedSwitch();
+int call_dvmJitHandleSparseSwitch();
+int call_dvmJitToInterpTraceSelectNoChain();
+int call_dvmJitToPatchPredictedChain();
+void call_dvmJitToInterpNormal();
+/** @brief helper function to call dvmJitToInterpBackwardBranch */
+void call_dvmJitToInterpBackwardBranch();
+void call_dvmJitToInterpTraceSelect();
+int call_dvmQuasiAtomicSwap64();
+int call_dvmQuasiAtomicRead64();
+int call_dvmCanPutArrayElement();
+int call_dvmFindInterfaceMethodInCache();
+int call_dvmHandleStackOverflow();
+int call_dvmResolveString();
+int call_dvmResolveInstField();
+int call_dvmResolveStaticField();
+#ifdef WITH_SELF_VERIFICATION
+int call_selfVerificationLoad(void);
+int call_selfVerificationStore(void);
+int call_selfVerificationLoadDoubleword(void);
+int call_selfVerificationStoreDoubleword(void);
+#endif
+
+//labels and branches
+//shared branch to resolve class: 2 specialized versions
+//OPTION 1: call & ret
+//OPTION 2: store jump back label in a fixed register or memory
+//jump to .class_resolve, then jump back
+//OPTION 3: share translator code
+/* global variables: ncg_rPC */
+int resolve_class(
+                  int startLR/*logical register index*/, bool isPhysical, int tmp/*const pool index*/,
+                  int thirdArg);
+/* EXPORT_PC; movl exceptionPtr, -8(%esp); movl descriptor, -4(%esp); lea; call; lea; jmp */
+int throw_exception_message(int exceptionPtr, int obj_reg, bool isPhysical,
+                            int startLR/*logical register index*/, bool startPhysical);
+/* EXPORT_PC; movl exceptionPtr, -8(%esp); movl imm, -4(%esp); lea; call; lea; jmp */
+int throw_exception(int exceptionPtr, int imm,
+                    int startLR/*logical register index*/, bool startPhysical);
+
+void freeShortMap();
+int insertDataWorklist(s4 relativePC, char* codePtr1);
+#ifdef ENABLE_TRACING
+int insertMapWorklist(s4 BCOffset, s4 NCGOffset, int isStartOfPC);
+#endif
+int performNCGWorklist();
+int performDataWorklist();
+void performLabelWorklist();
+void performMethodLabelWorklist();
+void freeLabelMap();
+void performSharedWorklist();
+void performChainingWorklist();
+void freeNCGWorklist();
+void freeDataWorklist();
+void freeLabelWorklist();
+/** @brief search chainingWorklist to return instruction offset address in move instruction */
+char* searchChainingWorklist(unsigned int blockId);
+/** @brief search globalNCGWorklist to find the jmp/jcc offset address */
+char* searchNCGWorklist(int blockId);
+/** @brief search globalWorklist to find the jmp/jcc offset address */
+char* searchLabelWorklist(char* label);
+void freeChainingWorklist();
+
+int common_backwardBranch();
+int common_exceptionThrown();
+int common_errNullObject();
+int common_errArrayIndex();
+int common_errArrayStore();
+int common_errNegArraySize();
+int common_errNoSuchMethod();
+int common_errDivideByZero();
+int common_periodicChecks_entry();
+int common_periodicChecks4();
+int common_gotoBail(void);
+int common_gotoBail_0(void);
+int common_errStringIndexOutOfBounds();
+
+#if defined VTUNE_DALVIK
+void sendLabelInfoToVTune(int startStreamPtr, int endStreamPtr, const char* labelName);
+#endif
+
+// Delay VRs freeing if bytecode can throw exception, then call lowerByteCode
+int lowerByteCodeCanThrowCheck(const Method* method, const MIR * mir, const u2 * dalvikPC);
+//lower a bytecode
+int lowerByteCode(const Method* method, const MIR * mir, const u2 * dalvikPC);
+
+int op_nop(const MIR * mir);
+int op_move(const MIR * mir);
+int op_move_from16(const MIR * mir);
+int op_move_16(const MIR * mir);
+int op_move_wide(const MIR * mir);
+int op_move_wide_from16(const MIR * mir);
+int op_move_wide_16(const MIR * mir);
+int op_move_result(const MIR * mir);
+int op_move_result_wide(const MIR * mir);
+int op_move_exception(const MIR * mir);
+
+int op_return_void(const MIR * mir);
+int op_return(const MIR * mir);
+int op_return_wide(const MIR * mir);
+int op_const_4(const MIR * mir);
+int op_const_16(const MIR * mir);
+int op_const(const MIR * mir);
+int op_const_high16(const MIR * mir);
+int op_const_wide_16(const MIR * mir);
+int op_const_wide_32(const MIR * mir);
+int op_const_wide(const MIR * mir);
+int op_const_wide_high16(const MIR * mir);
+int op_const_string(const MIR * mir);
+int op_const_string_jumbo(const MIR * mir);
+int op_const_class(const MIR * mir);
+int op_monitor_enter(const MIR * mir);
+int op_monitor_exit(const MIR * mir);
+int op_check_cast(const MIR * mir);
+int op_instance_of(const MIR * mir);
+
+int op_array_length(const MIR * mir);
+int op_new_instance(const MIR * mir);
+int op_new_array(const MIR * mir);
+int op_filled_new_array(const MIR * mir);
+int op_filled_new_array_range(const MIR * mir);
+int op_fill_array_data(const MIR * mir, const u2 * dalvikPC);
+int op_throw(const MIR * mir);
+int op_throw_verification_error(const MIR * mir);
+int op_goto(const MIR * mir);
+int op_goto_16(const MIR * mir);
+int op_goto_32(const MIR * mir);
+int op_packed_switch(const MIR * mir, const u2 * dalvikPC);
+int op_sparse_switch(const MIR * mir, const u2 * dalvikPC);
+int op_if_ge(const MIR * mir);
+int op_aget(const MIR * mir);
+int op_aget_wide(const MIR * mir);
+int op_aget_object(const MIR * mir);
+int op_aget_boolean(const MIR * mir);
+int op_aget_byte(const MIR * mir);
+int op_aget_char(const MIR * mir);
+int op_aget_short(const MIR * mir);
+int op_aput(const MIR * mir);
+int op_aput_wide(const MIR * mir);
+int op_aput_object(const MIR * mir);
+int op_aput_boolean(const MIR * mir);
+int op_aput_byte(const MIR * mir);
+int op_aput_char(const MIR * mir);
+int op_aput_short(const MIR * mir);
+int op_iget(const MIR * mir);
+int op_iget_wide(const MIR * mir, bool isVolatile);
+int op_iget_object(const MIR * mir);
+int op_iget_boolean(const MIR * mir);
+int op_iget_byte(const MIR * mir);
+int op_iget_char(const MIR * mir);
+int op_iget_short(const MIR * mir);
+int op_iput(const MIR * mir);
+int op_iput_wide(const MIR * mir, bool isVolatile);
+int op_iput_object(const MIR * mir);
+int op_iput_boolean(const MIR * mir);
+int op_iput_byte(const MIR * mir);
+int op_iput_char(const MIR * mir);
+int op_iput_short(const MIR * mir);
+int op_sget(const MIR * mir);
+int op_sget_wide(const MIR * mir, bool isVolatile);
+int op_sget_object(const MIR * mir);
+int op_sget_boolean(const MIR * mir);
+int op_sget_byte(const MIR * mir);
+int op_sget_char(const MIR * mir);
+int op_sget_short(const MIR * mir);
+int op_sput(const MIR * mir, bool isObj);
+int op_sput_wide(const MIR * mir, bool isVolatile);
+int op_sput_object(const MIR * mir);
+int op_sput_boolean(const MIR * mir);
+int op_sput_byte(const MIR * mir);
+int op_sput_char(const MIR * mir);
+int op_sput_short(const MIR * mir);
+int op_invoke_virtual(const MIR * mir);
+int op_invoke_super(const MIR * mir);
+int op_invoke_direct(const MIR * mir);
+int op_invoke_static(const MIR * mir);
+int op_invoke_interface(const MIR * mir);
+int op_invoke_virtual_range(const MIR * mir);
+int op_invoke_super_range(const MIR * mir);
+int op_invoke_direct_range(const MIR * mir);
+int op_invoke_static_range(const MIR * mir);
+int op_invoke_interface_range(const MIR * mir);
+int op_int_to_long(const MIR * mir);
+int op_add_long_2addr(const MIR * mir);
+int op_add_int_lit8(const MIR * mir);
+int op_cmpl_float(const MIR * mir);
+int op_cmpg_float(const MIR * mir);
+int op_cmpl_double(const MIR * mir);
+int op_cmpg_double(const MIR * mir);
+int op_cmp_long(const MIR * mir);
+int op_if_eq(const MIR * mir);
+int op_if_ne(const MIR * mir);
+int op_if_lt(const MIR * mir);
+int op_if_gt(const MIR * mir);
+int op_if_le(const MIR * mir);
+int op_if_eqz(const MIR * mir);
+int op_if_nez(const MIR * mir);
+int op_if_ltz(const MIR * mir);
+int op_if_gez(const MIR * mir);
+int op_if_gtz(const MIR * mir);
+int op_if_lez(const MIR * mir);
+int op_neg_int(const MIR * mir);
+int op_not_int(const MIR * mir);
+int op_neg_long(const MIR * mir);
+int op_not_long(const MIR * mir);
+int op_neg_float(const MIR * mir);
+int op_neg_double(const MIR * mir);
+int op_int_to_float(const MIR * mir);
+int op_int_to_double(const MIR * mir);
+int op_long_to_int(const MIR * mir);
+int op_long_to_float(const MIR * mir);
+int op_long_to_double(const MIR * mir);
+int op_float_to_int(const MIR * mir);
+int op_float_to_long(const MIR * mir);
+int op_float_to_double(const MIR * mir);
+int op_double_to_int(const MIR * mir);
+int op_double_to_long(const MIR * mir);
+int op_double_to_float(const MIR * mir);
+int op_int_to_byte(const MIR * mir);
+int op_int_to_char(const MIR * mir);
+int op_int_to_short(const MIR * mir);
+int op_add_int(const MIR * mir);
+int op_sub_int(const MIR * mir);
+int op_mul_int(const MIR * mir);
+int op_div_int(const MIR * mir);
+int op_rem_int(const MIR * mir);
+int op_and_int(const MIR * mir);
+int op_or_int(const MIR * mir);
+int op_xor_int(const MIR * mir);
+int op_shl_int(const MIR * mir);
+int op_shr_int(const MIR * mir);
+int op_ushr_int(const MIR * mir);
+int op_add_long(const MIR * mir);
+int op_sub_long(const MIR * mir);
+int op_mul_long(const MIR * mir);
+int op_div_long(const MIR * mir);
+int op_rem_long(const MIR * mir);
+int op_and_long(const MIR * mir);
+int op_or_long(const MIR * mir);
+int op_xor_long(const MIR * mir);
+int op_shl_long(const MIR * mir);
+int op_shr_long(const MIR * mir);
+int op_ushr_long(const MIR * mir);
+int op_add_float(const MIR * mir);
+int op_sub_float(const MIR * mir);
+int op_mul_float(const MIR * mir);
+int op_div_float(const MIR * mir);
+int op_rem_float(const MIR * mir);
+int op_add_double(const MIR * mir);
+int op_sub_double(const MIR * mir);
+int op_mul_double(const MIR * mir);
+int op_div_double(const MIR * mir);
+int op_rem_double(const MIR * mir);
+int op_add_int_2addr(const MIR * mir);
+int op_sub_int_2addr(const MIR * mir);
+int op_mul_int_2addr(const MIR * mir);
+int op_div_int_2addr(const MIR * mir);
+int op_rem_int_2addr(const MIR * mir);
+int op_and_int_2addr(const MIR * mir);
+int op_or_int_2addr(const MIR * mir);
+int op_xor_int_2addr(const MIR * mir);
+int op_shl_int_2addr(const MIR * mir);
+int op_shr_int_2addr(const MIR * mir);
+int op_ushr_int_2addr(const MIR * mir);
+int op_sub_long_2addr(const MIR * mir);
+int op_mul_long_2addr(const MIR * mir);
+int op_div_long_2addr(const MIR * mir);
+int op_rem_long_2addr(const MIR * mir);
+int op_and_long_2addr(const MIR * mir);
+int op_or_long_2addr(const MIR * mir);
+int op_xor_long_2addr(const MIR * mir);
+int op_shl_long_2addr(const MIR * mir);
+int op_shr_long_2addr(const MIR * mir);
+int op_ushr_long_2addr(const MIR * mir);
+int op_add_float_2addr(const MIR * mir);
+int op_sub_float_2addr(const MIR * mir);
+int op_mul_float_2addr(const MIR * mir);
+int op_div_float_2addr(const MIR * mir);
+int op_rem_float_2addr(const MIR * mir);
+int op_add_double_2addr(const MIR * mir);
+int op_sub_double_2addr(const MIR * mir);
+int op_mul_double_2addr(const MIR * mir);
+int op_div_double_2addr(const MIR * mir);
+int op_rem_double_2addr(const MIR * mir);
+int op_add_int_lit16(const MIR * mir);
+int op_rsub_int(const MIR * mir);
+int op_mul_int_lit16(const MIR * mir);
+int op_div_int_lit16(const MIR * mir);
+int op_rem_int_lit16(const MIR * mir);
+int op_and_int_lit16(const MIR * mir);
+int op_or_int_lit16(const MIR * mir);
+int op_xor_int_lit16(const MIR * mir);
+int op_rsub_int_lit8(const MIR * mir);
+int op_mul_int_lit8(const MIR * mir);
+int op_div_int_lit8(const MIR * mir);
+int op_rem_int_lit8(const MIR * mir);
+int op_and_int_lit8(const MIR * mir);
+int op_or_int_lit8(const MIR * mir);
+int op_xor_int_lit8(const MIR * mir);
+int op_shl_int_lit8(const MIR * mir);
+int op_shr_int_lit8(const MIR * mir);
+int op_ushr_int_lit8(const MIR * mir);
+int op_execute_inline(const MIR * mir, bool isRange);
+int op_invoke_direct_empty(const MIR * mir);
+int op_iget_quick(const MIR * mir);
+int op_iget_wide_quick(const MIR * mir);
+int op_iget_object_quick(const MIR * mir);
+int op_iput_quick(const MIR * mir);
+int op_iput_wide_quick(const MIR * mir);
+int op_iput_object_quick(const MIR * mir);
+int op_invoke_virtual_quick(const MIR * mir);
+int op_invoke_virtual_quick_range(const MIR * mir);
+int op_invoke_super_quick(const MIR * mir);
+int op_invoke_super_quick_range(const MIR * mir);
+
+///////////////////////////////////////////////
+void set_reg_opnd(LowOpndReg* op_reg, int reg, bool isPhysical, LowOpndRegType type);
+void set_mem_opnd(LowOpndMem* mem, int disp, int base, bool isPhysical);
+void set_mem_opnd_scale(LowOpndMem* mem, int base, bool isPhysical, int disp, int index, bool indexPhysical, int scale);
+LowOpImm* dump_imm(Mnemonic m, OpndSize size, int imm);
+void dump_imm_update(int imm, char* codePtr, bool updateSecondOperand);
+LowOpBlock* dump_blockid_imm(Mnemonic m, int targetBlockId,
+        bool immediateNeedsAligned);
+LowOpMem* dump_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
+               int disp, int base_reg, bool isBasePhysical);
+LowOpReg* dump_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
+               int reg, bool isPhysical, LowOpndRegType type);
+LowOpReg* dump_reg_noalloc(Mnemonic m, OpndSize size,
+               int reg, bool isPhysical, LowOpndRegType type);
+LowOpImmMem* dump_imm_mem_noalloc(Mnemonic m, OpndSize size,
+                           int imm,
+                           int disp, int base_reg, bool isBasePhysical,
+                           MemoryAccessType mType, int mIndex);
+LowOpRegReg* dump_reg_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
+                   int reg, bool isPhysical,
+                   int reg2, bool isPhysical2, LowOpndRegType type);
+LowOpRegReg* dump_reg_reg_diff_types(Mnemonic m, AtomOpCode m2, OpndSize srcSize,
+                   int srcReg, int isSrcPhysical, LowOpndRegType srcType,
+                   OpndSize destSize, int destReg, int isDestPhysical,
+                   LowOpndRegType destType);
+LowOpRegReg* dump_movez_reg_reg(Mnemonic m, OpndSize size,
+                        int reg, bool isPhysical,
+                        int reg2, bool isPhysical2);
+LowOpMemReg* dump_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
+                   int disp, int base_reg, bool isBasePhysical,
+                   MemoryAccessType mType, int mIndex,
+                   int reg, bool isPhysical, LowOpndRegType type);
+LowOpMemReg* dump_mem_reg_noalloc(Mnemonic m, OpndSize size,
+                           int disp, int base_reg, bool isBasePhysical,
+                           MemoryAccessType mType, int mIndex,
+                           int reg, bool isPhysical, LowOpndRegType type);
+LowOpMemReg* dump_mem_scale_reg(Mnemonic m, OpndSize size,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         int reg, bool isPhysical, LowOpndRegType type);
+LowOpRegMem* dump_reg_mem_scale(Mnemonic m, OpndSize size,
+                         int reg, bool isPhysical,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         LowOpndRegType type);
+LowOpRegMem* dump_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
+                   int reg, bool isPhysical,
+                   int disp, int base_reg, bool isBasePhysical,
+                   MemoryAccessType mType, int mIndex, LowOpndRegType type);
+LowOpRegMem* dump_reg_mem_noalloc(Mnemonic m, OpndSize size,
+                           int reg, bool isPhysical,
+                           int disp, int base_reg, bool isBasePhysical,
+                           MemoryAccessType mType, int mIndex, LowOpndRegType type);
+LowOpImmReg* dump_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
+                   int imm, int reg, bool isPhysical, LowOpndRegType type, bool chaining);
+/**
+ * @brief generate a x86 instruction that takes one immediate and one physical reg operand
+ * @param m opcode mnemonic
+ * @param size width of the operand
+ * @param imm immediate value
+ * @param reg register number
+ * @param isPhysical TRUE if reg is a physical register, false otherwise
+ * @param type register type
+ * @return return a LowOp for immediate to register if scheduling is on, otherwise, return NULL
+ */
+LowOpImmReg* dump_imm_reg_noalloc(Mnemonic m, OpndSize size, int imm, int reg,
+                   bool isPhysical, LowOpndRegType type);
+LowOpImmMem* dump_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
+                   int imm,
+                   int disp, int base_reg, bool isBasePhysical,
+                   MemoryAccessType mType, int mIndex, bool chaining);
+LowOpRegMem* dump_fp_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+                  int disp, int base_reg, bool isBasePhysical,
+                  MemoryAccessType mType, int mIndex);
+LowOpMemReg* dump_mem_fp(Mnemonic m, AtomOpCode m2, OpndSize size,
+                  int disp, int base_reg, bool isBasePhysical,
+                  MemoryAccessType mType, int mIndex,
+                  int reg);
+LowOpLabel* dump_label(Mnemonic m, OpndSize size, int imm,
+               const char* label, bool isLocal);
+
+unsigned getJmpCallInstSize(OpndSize size, JmpCall_type type);
+bool lowerByteCodeJit(const Method* method, const u2* codePtr, MIR* mir);
+#if defined(WITH_JIT)
+bool lowerByteCodeJit(const Method* method, const MIR * mir, const u2 * dalvikPC);
+void startOfBasicBlock(struct BasicBlock* bb);
+extern struct BasicBlock* traceCurrentBB;
+extern JitMode traceMode;
+
+//Forward declarations
+class CompilationUnit_O1;
+class BasicBlock_O1;
+
+//Start of a trace call to reset certain elements
+void startOfTrace(const Method* method, int, CompilationUnit_O1*);
+
+//End of a trace call to reset certain elements
+void endOfTrace (CompilationUnit *cUnit);
+
+//Initiates all worklists to do their work
+void performWorklistWork (void);
+
+/**
+ * @brief Generates a conditional jump to taken child of current BB being generated.
+ * @details Implements semantics of "if" bytecode.
+ * @param takenCondition The condition for the taken branch
+ * @return Returns value >= 0 when successful and negative otherwise.
+ */
+int generateConditionalJumpToTakenBlock (ConditionCode takenCondition);
+
+LowOp* jumpToBasicBlock(char* instAddr, int targetId, bool targetIsChainingCell = false);
+LowOp* condJumpToBasicBlock(char* instAddr, ConditionCode cc, int targetId, bool immediateNeedsAligned = false);
+bool jumpToException(const char* target);
+int codeGenBasicBlockJit(const Method* method, BasicBlock* bb);
+void endOfBasicBlock(struct BasicBlock* bb);
+
+//Used to generate native code the extended MIRs
+bool handleExtendedMIR (CompilationUnit *cUnit, BasicBlock_O1 *bb, MIR *mir);
+
+int insertChainingWorklist(int bbId, char * codeStart);
+void startOfTraceO1(const Method* method, int exceptionBlockId, CompilationUnit *cUnit);
+/** @brief search globalMap to find the entry for the given label */
+char* findCodeForLabel(const char* label);
+/* Find a label offset given a BasicBlock index */
+int getLabelOffset (unsigned int bbIdx);
+#endif
+int isPowerOfTwo(int imm);
+
+/** @brief calculate the magic number and shift for a given divisor
+    @details For a division by a signed integer constant, we can always
+     find a magic number M and a shift S. Thus we can transform the div
+     operation to a serial of multiplies, adds, shifts. This function
+     is used to calcuate the magic number and shift for a given divisor.
+     For the detailed desrciption and proof of
+     this optimization, please refer to "Hacker's Delight", Henry S.
+     Warren, Jr., chapter 10.
+    @param divisor the given divisor we need to calculate
+    @param magic pointer to hold the magic number
+    @param shift pointer to hold the shift
+*/
+void calculateMagicAndShift(int divisor, int* magic, int* shift);
+
+void move_chain_to_mem(OpndSize size, int imm,
+                        int disp, int base_reg, bool isBasePhysical);
+void move_chain_to_reg(OpndSize size, int imm, int reg, bool isPhysical);
+
+bool isInMemory(int regNum, OpndSize size);
+int touchEbx();
+int boundCheck(int vr_array, int reg_array, bool isPhysical_array,
+               int vr_index, int reg_index, bool isPhysical_index,
+               int exceptionNum);
+int getRelativeOffset(const char* target, bool isShortTerm, JmpCall_type type, bool* unknown,
+                      OpndSize* immSize);
+int getRelativeNCG(s4 tmp, JmpCall_type type, bool* unknown, OpndSize* size);
+void freeAtomMem();
+OpndSize estOpndSizeFromImm(int target);
+
+//Preprocess a BasicBlock before being lowered
+int preprocessingBB (CompilationUnit *cUnit, BasicBlock *bb);
+/** @brief align the relative offset of jmp/jcc and movl within 16B */
+void alignOffset(int cond);
+
+/** @brief align a pointer to n-bytes aligned */
+char* align(char* addr, int n);
+bool doesJumpToBBNeedAlignment(BasicBlock * bb);
+
+/**
+ * @brief Architecture specific BasicBlock creator
+ * @details Initializes x86 specific BasicBlock fields
+ * @return newly created BasicBlock
+ */
+BasicBlock *x86StandAloneArchSpecificNewBB (void);
+
+/**
+ * @brief Architecture specific BasicBlock printing
+ * @param cUnit the CompilationUnit
+ * @param bb the BasicBlock
+ * @param file the File in which to dump the BasicBlock
+ * @param beforeMIRs is this call performed before generating the dumps for the MIRs
+ */
+void x86StandAloneArchSpecificDumpBB (CompilationUnit *cUnit, BasicBlock *bb, FILE *file, bool beforeMIRs);
+
+/**
+ * @brief Handle the invoke label
+ * @param value the form of the arguments
+ * @return the section label's name
+ */
+const char *dvmCompilerHandleInvokeArgsHeader (int value);
+
+void pushCallerSavedRegs(void);
+void popCallerSavedRegs(void);
+
+//Print the emitted code
+void printEmittedCodeBlock(unsigned char *startAddr, unsigned char *endAddr);
+
+#endif
diff --git a/vm/compiler/codegen/x86/lightcg/LowerAlu.cpp b/vm/compiler/codegen/x86/lightcg/LowerAlu.cpp
new file mode 100644
index 0000000..8678217
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/LowerAlu.cpp
@@ -0,0 +1,2557 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file LowerAlu.cpp
+    \brief This file lowers ALU bytecodes.
+*/
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "NcgAot.h"
+#include "enc_wrapper.h"
+
+/////////////////////////////////////////////
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecode neg-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_neg_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEG_INT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    alu_unary_reg(OpndSize_32, neg_opc, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode not-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_not_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NOT_INT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    alu_unary_reg(OpndSize_32, not_opc, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+#undef P_GPR_1
+
+/**
+ * @brief Generate native code for bytecode neg-long
+ * @details Implementation uses XMM registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_neg_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEG_LONG);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_64, 1, false);
+    alu_binary_reg_reg(OpndSize_64, xor_opc, 2, false, 2, false);
+    alu_binary_reg_reg(OpndSize_64, sub_opc, 1, false, 2, false);
+    set_virtual_reg(vA, OpndSize_64, 2, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode not-long
+ * @details Implementation uses XMM registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_not_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NOT_LONG);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_64, 1, false);
+    load_global_data_API("64bits", OpndSize_64, 2, false);
+    alu_binary_reg_reg(OpndSize_64, andn_opc, 2, false, 1, false);
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 0;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+/**
+ * @brief Generate native code for bytecode neg-float
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_neg_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEG_FLOAT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    alu_binary_imm_reg(OpndSize_32, add_opc, 0x80000000, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+#undef P_GPR_1
+
+/**
+ * @brief Generate native code for bytecode neg-double
+ * @details Implementation uses XMM registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_neg_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEG_DOUBLE);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_64, 1, false);
+    load_global_data_API("doubNeg", OpndSize_64, 2, false);
+    alu_binary_reg_reg(OpndSize_64, xor_opc, 2, false, 1, false);
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode int-to-long
+ * @details Implementation uses native instruction cdq
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_LONG);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, PhysicalReg_EAX, true);
+    convert_integer(OpndSize_32, OpndSize_64);
+    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+    set_virtual_reg(vA+1, OpndSize_32, PhysicalReg_EDX, true);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode int-to-float
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_FLOAT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    load_int_fp_stack_VR(OpndSize_32, vB); //fildl
+    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode int-to-double
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_DOUBLE);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    convert_int_to_fp(1, false, 2, false, true /* isDouble */);
+    set_virtual_reg(vA, OpndSize_64, 2, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode long-to-float
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_long_to_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_LONG_TO_FLOAT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    load_int_fp_stack_VR(OpndSize_64, vB); //fildll
+    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode long-to-double
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_long_to_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_LONG_TO_DOUBLE);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    load_int_fp_stack_VR(OpndSize_64, vB); //fildll
+    store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode float-to-double
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_float_to_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FLOAT_TO_DOUBLE);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    load_fp_stack_VR(OpndSize_32, vB); //flds
+    store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode double-to-float
+ * @details Implementation uses FP stack
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_double_to_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DOUBLE_TO_FLOAT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    load_fp_stack_VR(OpndSize_64, vB); //fldl
+    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
+    return 0;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+/**
+ * @brief Generate native code for bytecode long-to-int
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_long_to_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_LONG_TO_INT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+#undef P_GPR_1
+
+//! common code to convert a float or double to integer
+
+//! It uses FP stack
+int common_fp_to_int(bool isDouble, int vA, int vB) {
+    if(isDouble) {
+        load_fp_stack_VR(OpndSize_64, vB); //fldl
+    }
+    else {
+        load_fp_stack_VR(OpndSize_32, vB); //flds
+    }
+
+    load_fp_stack_global_data_API("intMax", OpndSize_32);
+    load_fp_stack_global_data_API("intMin", OpndSize_32);
+
+    //ST(0) ST(1) ST(2) --> LintMin LintMax value
+    compare_fp_stack(true, 2, false/*isDouble*/); //ST(2)
+    //ST(0) ST(1) --> LintMax value
+    conditional_jump(Condition_AE, ".float_to_int_negInf", true);
+    rememberState(1);
+    compare_fp_stack(true, 1, false/*isDouble*/); //ST(1)
+    //ST(0) --> value
+    rememberState(2);
+    conditional_jump(Condition_C, ".float_to_int_nanInf", true);
+    //fnstcw, orw, fldcw, xorw
+    load_effective_addr(-2, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    store_fpu_cw(false/*checkException*/, 0, PhysicalReg_ESP, true);
+    alu_binary_imm_mem(OpndSize_16, or_opc, 0xc00, 0, PhysicalReg_ESP, true);
+    load_fpu_cw(0, PhysicalReg_ESP, true);
+    alu_binary_imm_mem(OpndSize_16, xor_opc, 0xc00, 0, PhysicalReg_ESP, true);
+    store_int_fp_stack_VR(true/*pop*/, OpndSize_32, vA); //fistpl
+    //fldcw
+    load_fpu_cw(0, PhysicalReg_ESP, true);
+    load_effective_addr(2, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    rememberState(3);
+    unconditional_jump(".float_to_int_okay", true);
+    if (insertLabel(".float_to_int_nanInf", true) == -1)
+        return -1;
+    conditional_jump(Condition_NP, ".float_to_int_posInf", true);
+    //fstps CHECK
+    goToState(2);
+    store_fp_stack_VR(true, OpndSize_32, vA);
+    set_VR_to_imm(vA, OpndSize_32, 0);
+    transferToState(3);
+    unconditional_jump(".float_to_int_okay", true);
+    if (insertLabel(".float_to_int_posInf", true) == -1)
+        return -1;
+    //fstps CHECK
+    goToState(2);
+    store_fp_stack_VR(true, OpndSize_32, vA);
+    set_VR_to_imm(vA, OpndSize_32, 0x7fffffff);
+    transferToState(3);
+    unconditional_jump(".float_to_int_okay", true);
+    if (insertLabel(".float_to_int_negInf", true) == -1)
+        return -1;
+    goToState(1);
+    //fstps CHECK
+    store_fp_stack_VR(true, OpndSize_32, vA);
+    store_fp_stack_VR(true, OpndSize_32, vA);
+    set_VR_to_imm(vA, OpndSize_32, 0x80000000);
+    transferToState(3);
+    if (insertLabel(".float_to_int_okay", true) == -1)
+        return -1;
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode float-to-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_float_to_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FLOAT_TO_INT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    int retval = common_fp_to_int(false, vA, vB);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode double-to-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_double_to_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DOUBLE_TO_INT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    int retval = common_fp_to_int(true, vA, vB);
+    return retval;
+}
+
+//! common code to convert float or double to long
+
+//! It uses FP stack
+int common_fp_to_long(bool isDouble, int vA, int vB) {
+    if(isDouble) {
+        load_fp_stack_VR(OpndSize_64, vB); //fldl
+    }
+    else {
+        load_fp_stack_VR(OpndSize_32, vB); //flds
+    }
+
+    //Check if it is the special Negative Infinity value
+    load_fp_stack_global_data_API("valueNegInfLong", OpndSize_64);
+    //Stack status: ST(0) ST(1) --> LlongMin value
+    compare_fp_stack(true, 1, false/*isDouble*/); // Pops ST(1)
+    conditional_jump(Condition_AE, ".float_to_long_negInf", true);
+    rememberState(1);
+
+    //Check if it is the special Positive Infinity value
+    load_fp_stack_global_data_API("valuePosInfLong", OpndSize_64);
+    //Stack status: ST(0) ST(1) --> LlongMax value
+    compare_fp_stack(true, 1, false/*isDouble*/); // Pops ST(1)
+    rememberState(2);
+    conditional_jump(Condition_C, ".float_to_long_nanInf", true);
+
+    //Normal Case
+    //We want to truncate to 0 for conversion. That will be rounding mode 0x11
+    load_effective_addr(-2, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    store_fpu_cw(false/*checkException*/, 0, PhysicalReg_ESP, true);
+    //Change control word to rounding mode 11:
+    alu_binary_imm_mem(OpndSize_16, or_opc, 0xc00, 0, PhysicalReg_ESP, true);
+    //Load the control word
+    load_fpu_cw(0, PhysicalReg_ESP, true);
+    //Reset the control word
+    alu_binary_imm_mem(OpndSize_16, xor_opc, 0xc00, 0, PhysicalReg_ESP, true);
+    //Perform the actual conversion
+    store_int_fp_stack_VR(true/*pop*/, OpndSize_64, vA); //fistpll
+    // Restore the original control word
+    load_fpu_cw(0, PhysicalReg_ESP, true);
+    load_effective_addr(2, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    rememberState(3);
+    /* NOTE: We do not need to pop out the original value we pushed
+     * since load_fpu_cw above already clears the stack for
+     * normal values.
+     */
+    unconditional_jump(".float_to_long_okay", true);
+
+    //We can be here for positive infinity or NaN. Check parity bit
+    if (insertLabel(".float_to_long_nanInf", true) == -1)
+        return -1;
+    conditional_jump(Condition_NP, ".float_to_long_posInf", true);
+    goToState(2);
+    //Save corresponding Long NaN value
+    load_global_data_API("valueNanLong", OpndSize_64, 1, false);
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    transferToState(3);
+    //Pop out the original value we pushed
+    compare_fp_stack(true, 0, false/*isDouble*/); //ST(0)
+    unconditional_jump(".float_to_long_okay", true);
+
+    if (insertLabel(".float_to_long_posInf", true) == -1)
+        return -1;
+    goToState(2);
+    //Save corresponding Long Positive Infinity value
+    load_global_data_API("valuePosInfLong", OpndSize_64, 2, false);
+    set_virtual_reg(vA, OpndSize_64, 2, false);
+    transferToState(3);
+    //Pop out the original value we pushed
+    compare_fp_stack(true, 0, false/*isDouble*/); //ST(0)
+    unconditional_jump(".float_to_long_okay", true);
+
+    if (insertLabel(".float_to_long_negInf", true) == -1)
+        return -1;
+    //fstpl
+    goToState(1);
+    //Load corresponding Long Negative Infinity value
+    load_global_data_API("valueNegInfLong", OpndSize_64, 3, false);
+    set_virtual_reg(vA, OpndSize_64, 3, false);
+    transferToState(3);
+    //Pop out the original value we pushed
+    compare_fp_stack(true, 0, false/*isDouble*/); //ST(0)
+
+    if (insertLabel(".float_to_long_okay", true) == -1)
+        return -1;
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode float-to-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_float_to_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FLOAT_TO_LONG);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    int retval = common_fp_to_long(false, vA, vB);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode double-to-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_double_to_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DOUBLE_TO_LONG);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    int retval = common_fp_to_long(true, vA, vB);
+    return retval;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+/**
+ * @brief Generate native code for bytecode int-to-byte
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_BYTE);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    moves_reg_to_reg(OpndSize_8, 1, false, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode int-to-char
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_CHAR);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    alu_binary_imm_reg(OpndSize_32, sal_opc, 16, 1, false);
+    alu_binary_imm_reg(OpndSize_32, shr_opc, 16, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode int-to-short
+ * @details Implementation uses general purpose registers
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_int_to_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INT_TO_SHORT);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    moves_reg_to_reg(OpndSize_16, 1, false, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+//! common code to handle integer ALU ops
+
+//! It uses GPR
+int common_alu_int(ALU_Opcode opc, int vA, int v1, int v2) { //except div and rem
+    get_virtual_reg(v1, OpndSize_32, 1, false);
+    //in encoder, reg is first operand, which is the destination
+    //gpr_1 op v2(rFP) --> gpr_1
+    //shift only works with reg cl, v2 should be in %ecx
+    alu_binary_VR_reg(OpndSize_32, opc, v2, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+#undef P_GPR_1
+#define P_GPR_1 PhysicalReg_EBX
+//! common code to handle integer shift ops
+
+//! It uses GPR
+int common_shift_int(ALU_Opcode opc, int vA, int v1, int v2) {
+    get_virtual_reg(v2, OpndSize_32, PhysicalReg_ECX, true);
+    get_virtual_reg(v1, OpndSize_32, 1, false);
+    //in encoder, reg2 is first operand, which is the destination
+    //gpr_1 op v2(rFP) --> gpr_1
+    //shift only works with reg cl, v2 should be in %ecx
+    alu_binary_reg_reg(OpndSize_32, opc, PhysicalReg_ECX, true, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+#undef p_GPR_1
+
+/**
+ * @brief Generate native code for bytecode add-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_int(add_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode sub-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_int(sub_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_int(imul_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode and-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_int(and_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode or-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_int(or_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode xor-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_int(xor_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode shl-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_shift_int(shl_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode shr-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_shift_int(sar_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode ushr-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_shift_int(shr_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode add-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_INT_2ADDR);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = vA;
+    v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_int(add_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode sub-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_INT_2ADDR);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = vA;
+    v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_int(sub_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_INT_2ADDR);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = vA;
+    v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_int(imul_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode and-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_INT_2ADDR);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = vA;
+    v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_int(and_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode or-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_INT_2ADDR);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = vA;
+    v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_int(or_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode xor-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_INT_2ADDR);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = vA;
+    v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_int(xor_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode shl-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_INT_2ADDR);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = vA;
+    v2 = mir->dalvikInsn.vB;
+    int retval = common_shift_int(shl_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode shr-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_INT_2ADDR);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = vA;
+    v2 = mir->dalvikInsn.vB;
+    int retval = common_shift_int(sar_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode ushr-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_INT_2ADDR);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = vA;
+    v2 = mir->dalvikInsn.vB;
+    int retval = common_shift_int(shr_opc, vA, v1, v2);
+    return retval;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+/**
+ * @brief common code to handle integer DIV & REM, it used GPR
+ *  If the divisor is a constant at compiler time, use the algorithm from "Hacker's Delight", Henry S.
+ *  Warren, Jr., chapter 10. to simplify the code.
+ *  The special case: when op0 == minint && op1 == -1, return 0 for isRem, return 0x80000000 for isDiv
+ *  There are four merge points in the control flow for this bytecode
+ *  make sure the reg. alloc. state is the same at merge points by calling transferToState
+ * @param isRem true for REM, false for DIV
+ * @param vA the destination VR
+ * @param v1 the source VR for numerator
+ * @param v2 the source VR for divisor
+ * @return value >= 0 when handled
+ */
+int common_div_rem_int(bool isRem, int vA, int v1, int v2) {
+    get_virtual_reg(v1, OpndSize_32, PhysicalReg_EAX, true);
+    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EDX, true);
+    get_virtual_reg(v2, OpndSize_32, 2, false);
+
+    // Handle the div 0 case
+    compare_imm_reg(OpndSize_32, 0, 2, false);
+    handlePotentialException(
+                                       Condition_E, Condition_NE,
+                                       1, "common_errDivideByZero");
+
+    //Check if numerator is 0
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    rememberState(2);
+    conditional_jump(Condition_Z, ".common_div_rem_int_divdone", true);
+
+    transferToState(1);
+
+    // Handle the case where the divisor is a constant at compile time
+    int divisor[2];
+    int isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, divisor, false);
+    // For now we just use the generic code generation for division by -1
+    if (isConst > 0 && divisor[0] != -1) {
+        if (divisor[0] == 0) {
+            // Division by zero: we don't need to generate the code at all.
+            transferToState(2);
+            return 0;
+        } else if (divisor[0] != 1) { // There is nothing to do with division by 1
+            int magic;
+            int shift;
+            calculateMagicAndShift(divisor[0], &magic, &shift);
+
+            // According to H.S.Warren's Hacker's Delight Chapter 10 and
+            // T,Grablund, P.L.Montogomery's Division by invariant integers using multiplication
+            // For a integer divided by a constant,
+            // we can always find a magic number M and a shift S. Thus,
+            // For d >= 2,
+            //     int(n/d) = floor(n/d) = floor(M*n/2^S), while n > 0
+            //     int(n/d) = ceil(n/d) = floor(M*n/2^S) +1, while n < 0.
+            // For d <= -2,
+            //     int(n/d) = ceil(n/d) = floor(M*n/2^S) +1 , while n > 0
+            //     int(n/d) = floor(n/d) = floor(M*n/2^S), while n < 0.
+            // We implement this algorithm in the following way:
+            // 1. multiply magic number m and numerator n, get the higher 32bit result in EDX
+            // 2. if divisor > 0 and magic < 0, add numerator to EDX
+            //    if divisor < 0 and magic > 0, sub numerator to EDX
+            // 3. if S !=0, SAR S bits for EDX
+            // 4. add 1 to EDX if EDX < 0
+            // 5. Thus, EDX is the quotient
+
+            // mov %eax, %tmp1
+            // mov magic, %tmp2
+            // imul %tmp2
+
+            move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 1, false);
+            move_imm_to_reg(OpndSize_32, magic, 2, false);
+            alu_unary_reg(OpndSize_32, imul_opc, 2, false);
+            // v2>0 && M<0
+            if (divisor[0] > 0 && magic < 0){
+                alu_binary_reg_reg(OpndSize_32, add_opc, 1, false, PhysicalReg_EDX, true);
+            }else if (divisor[0] < 0 && magic > 0){
+            // v2<0 && M>0
+                alu_binary_reg_reg(OpndSize_32, sub_opc, 1, false, PhysicalReg_EDX, true);
+            }
+            // sarl shift, %edx
+            if (shift != 0)
+                alu_binary_imm_reg(OpndSize_32, sar_opc, shift, PhysicalReg_EDX, true);
+            // mov %edx, %tmp2
+            // shrl 31, %edx
+            // add %tmp2, %edx
+            move_reg_to_reg(OpndSize_32, PhysicalReg_EDX, true, 2, false);
+            alu_binary_imm_reg(OpndSize_32, shr_opc, 31, PhysicalReg_EDX, true);
+            alu_binary_reg_reg(OpndSize_32, add_opc, 2, false, PhysicalReg_EDX, true);
+
+            if (isRem == true) {
+                 //mov %edx, %eax
+                 //mov v2, %tmp2
+                 //imul %tmp2
+                 //sub %eax, %tmp1
+                 //mov %tmp1, edx
+                move_reg_to_reg(OpndSize_32, PhysicalReg_EDX, true, PhysicalReg_EAX, true);
+                move_imm_to_reg(OpndSize_32, divisor[0], 2, false);
+                alu_unary_reg(OpndSize_32, imul_opc, 2, false);
+                alu_binary_reg_reg(OpndSize_32, sub_opc, PhysicalReg_EAX, true, 1, false);
+                move_reg_to_reg(OpndSize_32, 1, false, PhysicalReg_EDX, true);
+            }
+        }
+    } else { //It is a general case. Both divisor and numerator are variables.
+ 
+        //Find out Numerator | Denominator
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 3, false);
+        alu_binary_reg_reg(OpndSize_32, or_opc, 2, false, 3, false);
+
+        //If both arguments are less than 8-bits (and positive), do 8-bit divide
+        test_imm_reg(OpndSize_32, 0xFFFFFF00, 3, false);
+
+        rememberState(3);
+        conditional_jump(Condition_Z, ".common_div_rem_int_8", true);
+
+        //If both arguments are less than 16-bits (and positive), do 16-bit divide
+        test_imm_reg(OpndSize_32, 0xFFFF0000, 3, false);
+        conditional_jump(Condition_Z, ".common_div_rem_int_16", true);
+
+        //Handle special cases:
+        //0x80000000 / -1 should result in a quotient of 0x80000000
+        //and a remainder of 0.
+        //Check for -1:
+        compare_imm_reg(OpndSize_32, -1, 2, false);
+        rememberState(4);
+        conditional_jump(Condition_NE, ".common_div_rem_int_32", true);
+        //Check for 0x80000000 (MinInt)
+        compare_imm_reg(OpndSize_32, 0x80000000, PhysicalReg_EAX, true);
+        //Special case, no division is needed.
+        //We set the quotient to 0x800000000 (EAX is already that),
+        //and remainder to 0
+        transferToState(2);
+        conditional_jump(Condition_E, ".common_div_rem_int_divdone", true);
+
+
+        goToState(4);
+        if (insertLabel(".common_div_rem_int_32", true) == -1) //merge point
+            return -1;
+        convert_integer(OpndSize_32, OpndSize_64); //cdq
+        //idiv: dividend in edx:eax; quotient in eax; remainder in edx
+        alu_unary_reg(OpndSize_32, idiv_opc, 2, false);
+        transferToState(2);
+        unconditional_jump(".common_div_rem_int_divdone", true);
+
+        //Do 8-bit unsigned divide:
+        //div: dividend in ax; quotient in al; remainder in ah
+        //We are forced to use a hard-coded register, since the register allocator
+        //can allocate a register not capable of 8-bit operation, like ESI,
+        //which will cause undefined behaviour.
+        goToState(3);
+        if (insertLabel(".common_div_rem_int_8", true) == -1)
+            return -1;
+        move_reg_to_reg(OpndSize_32, 2, false, 4, false);
+        alu_unary_reg(OpndSize_8, div_opc, 4, false);
+        if (isRem) {
+            move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_EDX, true);
+            alu_binary_imm_reg(OpndSize_32, shr_opc, 8, PhysicalReg_EDX, true);
+        } else
+            alu_binary_imm_reg(OpndSize_32, and_opc, 0x000000FF, PhysicalReg_EAX, true);
+        transferToState(2);
+        unconditional_jump(".common_div_rem_int_divdone", true);
+
+        //Do 16-bit divide:
+        //div: dividend in dx:ax; quotient in ax; remainder in dx
+        goToState(3);
+        if (insertLabel(".common_div_rem_int_16", true) == -1)
+            return -1;
+        alu_unary_reg(OpndSize_16, div_opc, 2, false);
+    }
+
+    transferToState(2);
+    if (insertLabel(".common_div_rem_int_divdone", true) == -1)
+        return -1;
+    if(isRem)
+        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EDX, true);
+    else //divide: quotient in %eax
+        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+
+    return 0;
+}
+#undef P_GPR_1
+
+/**
+ * @brief Generate native code for bytecode div-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int(false, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode rem-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_INT);
+    int vA, v1, v2;
+    vA = mir->dalvikInsn.vA;
+    v1 = mir->dalvikInsn.vB;
+    v2 = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int(true, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode div-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_INT_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_div_rem_int(false, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode rem-int/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_int_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_INT_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_div_rem_int(true, vA, v1, v2);
+    return retval;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+/**
+ * @brief Common function to handle alu operations involving literals
+ * @param opc The Opcode to perform
+ * @param vA The destination VR
+ * @param vB The source VR
+ * @param imm The literal value
+ * @return value >= 0 when handled
+ */
+int common_alu_int_lit(ALU_Opcode opc, int vA, int vB, s2 imm) { //except div and rem
+    // For add and sub, try if we can operate directly on VRs
+    if ((opc == add_opc) || (opc == sub_opc)) {
+        bool success = alu_imm_to_VR(OpndSize_32, opc, vB, vA, imm, 1, false, NULL);
+        //If succeeded, we are done
+        if (success == true) {
+            return 0;
+        }
+        //Otherwise, go the normal path
+    }
+
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    alu_binary_imm_reg(OpndSize_32, opc, imm, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+
+//! calls common_alu_int_lit
+int common_shift_int_lit(ALU_Opcode opc, int vA, int vB, s2 imm) {
+    return common_alu_int_lit(opc, vA, vB, imm);
+}
+#undef p_GPR_1
+
+/**
+ * @brief Generate native code for bytecode add-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_INT_LIT16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(add_opc, vA, vB, literal);
+    return retval;
+}
+
+int alu_rsub_int(ALU_Opcode opc, int vA, s2 imm, int vB) {
+    move_imm_to_reg(OpndSize_32, imm, 2, false);
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    alu_binary_reg_reg(OpndSize_32, opc, 1, false, 2, false);
+    set_virtual_reg(vA, OpndSize_32, 2, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode rsub-int
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rsub_int(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RSUB_INT);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = alu_rsub_int(sub_opc, vA, literal, vB);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_INT_LIT16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(imul_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode and-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_INT_LIT16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(and_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode or-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_INT_LIT16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(or_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode xor-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_INT_LIT16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(xor_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode add-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+
+    //try if we can operate directly on VRs
+    bool success = alu_imm_to_VR(OpndSize_32, add_opc, vB, vA, literal, 1, false, mir);
+
+    //If succeeded, we are done
+    if (success == true) {
+        return 0;
+    }
+
+    //Otherwise, go the normal path
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    alu_binary_imm_reg(OpndSize_32, add_opc, literal, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode rsub-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rsub_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RSUB_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = alu_rsub_int(sub_opc, vA, literal, vB);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(imul_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode and-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(and_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode or-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(or_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode xor-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_alu_int_lit(xor_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode shl-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_shift_int_lit(shl_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode shr-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_shift_int_lit(sar_opc, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode ushr-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_shift_int_lit(shr_opc, vA, vB, literal);
+    return retval;
+}
+
+int isPowerOfTwo(int imm) {
+    int i;
+    for(i = 1; i < 17; i++) {
+        if(imm == (1 << i)) return i;
+    }
+    return -1;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+int div_lit_strength_reduction(int vA, int vB, s2 imm) {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        //strength reduction for div by 2,4,8,...
+        int power = isPowerOfTwo(imm);
+        if(power < 1) return 0;
+        //tmp2 is not updated, so it can share with vB
+        get_virtual_reg(vB, OpndSize_32, 2, false);
+        //if imm is 2, power will be 1
+        if(power == 1) {
+            /* mov tmp1, tmp2
+               shrl $31, tmp1
+               addl tmp2, tmp1
+               sarl $1, tmp1 */
+            move_reg_to_reg(OpndSize_32, 2, false, 1, false);
+            alu_binary_imm_reg(OpndSize_32, shr_opc, 31, 1, false);
+            alu_binary_reg_reg(OpndSize_32, add_opc, 2, false, 1, false);
+            alu_binary_imm_reg(OpndSize_32, sar_opc, 1, 1, false);
+            set_virtual_reg(vA, OpndSize_32, 1, false);
+            return 1;
+        }
+        //power > 1
+        /* mov tmp1, tmp2
+           sarl $power-1, tmp1
+           shrl 32-$power, tmp1
+           addl tmp2, tmp1
+           sarl $power, tmp1 */
+        move_reg_to_reg(OpndSize_32, 2, false, 1, false);
+        alu_binary_imm_reg(OpndSize_32, sar_opc, power-1, 1, false);
+        alu_binary_imm_reg(OpndSize_32, shr_opc, 32-power, 1, false);
+        alu_binary_reg_reg(OpndSize_32, add_opc, 2, false, 1, false);
+        alu_binary_imm_reg(OpndSize_32, sar_opc, power, 1, false);
+        set_virtual_reg(vA, OpndSize_32, 1, false);
+        return 1;
+    }
+    return 0;
+}
+
+////////// throws exception!!!
+//! common code to handle integer DIV & REM with literal
+
+//! It uses GPR
+int common_div_rem_int_lit(bool isRem, int vA, int vB, s2 imm) {
+    if(!isRem) {
+        int retCode = div_lit_strength_reduction(vA, vB, imm);
+        if(retCode > 0) return 0;
+    }
+    if(imm == 0) {
+        export_pc(); //use %edx
+#ifdef DEBUG_EXCEPTION
+        ALOGI("EXTRA code to handle exception");
+#endif
+        beforeCall("exception"); //dump GG, GL VRs
+        unconditional_jump_global_API(
+                          "common_errDivideByZero", false);
+
+        return 0;
+    }
+    get_virtual_reg(vB, OpndSize_32, PhysicalReg_EAX, true);
+    //check against -1 for DIV_INT??
+    if(imm == -1) {
+        compare_imm_reg(OpndSize_32, 0x80000000, PhysicalReg_EAX, true);
+        conditional_jump(Condition_E, ".div_rem_int_lit_special", true);
+        rememberState(1);
+    }
+    move_imm_to_reg(OpndSize_32, imm, 2, false);
+    convert_integer(OpndSize_32, OpndSize_64); //cdq
+    //idiv: dividend in edx:eax; quotient in eax; remainder in edx
+    alu_unary_reg(OpndSize_32, idiv_opc, 2, false);
+    if(isRem)
+        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EDX, true);
+    else
+        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+
+    if(imm == -1) {
+        unconditional_jump(".div_rem_int_lit_okay", true);
+        rememberState(2);
+
+        if (insertLabel(".div_rem_int_lit_special", true) == -1)
+            return -1;
+        goToState(1);
+        if(isRem)
+            set_VR_to_imm(vA, OpndSize_32, 0);
+        else
+            set_VR_to_imm(vA, OpndSize_32, 0x80000000);
+        transferToState(2);
+    }
+
+    if (insertLabel(".div_rem_int_lit_okay", true) == -1)
+        return -1; //merge point 2
+    return 0;
+}
+#undef P_GPR_1
+
+/**
+ * @brief Generate native code for bytecode div-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_INT_LIT16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int_lit(false, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode rem-int/lit16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_int_lit16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_INT_LIT16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int_lit(true, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode div-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int_lit(false, vA, vB, literal);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode rem-int/lit8
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_int_lit8(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_INT_LIT8);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    s2 literal = mir->dalvikInsn.vC;
+    int retval = common_div_rem_int_lit(true, vA, vB, literal);
+    return retval;
+}
+//! common code to hanle long ALU ops
+
+//! It uses XMM
+//!all logical operations and sub operation of long type (no add mul div or rem)
+int common_alu_long(ALU_Opcode opc, int vA, int v1, int v2) {
+    int value[2];
+    int isConst = isVirtualRegConstant(v2, LowOpndRegType_xmm, value, false);
+
+    get_virtual_reg(v1, OpndSize_64, 1, false);
+    if (isConst == 3) {                                           //operate on constants stored in code stream
+        alu_binary_VR_reg(OpndSize_64, opc, v2, 1, false);        //opc const, XMM
+    } else {
+        get_virtual_reg(v2, OpndSize_64, 2, false);               //operate on XMM registers
+        alu_binary_reg_reg(OpndSize_64, opc, 2, false, 1, false); //opc XMM, XMM
+    }
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 0;
+}
+
+//! Use general purpose registers during the lowering for add-long and add-long/2addr
+int common_add_long(int vA, int v1, int v2) {
+    get_virtual_reg(v1, OpndSize_32, 1, false);
+    get_virtual_reg(v1+1, OpndSize_32, 2, false);
+    alu_binary_VR_reg(OpndSize_32, add_opc, v2, 1, false);
+    alu_binary_VR_reg(OpndSize_32, adc_opc, (v2+1), 2, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    set_virtual_reg(vA+1, OpndSize_32, 2, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode add-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_add_long(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode sub-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_long(sub_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode and-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_long(and_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode or-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_long(or_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode xor-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_long(xor_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode add-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_add_long(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode sub-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_long(sub_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode and-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_and_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AND_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_long(and_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode or-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_or_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_OR_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_long(or_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode xor-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_xor_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_XOR_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_long(xor_opc, vA, v1, v2);
+    return retval;
+}
+
+//signed vs unsigned imul and mul?
+//! common code to handle multiplication of long
+
+//! It uses GPR
+int common_mul_long(int vA, int v1, int v2) {
+    get_virtual_reg(v2, OpndSize_32, 1, false);
+    move_reg_to_reg(OpndSize_32, 1, false, PhysicalReg_EAX, true);
+    //imul: 2L * 1H update temporary 1
+    alu_binary_VR_reg(OpndSize_32, imul_opc, (v1+1), 1, false);
+    get_virtual_reg(v1, OpndSize_32, 3, false);
+    move_reg_to_reg(OpndSize_32, 3, false, 2, false);
+    //imul: 1L * 2H
+    alu_binary_VR_reg(OpndSize_32, imul_opc, (v2+1), 2, false);
+    alu_binary_reg_reg(OpndSize_32, add_opc, 2, false, 1, false);
+    alu_unary_reg(OpndSize_32, mul_opc, 3, false);
+    alu_binary_reg_reg(OpndSize_32, add_opc, PhysicalReg_EDX, true, 1, false);
+    set_virtual_reg(vA+1, OpndSize_32, 1, false);
+    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+    return 0;
+}
+
+//! common code to handle multiplication when multiplicands of long type are the same
+
+//! It uses GPR
+int common_mul_long_square(int vA, int v1) {
+    get_virtual_reg(v1, OpndSize_32, 1, false);
+    move_reg_to_reg(OpndSize_32, 1, false, PhysicalReg_EAX, true);
+    move_reg_to_reg(OpndSize_32,1, false, PhysicalReg_EDX, true);
+    //imul: 1L * 1H update temporary 1.
+    //same as 2L * 1H or 1L * 2H, thus eliminating need for second imul.
+    alu_binary_VR_reg(OpndSize_32, imul_opc, (v1+1), 1, false);
+    alu_binary_reg_reg(OpndSize_32, add_opc, 1, false, 1, false);
+    alu_unary_reg(OpndSize_32, mul_opc, PhysicalReg_EDX, true);
+    alu_binary_reg_reg(OpndSize_32, add_opc, PhysicalReg_EDX, true, 1, false);
+    set_virtual_reg(vA+1, OpndSize_32, 1, false);
+    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-long
+ * @details when multiplicands are same, use special case for square
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval;
+    if (v1 != v2){
+      retval = common_mul_long(vA, v1, v2);
+    }
+    else{
+      retval = common_mul_long_square(vA, v1);
+    }
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-long/2addr
+ * @details when multiplicands are same, use special case for square
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval;
+    if (v1 != v2){
+      retval = common_mul_long(vA, v1, v2);
+    }
+    else{
+      retval = common_mul_long_square(vA, v1);
+    }
+    return retval;
+}
+
+//! common code to handle DIV & REM of long
+
+//! It uses GPR & XMM; and calls call_moddi3 & call_divdi3
+int common_div_rem_long(bool isRem, int vA, int v1, int v2) {
+    get_virtual_reg(v2, OpndSize_32, 1, false);
+    get_virtual_reg(v2+1, OpndSize_32, 2, false);
+    //save to native stack before changing register 1, esp-8 is unused area
+    move_reg_to_mem(OpndSize_32, 1, false, 8-16, PhysicalReg_ESP, true);
+    alu_binary_reg_reg(OpndSize_32, or_opc, 2, false, 1, false);
+
+    handlePotentialException(
+                                       Condition_E, Condition_NE,
+                                       1, "common_errDivideByZero");
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 2, false, 12, PhysicalReg_ESP, true);
+    get_virtual_reg(v1, OpndSize_64, 1, false);
+    move_reg_to_mem(OpndSize_64, 1, false, 0, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    nextVersionOfHardReg(PhysicalReg_EDX, 2); //next version has 2 refs
+    if(isRem)
+        call_moddi3();
+    else
+        call_divdi3();
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    set_virtual_reg(vA+1, OpndSize_32,PhysicalReg_EDX, true);
+    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode div-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_div_rem_long(false, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode rem-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_div_rem_long(true, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode div-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_div_rem_long(false, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode rem-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_div_rem_long(true, vA, v1, v2);
+    return retval;
+}
+
+//! common code to handle SHL long
+
+//! It uses XMM
+int common_shl_long(int vA, int v1, int v2) {
+    get_VR_ss(v2, 2, false);
+    get_virtual_reg(v1, OpndSize_64, 1, false);
+
+    int value[2];
+    int isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, value, false); //do not update refCount
+    if (isConst == 3) {                          // case where shift amount is available
+        int shiftImm = (0x3f) & (value[0]); // compute masked shift amount statically
+        alu_binary_imm_reg(OpndSize_64, sll_opc, shiftImm, 1, false);
+    } else {                                // case where shift count to be read from VR
+        load_global_data_API("shiftMask", OpndSize_64, 3, false);
+        alu_binary_reg_reg(OpndSize_64, and_opc, 3, false, 2, false);
+        alu_binary_reg_reg(OpndSize_64, sll_opc, 2, false, 1, false);
+    }
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 0;
+}
+
+//! common code to handle SHR long
+
+//! It uses XMM
+int common_shr_long(int vA, int v1, int v2) {
+    get_VR_ss(v2, 2, false);
+
+    load_global_data_API("shiftMask", OpndSize_64, 3, false);
+
+    get_virtual_reg(v1, OpndSize_64, 1, false);
+    alu_binary_reg_reg(OpndSize_64, and_opc, 3, false, 2, false);
+    alu_binary_reg_reg(OpndSize_64, srl_opc, 2, false, 1, false);
+    compare_imm_VR(OpndSize_32, 0, (v1+1));
+    conditional_jump(Condition_GE, ".common_shr_long_special", true);
+    rememberState(1);
+
+    load_global_data_API("value64", OpndSize_64, 4, false);
+
+    alu_binary_reg_reg(OpndSize_64, sub_opc, 2, false, 4, false);
+
+    load_global_data_API("64bits", OpndSize_64, 5, false);
+
+    alu_binary_reg_reg(OpndSize_64, sll_opc, 4, false, 5, false);
+    alu_binary_reg_reg(OpndSize_64, or_opc, 5, false, 1, false);
+    rememberState(2);
+    //check whether the target is next instruction TODO
+    unconditional_jump(".common_shr_long_done", true);
+
+    if (insertLabel(".common_shr_long_special", true) == -1)
+        return -1;
+    goToState(1);
+    transferToState(2);
+    if (insertLabel(".common_shr_long_done", true) == -1)
+        return -1;
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 0;
+}
+
+//! common code to handle USHR long
+
+//! It uses XMM
+int common_ushr_long(int vA, int v1, int v2) {
+    get_VR_sd(v1, 1, false);
+    get_VR_ss(v2, 2, false);
+
+    int value[2];
+    int isConst = isVirtualRegConstant(v2, LowOpndRegType_gp, value, false); //do not update refCount
+    if (isConst == 3) {                     // case where shift amount is available
+        int shiftImm = (0x3f) & (value[0]); // compute masked shift amount statically
+        alu_binary_imm_reg(OpndSize_64, srl_opc, shiftImm, 1, false);
+    } else {                                // case where shift count to be read from VR
+        load_sd_global_data_API("shiftMask", 3, false);
+        alu_binary_reg_reg(OpndSize_64, and_opc, 3, false, 2, false);
+        alu_binary_reg_reg(OpndSize_64, srl_opc, 2, false, 1, false);
+    }
+    set_VR_sd(vA, 1, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode shl-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_shl_long(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode shl-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shl_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHL_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_shl_long(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode shr-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_shr_long(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode shr-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_shr_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SHR_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_shr_long(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode ushr-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_ushr_long(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode ushr-long/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_ushr_long_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_USHR_LONG_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_ushr_long(vA, v1, v2);
+    return retval;
+}
+#define USE_MEM_OPERAND
+///////////////////////////////////////////
+//! common code to handle ALU of floats
+
+//! It uses XMM
+int common_alu_float(ALU_Opcode opc, int vA, int v1, int v2) {//add, sub, mul
+    get_VR_ss(v1, 1, false);
+#ifdef USE_MEM_OPERAND
+    alu_sd_binary_VR_reg(opc, v2, 1, false, false/*isSD*/);
+#else
+    get_VR_ss(v2, 2, false);
+    alu_ss_binary_reg_reg(opc, 2, false, 1, false);
+#endif
+    set_VR_ss(vA, 1, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode add-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_FLOAT);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_float(add_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode sub-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_FLOAT);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_float(sub_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_FLOAT);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_float(mul_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode add-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_FLOAT_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_float(add_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode sub-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_FLOAT_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_float(sub_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_FLOAT_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_float(mul_opc, vA, v1, v2);
+    return retval;
+}
+//! common code to handle DIV of float
+
+//! It uses FP stack
+int common_div_float(int vA, int v1, int v2) {
+    load_fp_stack_VR(OpndSize_32, v1); //flds
+    fpu_VR(div_opc, OpndSize_32, v2);
+    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode div-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_FLOAT);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_float(div_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode div-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_FLOAT_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_float(div_opc, vA, v1, v2);
+    return retval;
+}
+//! common code to handle DIV of double
+
+//! It uses XMM
+int common_alu_double(ALU_Opcode opc, int vA, int v1, int v2) {//add, sub, mul
+    get_VR_sd(v1, 1, false);
+#ifdef USE_MEM_OPERAND
+    alu_sd_binary_VR_reg(opc, v2, 1, false, true /*isSD*/);
+#else
+    get_VR_sd(v2, 2, false);
+    alu_sd_binary_reg_reg(opc, 2, false, 1, false);
+#endif
+    set_VR_sd(vA, 1, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode add-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_DOUBLE);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_double(add_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode sub-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_DOUBLE);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_double(sub_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_DOUBLE);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_double(mul_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode add-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_add_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ADD_DOUBLE_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_double(add_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode sub-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sub_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SUB_DOUBLE_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_double(sub_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode mul-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_mul_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MUL_DOUBLE_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_double(mul_opc, vA, v1, v2);
+    return retval;
+}
+//! common code to handle DIV of double
+
+//! It uses FP stack
+int common_div_double(int vA, int v1, int v2) {
+    load_fp_stack_VR(OpndSize_64, v1); //fldl
+    fpu_VR(div_opc, OpndSize_64, v2); //fdivl
+    store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode div-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_DOUBLE);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_alu_double(div_opc, vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode div-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_div_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_DIV_DOUBLE_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_alu_double(div_opc, vA, v1, v2);
+    return retval;
+}
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+//! common code to handle REM of float
+
+//! It uses GPR & calls call_fmodf
+int common_rem_float(int vA, int v1, int v2) {
+    get_virtual_reg(v1, OpndSize_32, 1, false);
+    get_virtual_reg(v2, OpndSize_32, 2, false);
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 1, false, 0, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 2, false, 4, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    call_fmodf(); //(float x, float y) return float
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    store_fp_stack_VR(true, OpndSize_32, vA); //fstps
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+
+/**
+ * @brief Generate native code for bytecode rem-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_FLOAT);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_rem_float(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode rem-float/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_float_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_FLOAT_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_rem_float(vA, v1, v2);
+    return retval;
+}
+//! common code to handle REM of double
+
+//! It uses XMM & calls call_fmod
+int common_rem_double(int vA, int v1, int v2) {
+    get_virtual_reg(v1, OpndSize_64, 1, false);
+    get_virtual_reg(v2, OpndSize_64, 2, false);
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_64, 1, false, 0, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_64, 2, false, 8, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    call_fmod(); //(long double x, long double y) return double
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    store_fp_stack_VR(true, OpndSize_64, vA); //fstpl
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode rem-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_DOUBLE);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    int retval = common_rem_double(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode rem-double/2addr
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_rem_double_2addr(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_REM_DOUBLE_2ADDR);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = vA;
+    int v2 = mir->dalvikInsn.vB;
+    int retval = common_rem_double(vA, v1, v2);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode cmpl-float
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmpl_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMPL_FLOAT);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    get_VR_ss(v1, 1, false); //xmm
+    move_imm_to_reg(OpndSize_32, 0, 1, false);
+    move_imm_to_reg(OpndSize_32, 1, 2, false);
+    move_imm_to_reg(OpndSize_32, 0xffffffff, 3, false);
+    compare_VR_ss_reg(v2, 1, false);
+    //default: 0xffffffff??
+    move_imm_to_reg(OpndSize_32,
+                                 0xffffffff, 4, false);
+    //ORDER of cmov matters !!! (Z,P,A)
+    //finalNaN: unordered 0xffffffff
+    conditional_move_reg_to_reg(OpndSize_32, Condition_Z,
+                                             1, false, 4, false);
+    conditional_move_reg_to_reg(OpndSize_32, Condition_P,
+                                             3, false, 4, false);
+    conditional_move_reg_to_reg(OpndSize_32, Condition_A,
+                                             2, false, 4, false);
+    set_virtual_reg(vA, OpndSize_32, 4, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode "cmpg-float vAA, vBB, vCC
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmpg_float(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMPG_FLOAT);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+
+    //Operands are reversed here. Comparing vCC and vBB
+    get_VR_ss(v2, 1, false);
+    compare_VR_ss_reg(v1, 1, false);
+
+    rememberState(1);
+
+    //if vCC > vBB, jump to label ".cmp_float_less"
+    conditional_jump(Condition_A, ".cmp_float_less", true);
+
+    //if vCC < vBB, jump to label ".cmp_float_greater". Handles < and NaN
+    conditional_jump(Condition_B, ".cmp_float_greater", true);
+
+    //if vCC = vBB, move 0 to vAA
+    set_VR_to_imm(vA, OpndSize_32, 0);
+
+    rememberState(2);
+    unconditional_jump(".cmp_float_done", true);
+
+    // if vCC < vBB, i.e (if vBB > vCC) or if one of the operand is a NaN,  move +1 to vAA
+    if (insertLabel(".cmp_float_greater", true) == -1)
+       return -1;
+    goToState(1);
+    set_VR_to_imm(vA, OpndSize_32, 1);
+    transferToState(2);
+    unconditional_jump(".cmp_float_done", true);
+
+    // if vCC > vBB, i.e (if vBB < vCC), move -1 to vAA
+    if (insertLabel(".cmp_float_less", true) == -1)
+       return -1;
+    goToState(1);
+    set_VR_to_imm(vA, OpndSize_32, 0xffffffff);
+    transferToState(2);
+
+    //cmpg_float handling over
+    if (insertLabel(".cmp_float_done", true) == -1)
+       return -1;
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode cmpl-double
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmpl_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMPL_DOUBLE);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    get_VR_sd(v1, 1, false);
+    compare_VR_sd_reg(v2, 1, false);
+    move_imm_to_reg(OpndSize_32, 0, 1, false);
+    move_imm_to_reg(OpndSize_32, 1, 2, false);
+    move_imm_to_reg(OpndSize_32, 0xffffffff, 3, false);
+
+    //default: 0xffffffff??
+    move_imm_to_reg(OpndSize_32, 0xffffffff, 4, false);
+    conditional_move_reg_to_reg(OpndSize_32, Condition_Z,
+                                             1, false, 4, false);
+    conditional_move_reg_to_reg(OpndSize_32, Condition_P,
+                                             3, false, 4, false);
+    conditional_move_reg_to_reg(OpndSize_32, Condition_A,
+                                             2, false, 4, false);
+    set_virtual_reg(vA, OpndSize_32, 4, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode cmpg-double vAA, vBB, vCC
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmpg_double(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMPG_DOUBLE);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+
+    //Operands are reversed here. Comparing vCC and vBB
+    get_VR_sd(v2, 1, false);
+    compare_VR_sd_reg(v1, 1, false);
+
+    rememberState(1);
+
+    //if vCC > vBB, jump to label ".cmp_double_less"
+    conditional_jump(Condition_A, ".cmp_double_less", true);
+
+    //if vCC < vBB, jump to label ".cmp_double_greater". Handles < and NaN
+    conditional_jump(Condition_B, ".cmp_double_greater", true);
+
+    //if vCC = vBB, move 0 to vAA
+    set_VR_to_imm(vA, OpndSize_32, 0);
+
+    rememberState(2);
+    unconditional_jump(".cmp_double_done", true);
+
+    // if vCC < vBB, i.e (if vBB > vCC) or if one of the operand is a NaN, move +1 to vAA
+    if (insertLabel(".cmp_double_greater", true) == -1)
+       return -1;
+    goToState(1);
+    set_VR_to_imm(vA, OpndSize_32, 1);
+    transferToState(2);
+    unconditional_jump(".cmp_double_done", true);
+
+    // if vCC > vBB, i.e (if vBB < vCC), move -1 to vAA
+    if (insertLabel(".cmp_double_less", true) == -1)
+       return -1;
+    goToState(1);
+    set_VR_to_imm(vA, OpndSize_32, 0xffffffff);
+    transferToState(2);
+
+    //cmpg_double handling over
+    if (insertLabel(".cmp_double_done", true) == -1)
+       return -1;
+    return 0;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_ESI
+#define P_SCRATCH_1 PhysicalReg_EDX
+#define P_SCRATCH_2 PhysicalReg_EAX
+
+/**
+ * @brief Generate native code for bytecode cmp-long
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_cmp_long(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CMP_LONG);
+    int vA = mir->dalvikInsn.vA;
+    int v1 = mir->dalvikInsn.vB;
+    int v2 = mir->dalvikInsn.vC;
+    get_virtual_reg(v1+1, OpndSize_32, 2, false);
+
+    //Compare higher 32 bits
+    compare_VR_reg(OpndSize_32,v2+1, 2, false);
+    rememberState(1);
+    //If equal on higher 32 bits, goto compare of lower 32 bits
+    conditional_jump(Condition_E, ".cmp_long_higher_32b_equal", true);
+    //If less on higher 32 bits, it is less on 64 bits
+    conditional_jump(Condition_L, ".cmp_long_higher_32b_less", true);
+    //If greater on higher 32 bits, it is greater on 64 bits
+    set_VR_to_imm(vA, OpndSize_32, 1);
+    rememberState(2);
+    unconditional_jump(".cmp_long_done", true);
+
+    //If higher 32 bits are equal, compare lower 32 bits
+    if (insertLabel(".cmp_long_higher_32b_equal",true) == -1)
+       return -1;
+    goToState(1);
+    get_virtual_reg(v1, OpndSize_32, 1, false);
+
+    //Compare lower 32 bits
+    compare_VR_reg(OpndSize_32, v2, 1, false);
+    rememberState(3);
+    //Less on lower 32 bits
+    conditional_jump(Condition_B, ".cmp_long_lower_32b_less", true);
+    //Equal on lower 32 bits
+    conditional_jump(Condition_E, ".cmp_long_lower_32b_equal", true);
+    //Greater on lower 32 bits
+    set_VR_to_imm(vA, OpndSize_32, 1);
+    transferToState(2);
+    unconditional_jump(".cmp_long_done", true);
+
+    if (insertLabel(".cmp_long_higher_32b_less", true) == -1)
+       return -1;
+    goToState(1);
+    set_VR_to_imm(vA, OpndSize_32, 0xffffffff);
+    transferToState(2);
+    unconditional_jump(".cmp_long_done", true);
+
+    if (insertLabel(".cmp_long_lower_32b_less", true) == -1)
+       return -1;
+    goToState(3);
+    set_VR_to_imm(vA, OpndSize_32, 0xffffffff);
+    transferToState(2);
+    unconditional_jump(".cmp_long_done", true);
+
+    if (insertLabel(".cmp_long_lower_32b_equal", true) == -1)
+       return -1;
+    goToState(3);
+    set_VR_to_imm(vA, OpndSize_32, 0);
+    transferToState(2);
+
+    if (insertLabel(".cmp_long_done", true) == -1)
+       return -1;
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
diff --git a/vm/compiler/codegen/x86/lightcg/LowerConst.cpp b/vm/compiler/codegen/x86/lightcg/LowerConst.cpp
new file mode 100644
index 0000000..0fe1860
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/LowerConst.cpp
@@ -0,0 +1,240 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file LowerConst.cpp
+    \brief This file lowers the following bytecodes: CONST_XXX
+
+    Functions are called from the lowered native sequence:
+    1> const_string_resolve
+       INPUT: const pool index in %eax
+       OUTPUT: resolved string in %eax
+       The only register that is still live after this function is ebx
+    2> class_resolve
+       INPUT: const pool index in %eax
+       OUTPUT: resolved class in %eax
+       The only register that is still live after this function is ebx
+*/
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "NcgAot.h"
+#include "enc_wrapper.h"
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+
+//! LOWER bytecode CONST_STRING without usage of helper function
+
+//! It calls const_string_resolve (%ebx is live across the call)
+//! Since the register allocator does not handle control flow within the lowered native sequence,
+//!   we define an interface between the lowering module and register allocator:
+//!     rememberState, gotoState, transferToState
+//!   to make sure at the control flow merge point the state of registers is the same
+int const_string_common_nohelper(u4 tmp, int vA) {
+    /* for trace-based JIT, the string is already resolved since this code has been executed */
+    void *strPtr = (void*)
+              (currentMethod->clazz->pDvmDex->pResStrings[tmp]);
+    assert(strPtr != NULL);
+    set_VR_to_imm(vA, OpndSize_32, (int) strPtr );
+    return 0;
+}
+//! dispatcher to select either const_string_common_helper or const_string_common_nohelper
+
+//!
+int const_string_common(u4 tmp, int vA) {
+    return const_string_common_nohelper(tmp, vA);
+}
+#undef P_GPR_1
+#undef P_GPR_2
+
+/**
+ * @brief Generate native code for bytecode const/4
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_4(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_4);
+    int vA = mir->dalvikInsn.vA;
+    s4 tmp = mir->dalvikInsn.vB;
+    set_VR_to_imm(vA, OpndSize_32, tmp);
+    return 1;
+}
+
+/**
+ * @brief Generate native code for bytecode const/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_16);
+    u2 BBBB = mir->dalvikInsn.vB;
+    int vA = mir->dalvikInsn.vA;
+    set_VR_to_imm(vA, OpndSize_32, (s2)BBBB);
+    return 1;
+}
+
+/**
+ * @brief Generate native code for bytecode const
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+    set_VR_to_imm(vA, OpndSize_32, (s4)tmp);
+    return 1;
+}
+
+/**
+ * @brief Generate native code for bytecode const/high16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_high16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_HIGH16);
+    int vA = mir->dalvikInsn.vA;
+    u2 tmp = mir->dalvikInsn.vB;
+    set_VR_to_imm(vA, OpndSize_32, ((s4)tmp)<<16);
+    return 1;
+}
+
+/**
+ * @brief Generate native code for bytecode const-wide/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_wide_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE_16);
+    int vA = mir->dalvikInsn.vA;
+    u2 tmp = mir->dalvikInsn.vB;
+    set_VR_to_imm(vA, OpndSize_32, (s2)tmp);
+    set_VR_to_imm(vA+1, OpndSize_32, ((s2)tmp)>>31);
+    return 2;
+}
+
+/**
+ * @brief Generate native code for bytecode const-wide/32
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_wide_32(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE_32);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+    set_VR_to_imm(vA, OpndSize_32, (s4)tmp);
+    set_VR_to_imm(vA+1, OpndSize_32, ((s4)tmp)>>31);
+    return 2;
+}
+
+/**
+ * @brief Generate native code for bytecode const-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE);
+    int vA = mir->dalvikInsn.vA;
+    u8 tmp = mir->dalvikInsn.vB_wide;
+    set_VR_to_imm(vA, OpndSize_32, (s4)tmp);
+    set_VR_to_imm(vA+1, OpndSize_32, (s4)(tmp >> 32));
+    return 2;
+}
+
+/**
+ * @brief Generate native code for bytecode const-wide/high16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_wide_high16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_WIDE_HIGH16);
+    int vA = mir->dalvikInsn.vA;
+    u2 tmp = mir->dalvikInsn.vB;
+    set_VR_to_imm(vA, OpndSize_32, 0);
+    set_VR_to_imm(vA+1, OpndSize_32, ((s4)tmp)<<16);
+    return 2;
+}
+
+/**
+ * @brief Generate native code for bytecode const-string
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_string(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_STRING);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+    int retval = const_string_common(tmp, vA);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode const-string/jumbo
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_string_jumbo(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_STRING_JUMBO);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+    int retval = const_string_common(tmp, vA);
+    return retval;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+/**
+ * @brief Generate native code for bytecode const-class
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_const_class(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CONST_CLASS);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+#if !defined(WITH_JIT)
+    // It calls class_resolve (%ebx is live across the call)
+    // Since the register allocator does not handle control flow within the lowered native sequence,
+    //   we define an interface between the lowering module and register allocator:
+    //     rememberState, gotoState, transferToState
+    //   to make sure at the control flow merge point the state of registers is the same
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+    get_res_classes(3, false);
+    move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_NE, ".const_class_resolved", true);
+    rememberState(1);
+    export_pc();
+    move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+    call_helper_API(".class_resolve");
+    transferToState(1);
+    if (insertLabel(".const_class_resolved", true) == -1)
+        return -1;
+    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+#else
+    /* for trace-based JIT, the class is already resolved since this code has been executed */
+    void *classPtr = (void*)
+       (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
+    assert(classPtr != NULL);
+    set_VR_to_imm(vA, OpndSize_32, (int) classPtr );
+#endif
+
+    return 0;
+}
+
+#undef P_GPR_1
+
diff --git a/vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp b/vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp
new file mode 100644
index 0000000..e5b8f1d
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp
@@ -0,0 +1,1873 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file compiler/codegen/x86/LowerGetPut.cpp
+    \brief This file lowers the following bytecodes: XGET|PUT_XXX
+*/
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "NcgAot.h"
+#include "enc_wrapper.h"
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_ESI
+#define P_GPR_4 PhysicalReg_EDX
+
+/**
+ * @brief Common function for generating native code for aget variants
+ * @details Includes null check and bound check.
+ * @param flag
+ * @param vA destination VR
+ * @param vref VR holding reference
+ * @param vindex VR holding index
+ * @param mirOptFlags optimization flags for current bytecode
+ * @return value >= 0 when handled
+ */
+int aget_common_nohelper(ArrayAccess flag, int vA, int vref, int vindex, int mirOptFlags) {
+    ////////////////////////////
+    // Request VR free delays before register allocation for the temporaries
+    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK))
+        requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
+    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
+        requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
+        requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
+    }
+
+    get_virtual_reg(vref, OpndSize_32, 1, false); //array
+    get_virtual_reg(vindex, OpndSize_32, 2, false); //index
+
+    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK)) {
+        //last argument is the exception number for this bytecode
+        nullCheck(1, false, 1, vref); //maybe optimized away, if not, call
+        cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
+    } else {
+        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
+    }
+
+    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
+        boundCheck(vref, 1, false,
+                             vindex, 2, false,
+                             2);
+        cancelVRFreeDelayRequest(vref,VRDELAY_BOUNDCHECK);
+        cancelVRFreeDelayRequest(vindex,VRDELAY_BOUNDCHECK);
+    } else {
+        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
+        updateRefCount2(2, LowOpndRegType_gp, false); //update reference count for tmp2
+    }
+
+    if(flag == AGET) {
+#ifndef WITH_SELF_VERIFICATION
+        move_mem_disp_scale_to_reg(OpndSize_32, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 4, false);
+#else
+        // Load address into temp 5
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 5, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 5 to (esp)
+        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+        // Mov opnd size to 4(esp)
+        move_imm_to_mem(OpndSize_32, int(OpndSize_32), 4, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationLoad();
+        // Restore ESP
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move result of self verification load into temp4
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+    else if(flag == AGET_WIDE) {
+#ifndef WITH_SELF_VERIFICATION
+        move_mem_disp_scale_to_reg(OpndSize_64, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 8, 1, false);
+#else
+        // Load address into temp 5 (scale of 8 due to opnd size 64), temp 1 is base gp
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 8, 5, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 5 to (esp)
+        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationLoadDoubleword();
+        // Restore ESP
+        load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move result of self verification load into temp 1(XMM)
+        move_reg_to_reg(OpndSize_64, PhysicalReg_XMM7, true, 1, false);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+    else if(flag == AGET_CHAR) {
+#ifndef WITH_SELF_VERIFICATION
+        movez_mem_disp_scale_to_reg(OpndSize_16, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 4, false);
+#else
+        // Load address into temp 5
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 5, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 5 to (esp) //address
+        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+        // Mov opnd size to 4(esp)  // op_size
+        move_imm_to_mem(OpndSize_32, int(OpndSize_16), 4, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationLoad();
+        // Restore ESP
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move result of self verification load into temp 4
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+    else if(flag == AGET_SHORT) {
+#ifndef WITH_SELF_VERIFICATION
+        moves_mem_disp_scale_to_reg(OpndSize_16, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 4, false);
+#else
+        // Load address into temp 5
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 5, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 5 to (esp) //address
+        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+        // Mov opnd size to 4(esp)  // op_size
+        move_imm_to_mem(OpndSize_32, int(0x22), 4, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationLoad();
+        // Restore ESP
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move result of self verification load into temp 4
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+    else if(flag == AGET_BOOLEAN) {
+
+#ifndef WITH_SELF_VERIFICATION
+        movez_mem_disp_scale_to_reg(OpndSize_8, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 4, false);
+#else
+        // Load address into temp 5
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 5, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 5 to (esp) //address
+        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+        // Mov opnd size to 4(esp)  // op_size
+        move_imm_to_mem(OpndSize_32, int(OpndSize_8), 4, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationLoad();
+        // Restore ESP
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move result of self verification load into temp 4
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+    else if(flag == AGET_BYTE) {
+#ifndef WITH_SELF_VERIFICATION
+        moves_mem_disp_scale_to_reg(OpndSize_8, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 4, false);
+#else
+        // Load address into temp 5
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 5, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 5 to (esp) //address
+        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+        // Mov opnd size to 4(esp)  // op_size
+        move_imm_to_mem(OpndSize_32, int(0x11), 4, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationLoad();
+        // Restore ESP
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move result of self verification load into temp 4
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 4, false);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+
+    if(flag == AGET_WIDE) {
+        set_virtual_reg(vA, OpndSize_64, 1, false);
+    }
+    else {
+        set_virtual_reg(vA, OpndSize_32, 4, false);
+    }
+    //////////////////////////////////
+    return 0;
+}
+#if 0 /* Code is deprecated. If reenabled, needs additional parameter
+         for optimization flags*/
+//! wrapper to call either aget_common_helper or aget_common_nohelper
+
+//!
+int aget_common(int flag, int vA, int vref, int vindex) {
+    return aget_common_nohelper(flag, vA, vref, vindex);
+}
+#endif
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef P_GPR_4
+
+/**
+ * @brief Generate native code for bytecode aget and aget-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET
+            || mir->dalvikInsn.opcode == OP_AGET_OBJECT);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aget-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_WIDE);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_WIDE, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aget-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_OBJECT);
+    return op_aget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode aget-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_BOOLEAN);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_BOOLEAN, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aget-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_BYTE);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_BYTE, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aget-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_CHAR);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_CHAR, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aget-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aget_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_AGET_SHORT);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aget_common_nohelper(AGET_SHORT, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_ESI
+#define P_GPR_4 PhysicalReg_EDX
+
+/**
+ * @brief Common function for generating native code for aput variants
+ * @details Includes null check and bound check.
+ * @param flag
+ * @param vA destination VR
+ * @param vref VR holding reference
+ * @param vindex VR holding index
+ * @param mirOptFlags optimization flags for current bytecode
+ * @return value >= 0 when handled
+ */
+int aput_common_nohelper(ArrayAccess flag, int vA, int vref, int vindex, int mirOptFlags) {
+    //////////////////////////////////////
+    // Request VR free delays before register allocation for the temporaries.
+    // No need to request delay for vA since it will be transferred to temporary
+    // after the null check and bound check.
+    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK))
+        requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
+    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
+        requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
+        requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
+    }
+
+    get_virtual_reg(vref, OpndSize_32, 1, false); //array
+    get_virtual_reg(vindex, OpndSize_32, 2, false); //index
+
+    if(!(mirOptFlags & MIR_IGNORE_NULL_CHECK)) {
+        //last argument is the exception number for this bytecode
+        nullCheck(1, false, 1, vref); //maybe optimized away, if not, call
+        cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
+    } else {
+        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
+    }
+
+    if(!(mirOptFlags & MIR_IGNORE_RANGE_CHECK)) {
+        boundCheck(vref, 1, false,
+                             vindex, 2, false,
+                             2);
+        cancelVRFreeDelayRequest(vref,VRDELAY_BOUNDCHECK);
+        cancelVRFreeDelayRequest(vindex,VRDELAY_BOUNDCHECK);
+    } else {
+        updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
+        updateRefCount2(2, LowOpndRegType_gp, false); //update reference count for tmp2
+    }
+
+    if(flag == APUT_WIDE) {
+        get_virtual_reg(vA, OpndSize_64, 1, false);
+    }
+    else {
+        get_virtual_reg(vA, OpndSize_32, 4, false);
+    }
+    if(flag == APUT) {
+#ifndef WITH_SELF_VERIFICATION
+        move_reg_to_mem_disp_scale(OpndSize_32, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4);
+#else
+        // Load address into temp 5
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 5, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 5 namely the address to (esp)
+        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+        // Store value from temp 4 namely the data to 4(esp)
+        move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);
+        // Mov opnd size to 8(esp)
+        move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationStore();
+        // Restore ESP
+        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    } else if(flag == APUT_WIDE) {
+#ifndef WITH_SELF_VERIFICATION
+        move_reg_to_mem_disp_scale(OpndSize_64, 1, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 8);
+#else
+        // Load address into temp 4
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 8, 4, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 4 namely the address to (esp)
+        move_reg_to_mem(OpndSize_32, 4, false, 0, PhysicalReg_ESP, true);
+        // Store value from temp 1(XMM) namely the data to 4(esp)
+        move_reg_to_mem(OpndSize_64, 1, false, 4, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationStoreDoubleword();
+        // Restore ESP
+        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+    else if(flag == APUT_CHAR || flag == APUT_SHORT) {
+#ifndef WITH_SELF_VERIFICATION
+        move_reg_to_mem_disp_scale(OpndSize_16, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2);
+#else
+        // Load address into temp 5
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 2, 5, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 5 namely the address to (esp)
+        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+        // Store value from temp 4 namely the data to 4(esp)
+        move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);
+        // Mov opnd size to 8(esp)
+        move_imm_to_mem(OpndSize_32, int(OpndSize_16), 8, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationStore();
+        // Restore ESP
+        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+    else if(flag == APUT_BOOLEAN || flag == APUT_BYTE) {
+#ifndef WITH_SELF_VERIFICATION
+        move_reg_to_mem_disp_scale(OpndSize_8, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1);
+#else
+        // Load address into temp 5
+        load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 1, 5, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 5 namely the address to (esp)
+        move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+        // Store value from temp 4 namely the data to 4(esp)
+        move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);
+        // Mov opnd size to 8(esp)
+        move_imm_to_mem(OpndSize_32, int(OpndSize_8), 8, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationStore();
+        // Restore ESP
+        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+    //////////////////////////////////
+    return 0;
+}
+#if 0 /* Code is deprecated. If reenabled, needs additional parameter
+         for optimization flags*/
+//! wrapper to call either aput_common_helper or aput_common_nohelper
+
+//!
+int aput_common(int flag, int vA, int vref, int vindex) {
+    return aput_common_nohelper(flag, vA, vref, vindex);
+}
+#endif
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef P_GPR_4
+
+/**
+ * @brief Generate native code for bytecode aput
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aput-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_WIDE);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_WIDE, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aput-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_BOOLEAN);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_BOOLEAN, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aput-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_BYTE);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_BYTE, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aput-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_CHAR);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_CHAR, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode aput-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_APUT_SHORT);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+    int retval = aput_common_nohelper(APUT_SHORT, vA, vref, vindex,
+            mir->OptimizationFlags);
+    return retval;
+}
+
+#define P_GPR_1 PhysicalReg_EBX //callee-saved valid after CanPutArray
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_ESI //callee-saved
+#define P_SCRATCH_1 PhysicalReg_EDX
+#define P_SCRATCH_2 PhysicalReg_EAX
+#define P_SCRATCH_3 PhysicalReg_EDX
+
+void markCard_notNull(int tgtAddrReg, int scratchReg, bool isPhysical);
+
+/**
+ * @brief Generate native code for bytecode aput-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_aput_object(const MIR * mir) { //type checking
+    assert(mir->dalvikInsn.opcode == OP_APUT_OBJECT);
+    int vA = mir->dalvikInsn.vA;
+    int vref = mir->dalvikInsn.vB;
+    int vindex = mir->dalvikInsn.vC;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[6]) {
+        export_pc(); //use %edx
+        move_imm_to_reg(OpndSize_32, vA, P_SCRATCH_1, true);
+        move_imm_to_reg(OpndSize_32, vref, P_SCRATCH_2, true);
+        move_imm_to_reg(OpndSize_32, vindex, P_GPR_2, true);
+
+        spillVirtualReg(vref, LowOpndRegType_gp, true);
+        spillVirtualReg(vindex, LowOpndRegType_gp, true);
+        spillVirtualReg(vA, LowOpndRegType_gp, true);
+        call_helper_API(".aput_obj_helper");
+    }
+    else
+#endif
+    {
+        ///////////////////////////
+        // Request VR free delays before register allocation for the temporaries
+        // No need to request delay for vA since it will be transferred to temporary
+        // after the null check and bound check.
+        if(!(mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK))
+            requestVRFreeDelay(vref,VRDELAY_NULLCHECK);
+        if(!(mir->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
+            requestVRFreeDelay(vref,VRDELAY_BOUNDCHECK);
+            requestVRFreeDelay(vindex,VRDELAY_BOUNDCHECK);
+        }
+
+        get_virtual_reg(vref, OpndSize_32, 1, false); //array
+        if(!(mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK)) {
+            nullCheck(1, false, 1, vref); //maybe optimized away, if not, call
+            cancelVRFreeDelayRequest(vref,VRDELAY_NULLCHECK);
+        } else {
+            updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
+        }
+
+        get_virtual_reg(vindex, OpndSize_32, 2, false); //index
+        if(!(mir->OptimizationFlags & MIR_IGNORE_RANGE_CHECK)) {
+            boundCheck(vref, 1, false, vindex, 2, false, 2);
+            cancelVRFreeDelayRequest(vref,VRDELAY_BOUNDCHECK);
+            cancelVRFreeDelayRequest(vindex,VRDELAY_BOUNDCHECK);
+        } else {
+            updateRefCount2(1, LowOpndRegType_gp, false); //update reference count for tmp1
+            updateRefCount2(2, LowOpndRegType_gp, false); //update reference count for tmp2
+        }
+
+    get_virtual_reg(vA, OpndSize_32, 4, false);
+    compare_imm_reg(OpndSize_32, 0, 4, false);
+    conditional_jump(Condition_E, ".aput_object_skip_check", true);
+    rememberState(1);
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), 4, false, 5, false);
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), 1, false, 6, false);
+    move_reg_to_mem(OpndSize_32, 6, false, 4, PhysicalReg_ESP, true);
+
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    call_dvmCanPutArrayElement(); //scratch??
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump_global_API(Condition_E, "common_errArrayStore", false);
+
+#ifndef WITH_SELF_VERIFICATION
+    //NOTE: "2, false" is live through function call
+    move_reg_to_mem_disp_scale(OpndSize_32, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4);
+#else
+    // lea to temp 7, temp 4 contains the data
+    load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 7, false);
+    pushCallerSavedRegs();
+    // make space on the stack and push 3 args (address, data, operand size)
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 7, false, 0, PhysicalReg_ESP, true);  // address
+    move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);  //data
+    move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    call_selfVerificationStore();
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    popCallerSavedRegs();
+#endif
+    markCard_notNull(1, 11, false);
+    rememberState(2);
+    ////TODO NCG O1 + code cache
+    unconditional_jump(".aput_object_after_check", true);
+
+    if (insertLabel(".aput_object_skip_check", true) == -1)
+        return -1;
+    goToState(1);
+#ifndef WITH_SELF_VERIFICATION
+    //NOTE: "2, false" is live through function call
+    move_reg_to_mem_disp_scale(OpndSize_32, 4, false, 1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4);
+#else
+    // lea to temp 7, temp 4 contains the data
+    load_effective_addr_scale_disp(1, false, OFFSETOF_MEMBER(ArrayObject, contents), 2, false, 4, 7, false);
+    pushCallerSavedRegs();
+    // make space on the stack and push 3 args (address, data, operand size)
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 7, false, 0, PhysicalReg_ESP, true); //address
+    move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true); //data
+    move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    call_selfVerificationStore();
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    popCallerSavedRegs();
+#endif
+    transferToState(2);
+    if (insertLabel(".aput_object_after_check", true) == -1)
+        return -1;
+    ///////////////////////////////
+  }
+  return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef P_SCRATCH_1
+#undef P_SCRATCH_2
+#undef P_SCRATCH_3
+
+//////////////////////////////////////////
+#define P_GPR_1 PhysicalReg_ECX
+#define P_GPR_2 PhysicalReg_EBX //should be callee-saved to avoid overwritten by inst_field_resolve
+#define P_GPR_3 PhysicalReg_ESI
+#define P_SCRATCH_1 PhysicalReg_EDX
+
+/*
+   movl offThread_cardTable(self), scratchReg
+   compare_imm_reg 0, valReg (testl valReg, valReg)
+   je .markCard_skip
+   shrl $GC_CARD_SHIFT, tgtAddrReg
+   movb %, (scratchReg, tgtAddrReg)
+   NOTE: scratchReg can be accessed with the corresponding byte
+         tgtAddrReg will be updated
+   for O1, update the corresponding reference count
+*/
+void markCard(int valReg, int tgtAddrReg, bool targetPhysical, int scratchReg, bool isPhysical) {
+   get_self_pointer(PhysicalReg_SCRATCH_6, isScratchPhysical);
+   move_mem_to_reg(OpndSize_32, offsetof(Thread, cardTable), PhysicalReg_SCRATCH_6, isScratchPhysical, scratchReg, isPhysical);
+   compare_imm_reg(OpndSize_32, 0, valReg, isPhysical);
+   conditional_jump(Condition_E, ".markCard_skip", true);
+   alu_binary_imm_reg(OpndSize_32, shr_opc, GC_CARD_SHIFT, tgtAddrReg, targetPhysical);
+   move_reg_to_mem_disp_scale(OpndSize_8, scratchReg, isPhysical, scratchReg, isPhysical, 0, tgtAddrReg, targetPhysical, 1);
+   if (insertLabel(".markCard_skip", true) == -1) {
+       return;
+   }
+}
+
+void markCard_notNull(int tgtAddrReg, int scratchReg, bool isPhysical) {
+   get_self_pointer(PhysicalReg_SCRATCH_2, isScratchPhysical);
+   move_mem_to_reg(OpndSize_32, offsetof(Thread, cardTable), PhysicalReg_SCRATCH_2, isScratchPhysical, scratchReg, isPhysical);
+   alu_binary_imm_reg(OpndSize_32, shr_opc, GC_CARD_SHIFT, tgtAddrReg, isPhysical);
+   move_reg_to_mem_disp_scale(OpndSize_8, scratchReg, isPhysical, scratchReg, isPhysical, 0, tgtAddrReg, isPhysical, 1);
+}
+
+void markCard_filled(int tgtAddrReg, bool isTgtPhysical, int scratchReg, bool isScratchPhysical) {
+   get_self_pointer(PhysicalReg_SCRATCH_2, false/*isPhysical*/);
+   move_mem_to_reg(OpndSize_32, offsetof(Thread, cardTable), PhysicalReg_SCRATCH_2, isScratchPhysical, scratchReg, isScratchPhysical);
+   alu_binary_imm_reg(OpndSize_32, shr_opc, GC_CARD_SHIFT, tgtAddrReg, isTgtPhysical);
+   move_reg_to_mem_disp_scale(OpndSize_8, scratchReg, isScratchPhysical, scratchReg, isScratchPhysical, 0, tgtAddrReg, isTgtPhysical, 1);
+}
+
+/**
+ * @brief Common function for generating native code for iget and iput variants
+ * @details Includes null check
+ * @param referenceIndex instance field index
+ * @param flag type of instance access
+ * @param vA value register
+ * @param vB object register
+ * @param isObj true iff mnemonic is object variant
+ * @param isVolatile iff mnemonic is volatile variant
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int iget_iput_common_nohelper(u2 referenceIndex, InstanceAccess flag, int vA,
+        int vB, bool isObj, bool isVolatile, const MIR * mir) {
+#if !defined(WITH_JIT)
+    ///////////////////////////////
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+    get_res_fields(3, false);
+    //move_mem_to_reg(OpndSize_32, referenceIndex*4, 3, false, 4, false);
+    compare_imm_mem(OpndSize_32, 0, referenceIndex*4, 3, false);
+    move_mem_to_reg(OpndSize_32, referenceIndex*4, 3, false, PhysicalReg_EAX, true);
+    /*********************************
+    compare_imm_reg(OpndSize_32, 0, 4, false);
+    **********************************/
+    export_pc(); //use %edx
+    conditional_jump(Condition_NE, ".iget_iput_resolved", true);
+    rememberState(1);
+    move_imm_to_reg(OpndSize_32, referenceIndex, PhysicalReg_EAX, true);
+    call_helper_API(".inst_field_resolve");
+    transferToState(1);
+    if (insertLabel(".iget_iput_resolved", true) == -1)
+        return -1;
+#else
+
+    const Method *method =
+            (mir->OptimizationFlags & MIR_CALLEE) ?
+                    mir->meta.calleeMethod : currentMethod;
+    InstField *pInstField =
+            (InstField *) method->clazz->pDvmDex->pResFields[referenceIndex];
+
+    int fieldOffset;
+
+    assert(pInstField != NULL);
+    fieldOffset = pInstField->byteOffset;
+    move_imm_to_reg(OpndSize_32, fieldOffset, 8, false);
+#endif
+
+    // Request VR delay before transfer to temporary. Only vB needs delay.
+    // vA will have non-zero reference count since transfer to temporary for
+    // it happens after null check, thus no delay is needed.
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        requestVRFreeDelay(vB,VRDELAY_NULLCHECK);
+    }
+    get_virtual_reg(vB, OpndSize_32, 7, false);
+    //If we can't ignore the NULL check
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        nullCheck(7, false, 2, vB); //maybe optimized away, if not, call
+        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
+    }
+
+#if !defined(WITH_JIT)
+    move_mem_to_reg(OpndSize_32, offInstField_byteOffset, PhysicalReg_EAX, true, 8, false); //byte offest
+#endif
+    if(flag == IGET) {
+#ifndef WITH_SELF_VERIFICATION
+        move_mem_scale_to_reg(OpndSize_32, 7, false, 8, false, 1, 9, false);
+        set_virtual_reg(vA, OpndSize_32, 9, false);
+#else
+        // Load address into temp reg 10
+        load_effective_addr_scale(7, false, 8, false, 1, 10, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp reg 10 to (esp)
+        move_reg_to_mem(OpndSize_32, 10, false, 0, PhysicalReg_ESP, true);
+        // Mov opnd size to 4(esp)
+        move_imm_to_mem(OpndSize_32, int(OpndSize_32), 4, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationLoad();
+        // Restore ESP
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move result of self verification load into tenp9
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 9, false);
+        // pop caller saved registers
+        popCallerSavedRegs();
+        set_virtual_reg(vA, OpndSize_32, 9, false);
+#endif
+
+#ifdef DEBUG_IGET_OBJ
+        if(isObj > 0) {
+            pushAllRegs();
+            load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            move_reg_to_mem(OpndSize_32, 9, false, 12, PhysicalReg_ESP, true); //field
+            move_reg_to_mem(OpndSize_32, 7, false, 8, PhysicalReg_ESP, true); //object
+            move_imm_to_mem(OpndSize_32, referenceIndex, 4, PhysicalReg_ESP, true); //field
+            move_imm_to_mem(OpndSize_32, 0, 0, PhysicalReg_ESP, true); //iget
+            call_dvmDebugIgetIput();
+            load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            popAllRegs();
+        }
+#endif
+    } else if(flag == IGET_WIDE) {
+#ifndef WITH_SELF_VERIFICATION
+        if(isVolatile) {
+            /* call dvmQuasiAtomicRead64(addr) */
+            load_effective_addr(fieldOffset, 7, false, 9, false);
+            move_reg_to_mem(OpndSize_32, 9, false, -4, PhysicalReg_ESP, true); //1st argument
+            load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            nextVersionOfHardReg(PhysicalReg_EAX, 2);
+            nextVersionOfHardReg(PhysicalReg_EDX, 2);
+            scratchRegs[0] = PhysicalReg_SCRATCH_3;
+            call_dvmQuasiAtomicRead64();
+            load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            //memory content in %edx, %eax
+            set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+            set_virtual_reg(vA+1, OpndSize_32, PhysicalReg_EDX, true);
+        } else {
+            move_mem_scale_to_reg(OpndSize_64, 7, false, 8, false, 1, 1, false); //access field
+            set_virtual_reg(vA, OpndSize_64, 1, false);
+        }
+#else
+        // Load address into temp 10
+        if(isVolatile) {
+            load_effective_addr(fieldOffset, 7, false, 10, false);
+        } else {
+            load_effective_addr_scale(7, false, 8, false, 1, 10, false);
+        }
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 10 to (esp)
+        move_reg_to_mem(OpndSize_32, 10, false, 0, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_5;
+        // Load from shadow heap
+        call_selfVerificationLoadDoubleword();
+        // Restore ESP
+        load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move result of self verification load into temp 1(XMM)
+        move_reg_to_reg(OpndSize_64, PhysicalReg_XMM7, true, 1, false);
+        // pop caller saved registers
+        popCallerSavedRegs();
+        set_virtual_reg(vA, OpndSize_64, 1, false);
+#endif
+    } else if(flag == IPUT) {
+        get_virtual_reg(vA, OpndSize_32, 9, false);
+#ifndef WITH_SELF_VERIFICATION
+        move_reg_to_mem_scale(OpndSize_32, 9, false, 7, false, 8, false, 1); //access field
+#else
+        // Load address into temp 10; reg temp 9 will contain the data
+        load_effective_addr_scale(7, false, 8, false, 1, 10, false);
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 10 namely the address to (esp)
+        move_reg_to_mem(OpndSize_32, 10, false, 0, PhysicalReg_ESP, true);
+        // Store value from temp 9 namely the data to 4(esp)
+        move_reg_to_mem(OpndSize_32, 9, false, 4, PhysicalReg_ESP, true);
+        // Mov opnd size to 8(esp)
+        move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_1;
+        // Load from shadow heap
+        call_selfVerificationStore();
+        // Restore ESP
+        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+        if(isObj) {
+            markCard(9, 7, false, 11, false);
+        }
+    } else if(flag == IPUT_WIDE) {
+        get_virtual_reg(vA, OpndSize_64, 1, false);
+#ifndef WITH_SELF_VERIFICATION
+        if(isVolatile) {
+            /* call dvmQuasiAtomicSwap64(val, addr) */
+            load_effective_addr(fieldOffset, 7, false, 9, false);
+            move_reg_to_mem(OpndSize_32, 9, false, -4, PhysicalReg_ESP, true); //2nd argument
+            move_reg_to_mem(OpndSize_64, 1, false, -12, PhysicalReg_ESP, true); //1st argument
+            load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            scratchRegs[0] = PhysicalReg_SCRATCH_3;
+            call_dvmQuasiAtomicSwap64();
+            load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        }
+        else {
+            move_reg_to_mem_scale(OpndSize_64, 1, false, 7, false, 8, false, 1);
+        }
+#else
+        //TODO: handle the volatile type correctly..
+        // Load address into temp 10
+        if(isVolatile) {
+            load_effective_addr(fieldOffset, 7, false, 10, false);
+        } else {
+            load_effective_addr_scale(7, false, 8, false, 1, 10, false);
+        }
+        // push caller saved registers
+        pushCallerSavedRegs();
+        // Set up arguments
+        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // Move value in temp 10 namely the address to (esp)
+        move_reg_to_mem(OpndSize_32, 10, false, 0, PhysicalReg_ESP, true);
+        // Store value from temp 1(XMM) namely the data to 4(esp)
+        move_reg_to_mem(OpndSize_64, 1, false, 4, PhysicalReg_ESP, true);
+        // In order to call, the scratch reg must be set
+        scratchRegs[0] = PhysicalReg_SCRATCH_5;
+        // Load from shadow heap
+        call_selfVerificationStoreDoubleword();
+        // Restore ESP
+        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        // pop caller saved registers
+        popCallerSavedRegs();
+#endif
+    }
+    ///////////////////////////
+    return 0;
+}
+
+#if 0 /* Code is deprecated. If reenabled, needs additional parameter
+         for optimization flags*/
+//! wrapper to call either iget_iput_common_helper or iget_iput_common_nohelper
+
+//!
+int iget_iput_common(int tmp, int flag, int vA, int vB, int isObj, bool isVolatile) {
+    return iget_iput_common_nohelper(tmp, flag, vA, vB, isObj, isVolatile);
+}
+#endif
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef P_SCRATCH_1
+
+/**
+ * @brief Generate native code for bytecodes iget, iget-boolean,
+ * iget-byte, iget-char, iget-short, and iget/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET
+            || mir->dalvikInsn.opcode == OP_IGET_BOOLEAN
+            || mir->dalvikInsn.opcode == OP_IGET_BYTE
+            || mir->dalvikInsn.opcode == OP_IGET_CHAR
+            || mir->dalvikInsn.opcode == OP_IGET_SHORT
+            || mir->dalvikInsn.opcode == OP_IGET_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IGET, vA, vB, false,
+            false, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecodes iget-wide and iget-wide/volatile
+ * @param mir bytecode representation
+ * @param isVolatile is the iget a volatile access or not?
+ * @return value >= 0 when handled
+ */
+int op_iget_wide(const MIR * mir, bool isVolatile) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_WIDE
+            || mir->dalvikInsn.opcode == OP_IGET_WIDE_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IGET_WIDE, vA, vB,
+            false, isVolatile, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecodes iget-object
+ * and iget-object/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_OBJECT
+            || mir->dalvikInsn.opcode == OP_IGET_OBJECT_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IGET, vA, vB, true,
+            false, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode iget-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_BOOLEAN);
+    return op_iget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode iget-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_BYTE);
+    return op_iget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode iget-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_CHAR);
+    return op_iget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode iget-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_SHORT);
+    return op_iget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecodes iput, iput-boolean,
+ * iput-byte, iput-char, iput-short, and iput/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT
+            || mir->dalvikInsn.opcode == OP_IPUT_BOOLEAN
+            || mir->dalvikInsn.opcode == OP_IPUT_BYTE
+            || mir->dalvikInsn.opcode == OP_IPUT_CHAR
+            || mir->dalvikInsn.opcode == OP_IPUT_SHORT
+            || mir->dalvikInsn.opcode == OP_IPUT_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IPUT, vA, vB, false,
+            false, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecodes iput-wide and iput-wide/volatile
+ * @param mir bytecode representation
+ * @param isVolatile is the iput a volatile access or not?
+ * @return value >= 0 when handled
+ */
+int op_iput_wide(const MIR * mir, bool isVolatile) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_WIDE
+            || mir->dalvikInsn.opcode == OP_IPUT_WIDE_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IPUT_WIDE, vA, vB,
+            false, isVolatile, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecodes iput-object and iput-object/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_OBJECT
+            || mir->dalvikInsn.opcode == OP_IPUT_OBJECT_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    u2 referenceIndex = mir->dalvikInsn.vC;
+    int retval = iget_iput_common_nohelper(referenceIndex, IPUT, vA, vB, true,
+            false, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode iput-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_BOOLEAN);
+    return op_iput(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode iput-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_BYTE);
+    return op_iput(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode iput-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_CHAR);
+    return op_iput(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode iput-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_SHORT);
+    return op_iput(mir);
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_EDX //used by helper only
+
+/**
+ * @brief Common function for generating native code for sget and sput variants
+ * @details Includes null check
+ * @param flag type of static access
+ * @param vA value register
+ * @param referenceIndex static field index
+ * @param isObj true iff mnemonic is object variant
+ * @param isVolatile iff mnemonic is volatile variant
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int sget_sput_common(StaticAccess flag, int vA, u2 referenceIndex, bool isObj,
+        bool isVolatile, const MIR * mir) {
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[5]) {
+        return sget_sput_common_helper(flag, vA, referenceIndex, isObj);
+    }
+    else
+#endif
+    {
+        //call assembly static_field_resolve
+        //no exception
+        //glue: get_res_fields
+        //hard-coded: eax (one version?)
+        //////////////////////////////////////////
+#if !defined(WITH_JIT)
+        scratchRegs[2] = PhysicalReg_EDX; scratchRegs[3] = PhysicalReg_Null;
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        get_res_fields(3, false);
+        move_mem_to_reg(OpndSize_32, referenceIndex*4, 3, false, PhysicalReg_EAX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //InstanceField
+        conditional_jump(Condition_NE, ".sget_sput_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, referenceIndex, PhysicalReg_EAX, true);
+
+        export_pc(); //use %edx
+        call_helper_API(".static_field_resolve");
+        transferToState(1);
+        if (insertLabel(".sget_sput_resolved", true) == -1)
+            return -1;
+#else
+
+        const Method *method =
+                (mir->OptimizationFlags & MIR_CALLEE) ?
+                        mir->meta.calleeMethod : currentMethod;
+        void *fieldPtr =
+                (void*) (method->clazz->pDvmDex->pResFields[referenceIndex]);
+
+        /* Usually, fieldPtr should not be null. The interpreter should resolve
+         * it before we come here, or not allow this opcode in a trace. However,
+         * we can be in a loop trace and this opcode might have been picked up
+         * by exhaustTrace. Sending a -1 here will terminate the loop formation
+         * and fall back to normal trace, which will not have this opcode.
+         */
+        if (!fieldPtr) {
+            ALOGI("JIT_INFO: Unresolved fieldPtr at sget_sput_common");
+            SET_JIT_ERROR(kJitErrorUnresolvedField);
+            return -1;
+        }
+
+    move_imm_to_reg(OpndSize_32, (int)fieldPtr, PhysicalReg_EAX, true);
+#endif
+    if(flag == SGET) {
+#ifndef WITH_SELF_VERIFICATION
+        move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 7, false); //access field
+        set_virtual_reg(vA, OpndSize_32, 7, false);
+#else
+            // Load address into temp reg 8
+            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 8, false);
+            // push caller saved registers
+            pushCallerSavedRegs();
+            // Set up arguments
+            load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            // Move value in temp reg 8 to (esp)
+            move_reg_to_mem(OpndSize_32, 8, false, 0, PhysicalReg_ESP, true);
+            // Mov opnd size to 4(esp)
+            move_imm_to_mem(OpndSize_32, int(OpndSize_32), 4, PhysicalReg_ESP, true);
+            // In order to call, the scratch reg must be set
+            scratchRegs[0] = PhysicalReg_SCRATCH_5;
+            // Load from shadow heap
+            call_selfVerificationLoad();
+            // Restore ESP
+            load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            // Move result of self verification load into temp 7
+            move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 7, false);
+            // pop caller saved registers
+            popCallerSavedRegs();
+            set_virtual_reg(vA, OpndSize_32, 7, false);
+#endif
+    } else if(flag == SGET_WIDE) {
+#ifndef WITH_SELF_VERIFICATION
+        if(isVolatile) {
+            /* call dvmQuasiAtomicRead64(addr) */
+            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 9, false);
+            move_reg_to_mem(OpndSize_32, 9, false, -4, PhysicalReg_ESP, true); //1st argument
+            load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            nextVersionOfHardReg(PhysicalReg_EAX, 2);
+            nextVersionOfHardReg(PhysicalReg_EDX, 2);
+            scratchRegs[0] = PhysicalReg_SCRATCH_3;
+            call_dvmQuasiAtomicRead64();
+            load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            //memory content in %edx, %eax
+            set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+            set_virtual_reg(vA+1, OpndSize_32, PhysicalReg_EDX, true);
+        }
+        else {
+            move_mem_to_reg(OpndSize_64, OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 1, false); //access field
+            set_virtual_reg(vA, OpndSize_64, 1, false);
+        }
+#else
+            // TODO: the volatile 64 bit type is not handled;
+            // write a C function to only return the mapped shadow address(Read)
+            // Load address into temp 4
+            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 4, false);
+            // push caller saved registers
+            pushCallerSavedRegs();
+            // Set up arguments
+            load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            // Move value in temp 4 (address) to 0(esp)
+            move_reg_to_mem(OpndSize_32, 4, false, 0, PhysicalReg_ESP, true);
+            // In order to call, the scratch reg must be set
+            scratchRegs[0] = PhysicalReg_SCRATCH_5;
+            // Load from shadow heap
+            call_selfVerificationLoadDoubleword();
+            // Restore ESP
+            load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            // Move result of self verification load from XMM7 to temp 1(XMM)
+            move_reg_to_reg(OpndSize_64, PhysicalReg_XMM7, true, 1, false);
+            // pop caller saved registers
+            popCallerSavedRegs();
+            set_virtual_reg(vA, OpndSize_64, 1, false);
+#endif
+    } else if(flag == SPUT) {
+        get_virtual_reg(vA, OpndSize_32, 7, false);
+#ifndef WITH_SELF_VERIFICATION
+        move_reg_to_mem(OpndSize_32, 7, false, OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true); //access field
+#else
+            // Load address into temp 8; reg temp 7 will contain the data
+            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 8, false);
+            // push caller saved registers
+            pushCallerSavedRegs();
+            // Set up arguments
+            load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            // Move value in temp 8 namely the address to (esp)
+            move_reg_to_mem(OpndSize_32, 8, false, 0, PhysicalReg_ESP, true);
+            // Store value from temp 7 namely the data to 4(esp)
+            move_reg_to_mem(OpndSize_32, 7, false, 4, PhysicalReg_ESP, true);
+            // Mov opnd size to 8(esp)
+            move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
+            // In order to call, the scratch reg must be set
+            scratchRegs[0] = PhysicalReg_SCRATCH_5;
+            // Load from shadow heap
+            call_selfVerificationStore();
+            // Restore ESP
+            load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            // pop caller saved registers
+            popCallerSavedRegs();
+#endif
+        if(isObj) {
+            /* get clazz object, then use clazz object to mark card */
+            move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Field, clazz), PhysicalReg_EAX, true, 12, false);
+            markCard(7/*valReg*/, 12, false, 11, false);
+        }
+    } else if(flag == SPUT_WIDE) {
+        get_virtual_reg(vA, OpndSize_64, 1, false);
+#ifndef WITH_SELF_VERIFICATION
+        if(isVolatile) {
+            /* call dvmQuasiAtomicSwap64(val, addr) */
+            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 9, false);
+            move_reg_to_mem(OpndSize_32, 9, false, -4, PhysicalReg_ESP, true); //2nd argument
+            move_reg_to_mem(OpndSize_64, 1, false, -12, PhysicalReg_ESP, true); //1st argument
+            load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            scratchRegs[0] = PhysicalReg_SCRATCH_3;
+            call_dvmQuasiAtomicSwap64();
+            load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        }
+        else {
+            move_reg_to_mem(OpndSize_64, 1, false, OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true); //access field
+        }
+#else
+            // Load address into temp 4; reg temp 1 will contain the data
+            load_effective_addr(OFFSETOF_MEMBER(StaticField, value), PhysicalReg_EAX, true, 4, false);
+            // push caller saved registers
+            pushCallerSavedRegs();
+            // Set up arguments
+            load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            // Move value in temp 4 namely the address to (esp)
+            move_reg_to_mem(OpndSize_32, 4, false, 0, PhysicalReg_ESP, true);
+            // Store value from temp 1(XMM) namely the data to 4(esp)
+            move_reg_to_mem(OpndSize_64, 1, false, 4, PhysicalReg_ESP, true);
+            // In order to call, the scratch reg must be set
+            scratchRegs[0] = PhysicalReg_SCRATCH_5;
+            // Load from shadow heap
+            call_selfVerificationStoreDoubleword();
+            // Restore ESP
+            load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+            // pop caller saved registers
+            popCallerSavedRegs();
+#endif
+        }
+    //////////////////////////////////////////////
+  }
+  return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+
+/**
+ * @brief Generate native code for bytecodes sget, sget-boolean, sget-byte, sget-char, sget-object, sget-short, sget/volatile and sget-object/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET
+            || mir->dalvikInsn.opcode == OP_SGET_BOOLEAN
+            || mir->dalvikInsn.opcode == OP_SGET_BYTE
+            || mir->dalvikInsn.opcode == OP_SGET_CHAR
+            || mir->dalvikInsn.opcode == OP_SGET_OBJECT
+            || mir->dalvikInsn.opcode == OP_SGET_SHORT
+            || mir->dalvikInsn.opcode == OP_SGET_VOLATILE
+            || mir->dalvikInsn.opcode == OP_SGET_OBJECT_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    u2 referenceIndex = mir->dalvikInsn.vB;
+    int retval = sget_sput_common(SGET, vA, referenceIndex, false, false, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecodes sget-wide and sget-wide/volatile
+ * @param mir bytecode representation
+ * @param isVolatile is the sget a volatile access or not?
+ * @return value >= 0 when handled
+ */
+int op_sget_wide(const MIR * mir, bool isVolatile) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_WIDE
+            || mir->dalvikInsn.opcode == OP_SGET_WIDE_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    u2 referenceIndex = mir->dalvikInsn.vB;
+    int retval = sget_sput_common(SGET_WIDE, vA, referenceIndex, false,
+            isVolatile, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecodes sget-object and sget-object/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_OBJECT
+            || mir->dalvikInsn.opcode == OP_SGET_OBJECT_VOLATILE);
+    return op_sget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode sget-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_BOOLEAN);
+    return op_sget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode sget-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_BYTE);
+    return op_sget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode sget-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_CHAR);
+    return op_sget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecode sget-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sget_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SGET_SHORT);
+    return op_sget(mir);
+}
+
+/**
+ * @brief Generate native code for bytecodes sput, sput-boolean,
+ * sput-byte, sput-char, sput-object, sput-short, sput/volatile and sput-object/volatile
+ * @param mir bytecode representation
+ * @param isObj is the store an object?
+ * @return value >= 0 when handled
+ */
+int op_sput(const MIR * mir, bool isObj) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT
+            || mir->dalvikInsn.opcode == OP_SPUT_BOOLEAN
+            || mir->dalvikInsn.opcode == OP_SPUT_BYTE
+            || mir->dalvikInsn.opcode == OP_SPUT_CHAR
+            || mir->dalvikInsn.opcode == OP_SPUT_OBJECT
+            || mir->dalvikInsn.opcode == OP_SPUT_SHORT
+            || mir->dalvikInsn.opcode == OP_SPUT_VOLATILE
+            || mir->dalvikInsn.opcode == OP_SPUT_OBJECT_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    u2 referenceIndex = mir->dalvikInsn.vB;
+    int retval = sget_sput_common(SPUT, vA, referenceIndex, isObj, false, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecodes sput-wide
+ * and sput-wide/volatile
+ * @param mir bytecode representation
+ * @param isVolatile is the sput a volatile access or not?
+ * @return value >= 0 when handled
+ */
+int op_sput_wide(const MIR * mir, bool isVolatile) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_WIDE
+            || mir->dalvikInsn.opcode == OP_SPUT_WIDE_VOLATILE);
+    int vA = mir->dalvikInsn.vA;
+    u2 referenceIndex = mir->dalvikInsn.vB;
+    int retval = sget_sput_common(SPUT_WIDE, vA, referenceIndex, false,
+            isVolatile, mir);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecodes sput-object and
+ * sput-object/volatile
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_object(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_OBJECT
+            || mir->dalvikInsn.opcode == OP_SPUT_OBJECT_VOLATILE);
+    return op_sput(mir, true /*isObj*/);
+}
+
+/**
+ * @brief Generate native code for bytecode sput-boolean
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_boolean(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_BOOLEAN);
+    return op_sput(mir, false /*isObj*/);
+}
+
+/**
+ * @brief Generate native code for bytecode sput-byte
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_byte(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_BYTE);
+    return op_sput(mir, false /*isObj*/);
+}
+
+/**
+ * @brief Generate native code for bytecode sput-char
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_char(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_CHAR);
+    return op_sput(mir, false /*isObj*/);
+}
+
+/**
+ * @brief Generate native code for bytecode sput-short
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_sput_short(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_SPUT_SHORT);
+    return op_sput(mir, false /*isObj*/);
+}
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+
+/**
+ * @brief Generate native code for bytecodes iget-quick and iget-object-quick
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_QUICK
+            || mir->dalvikInsn.opcode == OP_IGET_OBJECT_QUICK);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB; //object
+    u2 fieldByteOffset = mir->dalvikInsn.vC;
+
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
+    }
+
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+
+    //If we can't ignore the NULL check
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
+        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
+    }
+
+#ifndef WITH_SELF_VERIFICATION
+    move_mem_to_reg(OpndSize_32, fieldByteOffset, 1, false, 2, false);
+    set_virtual_reg(vA, OpndSize_32, 2, false);
+#else
+    // Load address into temp reg 3
+    load_effective_addr(fieldByteOffset, 1, false, 3, false);
+    // push caller saved registers
+    pushCallerSavedRegs();
+    // Set up arguments
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    // Move value in temp reg 3 to (esp)
+    move_reg_to_mem(OpndSize_32, 3, false, 0, PhysicalReg_ESP, true);
+    // Mov opnd size to 4(esp)
+    move_imm_to_mem(OpndSize_32, int(OpndSize_32), 4, PhysicalReg_ESP, true);
+    // In order to call, the scratch reg must be set
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    // Load from shadow heap
+    call_selfVerificationLoad();
+    // Restore ESP
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    // Move result of self verification load into temp 2
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 2, false);
+    // pop caller saved registers
+    popCallerSavedRegs();
+    set_virtual_reg(vA, OpndSize_32, 2, false);
+#endif
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_wide_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_WIDE_QUICK);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB; //object
+    u2 fieldByteOffset = mir->dalvikInsn.vC;
+
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
+    }
+
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+
+    //If we can't ignore the NULL check
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
+        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
+    }
+
+#ifndef WITH_SELF_VERIFICATION
+    move_mem_to_reg(OpndSize_64, fieldByteOffset, 1, false, 1, false);
+#else
+    // Load address into temp 3
+    load_effective_addr(fieldByteOffset, 1, false, 3, false);
+    // push caller saved registers
+    pushCallerSavedRegs();
+    // Set up arguments
+    load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    // Move value in temp 3 (address) to 0(esp)
+    move_reg_to_mem(OpndSize_32, 3, false, 0, PhysicalReg_ESP, true);
+    // In order to call, the scratch reg must be set
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    // Load from shadow heap
+    call_selfVerificationLoadDoubleword();
+    // Restore ESP
+    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    // Move result of self verification load from XMM7 to temp 1(XMM)
+    move_reg_to_reg(OpndSize_64, PhysicalReg_XMM7, true, 1, false);
+    // pop caller saved registers
+    popCallerSavedRegs();
+#endif
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 0;
+}
+#undef P_GPR_1
+
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iget_object_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IGET_OBJECT_QUICK);
+    return op_iget_quick(mir);
+}
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+
+/**
+ *
+ * @param mir
+ * @param isObj
+ * @return
+ */
+int iput_quick_common(const MIR * mir, bool isObj) {
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB; //object
+    u2 fieldByteOffset = mir->dalvikInsn.vC;
+
+    // Request VR delay before transfer to temporary. Only vB needs delay.
+    // vA will have non-zero reference count since transfer to temporary for
+    // it happens after null check, thus no delay is needed.
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        requestVRFreeDelay(vB,VRDELAY_NULLCHECK);
+    }
+
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+
+    //If we can't ignore the NULL check
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
+        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
+    }
+
+    get_virtual_reg(vA, OpndSize_32, 2, false);
+#ifndef WITH_SELF_VERIFICATION
+    move_reg_to_mem(OpndSize_32, 2, false, fieldByteOffset, 1, false);
+#else
+    // Load address into temp 3; reg temp 2 will contain the data
+    load_effective_addr(fieldByteOffset, 1, false, 3, false);
+    // push caller saved registers
+    pushCallerSavedRegs();
+    // Set up arguments
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    // Move value in temp 3 namely the address to (esp)
+    move_reg_to_mem(OpndSize_32, 3, false, 0, PhysicalReg_ESP, true);
+    // Store value from temp 2 namely the data to 4(esp)
+    move_reg_to_mem(OpndSize_32, 2, false, 4, PhysicalReg_ESP, true);
+    // Mov opnd size to 8(esp)
+    move_imm_to_mem(OpndSize_32, int(OpndSize_32), 8, PhysicalReg_ESP, true);
+    // In order to call, the scratch reg must be set
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    // Load from shadow heap
+    call_selfVerificationStore();
+    // Restore ESP
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    // pop caller saved registers
+    popCallerSavedRegs();
+#endif
+    if(isObj) {
+        markCard(2/*valReg*/, 1, false, 11, false);
+    }
+    return 0;
+}
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_QUICK);
+    return iput_quick_common(mir, false /*isObj*/);
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_wide_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_WIDE_QUICK);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB; //object
+    u2 fieldByteOffset = mir->dalvikInsn.vC;
+
+    // Request VR delay before transfer to temporary. Only vB needs delay.
+    // vA will have non-zero reference count since transfer to temporary for
+    // it happens after null check, thus no delay is needed.
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        requestVRFreeDelay(vB,VRDELAY_NULLCHECK);
+    }
+
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+
+    //If we can't ignore the NULL check
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        nullCheck(1, false, 1, vB); //maybe optimized away, if not, call
+        cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
+    }
+
+    get_virtual_reg(vA, OpndSize_64, 1, false);
+
+#ifndef WITH_SELF_VERIFICATION
+    move_reg_to_mem(OpndSize_64, 1, false, fieldByteOffset, 1, false);
+#else
+    // Load address into temp 3; reg temp 1 will contain the data
+    load_effective_addr(fieldByteOffset, 1, false, 3, false);
+    // push caller saved registers
+    pushCallerSavedRegs();
+    // Set up arguments
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    // Move value in temp 3 namely the address to (esp)
+    move_reg_to_mem(OpndSize_32, 3, false, 0, PhysicalReg_ESP, true);
+    // Store value from temp 1(XMM) namely the data to 4(esp)
+    move_reg_to_mem(OpndSize_64, 1, false, 4, PhysicalReg_ESP, true);
+    // In order to call, the scratch reg must be set
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    // Load from shadow heap
+    call_selfVerificationStoreDoubleword();
+    // Restore ESP
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    // pop caller saved registers
+    popCallerSavedRegs();
+#endif
+    return 0;
+}
+#undef P_GPR_1
+
+/**
+ * @brief Generate native code for bytecode
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_iput_object_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IPUT_OBJECT_QUICK);
+    return iput_quick_common(mir, true /*isObj*/);
+}
+
diff --git a/vm/compiler/codegen/x86/lightcg/LowerHelper.cpp b/vm/compiler/codegen/x86/lightcg/LowerHelper.cpp
new file mode 100644
index 0000000..1716771
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/LowerHelper.cpp
@@ -0,0 +1,4277 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file LowerHelper.cpp
+    \brief This file implements helper functions for lowering
+
+With NCG O0: all registers are hard-coded ;
+With NCG O1: the lowering module will use variables that will be allocated to a physical register by the register allocator.
+
+register types: FS 32-bit or 64-bit;
+                XMM: SS(32-bit) SD (64-bit);
+                GPR: 8-bit, 16-bit, 32-bit;
+LowOpndRegType tells whether it is gpr, xmm or fs;
+OpndSize can be OpndSize_8, OpndSize_16, OpndSize_32, OpndSize_64
+
+A single native instruction can use multiple physical registers.
+  we can't call freeReg in the middle of emitting a native instruction,
+  since it may free the physical register used by an operand and cause two operands being allocated to the same physical register.
+
+When allocating a physical register for an operand, we can't spill the operands that are already allocated. To avoid that, we call startNativeCode before each native instruction, it resets the spill information to true for each physical register;
+  when a physical register is allocated, we set its corresponding flag to false;
+  at end of each native instruction, call endNativeCode to also reset the flags to true.
+*/
+
+#include "CompilationUnit.h"
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "NcgAot.h"
+#include "enc_wrapper.h"
+#include "vm/mterp/Mterp.h"
+#include "vm/mterp/common/FindInterface.h"
+#include "NcgHelper.h"
+#include <math.h>
+#include "interp/InterpState.h"
+#include "Scheduler.h"
+#include "Singleton.h"
+#include "ExceptionHandling.h"
+#include "compiler/Dataflow.h"
+
+extern "C" int64_t __divdi3(int64_t, int64_t);
+extern "C" int64_t __moddi3(int64_t, int64_t);
+bool isScratchPhysical;
+
+//4 tables are defined: GPR integer ALU ops, ALU ops in FPU, SSE 32-bit, SSE 64-bit
+//the index to the table is the opcode
+//add_opc,    or_opc,     adc_opc,    sbb_opc,
+//and_opc,    sub_opc,    xor_opc,    cmp_opc,
+//mul_opc,    imul_opc,   div_opc,    idiv_opc,
+//sll_opc,    srl_opc,    sra, (SSE)
+//shl_opc,    shr_opc,    sal_opc,    sar_opc, //integer shift
+//neg_opc,    not_opc,    andn_opc, (SSE)
+//n_alu
+//!mnemonic for integer ALU operations
+const  Mnemonic map_of_alu_opcode_2_mnemonic[] = {
+    Mnemonic_ADD,  Mnemonic_OR,   Mnemonic_ADC,  Mnemonic_SBB,
+    Mnemonic_AND,  Mnemonic_SUB,  Mnemonic_XOR,  Mnemonic_CMP,
+    Mnemonic_MUL,  Mnemonic_IMUL, Mnemonic_DIV,  Mnemonic_IDIV,
+    Mnemonic_Null, Mnemonic_Null, Mnemonic_Null,
+    Mnemonic_SHL,  Mnemonic_SHR,  Mnemonic_SAL,  Mnemonic_SAR,
+    Mnemonic_NEG,  Mnemonic_NOT,  Mnemonic_Null,
+    Mnemonic_Null
+};
+//!mnemonic for ALU operations in FPU
+const  Mnemonic map_of_fpu_opcode_2_mnemonic[] = {
+    Mnemonic_FADD,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_Null,  Mnemonic_FSUB,  Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_FMUL,  Mnemonic_Null,  Mnemonic_FDIV,  Mnemonic_Null,
+    Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_Null
+};
+//!mnemonic for SSE 32-bit
+const  Mnemonic map_of_sse_opcode_2_mnemonic[] = {
+    Mnemonic_ADDSD,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_Null,   Mnemonic_SUBSD, Mnemonic_XORPD, Mnemonic_Null,
+    Mnemonic_MULSD,  Mnemonic_Null,  Mnemonic_DIVSD,  Mnemonic_Null,
+    Mnemonic_Null,   Mnemonic_Null,
+    Mnemonic_Null,   Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_Null,   Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_Null
+};
+//!mnemonic for SSE 64-bit integer
+const  Mnemonic map_of_64_opcode_2_mnemonic[] = {
+    Mnemonic_PADDQ, Mnemonic_POR,   Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_PAND,  Mnemonic_PSUBQ, Mnemonic_PXOR,  Mnemonic_Null,
+    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_PSLLQ, Mnemonic_PSRLQ, Mnemonic_Null,
+    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,  Mnemonic_Null,
+    Mnemonic_Null,  Mnemonic_Null,  Mnemonic_PANDN,
+    Mnemonic_Null
+};
+
+//! \brief Simplifies update of LowOpndReg fields.
+void set_reg_opnd(LowOpndReg* op_reg, int reg, bool isPhysical,
+        LowOpndRegType type) {
+    op_reg->regType = type;
+    op_reg->regNum = reg;
+    op_reg->isPhysical = isPhysical;
+}
+
+//! \brief Simplifies update of LowOpndMem fields when only base and
+//! displacement is used.
+void set_mem_opnd(LowOpndMem* mem, int disp, int base, bool isPhysical) {
+    mem->m_disp.value = disp;
+    mem->hasScale = false;
+    mem->m_base.regType = LowOpndRegType_gp;
+    mem->m_base.regNum = base;
+    mem->m_base.isPhysical = isPhysical;
+}
+
+//! \brief Simplifies update of LowOpndMem fields when base, displacement, index,
+//! and scaling is used.
+void set_mem_opnd_scale(LowOpndMem* mem, int base, bool isPhysical, int disp,
+        int index, bool indexPhysical, int scale) {
+    mem->hasScale = true;
+    mem->m_base.regType = LowOpndRegType_gp;
+    mem->m_base.regNum = base;
+    mem->m_base.isPhysical = isPhysical;
+    mem->m_index.regNum = index;
+    mem->m_index.isPhysical = indexPhysical;
+    mem->m_disp.value = disp;
+    mem->m_scale.value = scale;
+}
+
+//! \brief Return either LowOpndRegType_xmm or LowOpndRegType_gp
+//! depending on operand size.
+//! \param size
+inline LowOpndRegType getTypeFromIntSize(OpndSize size) {
+    return size == OpndSize_64 ? LowOpndRegType_xmm : LowOpndRegType_gp;
+}
+
+//! \brief Thin layer over encoder that makes scheduling decision and
+//! is used for dumping instruction whose immediate is a target label.
+//! \param m x86 mnemonic
+//! \param size operand size
+//! \param imm When scheduling is disabled, this is the actual immediate.
+//! When scheduling is enabled, this is 0 because immediate has not been
+//! generated yet.
+//! \param label name of label for which we need to generate immediate for
+//! using the label address.
+//! \param isLocal Used to hint the distance from this instruction to label.
+//! When this is true, it means that 8 bits should be enough.
+inline LowOpLabel* lower_label(Mnemonic m, OpndSize size, int imm,
+        const char* label, bool isLocal) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_imm(m, size, imm, stream);
+        return NULL;
+    }
+    LowOpLabel * op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpLabel>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Label;
+    op->numOperands = 1;
+    snprintf(op->labelOpnd.label, LABEL_SIZE, "%s", label);
+    op->labelOpnd.isLocal = isLocal;
+    singletonPtr<Scheduler>()->updateUseDefInformation_imm(op);
+    return op;
+}
+
+//! \brief Interface to encoder.
+LowOpLabel* dump_label(Mnemonic m, OpndSize size, int imm, const char* label,
+        bool isLocal) {
+    return lower_label(m, size, imm, label, isLocal);
+}
+
+//! Used for dumping an instruction with a single immediate to the code stream
+//! but the immediate is not yet known because the target MIR block still needs
+//! code generated for it. This is only valid when scheduling is on.
+//! \pre Instruction scheduling must be enabled
+//! \param m x86 mnemonic
+//! \param targetBlockId id of the MIR block
+//! \param immediateNeedsAligned if the immediate in the instruction need to be aligned within 16B
+LowOpBlock* dump_blockid_imm(Mnemonic m, int targetBlockId,
+        bool immediateNeedsAligned) {
+    assert(gDvmJit.scheduling && "Scheduling must be turned on before "
+                "calling dump_blockid_imm");
+    LowOpBlock* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpBlock>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndSrc.type = LowOpndType_BlockId;
+    op->numOperands = 1;
+    op->blockIdOpnd.value = targetBlockId;
+    op->blockIdOpnd.immediateNeedsAligned = immediateNeedsAligned;
+    singletonPtr<Scheduler>()->updateUseDefInformation_imm(op);
+    return op;
+}
+
+//! \brief Thin layer over encoder that makes scheduling decision and
+//! is used for dumping instruction with a known immediate.
+//! \param m x86 mnemonic
+//! \param size operand size
+//! \param imm immediate
+LowOpImm* lower_imm(Mnemonic m, OpndSize size, int imm) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_imm(m, size, imm, stream);
+        return NULL;
+    }
+    LowOpImm* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpImm>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Imm;
+    op->numOperands = 1;
+    op->immOpnd.value = imm;
+    singletonPtr<Scheduler>()->updateUseDefInformation_imm(op);
+    return op;
+}
+
+//! \brief Interface to encoder.
+LowOpImm* dump_imm(Mnemonic m, OpndSize size, int imm) {
+    return lower_imm(m, size, imm);
+}
+
+//! \brief Used to update the immediate of an instruction already in the
+//! code stream.
+//! \warning This assumes that the instruction to update is already in the
+//! code stream. If it is not, the VM will abort.
+//! \param imm new immediate to use
+//! \param codePtr pointer to location in code stream where the instruction
+//! whose immediate needs updated
+//! \param updateSecondOperand This is true when second operand needs updated
+void dump_imm_update(int imm, char* codePtr, bool updateSecondOperand) {
+    // These encoder call do not need to go through scheduler since they need
+    // to be dumped at a specific location in code stream.
+    if(updateSecondOperand)
+        encoder_update_imm_rm(imm, codePtr);
+    else // update first operand
+        encoder_update_imm(imm, codePtr);
+}
+
+//! \brief Thin layer over encoder that makes scheduling decision and
+//! is used for dumping instruction with a single memory operand.
+//! \param m x86 mnemonic
+//! \param m2 Atom pseudo-mnemonic
+//! \param size operand size
+//! \param disp displacement offset
+//! \param base_reg physical register (PhysicalReg type) or a logical register
+//! \param isBasePhysical notes if base_reg is a physical register. It must
+//! be true when scheduling is enabled or else VM will abort.
+LowOpMem* lower_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_mem(m, size, disp, base_reg, isBasePhysical, stream);
+        return NULL;
+    }
+
+    if (!isBasePhysical) {
+        ALOGI("JIT_INFO: Base register not physical in lower_mem");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+    LowOpMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpMem>();
+
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Mem;
+    op->numOperands = 1;
+    op->memOpnd.mType = MemoryAccess_Unknown;
+    op->memOpnd.index = -1;
+    set_mem_opnd(&(op->memOpnd), disp, base_reg, isBasePhysical);
+    singletonPtr<Scheduler>()->updateUseDefInformation_mem(op);
+    return op;
+}
+
+//! \brief Interface to encoder which includes register allocation
+//! decision.
+//! \details With NCG O1, call freeReg to free up physical registers,
+//! then call registerAlloc to allocate a physical register for memory base
+LowOpMem* dump_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        freeReg(false);
+        //type of the base is gpr
+        int regAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true, true);
+        return lower_mem(m, m2, size, disp, regAll, true /*isBasePhysical*/);
+    } else {
+        return lower_mem(m, m2, size, disp, base_reg, isBasePhysical);
+    }
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes a single reg operand
+LowOpReg* lower_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        LowOpndRegType type, bool isPhysical) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_reg(m, size, reg, isPhysical, type, stream);
+        return NULL;
+    }
+
+    if (!isPhysical) {
+        ALOGI("JIT_INFO: Register not physical at lower_reg");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+    LowOpReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpReg>();
+
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 1;
+    set_reg_opnd(&(op->regOpnd), reg, isPhysical, type);
+    singletonPtr<Scheduler>()->updateUseDefInformation_reg(op);
+    return op;
+}
+
+//!With NCG O1, wecall freeReg to free up physical registers, then call registerAlloc to allocate a physical register for the single operand
+LowOpReg* dump_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        bool isPhysical, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        freeReg(false);
+        if (m == Mnemonic_MUL || m == Mnemonic_IMUL || m == Mnemonic_DIV
+                || m == Mnemonic_IDIV) {
+            //these four instructions use eax & edx implicitly
+            touchEax();
+            touchEdx();
+        }
+        int regAll = registerAlloc(type, reg, isPhysical, true);
+        return lower_reg(m, m2, size, regAll, type, true /*isPhysical*/);
+    } else {
+        return lower_reg(m, m2, size, reg, type, isPhysical);
+    }
+}
+
+LowOpReg* dump_reg_noalloc(Mnemonic m, OpndSize size, int reg, bool isPhysical,
+        LowOpndRegType type) {
+    return lower_reg(m, ATOM_NORMAL, size, reg, type, true /*isPhysical*/);
+}
+
+//! \brief Update fields of LowOp to generate an instruction with
+//! two register operands
+//!
+//! \details For MOVZX and MOVSX, allows source and destination
+//! operand sizes to be different, and fixes type to general purpose.
+//! \param m x86 mnemonic
+//! \param m2 Atom pseudo-mnemonic
+//! \param size operand size
+//! \param regSrc source register
+//! \param isPhysical if regSrc is a physical register
+//! \param regDest destination register
+//! \param isPhysical2 if regDest is a physical register
+//! \param type the register type. For MOVSX and MOVZX, type is fixed
+//! as general purpose
+//! \return a LowOp corresponding to the reg-reg operation
+LowOpRegReg* lower_reg_to_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int regSrc,
+        bool isPhysical, int regDest, bool isPhysical2, LowOpndRegType type) {
+
+    OpndSize srcSize = size;
+    OpndSize destSize = size;
+    LowOpndRegType srcType = type;
+    LowOpndRegType destType = type;
+
+    //We may need to override the default size and type if src and dest can be
+    //of different size / type, as follows:
+
+    //For MOVSX and MOVZX, fix the destination size and type to 32-bit and GP
+    //respectively. Note that this is a rigid requirement, and for now won't
+    //allow, for example, MOVSX Sz8, Sz16
+    if (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX) {
+        destSize = OpndSize_32;
+    }
+    //For CVTSI2SD or CVTSI2SS, the source needs to be fixed at 32-bit GP
+    else if (m == Mnemonic_CVTSI2SD || m == Mnemonic_CVTSI2SS) {
+        srcSize = OpndSize_32;
+        srcType = LowOpndRegType_gp;
+    }
+
+    if (!gDvmJit.scheduling) {
+        if (m == Mnemonic_FUCOMP || m == Mnemonic_FUCOM) {
+            stream = encoder_compare_fp_stack(m == Mnemonic_FUCOMP, regSrc - regDest,
+                    size == OpndSize_64, stream);
+        } else {
+            stream = encoder_reg_reg_diff_sizes(m, srcSize, regSrc, isPhysical, destSize,
+                    regDest, isPhysical2, destType, stream);
+        }
+        return NULL;
+    }
+
+    if (!isPhysical && !isPhysical2) {
+        ALOGI("JIT_INFO: Registers not physical at lower_reg_to_reg");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+
+    LowOpRegReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpRegReg>();
+
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = destSize;
+    op->opndDest.type = LowOpndType_Reg;
+    op->opndSrc.size = srcSize;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regDest), regDest, isPhysical2, destType);
+    set_reg_opnd(&(op->regSrc), regSrc, isPhysical, srcType);
+    singletonPtr<Scheduler>()->updateUseDefInformation_reg_to_reg(op);
+
+    return op;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes two reg operands
+
+//!Here, both registers are physical
+LowOpRegReg* dump_reg_reg_noalloc(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
+    return lower_reg_to_reg(m, ATOM_NORMAL, size, reg, true /*isPhysical*/, reg2,
+            true /*isPhysical2*/, type);
+}
+
+//! \brief Check if we have a MOV instruction which can be redundant
+//!
+//! \details Checks if the Mnemonic is a MOV which can possibly be
+//! optimized. For example, MOVSX %ax, %eax cannot be optimized, while
+//! MOV %eax, %eax is a NOP, and can be treated as such.
+//! \param m Mnemonic to check for
+//! \return whether the move can possibly be optimized away
+inline bool isMoveOptimizable(Mnemonic m) {
+    return (m == Mnemonic_MOV || m == Mnemonic_MOVQ || m == Mnemonic_MOVSS
+            || m == Mnemonic_MOVSD);
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes two reg operands
+
+//!here dst reg is already allocated to a physical reg
+//! we should not spill the physical register for dst when allocating for src
+LowOpRegReg* dump_reg_reg_noalloc_dst(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        //TODO should mark reg2 as written
+        int regAll = registerAlloc(type, reg, isPhysical, true);
+        /* remove move from one register to the same register */
+        if (isMoveOptimizable(m) && regAll == reg2)
+            return NULL;
+        return lower_reg_to_reg(m, ATOM_NORMAL, size, regAll, true /*isPhysical*/,
+                reg2, true /*isPhysical2*/, type);
+    } else {
+        return lower_reg_to_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg2,
+                isPhysical2, type);
+    }
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes two reg operands
+
+//!here src reg is already allocated to a physical reg
+LowOpRegReg* dump_reg_reg_noalloc_src(Mnemonic m, AtomOpCode m2, OpndSize size,
+        int reg, bool isPhysical, int reg2, bool isPhysical2,
+        LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        int regAll2;
+        if(isMoveOptimizable(m) && checkTempReg2(reg2, type, isPhysical2, reg, -1)) { //dst reg is logical
+            //only from get_virtual_reg_all
+            regAll2 = registerAllocMove(reg2, type, isPhysical2, reg, true);
+        } else {
+            regAll2 = registerAlloc(type, reg2, isPhysical2, true, true);
+            return lower_reg_to_reg(m, m2, size, reg, true /*isPhysical*/, regAll2,
+                    true /*isPhysical2*/, type);
+        }
+    } else {
+        return lower_reg_to_reg(m, m2, size, reg, isPhysical, reg2, isPhysical2,
+                type);
+    }
+    return NULL;
+}
+
+//! \brief Wrapper around lower_reg_to_reg with reg allocation
+//! \details Allocates both registers, checks for optimizations etc,
+//! and calls lower_reg_to_reg
+//! \param m The mnemonic
+//! \param m2 The ATOM mnemonic type
+//! \param srcSize Size of the source operand
+//! \param srcReg The source register itself
+//! \param isSrcPhysical Whether source is physical
+//! \param srcType The type of source register
+//! \param destSize Size of the destination operand
+//! \param destReg The destination register itself
+//! \param isDestPhysical Whether destination is physical
+//! \param destType The type of destination register
+//! \return The generated LowOp
+LowOpRegReg* dump_reg_reg_diff_types(Mnemonic m, AtomOpCode m2, OpndSize srcSize,
+        int srcReg, int isSrcPhysical, LowOpndRegType srcType, OpndSize destSize,
+        int destReg, int isDestPhysical, LowOpndRegType destType) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        startNativeCode(-1, -1);
+        //reg is source if m is MOV
+        freeReg(false);
+        int regAll = registerAlloc(srcType, srcReg, isSrcPhysical, true);
+        int regAll2;
+        LowOpRegReg* op = NULL;
+#ifdef MOVE_OPT2
+        if(isMoveOptimizable(m) &&
+                ((reg != PhysicalReg_EDI && srcReg != PhysicalReg_ESP && srcReg != PhysicalReg_EBP) || (!isSrcPhysical)) &&
+                isDestPhysical == false) { //dst reg is logical
+            //called from move_reg_to_reg
+            regAll2 = registerAllocMove(regDest, destType, isDestPhysical, regAll, true);
+        } else {
+#endif
+        //Do not spill regAll
+        gCompilationUnit->setCanSpillRegister (regAll, false);
+
+        regAll2 = registerAlloc(destType, destReg, isDestPhysical, true, true);
+
+        // NOTE: The use of (destSize, destType) as THE (size, type) can be confusing. In most
+        // cases, we are using this function through dump_reg_reg, so the (size, type) doesn't
+        // matter. For MOVSX and MOVZX, the size passed to dump_reg_reg is the srcSize (8 or 16),
+        // so destSize is technically the srcSize, (type is gpr) and we override destSize inside
+        // lower_reg_to_reg to 32. For CVTSI2SS and CVTSI2SD, the destSize is 64-bit, and we
+        // override the srcSize inside lower_reg_to_reg.
+        op = lower_reg_to_reg(m, m2, destSize, regAll, true /*isPhysical*/, regAll2,
+                true /*isPhysical2*/, destType);
+#ifdef MOVE_OPT2
+    }
+#endif
+        endNativeCode();
+        return op;
+    } else {
+        return lower_reg_to_reg(m, m2, destSize, srcReg, isSrcPhysical, destReg, isDestPhysical,
+                destType);
+    }
+    return NULL;
+}
+
+//! \brief Wrapper around dump_reg_reg_diff_types assuming sizes and types are same
+//! \param m The mnemonic
+//! \param m2 The ATOM mnemonic type
+//! \param size Size of the source and destination operands
+//! \param reg The source register
+//! \param isPhysical Whether source is physical
+//! \param reg2 The destination register
+//! \param isPhysical2 Whether destination is physical
+//! \param type The type of operation
+//! \return The generated LowOp
+LowOpRegReg* dump_reg_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
+    return dump_reg_reg_diff_types(m, m2, size, reg, isPhysical, type, size,
+            reg2, isPhysical2, type);
+}
+
+LowOpMemReg* lower_mem_to_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
+        int reg, bool isPhysical, LowOpndRegType type, struct ConstInfo** listPtr) {
+    bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
+    OpndSize overridden_size = isMovzs ? OpndSize_32 : size;
+    LowOpndRegType overridden_type = isMovzs ? LowOpndRegType_gp : type;
+    if (!gDvmJit.scheduling) {
+        stream = encoder_mem_to_reg_diff_sizes(m, size, disp, base_reg, isBasePhysical,
+                overridden_size, reg, isPhysical, overridden_type, stream);
+        return NULL;
+    }
+
+    if (!isBasePhysical && !isPhysical) {
+        ALOGI("JIT_INFO: Base register or operand register not physical in lower_mem_to_reg");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+
+    LowOpMemReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpMemReg>();
+    if (listPtr != NULL) {
+        op->constLink = *listPtr;
+    } else {
+        op->constLink = NULL;
+    }
+
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = overridden_size;
+    op->opndDest.type = LowOpndType_Reg;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Mem;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regDest), reg, isPhysical, overridden_type);
+    set_mem_opnd(&(op->memSrc), disp, base_reg, isBasePhysical);
+    op->memSrc.mType = mType;
+    op->memSrc.index = mIndex;
+    singletonPtr<Scheduler>()->updateUseDefInformation_mem_to_reg(op);
+    return op;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
+
+//!Here, operands are already allocated to physical registers
+LowOpMemReg* dump_mem_reg_noalloc(Mnemonic m, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
+        int reg, bool isPhysical, LowOpndRegType type) {
+    return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, base_reg,
+            true /*isBasePhysical*/, mType, mIndex, reg, true /*isPhysical*/,
+            type, NULL);
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
+
+//!Here, memory operand is already allocated to physical register
+LowOpMemReg* dump_mem_reg_noalloc_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
+        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
+        int mIndex, int reg, bool isPhysical, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        int regAll = registerAlloc(type, reg, isPhysical, true, true);
+        return lower_mem_to_reg(m, m2, size, disp, base_reg,
+                true /*isBasePhysical*/, mType, mIndex, regAll,
+                true /*isPhysical*/, type, NULL);
+    } else {
+        return lower_mem_to_reg(m, m2, size, disp, base_reg, isBasePhysical, mType,
+                mIndex, reg, isPhysical, type, NULL);
+    }
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
+
+//!
+LowOpMemReg* dump_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
+        int reg, bool isPhysical, LowOpndRegType type, struct ConstInfo** listPtr) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        startNativeCode(-1, -1);
+        freeReg(false);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+        //it is okay to use the same physical register
+        if (isMoveOptimizable(m)) {
+            freeReg(false);
+        } else {
+            //Do not spill baseAll
+            gCompilationUnit->setCanSpillRegister (baseAll, false);
+        }
+        int regAll = registerAlloc(type, reg, isPhysical, true, true);
+        endNativeCode();
+        return lower_mem_to_reg(m, m2, size, disp, baseAll,
+                true /*isBasePhysical*/, mType, mIndex, regAll,
+                true /*isPhysical*/, type, listPtr);
+    } else {
+        return lower_mem_to_reg(m, m2, size, disp, base_reg, isBasePhysical, mType,
+                mIndex, reg, isPhysical, type, NULL);
+    }
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
+
+//!
+LowOpMemReg* dump_moves_mem_reg(Mnemonic m, OpndSize size,
+                         int disp, int base_reg, bool isBasePhysical,
+             int reg, bool isPhysical) {
+#if 0 /* Commented out because it is dead code. If re-enabling, this needs to be updated
+         to work with instruction scheduling and cannot call encoder directly. Please see
+         dump_movez_mem_reg for an example */
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        startNativeCode(-1, -1);
+        freeReg(true);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
+
+        //Do not spill baseAll
+        gCompilationUnit->setCanSpillRegister (baseAll, false);
+
+        int regAll = registerAlloc(LowOpndRegType_gp, reg, isPhysical, true);
+        endNativeCode();
+        return lower_mem_reg(m, ATOM_NORMAL, size, disp, baseAll, MemoryAccess_Unknown, -1,
+            regAll, LowOpndRegType_gp, true/*moves*/);
+    } else {
+        stream = encoder_moves_mem_to_reg(size, disp, base_reg, isBasePhysical, reg, isPhysical, stream);
+    }
+#endif
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
+
+//!
+LowOpMemReg* dump_movez_mem_reg(Mnemonic m, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, int reg, bool isPhysical) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        startNativeCode(-1, -1);
+        freeReg(false);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+
+        //Do not spill baseAll
+        gCompilationUnit->setCanSpillRegister (baseAll, false);
+
+        int regAll = registerAlloc(LowOpndRegType_gp, reg, isPhysical, true, true);
+        endNativeCode();
+        return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, baseAll,
+                true /*isBasePhysical*/, MemoryAccess_Unknown, -1, regAll,
+                true /*isPhysical*/, LowOpndRegType_gp, NULL);
+    } else {
+        return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, base_reg,
+                isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical,
+                LowOpndRegType_gp, NULL);
+    }
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one reg operand
+
+//!
+LowOpRegReg* dump_movez_reg_reg(Mnemonic m, OpndSize size,
+             int reg, bool isPhysical,
+             int reg2, bool isPhysical2) {
+#if 0 /* Commented out because it is dead code. If re-enabling, this needs to be updated
+         to work with instruction scheduling and cannot call encoder directly. Please see
+         dump_movez_mem_reg for an example */
+    LowOpRegReg* op = (LowOpRegReg*)atomNew(sizeof(LowOpRegReg));
+    op->lop.opCode = m;
+    op->lop.opnd1.size = OpndSize_32;
+    op->lop.opnd1.type = LowOpndType_Reg;
+    op->lop.opnd2.size = size;
+    op->lop.opnd2.type = LowOpndType_Reg;
+    set_reg_opnd(&(op->regOpnd1), reg2, isPhysical2, LowOpndRegType_gp);
+    set_reg_opnd(&(op->regOpnd2), reg, isPhysical, LowOpndRegType_gp);
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        startNativeCode(-1, -1);
+        //reg is source if m is MOV
+        freeReg(true);
+        int regAll = registerAlloc(LowOpndRegType_gp, reg, isPhysical, true);
+
+        //Do not spill regAll
+        gCompilationUnit->setCanSpillRegister (regAll, false);
+
+        int regAll2 = registerAlloc(LowOpndRegType_gp, reg2, isPhysical2, true);
+        stream = encoder_movez_reg_to_reg(size, regAll, true, regAll2, true,
+                                          LowOpndRegType_gp, stream);
+        endNativeCode();
+    }
+    else {
+        stream = encoder_movez_reg_to_reg(size, reg, isPhysical, reg2,
+                                        isPhysical2, LowOpndRegType_gp, stream);
+    }
+#endif
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
+
+//!
+LowOpMemReg* lower_mem_scale_to_reg(Mnemonic m, OpndSize size, int base_reg,
+        bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical,
+        int scale, int reg, bool isPhysical, LowOpndRegType type) {
+    bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
+    OpndSize overridden_size = isMovzs ? OpndSize_32 : size;
+    LowOpndRegType overridden_type = isMovzs ? LowOpndRegType_gp : type;
+    if (!gDvmJit.scheduling) {
+        stream = encoder_mem_disp_scale_to_reg_diff_sizes(m, size, base_reg, isBasePhysical,
+                disp, index_reg, isIndexPhysical, scale, overridden_size, reg,
+                isPhysical, overridden_type, stream);
+        return NULL;
+    }
+
+    if (!isBasePhysical && !isIndexPhysical && !isPhysical) {
+        ALOGI("JIT_INFO: Base, index or operand register not physical at lower_mem_scale_to_reg");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+    LowOpMemReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpMemReg>();
+
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndDest.size = overridden_size;
+    op->opndDest.type = LowOpndType_Reg;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Mem;
+    op->numOperands = 2;
+    op->memSrc.mType = MemoryAccess_Unknown;
+    op->memSrc.index = -1;
+    set_reg_opnd(&(op->regDest), reg, isPhysical, overridden_type);
+    set_mem_opnd_scale(&(op->memSrc), base_reg, isBasePhysical, disp,
+            index_reg, isIndexPhysical, scale);
+    singletonPtr<Scheduler>()->updateUseDefInformation_mem_to_reg(op);
+    return op;
+}
+
+LowOpMemReg* dump_mem_scale_reg(Mnemonic m, OpndSize size, int base_reg,
+        bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical,
+        int scale, int reg, bool isPhysical, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        startNativeCode(-1, -1);
+        freeReg(false);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+
+        //Do not spill baseAll
+        gCompilationUnit->setCanSpillRegister (baseAll, false);
+
+        int indexAll = registerAlloc(LowOpndRegType_gp, index_reg,
+                isIndexPhysical, true);
+        if (isMoveOptimizable(m)) {
+            freeReg(false);
+
+            //We can now spill base
+            gCompilationUnit->setCanSpillRegister (baseAll, true);
+        } else {
+            //Do not spill indexAll
+            gCompilationUnit->setCanSpillRegister (indexAll, false);
+        }
+        bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
+        int regAll = registerAlloc(isMovzs ? LowOpndRegType_gp : type, reg,
+                isPhysical, true, true);
+        endNativeCode();
+        return lower_mem_scale_to_reg(m, size, baseAll, true /*isBasePhysical*/,
+                disp, indexAll, true /*isIndexPhysical*/, scale, regAll,
+                true /*isPhysical*/, type);
+    } else {
+        return lower_mem_scale_to_reg(m, size, base_reg, isBasePhysical, disp,
+                index_reg, isIndexPhysical, scale, reg, isPhysical, type);
+    }
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
+
+//!
+LowOpRegMem* lower_reg_to_mem_scale(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int base_reg, bool isBasePhysical, int disp,
+        int index_reg, bool isIndexPhysical, int scale, LowOpndRegType type) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_reg_mem_disp_scale(m, size, reg, isPhysical, base_reg,
+                isBasePhysical, disp, index_reg, isIndexPhysical, scale, type,
+                stream);
+        return NULL;
+    }
+
+    if (!isBasePhysical && !isIndexPhysical && !isPhysical) {
+        ALOGI("JIT_INFO: Base, index or operand register not physical in lower_reg_to_mem_scale");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+    LowOpRegMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpRegMem>();
+
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Mem;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    op->memDest.mType = MemoryAccess_Unknown;
+    op->memDest.index = -1;
+    set_reg_opnd(&(op->regSrc), reg, isPhysical, type);
+    set_mem_opnd_scale(&(op->memDest), base_reg, isBasePhysical, disp,
+            index_reg, isIndexPhysical, scale);
+    singletonPtr<Scheduler>()->updateUseDefInformation_reg_to_mem(op);
+    return op;
+}
+
+LowOpRegMem* dump_reg_mem_scale(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int base_reg, bool isBasePhysical, int disp,
+        int index_reg, bool isIndexPhysical, int scale, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        startNativeCode(-1, -1);
+        freeReg(false);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+
+        //Do not spill baseAll
+        gCompilationUnit->setCanSpillRegister (baseAll, false);
+
+        int indexAll = registerAlloc(LowOpndRegType_gp, index_reg,
+                isIndexPhysical, true);
+
+        //Do not spill indexAll
+        gCompilationUnit->setCanSpillRegister (indexAll, false);
+
+        int regAll = registerAlloc(type, reg, isPhysical, true, true);
+        endNativeCode();
+        return lower_reg_to_mem_scale(m, size, regAll, true /*isPhysical*/,
+                baseAll, true /*isBasePhysical*/, disp, indexAll,
+                true /*isIndexPhysical*/, scale, type);
+    } else {
+        return lower_reg_to_mem_scale(m, size, reg, isPhysical, base_reg,
+                isBasePhysical, disp, index_reg, isIndexPhysical, scale, type);
+    }
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
+
+//!Here operands are already allocated
+LowOpRegMem* lower_reg_to_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        bool isPhysical, int disp, int base_reg, bool isBasePhysical,
+        MemoryAccessType mType, int mIndex, LowOpndRegType type) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_reg_mem(m, size, reg, isPhysical, disp, base_reg,
+                isBasePhysical, type, stream);
+        return NULL;
+    }
+
+    if (!isBasePhysical && !isPhysical) {
+        ALOGI("JIT_INFO: Base or operand register not physical in lower_reg_to_mem");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+    LowOpRegMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpRegMem>();
+
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Mem;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regSrc), reg, isPhysical, type);
+    set_mem_opnd(&(op->memDest), disp, base_reg, isBasePhysical);
+    op->memDest.mType = mType;
+    op->memDest.index = mIndex;
+    singletonPtr<Scheduler>()->updateUseDefInformation_reg_to_mem(op);
+    return op;
+}
+
+LowOpRegMem* dump_reg_mem_noalloc(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int disp, int base_reg, bool isBasePhysical,
+        MemoryAccessType mType, int mIndex, LowOpndRegType type) {
+    return lower_reg_to_mem(m, ATOM_NORMAL, size, reg, true /*isPhysical*/, disp,
+            base_reg, true /*isBasePhysical*/, mType, mIndex, type);
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
+
+//!
+LowOpRegMem* dump_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        bool isPhysical, int disp, int base_reg, bool isBasePhysical,
+        MemoryAccessType mType, int mIndex, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        startNativeCode(-1, -1);
+        freeReg(false);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+
+        //Do not spill baseAll
+        gCompilationUnit->setCanSpillRegister (baseAll, false);
+
+        int regAll = registerAlloc(type, reg, isPhysical, true);
+        endNativeCode();
+        return lower_reg_to_mem(m, m2, size, regAll, true /*isPhysical*/, disp,
+                baseAll, true /*isBasePhysical*/, mType, mIndex, type);
+    } else {
+        return lower_reg_to_mem(m, m2, size, reg, isPhysical, disp, base_reg,
+                isBasePhysical, mType, mIndex, type);
+    }
+    return NULL;
+}
+
+//! \brief Checks if Mnemonic sign extends imm operand
+//! \details Information taken from Atom instruction manual
+//! \param mn Mnemonic to check for
+//! \return whether mn sign extends its imm operand
+bool mnemonicSignExtendsImm(Mnemonic mn) {
+    if ((mn == Mnemonic_ADD) || (mn == Mnemonic_ADC)
+            || (mn == Mnemonic_SUB) || (mn == Mnemonic_SBB)) {
+        return true;
+    }
+    return false;
+}
+
+//! \brief Returns minimum size to fit an imm
+//! \param imm The immediate value to check for
+//! \return the OpndSize befitting the imm
+OpndSize minSizeForImm(int imm) {
+    //Don't care about signed values
+    if (imm < 0)
+        return OpndSize_32;
+    if (imm < 128)
+        return OpndSize_8;
+    if (imm < 32768)
+        return OpndSize_16;
+
+    return OpndSize_32;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one immediate and one reg operand
+
+//!The reg operand is allocated already
+LowOpImmReg* lower_imm_to_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
+        int reg, bool isPhysical, LowOpndRegType type, bool chaining) {
+    // size of opnd1 can be different from size of opnd2:
+    OpndSize overridden_size = size;
+    if (m == Mnemonic_SAL || m == Mnemonic_SHR || m == Mnemonic_SHL
+                    || m == Mnemonic_SAR || m == Mnemonic_ROR) {
+                    overridden_size = OpndSize_8;
+    }
+    else if (mnemonicSignExtendsImm(m)) {
+        overridden_size = minSizeForImm(imm);
+    }
+    if (!gDvmJit.scheduling) {
+        stream = encoder_imm_reg_diff_sizes(m, overridden_size, imm, size, reg, isPhysical, type, stream);
+        return NULL;
+    }
+
+    if (!isPhysical) {
+        ALOGI("JIT_INFO: Operand register not physical in lower_imm_to_reg");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+    LowOpImmReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpImmReg>();
+
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    op->opndSrc.size = overridden_size;
+    op->opndSrc.type = chaining ? LowOpndType_Chain : LowOpndType_Imm;
+    set_reg_opnd(&(op->regDest), reg, isPhysical, type);
+    op->immSrc.value = imm;
+    singletonPtr<Scheduler>()->updateUseDefInformation_imm_to_reg(op);
+    return op;
+}
+
+LowOpImmReg* dump_imm_reg_noalloc(Mnemonic m, OpndSize size, int imm, int reg,
+        bool isPhysical, LowOpndRegType type) {
+    return lower_imm_to_reg(m, ATOM_NORMAL, size, imm, reg, true /*isPhysical*/,
+            type, false);
+}
+
+LowOpImmReg* dump_imm_reg_noalloc_alu(Mnemonic m, OpndSize size, int imm, int reg,
+        bool isPhysical, LowOpndRegType type) {
+    return lower_imm_to_reg(m, ATOM_NORMAL_ALU, size, imm, reg, true /*isPhysical*/,
+            type, false);
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one immediate and one reg operand
+
+//!
+LowOpImmReg* dump_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
+        int reg, bool isPhysical, LowOpndRegType type, bool chaining) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        freeReg(false);
+        int regAll = registerAlloc(type, reg, isPhysical, true, true);
+        return lower_imm_to_reg(m, m2, size, imm, regAll, true /*isPhysical*/,
+                type, chaining);
+    } else {
+        return lower_imm_to_reg(m, m2, size, imm, reg, isPhysical, type, chaining);
+    }
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one immediate and one mem operand
+
+//!The mem operand is already allocated
+LowOpImmMem* lower_imm_to_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
+        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
+        int mIndex, bool chaining) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_imm_mem(m, size, imm, disp, base_reg, isBasePhysical,
+                stream);
+        return NULL;
+    }
+
+    if (!isBasePhysical) {
+        ALOGI("JIT_INFO: Base register not physical in lower_imm_to_mem");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+    LowOpImmMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpImmMem>();
+
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Mem;
+    op->opndSrc.size = size;
+    op->opndSrc.type = chaining ? LowOpndType_Chain : LowOpndType_Imm;
+    op->numOperands = 2;
+    set_mem_opnd(&(op->memDest), disp, base_reg, isBasePhysical);
+    op->immSrc.value = imm;
+    op->memDest.mType = mType;
+    op->memDest.index = mIndex;
+    singletonPtr<Scheduler>()->updateUseDefInformation_imm_to_mem(op);
+    return op;
+}
+
+LowOpImmMem* dump_imm_mem_noalloc(Mnemonic m, OpndSize size, int imm, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex) {
+    return lower_imm_to_mem(m, ATOM_NORMAL, size, imm, disp, base_reg,
+            true /*isBasePhysical*/, mType, mIndex, false);
+}
+
+LowOpImmMem* dump_imm_mem_noalloc_alu(Mnemonic m, OpndSize size, int imm, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex) {
+    return lower_imm_to_mem(m, ATOM_NORMAL_ALU, size, imm, disp, base_reg,
+            true /*isBasePhysical*/, mType, mIndex, false);
+}
+
+//!update fields of LowOp and generate a x86 instruction that takes one immediate and one mem operand
+
+//!
+LowOpImmMem* dump_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
+        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
+        int mIndex, bool chaining) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        /* do not free register if the base is %edi, %esp, or %ebp
+         make sure dump_imm_mem will only generate a single instruction */
+        if (!isBasePhysical
+                || (base_reg != PhysicalReg_EDI && base_reg != PhysicalReg_ESP
+                        && base_reg != PhysicalReg_EBP)) {
+            freeReg(false);
+        }
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+        return lower_imm_to_mem(m, m2, size, imm, disp, baseAll,
+                true /*isBasePhysical*/, mType, mIndex, chaining);
+    } else {
+        return lower_imm_to_mem(m, m2, size, imm, disp, base_reg, isBasePhysical,
+                mType, mIndex, chaining);
+    }
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that uses the FP stack and takes one mem operand
+
+//!
+LowOpRegMem* lower_fp_to_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
+        int mIndex) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_fp_mem(m, size, reg, disp, base_reg, isBasePhysical,
+                stream);
+        return NULL;
+    }
+
+    if (!isBasePhysical) {
+        ALOGI("JIT_INFO: Base register not physical in lower_fp_to_mem");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+    LowOpRegMem* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpRegMem>();
+
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Mem;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regSrc), PhysicalReg_ST0 + reg, true,
+            LowOpndRegType_fs);
+    set_mem_opnd(&(op->memDest), disp, base_reg, isBasePhysical);
+    op->memDest.mType = mType;
+    op->memDest.index = mIndex;
+    singletonPtr<Scheduler>()->updateUseDefInformation_fp_to_mem(op);
+    return op;
+}
+
+LowOpRegMem* dump_fp_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
+        int mIndex) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        freeReg(false);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+        return lower_fp_to_mem(m, m2, size, reg, disp, baseAll,
+                true /*isBasePhysical*/, mType, mIndex);
+    } else {
+        return lower_fp_to_mem(m, m2, size, reg, disp, base_reg, isBasePhysical,
+                mType, mIndex);
+    }
+    return NULL;
+}
+
+//!update fields of LowOp and generate a x86 instruction that uses the FP stack and takes one mem operand
+
+//!
+LowOpMemReg* lower_mem_to_fp(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
+        int reg) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_mem_fp(m, size, disp, base_reg, isBasePhysical, reg,
+                stream);
+        return NULL;
+    }
+
+    if (!isBasePhysical) {
+        ALOGI("JIT_INFO: Base register not physical in lower_mem_to_fp");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return NULL;
+    }
+
+    LowOpMemReg* op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOpMemReg>();
+
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Reg;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Mem;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regDest), PhysicalReg_ST0 + reg, true,
+            LowOpndRegType_fs);
+    set_mem_opnd(&(op->memSrc), disp, base_reg, isBasePhysical);
+    op->memSrc.mType = mType;
+    op->memSrc.index = mIndex;
+    singletonPtr<Scheduler>()->updateUseDefInformation_mem_to_fp(op);
+    return op;
+}
+
+LowOpMemReg* dump_mem_fp(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
+        int reg) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        freeReg(false);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+        return lower_mem_to_fp(m, m2, size, disp, baseAll,
+                true /*isBasePhysical*/, mType, mIndex, reg);
+    } else {
+        return lower_mem_to_fp(m, m2, size, disp, base_reg, isBasePhysical,
+                mType, mIndex, reg);
+    }
+    return NULL;
+}
+///////////////////////////////////////////////////////////////
+///////////////////////////////////////////////////////////////
+//OPERAND ORDER:
+//LowOp same as EncoderBase destination first
+//parameter order of function: src first
+
+////////////////////////////////// IA32 native instructions //////////////
+//! generate a native instruction lea
+
+//!
+void load_effective_addr(int disp, int base_reg, bool isBasePhysical,
+                          int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_LEA;
+    dump_mem_reg(m, ATOM_NORMAL, OpndSize_32, disp, base_reg, isBasePhysical,
+        MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_gp, NULL);
+}
+//! generate a native instruction lea
+
+//! Computes the effective address of the source operand and stores it in the
+//! first operand. (lea reg, [base_reg + index_reg*scale])
+void load_effective_addr_scale(int base_reg, bool isBasePhysical,
+                int index_reg, bool isIndexPhysical, int scale,
+                int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_LEA;
+    dump_mem_scale_reg(m, OpndSize_32,
+                              base_reg, isBasePhysical, 0/*disp*/, index_reg, isIndexPhysical, scale,
+                              reg, isPhysical, LowOpndRegType_gp);
+}
+
+//! lea reg, [base_reg + index_reg*scale + disp]
+void load_effective_addr_scale_disp(int base_reg, bool isBasePhysical, int disp,
+                int index_reg, bool isIndexPhysical, int scale,
+                int reg, bool isPhysical) {
+    dump_mem_scale_reg(Mnemonic_LEA, OpndSize_32, base_reg, isBasePhysical, disp,
+            index_reg, isIndexPhysical, scale, reg, isPhysical,
+            LowOpndRegType_gp);
+}
+//!fldcw
+
+//!
+void load_fpu_cw(int disp, int base_reg, bool isBasePhysical) {
+    Mnemonic m = Mnemonic_FLDCW;
+    dump_mem(m, ATOM_NORMAL, OpndSize_16, disp, base_reg, isBasePhysical);
+}
+//!fnstcw
+
+//!
+void store_fpu_cw(bool checkException, int disp, int base_reg, bool isBasePhysical) {
+    assert(!checkException);
+    Mnemonic m = Mnemonic_FNSTCW;
+    dump_mem(m, ATOM_NORMAL, OpndSize_16, disp, base_reg, isBasePhysical);
+}
+//!cdq
+
+//!
+void convert_integer(OpndSize srcSize, OpndSize dstSize) { //cbw, cwd, cdq
+    assert(srcSize == OpndSize_32 && dstSize == OpndSize_64);
+    Mnemonic m = Mnemonic_CDQ;
+    dump_reg_reg(m, ATOM_NORMAL, OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_EDX, true, LowOpndRegType_gp);
+}
+
+//! \brief Generates the CVTSI2SD and CVTSI2SS opcodes
+//! \details performs cvtsi2** destReg, srcReg
+//! NOTE: Even for cvtsi2ss, the destination is still XMM
+//! and needs to be moved to a GPR.
+//! \param srcReg the src register
+//! \param isSrcPhysical if the srcReg is a physical register
+//! \param destReg the destination register
+//! \param isDestPhysical if destReg is a physical register
+//! \param isDouble if the destination needs to be a double value (float otherwise)
+void convert_int_to_fp(int srcReg, bool isSrcPhysical, int destReg, bool isDestPhysical, bool isDouble) {
+    Mnemonic m = isDouble ? Mnemonic_CVTSI2SD : Mnemonic_CVTSI2SS;
+    dump_reg_reg_diff_types(m, ATOM_NORMAL, OpndSize_32, srcReg, isSrcPhysical, LowOpndRegType_gp,
+            OpndSize_64, destReg, isDestPhysical, LowOpndRegType_xmm);
+}
+
+//!fld: load from memory (float or double) to stack
+
+//!
+void load_fp_stack(LowOp* op, OpndSize size, int disp, int base_reg, bool isBasePhysical) {//fld(s|l)
+    Mnemonic m = Mnemonic_FLD;
+    dump_mem_fp(m, ATOM_NORMAL, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, 0); //ST0
+}
+//! fild: load from memory (int or long) to stack
+
+//!
+void load_int_fp_stack(OpndSize size, int disp, int base_reg, bool isBasePhysical) {//fild(ll|l)
+    Mnemonic m = Mnemonic_FILD;
+    dump_mem_fp(m, ATOM_NORMAL, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, 0); //ST0
+}
+//!fild: load from memory (absolute addr)
+
+//!
+void load_int_fp_stack_imm(OpndSize size, int imm) {//fild(ll|l)
+    return load_int_fp_stack(size, imm, PhysicalReg_Null, true);
+}
+//!fst: store from stack to memory (float or double)
+
+//!
+void store_fp_stack(LowOp* op, bool pop, OpndSize size, int disp, int base_reg, bool isBasePhysical) {//fst(p)(s|l)
+    Mnemonic m = pop ? Mnemonic_FSTP : Mnemonic_FST;
+    dump_fp_mem(m, ATOM_NORMAL, size, 0, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1);
+}
+//!fist: store from stack to memory (int or long)
+
+//!
+void store_int_fp_stack(LowOp* op, bool pop, OpndSize size, int disp, int base_reg, bool isBasePhysical) {//fist(p)(l)
+    Mnemonic m = pop ? Mnemonic_FISTP : Mnemonic_FIST;
+    dump_fp_mem(m, ATOM_NORMAL, size, 0, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1);
+}
+//!cmp reg, mem
+
+//!
+void compare_reg_mem(LowOp* op, OpndSize size, int reg, bool isPhysical,
+              int disp, int base_reg, bool isBasePhysical) {
+    Mnemonic m = Mnemonic_CMP;
+    dump_reg_mem(m, ATOM_NORMAL, size, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, getTypeFromIntSize(size));
+}
+//!cmp mem, reg
+
+//!
+void compare_mem_reg(OpndSize size,
+              int disp, int base_reg, bool isBasePhysical,
+              int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_CMP;
+    dump_mem_reg(m, ATOM_NORMAL, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical, getTypeFromIntSize(size), NULL);
+}
+//! compare a VR with a temporary variable
+
+//!
+void compare_VR_reg_all(OpndSize size,
+             int vA,
+             int reg, bool isPhysical, Mnemonic m) {
+    LowOpndRegType type = getTypeFromIntSize(size);
+    LowOpndRegType pType = type;
+    if(m == Mnemonic_COMISS) {
+        size = OpndSize_32;
+        type = LowOpndRegType_ss;
+        pType = LowOpndRegType_xmm;
+    }
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        int tmpValue[2];
+        int isConst = isVirtualRegConstant(vA, type, tmpValue, true/*updateRefCount*/);
+        if(isConst == 3) {
+            if(m == Mnemonic_COMISS) {
+#ifdef DEBUG_NCG_O1
+                ALOGI("VR is const and SS in compare_VR_reg");
+#endif
+                bool storedAddr = false;
+
+                if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
+                    tmpValue[1] = 0;// set higher 32 bits to zero
+                    // create a new record of a constant
+                    addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, false);
+
+                    // save mem access location in constList
+                    const int offset = 3; // offset is 3 for COMISS
+                    storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
+
+                    ConstInfo* tmpPtr = gCompilationUnit->constListHead;
+                    if (storedAddr == true){
+#ifdef DEBUG_CONST
+                        ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
+                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL,
+                                tmpPtr->valueH, tmpPtr->valueH);
+#endif
+                    } else {
+                        ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
+                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL,
+                                tmpPtr->valueH, tmpPtr->valueH);
+                    }
+                }
+                // Lower mem_reg instruction with constant to be accessed from constant data section
+                if (storedAddr == true) {
+                    int dispAddr =  getGlobalDataAddr("64bits");
+                    dump_mem_reg(m, ATOM_NORMAL, OpndSize_32, dispAddr, PhysicalReg_Null, true,
+                                     MemoryAccess_Constants, vA, reg, isPhysical, pType,
+                                     &(gCompilationUnit->constListHead));
+                } else {
+                    writeBackConstVR(vA, tmpValue[0]);
+                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
+                }
+                return;
+            }
+            else if(size != OpndSize_64) {
+#ifdef DEBUG_NCG_O1
+                ALOGI("VR is const and 32 bits in compare_VR_reg");
+#endif
+                dump_imm_reg(m, ATOM_NORMAL, size, tmpValue[0], reg, isPhysical, pType, false);
+                return;
+            }
+            else if(size == OpndSize_64) {
+#ifdef DEBUG_NCG_O1
+                ALOGI("VR is const and 64 bits in compare_VR_reg");
+#endif
+                bool storedAddr = false;
+
+                if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
+                    // create a new record of a constant
+                    addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, false);
+                    const int offset = 4; // offset is 4 for COMISD
+
+                    // save mem access location in constList
+                    storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
+
+                    ConstInfo* tmpPtr = gCompilationUnit->constListHead;
+                    if (storedAddr == true){
+#ifdef DEBUG_CONST
+                        ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
+                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL,
+                                tmpPtr->valueH, tmpPtr->valueH);
+#endif
+                    } else {
+                        ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
+                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL,
+                                tmpPtr->valueH, tmpPtr->valueH);
+                    }
+                }
+                // Lower mem_reg instruction with constant to be accessed from constant data section
+                if (storedAddr == true) {
+                    int dispAddr =  getGlobalDataAddr("64bits");
+                    dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, dispAddr, PhysicalReg_Null, true,
+                                     MemoryAccess_Constants, vA, reg, isPhysical, LowOpndRegType_xmm,
+                                     &(gCompilationUnit->constListHead));
+                } else {
+                    writeBackConstVR(vA, tmpValue[0]);
+                    writeBackConstVR(vA+1, tmpValue[1]);
+                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
+                                    MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
+                }
+                return;
+            }
+        }
+        if(isConst == 1) writeBackConstVR(vA, tmpValue[0]);
+        if(isConst == 2) writeBackConstVR(vA+1, tmpValue[1]);
+        freeReg(false);
+        int regAll = checkVirtualReg(vA, type, 0/*do not update*/);
+        if(regAll != PhysicalReg_Null) { //do not spill regAll when allocating register for dst
+            startNativeCode(-1, -1);
+
+            //Do not spill regAll
+            gCompilationUnit->setCanSpillRegister (regAll, false);
+
+            dump_reg_reg_noalloc_src(m, ATOM_NORMAL, size, regAll, true, reg, isPhysical, pType);
+            endNativeCode();
+        }
+        else {
+            //virtual register is not allocated to a physical register
+            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
+                MemoryAccess_VR, vA, reg, isPhysical, pType);
+        }
+        updateRefCount(vA, type);
+        return;
+    } else {
+        dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
+            MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
+        return;
+    }
+}
+void compare_VR_reg(OpndSize size,
+             int vA,
+             int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_CMP;
+    return compare_VR_reg_all(size, vA, reg, isPhysical, m);
+}
+void compare_VR_ss_reg(int vA, int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_COMISS;
+    return compare_VR_reg_all(OpndSize_32, vA, reg, isPhysical, m);
+}
+void compare_VR_sd_reg(int vA, int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_COMISD;
+    return compare_VR_reg_all(OpndSize_64, vA, reg, isPhysical, m);
+}
+//!load VR to stack
+
+//!
+void load_fp_stack_VR_all(OpndSize size, int vB, Mnemonic m) {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        //can't load from immediate to fp stack
+        int tmpValue[2];
+        int isConst = isVirtualRegConstant(vB, getTypeFromIntSize(size), tmpValue, false/*updateRefCount*/);
+        if(isConst > 0) {
+            if(size != OpndSize_64) {
+#ifdef DEBUG_NCG_O1
+                ALOGI("VR is const and 32 bits in load_fp_stack");
+#endif
+                writeBackConstVR(vB, tmpValue[0]);
+            }
+            else {
+#ifdef DEBUG_NCG_O1
+                ALOGI("VR is const and 64 bits in load_fp_stack_VR");
+#endif
+                if(isConst == 1 || isConst == 3) writeBackConstVR(vB, tmpValue[0]);
+                if(isConst == 2 || isConst == 3) writeBackConstVR(vB+1, tmpValue[1]);
+            }
+        }
+        else { //if VR was updated by a def of gp, a xfer point was inserted
+            //if VR was updated by a def of xmm, a xfer point was inserted
+#if 0
+            int regAll = checkVirtualReg(vB, size, 1);
+            if(regAll != PhysicalReg_Null) //dump from register to memory
+                dump_reg_mem_noalloc(m, size, regAll, true, 4*vB, PhysicalReg_FP, true,
+                    MemoryAccess_VR, vB, getTypeFromIntSize(size));
+#endif
+        }
+        dump_mem_fp(m, ATOM_NORMAL, size, 4*vB, PhysicalReg_FP, true, MemoryAccess_VR, vB, 0);
+    } else {
+        dump_mem_fp(m, ATOM_NORMAL, size, 4*vB, PhysicalReg_FP, true, MemoryAccess_VR, vB, 0);
+    }
+}
+//!load VR(float or double) to stack
+
+//!
+void load_fp_stack_VR(OpndSize size, int vA) {//fld(s|l)
+    Mnemonic m = Mnemonic_FLD;
+    return load_fp_stack_VR_all(size, vA, m);
+}
+//!load VR(int or long) to stack
+
+//!
+void load_int_fp_stack_VR(OpndSize size, int vA) {//fild(ll|l)
+    Mnemonic m = Mnemonic_FILD;
+    return load_fp_stack_VR_all(size, vA, m);
+}
+//!store from stack to VR (float or double)
+
+//!
+void store_fp_stack_VR(bool pop, OpndSize size, int vA) {//fst(p)(s|l)
+    Mnemonic m = pop ? Mnemonic_FSTP : Mnemonic_FST;
+    dump_fp_mem(m, ATOM_NORMAL, size, 0, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        if(size == OpndSize_32)
+            updateVirtualReg(vA, LowOpndRegType_fs_s);
+        else
+            updateVirtualReg(vA, LowOpndRegType_fs);
+    }
+}
+//!store from stack to VR (int or long)
+
+//!
+void store_int_fp_stack_VR(bool pop, OpndSize size, int vA) {//fist(p)(l)
+    Mnemonic m = pop ? Mnemonic_FISTP : Mnemonic_FIST;
+    dump_fp_mem(m, ATOM_NORMAL, size, 0, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        if(size == OpndSize_32)
+            updateVirtualReg(vA, LowOpndRegType_fs_s);
+        else
+            updateVirtualReg(vA, LowOpndRegType_fs);
+    }
+}
+//! ALU ops in FPU, one operand is a VR
+
+//!
+void fpu_VR(ALU_Opcode opc, OpndSize size, int vA) {
+    Mnemonic m = map_of_fpu_opcode_2_mnemonic[opc];
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        int tmpValue[2];
+        int isConst = isVirtualRegConstant(vA, getTypeFromIntSize(size), tmpValue, false/*updateRefCount*/);
+        if(isConst > 0) {
+            if(size != OpndSize_64) {
+                //allocate a register for dst
+                writeBackConstVR(vA, tmpValue[0]);
+            }
+            else {
+                if((isConst == 1 || isConst == 3) && size == OpndSize_64) {
+                    writeBackConstVR(vA, tmpValue[0]);
+                }
+                if((isConst == 2 || isConst == 3) && size == OpndSize_64) {
+                    writeBackConstVR(vA+1, tmpValue[1]);
+                }
+            }
+        }
+        if(!isInMemory(vA, size)) {
+            ALOGI("JIT_INFO: VR not in memory for FPU operation");
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return;
+        }
+        dump_mem_fp(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, 0);
+    } else {
+        dump_mem_fp(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, 0);
+    }
+}
+//! cmp imm reg
+
+//!
+void compare_imm_reg(OpndSize size, int imm,
+              int reg, bool isPhysical) {
+    if(imm == 0) {
+        LowOpndRegType type = getTypeFromIntSize(size);
+        Mnemonic m = Mnemonic_TEST;
+        if(gDvm.executionMode == kExecutionModeNcgO1) {
+            freeReg(false);
+            int regAll = registerAlloc(type, reg, isPhysical, true);
+            lower_reg_to_reg(m, ATOM_NORMAL, size, regAll, true /*isPhysical*/, regAll, true /*isPhysical2*/, type);
+        } else {
+            lower_reg_to_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg, isPhysical, type);
+        }
+        return;
+    }
+    Mnemonic m = Mnemonic_CMP;
+    dump_imm_reg(m, ATOM_NORMAL, size, imm, reg, isPhysical, getTypeFromIntSize(size), false);
+}
+//! cmp imm mem
+
+//!
+void compare_imm_mem(OpndSize size, int imm,
+              int disp, int base_reg, bool isBasePhysical) {
+    Mnemonic m = Mnemonic_CMP;
+    dump_imm_mem(m, ATOM_NORMAL, size, imm, disp,
+                        base_reg, isBasePhysical, MemoryAccess_Unknown, -1, false);
+}
+//! cmp imm VR
+
+//!
+void compare_imm_VR(OpndSize size, int imm,
+             int vA) {
+    Mnemonic m = Mnemonic_CMP;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        if(size != OpndSize_32) {
+            ALOGI("JIT_INFO: Only 32 bits supported in compare_imm_VR");
+            SET_JIT_ERROR(kJitErrorRegAllocFailed);
+            return;
+        }
+        int tmpValue[2];
+        int isConst = isVirtualRegConstant(vA, getTypeFromIntSize(size), tmpValue, false/*updateRefCount*/);
+        if(isConst > 0) {
+            writeBackConstVR(vA, tmpValue[0]);
+        }
+        int regAll = checkVirtualReg(vA, getTypeFromIntSize(size), 0);
+        if(regAll != PhysicalReg_Null)
+            dump_imm_reg_noalloc(m, size, imm, regAll, true, LowOpndRegType_gp);
+        else
+            dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true,
+                MemoryAccess_VR, vA);
+        updateRefCount(vA, getTypeFromIntSize(size));
+    } else {
+        dump_imm_mem(m, ATOM_NORMAL, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, false);
+    }
+}
+//! cmp reg reg
+
+//!
+void compare_reg_reg(int reg1, bool isPhysical1,
+              int reg2, bool isPhysical2) {
+    Mnemonic m = Mnemonic_CMP;
+    dump_reg_reg(m, ATOM_NORMAL, OpndSize_32, reg1, isPhysical1, reg2, isPhysical2, LowOpndRegType_gp);
+}
+void compare_reg_reg_16(int reg1, bool isPhysical1,
+              int reg2, bool isPhysical2) {
+    Mnemonic m = Mnemonic_CMP;
+    dump_reg_reg(m, ATOM_NORMAL, OpndSize_16, reg1, isPhysical1, reg2, isPhysical2, LowOpndRegType_gp);
+}
+
+//! comiss mem reg
+
+//!SSE, XMM: comparison of floating point numbers
+void compare_ss_mem_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
+             int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_COMISS;
+    dump_mem_reg(m, ATOM_NORMAL, OpndSize_32, disp, base_reg, isBasePhysical,
+        MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_xmm, NULL);
+}
+//! comiss reg reg
+
+//!
+void compare_ss_reg_with_reg(LowOp* op, int reg1, bool isPhysical1,
+                  int reg2, bool isPhysical2) {
+    Mnemonic m = Mnemonic_COMISS;
+    dump_reg_reg(m,  ATOM_NORMAL, OpndSize_32, reg1, isPhysical1, reg2, isPhysical2, LowOpndRegType_xmm);
+}
+//! comisd mem reg
+
+//!
+void compare_sd_mem_with_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
+                  int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_COMISD;
+    dump_mem_reg(m, ATOM_NORMAL, OpndSize_64, disp, base_reg, isBasePhysical,
+        MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_xmm, NULL);
+}
+//! comisd reg reg
+
+//!
+void compare_sd_reg_with_reg(LowOp* op, int reg1, bool isPhysical1,
+                  int reg2, bool isPhysical2) {
+    Mnemonic m = Mnemonic_COMISD;
+    dump_reg_reg(m, ATOM_NORMAL, OpndSize_64, reg1, isPhysical1, reg2, isPhysical2, LowOpndRegType_xmm);
+}
+//! fucom[p]
+
+//!
+void compare_fp_stack(bool pop, int reg, bool isDouble) { //compare ST(0) with ST(reg)
+    Mnemonic m = pop ? Mnemonic_FUCOMP : Mnemonic_FUCOM;
+    lower_reg_to_reg(m, ATOM_NORMAL, isDouble ? OpndSize_64 : OpndSize_32,
+                  PhysicalReg_ST0+reg, true /*isPhysical*/, PhysicalReg_ST0, true /*isPhysical2*/, LowOpndRegType_fs);
+}
+
+/*!
+\brief generate a single return instruction
+
+*/
+inline LowOp* lower_return() {
+    if (gDvm.executionMode == kExecutionModeNcgO0 || !gDvmJit.scheduling) {
+        stream = encoder_return(stream);
+        return NULL;
+    }
+    LowOp * op = singletonPtr<Scheduler>()->allocateNewEmptyLIR<LowOp>();
+    op->numOperands = 0;
+    op->opCode = Mnemonic_RET;
+    op->opCode2 = ATOM_NORMAL;
+    singletonPtr<Scheduler>()->updateUseDefInformation(op);
+    return op;
+}
+
+void x86_return() {
+    lower_return();
+}
+
+//!test imm reg
+
+//!
+void test_imm_reg(OpndSize size, int imm, int reg, bool isPhysical) {
+    dump_imm_reg(Mnemonic_TEST, ATOM_NORMAL, size, imm, reg, isPhysical, getTypeFromIntSize(size), false);
+}
+//!test imm mem
+
+//!
+void test_imm_mem(OpndSize size, int imm, int disp, int reg, bool isPhysical) {
+    dump_imm_mem(Mnemonic_TEST, ATOM_NORMAL, size, imm, disp, reg, isPhysical, MemoryAccess_Unknown, -1, false);
+}
+//!alu unary op with one reg operand
+
+//!
+void alu_unary_reg(OpndSize size, ALU_Opcode opc, int reg, bool isPhysical) {
+    Mnemonic m;
+    if(size == OpndSize_64)
+        m = map_of_64_opcode_2_mnemonic[opc];
+    else
+        m = map_of_alu_opcode_2_mnemonic[opc];
+    dump_reg(m, ATOM_NORMAL_ALU, size, reg, isPhysical, getTypeFromIntSize(size));
+}
+//!alu unary op with one mem operand
+
+//!
+void alu_unary_mem(LowOp* op, OpndSize size, ALU_Opcode opc, int disp, int base_reg, bool isBasePhysical) {
+    Mnemonic m;
+    if(size == OpndSize_64)
+        m = map_of_64_opcode_2_mnemonic[opc];
+    else
+        m = map_of_alu_opcode_2_mnemonic[opc];
+    dump_mem(m, ATOM_NORMAL_ALU, size, disp, base_reg, isBasePhysical);
+}
+//!alu binary op with immediate and one mem operand
+
+//!
+void alu_binary_imm_mem(OpndSize size, ALU_Opcode opc, int imm, int disp, int base_reg, bool isBasePhysical) {
+    Mnemonic m;
+    if(size == OpndSize_64)
+        m = map_of_64_opcode_2_mnemonic[opc];
+    else
+        m = map_of_alu_opcode_2_mnemonic[opc];
+    dump_imm_mem(m, ATOM_NORMAL_ALU, size, imm, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, false);
+}
+//!alu binary op with immediate and one reg operand
+
+//!
+void alu_binary_imm_reg(OpndSize size, ALU_Opcode opc, int imm, int reg, bool isPhysical) {
+    Mnemonic m;
+    if(size == OpndSize_64)
+        m = map_of_64_opcode_2_mnemonic[opc];
+    else
+        m = map_of_alu_opcode_2_mnemonic[opc];
+    dump_imm_reg(m, ATOM_NORMAL_ALU, size, imm, reg, isPhysical, getTypeFromIntSize(size), false);
+}
+
+/**
+ * @brief Performs get_VR, alu_op and set_VR but with lesser instructions
+ * @details Only for 32-bit integers for now
+ * @param size The Operand size (Only 32-bit currently)
+ * @param opc The alu operation to perform (add or subtract)
+ * @param srcVR The source VR to fetch
+ * @param destVR The destination VR to set
+ * @param imm The literal value to be added to VR value
+ * @param tempReg A temporary register
+ * @param isTempPhysical Whether the tempReg is physical
+ * @param mir current lowered MIR
+ * @return whether we were successful. If false, caller needs to perform get_VR, alu_op, set_VR separately
+ */
+bool alu_imm_to_VR(OpndSize size, ALU_Opcode opc, int srcVR, int destVR, int imm, int tempReg, bool isTempPhysical, const MIR * mir) {
+    const LowOpndRegType pType = getTypeFromIntSize(size); //gp or xmm
+
+    //We accept only add_opc and sub_opc for now
+    if (opc != add_opc && opc != sub_opc) {
+        return false;
+    }
+
+    //We accept only 32-bit values for now
+    if (size != OpndSize_32) {
+        return false;
+    }
+
+    Mnemonic alu_mn = map_of_alu_opcode_2_mnemonic[opc];
+
+    enum CaseSrc {
+        SRC_IS_CONSTANT,
+        SRC_IN_MEMORY,
+        SRC_IS_ALLOCATED
+    };
+
+    enum CaseDest {
+        DEST_SAME_AS_SRC,
+        DEST_IN_MEMORY,
+        DEST_IS_ALLOCATED
+    };
+
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+
+        /*
+         * We have the following possibilities with the VRs
+         *
+         *  CaseSrc == 1: srcVR is constant
+         *          CaseDest == 1: destVR == srcVR (We do constant += IMM)
+         *          CaseDest == 2: destVR is in Memory (We do MOV IMM + const, MEM)
+         *          CaseDest == 3: destVR is allocated (We do MOV IMM + const, REG)
+         *
+         * CaseSrc == 2: srcVR is in memory
+         *          CaseDest == 1: destVR == srcVR (We do <op> IMM, MEM)
+         *          CaseDest == 2: destVR is in memory (worst case. We return from here and do normal op)
+         *          CaseDest == 3: destVR is allocated (We spill srcVR to same reg, then <op> imm, reg)
+         *
+         * CaseSrc == 3: srcVR is allocated
+         *          CaseDest == 1: destVR == srcVR (We do <op> IMM, REG)
+         *          CaseDest == 2: destVR is in memory (We LEA srcVR plus imm to a temp, and then set destVR to temp)
+         *          CaseDest == 3: destVR is allocated (LEA IMM(srcVR), destVR)
+         *
+         * Now depending on above, we find out the cases, and if needed, find out the const value of src,
+         * and reg allocated to dest and/or src. Memory locations, if needed, are (4*destVRNum/srcVRNum + PhysicalReg_FP)
+         */
+
+        //Initializing
+        CaseSrc caseSrc = SRC_IS_CONSTANT;
+        CaseDest caseDest = DEST_SAME_AS_SRC;
+        int constValSrc = 0;
+        int regDest = -1;
+        int regSrc = -1;
+
+        //Check the case for srcVR
+        int constValue[2];
+        int isConst = isVirtualRegConstant(srcVR, pType, constValue, true/*updateRefCount*/);
+        int tempPhysicalReg = checkVirtualReg(srcVR, pType, 0);
+        if (isConst == 3) {
+            caseSrc = SRC_IS_CONSTANT;
+            constValSrc = constValue[0];
+        }
+        else if (tempPhysicalReg != PhysicalReg_Null) {
+            caseSrc = SRC_IS_ALLOCATED;
+            regSrc = tempPhysicalReg;
+        }
+        else {
+            caseSrc = SRC_IN_MEMORY;
+        }
+
+        //Check the case for destVR
+        if (destVR != srcVR) {
+            tempPhysicalReg = checkVirtualReg(destVR, pType, 0);
+            if (tempPhysicalReg != PhysicalReg_Null) {
+                caseDest = DEST_IS_ALLOCATED;
+                regDest = tempPhysicalReg;
+            }
+            else {
+                caseDest = DEST_IN_MEMORY;
+            }
+        }
+        else {
+            caseDest = DEST_SAME_AS_SRC;
+        }
+
+        int signedImm = (opc == add_opc ? imm : -imm);
+        int finalSum = constValSrc + signedImm;
+
+        //Now handle the cases
+        switch (caseSrc) {
+            case SRC_IS_CONSTANT:
+                if (caseDest == DEST_SAME_AS_SRC) {
+                    //Add or subtract
+                    constValue[0] = finalSum;
+                    constValue[1] = 0; //To be safe
+                    return setVRToConst(destVR, size, constValue);
+                }
+                else if (caseDest == DEST_IN_MEMORY) {
+                    dump_imm_mem_noalloc(Mnemonic_MOV, size, finalSum, 4*destVR, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
+                    return true; //Successfully updated
+                }
+                else if (caseDest == DEST_IS_ALLOCATED) {
+                    dump_imm_reg_noalloc(Mnemonic_MOV, size, finalSum, regDest, true, pType);
+                    updateRefCount(destVR, pType);
+                    updateVirtualReg(destVR, pType);
+                    return true; //Successfully updated
+                }
+                break;
+
+            case SRC_IN_MEMORY:
+                if (caseDest == DEST_SAME_AS_SRC) {
+
+                    // for Silvermont platform
+                    if (strcmp(ARCH_VARIANT, "x86-slm") == 0) {
+
+                        /* Heuristic for inc optimization to avoid store/load REHABQ hazard.
+                           number of adjacent bytecodes which need to be checked for avoiding
+                           store/load REHABQ hazard for increment in memory */
+                        const int incOptMirWindow = 2;
+
+                        // Get SSA representation
+                        SSARepresentation *ssa = mir->ssaRep;
+
+                        // current add/sub mir should only have one def and we
+                        // only care if this def is used
+                        if(ssa != 0 && ssa->numDefs == 1 && ssa->usedNext != 0 &&
+                           ssa->usedNext[0] != 0 && ssa->usedNext[0]->mir != 0) {
+
+                            MIR * mirUse = ssa->usedNext[0]->mir;
+                            MIR * nextMIR = const_cast<MIR*>(mir);
+
+                            // check adjacent mirs window
+                            for (int i = 0; i < incOptMirWindow; i ++) {
+                                if (nextMIR != 0) {
+                                    nextMIR = nextMIR->next;
+
+                                    // if the define variable of mir is used in adjacent mir, return
+                                    // false to avoid add/sub in memory
+                                    if (mirUse == nextMIR) {
+                                        return false;
+                                    }
+                                }
+                            }
+                        }
+
+                        // when we reach here, we can use add/sub on memory directly based
+                        // on the fact that no uses of the mir's def in adjacent mirs window
+                        dump_imm_mem_noalloc_alu(alu_mn, size, imm, 4*destVR, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
+
+                        // Successfully updated
+                        return true;
+                    }
+
+                    // for other platforms
+                    else {
+                        dump_imm_mem_noalloc_alu(alu_mn, size, imm, 4*destVR, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
+
+                        // Successfully updated
+                        return true;
+                    }
+                }
+                else if (caseDest == DEST_IN_MEMORY) {
+                    //We can in no way do better than get_VR, add / sub, set_VR
+                    return false;
+                }
+                else if (caseDest == DEST_IS_ALLOCATED) {
+                    //Load srcVR to regDest, and then add the constant
+                    //Note that with MOVE_OPT on, this is as good as get_VR, add / sub , set_VR
+                    dump_mem_reg_noalloc(Mnemonic_MOV, size, 4*srcVR, PhysicalReg_FP, true, MemoryAccess_VR, srcVR, regDest, true, pType);
+                    dump_imm_reg_noalloc_alu(alu_mn, size, imm, regDest, true, pType);
+                    updateRefCount(destVR, pType);
+                    updateVirtualReg(destVR, pType);
+                    return true; //Successfully updated
+                }
+                break;
+
+            case SRC_IS_ALLOCATED:
+                if (caseDest == DEST_SAME_AS_SRC) {
+                    dump_imm_reg_noalloc_alu(alu_mn, size, imm, regSrc, true, pType);
+                    //We have to reduce refCounts twice. Let's call the VR with
+                    //different names, even though srcVR == destVR
+                    updateRefCount(srcVR, pType);
+                    updateRefCount(destVR, pType);
+                    updateVirtualReg(destVR, pType);
+                    return true; //Successfully updated
+                }
+                else if (caseDest == DEST_IN_MEMORY) {
+                    //We can write regSrc directly to destVR, and then ADD imm, destVR (which is 2 inst). But
+                    //if destVR gets used later, we will load it to a reg anyways. That makes it 3 instructions.
+                    //Instead, let's do LEA imm(regSrc), temp. And assign destVR to temp. Worst case we write
+                    // back destVR soon after, which is still 2 instructions. Best case we get away with just 1.
+                    dump_mem_reg_noalloc_mem(Mnemonic_LEA, ATOM_NORMAL, size, signedImm, regSrc, true, MemoryAccess_Unknown, -1, tempReg, isTempPhysical, pType);
+                    set_virtual_reg(destVR, size, tempReg, isTempPhysical);
+                    updateRefCount(srcVR, pType);
+                    return true; //Successfully updated
+                }
+                else if (caseDest == DEST_IS_ALLOCATED) {
+                    dump_mem_reg_noalloc(Mnemonic_LEA, size, signedImm, regSrc, true, MemoryAccess_Unknown, -1, regDest, true, pType);
+                    //Done with srcVR and destVR
+                    updateRefCount(srcVR, pType);
+                    updateRefCount(destVR, pType);
+                    updateVirtualReg(destVR, pType);
+                    return true; //Successfully updated
+                }
+                break;
+
+            default:
+                return false;
+        }
+    }
+
+    //No optimization for O0
+    return false;
+}
+
+//!alu binary op with one mem operand and one reg operand
+
+//!
+void alu_binary_mem_reg(OpndSize size, ALU_Opcode opc,
+             int disp, int base_reg, bool isBasePhysical,
+             int reg, bool isPhysical) {
+    Mnemonic m;
+    if(size == OpndSize_64)
+        m = map_of_64_opcode_2_mnemonic[opc];
+    else
+        m = map_of_alu_opcode_2_mnemonic[opc];
+    dump_mem_reg(m, ATOM_NORMAL_ALU, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical, getTypeFromIntSize(size), NULL);
+}
+
+void alu_sd_binary_VR_reg(ALU_Opcode opc, int vA, int reg, bool isPhysical, bool isSD) {
+    Mnemonic m;
+    if(isSD) m = map_of_sse_opcode_2_mnemonic[opc];
+    else m = (Mnemonic)(map_of_sse_opcode_2_mnemonic[opc]+1); //from SD to SS
+    OpndSize size = isSD ? OpndSize_64 : OpndSize_32;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        LowOpndRegType type = isSD ? LowOpndRegType_xmm : LowOpndRegType_ss; //type of the mem operand
+        int tmpValue[2];
+        int isConst = isVirtualRegConstant(vA, type, tmpValue,
+                          true/*updateRefCount*/);
+        if(isConst == 3 && !isSD) {            //isConst can be 0 or 3, mem32, use xmm
+            bool storedAddr = false;
+
+            if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
+                tmpValue[1] = 0;// set higher 32 bits to zero
+                // create a new record of a constant
+                addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, false);
+
+                // save mem access location in constList
+                const int offset = 4; // offset is 4 for OPC_(ADD,SUB,MUL,DIV) float operations
+                storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
+
+                ConstInfo* tmpPtr = gCompilationUnit->constListHead;
+                if (storedAddr == true){ // creating constant record and saving address to constant list was successful
+#ifdef DEBUG_CONST
+                    ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
+                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+#endif
+                } else {
+                    ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
+                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+                }
+            }
+            // Lower mem_reg instruction with constant to be accessed from constant data section
+            if (storedAddr == true){
+                int dispAddr =  getGlobalDataAddr("64bits");
+                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_32, dispAddr, PhysicalReg_Null, true,
+                                   MemoryAccess_Constants, vA, reg, isPhysical, LowOpndRegType_xmm,
+                                   &(gCompilationUnit->constListHead));
+            } else {
+                 writeBackConstVR(vA, tmpValue[0]);
+                 dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_32, 4*vA, PhysicalReg_FP, true,
+                                MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
+            }
+            return;
+        }
+        if(isConst == 3 && isSD) {
+            bool storedAddr = false;
+
+            if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
+                // create a new record of a constant
+                addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, false);
+
+                const int offset = 4; // offset is 4 for OPC_(ADD,SUB,MUL,DIV) double operations
+                // save mem access location in constList
+                storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
+
+                ConstInfo* tmpPtr = gCompilationUnit->constListHead;
+                if (storedAddr == true){ // creating constant record and saving address to constant list was successful
+#ifdef DEBUG_CONST
+                    ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
+                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+#endif
+                } else {
+                    ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
+                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+                }
+            }
+            // Lower mem_reg instruction with constant to be accessed from constant data section
+            if (storedAddr == true){
+                int dispAddr =  getGlobalDataAddr("64bits");
+                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, dispAddr, PhysicalReg_Null, true,
+                       MemoryAccess_Constants, vA, reg, isPhysical, LowOpndRegType_xmm, &(gCompilationUnit->constListHead));
+            } else {
+                writeBackConstVR(vA, tmpValue[0]);
+                writeBackConstVR(vA+1, tmpValue[1]);
+                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, 4*vA, PhysicalReg_FP, true,
+                       MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
+            }
+            return;
+        }
+        if(isConst == 1) writeBackConstVR(vA, tmpValue[0]);
+        if(isConst == 2) writeBackConstVR(vA+1, tmpValue[1]);
+        freeReg(false);
+
+        int regAll = checkVirtualReg(vA, type, 0/*do not update refCount*/);
+        if(regAll != PhysicalReg_Null) {
+            startNativeCode(-1, -1); //should we use vA, type
+            //CHECK: callupdateVRAtUse
+
+            //Do not spill regAll
+            gCompilationUnit->setCanSpillRegister (regAll, false);
+
+            dump_reg_reg_noalloc_src(m, ATOM_NORMAL_ALU, size, regAll, true, reg,
+                         isPhysical, LowOpndRegType_xmm);
+            endNativeCode();
+        }
+        else {
+            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
+                         MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm);
+        }
+        updateRefCount(vA, type);
+    }
+    else {
+        dump_mem_reg(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
+                    MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
+    }
+}
+
+//!alu binary op with a VR and one reg operand
+
+//!
+void alu_binary_VR_reg(OpndSize size, ALU_Opcode opc, int vA, int reg, bool isPhysical) {
+    Mnemonic m;
+    if(size == OpndSize_64)
+        m = map_of_64_opcode_2_mnemonic[opc];
+    else
+        m = map_of_alu_opcode_2_mnemonic[opc];
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        int tmpValue[2];
+        int isConst = isVirtualRegConstant(vA, getTypeFromIntSize(size), tmpValue,
+                          true/*updateRefCount*/);
+        if(isConst == 3 && size != OpndSize_64) {
+            //allocate a register for dst
+            dump_imm_reg(m, ATOM_NORMAL_ALU, size, tmpValue[0], reg, isPhysical,
+                       getTypeFromIntSize(size), false);
+            return;
+        }
+        if(isConst == 3 && size == OpndSize_64) {
+            bool storedAddr = false;
+            bool align = false;
+            if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
+
+                // create a new record of a constant
+                if (m == Mnemonic_PADDQ || Mnemonic_PSUBQ || Mnemonic_PAND || Mnemonic_POR || Mnemonic_PXOR) {
+                    align = true;
+                }
+                addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, align);
+                const int offset = 4; // offset is 4 for OPC_(ADD,SUB and logical) long operations
+                // save mem access location in constList
+                storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vA, stream, offset);
+
+                ConstInfo* tmpPtr = gCompilationUnit->constListHead;
+                if (storedAddr == true){ // creating constant record and saving address to constant list was successful
+#ifdef DEBUG_CONST
+                    ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
+                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+#endif
+                } else {
+                    ALOGI("JIT_INFO: Error creating constant failed for regnum %d, valueL %d(%x) valueH %d(%x)",
+                            tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+                }
+            }
+            // Lower mem_reg instruction with constant to be accessed from constant data section
+            if (storedAddr == true){
+                int dispAddr =  getGlobalDataAddr("64bits");
+                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, dispAddr, PhysicalReg_Null, true,
+                       MemoryAccess_Constants, vA, reg, isPhysical, LowOpndRegType_xmm, &(gCompilationUnit->constListHead));
+            } else {
+                writeBackConstVR(vA, tmpValue[0]);
+                writeBackConstVR(vA+1, tmpValue[1]);
+                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, 4*vA, PhysicalReg_FP, true,
+                       MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
+            }
+            return;
+        }
+        if(isConst == 1) writeBackConstVR(vA, tmpValue[0]);
+        if(isConst == 2) writeBackConstVR(vA+1, tmpValue[1]);
+
+        freeReg(false);
+        int regAll = checkVirtualReg(vA, getTypeFromIntSize(size), 0);
+        if(regAll != PhysicalReg_Null) {
+            startNativeCode(-1, -1);
+
+            //Do not spill regAll
+            gCompilationUnit->setCanSpillRegister (regAll, false);
+
+            dump_reg_reg_noalloc_src(m, ATOM_NORMAL_ALU, size, regAll, true, reg,
+                         isPhysical, getTypeFromIntSize(size));
+            endNativeCode();
+        }
+        else {
+            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
+                MemoryAccess_VR, vA, reg, isPhysical, getTypeFromIntSize(size));
+        }
+        updateRefCount(vA, getTypeFromIntSize(size));
+    }
+    else {
+        dump_mem_reg(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
+            MemoryAccess_VR, vA, reg, isPhysical, getTypeFromIntSize(size), NULL);
+    }
+}
+//!alu binary op with two reg operands
+
+//!
+void alu_binary_reg_reg(OpndSize size, ALU_Opcode opc,
+                         int reg1, bool isPhysical1,
+                         int reg2, bool isPhysical2) {
+    Mnemonic m;
+    if(size == OpndSize_64)
+        m = map_of_64_opcode_2_mnemonic[opc];
+    else
+        m = map_of_alu_opcode_2_mnemonic[opc];
+    dump_reg_reg(m, ATOM_NORMAL_ALU, size, reg1, isPhysical1, reg2, isPhysical2, getTypeFromIntSize(size));
+}
+//!alu binary op with one reg operand and one mem operand
+
+//!
+void alu_binary_reg_mem(OpndSize size, ALU_Opcode opc,
+             int reg, bool isPhysical,
+             int disp, int base_reg, bool isBasePhysical) { //destination is mem!!
+    Mnemonic m;
+    if(size == OpndSize_64)
+        m = map_of_64_opcode_2_mnemonic[opc];
+    else
+        m = map_of_alu_opcode_2_mnemonic[opc];
+    dump_reg_mem(m, ATOM_NORMAL_ALU, size, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, getTypeFromIntSize(size));
+}
+//!FPU ops with one mem operand
+
+//!
+void fpu_mem(LowOp* op, ALU_Opcode opc, OpndSize size, int disp, int base_reg, bool isBasePhysical) {
+    Mnemonic m = map_of_fpu_opcode_2_mnemonic[opc];
+    dump_mem_fp(m, ATOM_NORMAL_ALU, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, 0);
+}
+//!SSE 32-bit ALU
+
+//!
+void alu_ss_binary_reg_reg(ALU_Opcode opc, int reg, bool isPhysical,
+                int reg2, bool isPhysical2) {
+    Mnemonic m = (Mnemonic)(map_of_sse_opcode_2_mnemonic[opc]+1); //from SD to SS
+    dump_reg_reg(m, ATOM_NORMAL_ALU, OpndSize_32, reg, isPhysical, reg2, isPhysical2, LowOpndRegType_xmm);
+}
+//!SSE 64-bit ALU
+
+//!
+void alu_sd_binary_reg_reg(ALU_Opcode opc, int reg, bool isPhysical,
+                int reg2, bool isPhysical2) {
+    Mnemonic m = map_of_sse_opcode_2_mnemonic[opc];
+    dump_reg_reg(m, ATOM_NORMAL_ALU, OpndSize_64, reg, isPhysical, reg2, isPhysical2, LowOpndRegType_xmm);
+}
+//!push reg to native stack
+
+//!
+void push_reg_to_stack(OpndSize size, int reg, bool isPhysical) {
+    dump_reg(Mnemonic_PUSH, ATOM_NORMAL, size, reg, isPhysical, getTypeFromIntSize(size));
+}
+//!push mem to native stack
+
+//!
+void push_mem_to_stack(OpndSize size, int disp, int base_reg, bool isBasePhysical) {
+    dump_mem(Mnemonic_PUSH, ATOM_NORMAL, size, disp, base_reg, isBasePhysical);
+}
+//!move from reg to memory
+
+//!
+void move_reg_to_mem(OpndSize size,
+                      int reg, bool isPhysical,
+                      int disp, int base_reg, bool isBasePhysical) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_reg_mem(m, ATOM_NORMAL, size, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, getTypeFromIntSize(size));
+}
+//!move from reg to memory
+
+//!Operands are already allocated
+void move_reg_to_mem_noalloc(OpndSize size,
+                  int reg, bool isPhysical,
+                  int disp, int base_reg, bool isBasePhysical,
+                  MemoryAccessType mType, int mIndex) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_reg_mem_noalloc(m, size, reg, isPhysical, disp, base_reg, isBasePhysical, mType, mIndex, getTypeFromIntSize(size));
+}
+//!move from memory to reg
+
+//!
+LowOpMemReg* move_mem_to_reg(OpndSize size,
+                      int disp, int base_reg, bool isBasePhysical,
+                      int reg, bool isPhysical) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    return dump_mem_reg(m, ATOM_NORMAL, size, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical, getTypeFromIntSize(size), NULL);
+}
+//!move from memory to reg
+
+//!Operands are already allocated
+LowOpMemReg* move_mem_to_reg_noalloc(OpndSize size,
+                  int disp, int base_reg, bool isBasePhysical,
+                  MemoryAccessType mType, int mIndex,
+                  int reg, bool isPhysical) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    return dump_mem_reg_noalloc(m, size, disp, base_reg, isBasePhysical, mType, mIndex, reg, isPhysical, getTypeFromIntSize(size));
+}
+//!movss from memory to reg
+
+//!Operands are already allocated
+LowOpMemReg* move_ss_mem_to_reg_noalloc(int disp, int base_reg, bool isBasePhysical,
+                 MemoryAccessType mType, int mIndex,
+                 int reg, bool isPhysical) {
+    return dump_mem_reg_noalloc(Mnemonic_MOVSS, OpndSize_32, disp, base_reg, isBasePhysical, mType, mIndex, reg, isPhysical, LowOpndRegType_xmm);
+}
+//!movss from reg to memory
+
+//!Operands are already allocated
+LowOpRegMem* move_ss_reg_to_mem_noalloc(int reg, bool isPhysical,
+                 int disp, int base_reg, bool isBasePhysical,
+                 MemoryAccessType mType, int mIndex) {
+    return dump_reg_mem_noalloc(Mnemonic_MOVSS, OpndSize_32, reg, isPhysical, disp, base_reg, isBasePhysical, mType, mIndex, LowOpndRegType_xmm);
+}
+//!movzx from memory to reg
+
+//!
+void movez_mem_to_reg(OpndSize size,
+               int disp, int base_reg, bool isBasePhysical,
+               int reg, bool isPhysical) {
+    dump_movez_mem_reg(Mnemonic_MOVZX, size, disp, base_reg, isBasePhysical, reg, isPhysical);
+}
+
+//!movzx from one reg to another reg
+
+//!
+void movez_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical,
+                      int reg2, bool isPhysical2) {
+    Mnemonic m = Mnemonic_MOVZX;
+    dump_movez_reg_reg(m, size, reg, isPhysical, reg2, isPhysical2);
+}
+
+void movez_mem_disp_scale_to_reg(OpndSize size,
+                 int base_reg, bool isBasePhysical,
+                 int disp, int index_reg, bool isIndexPhysical, int scale,
+                 int reg, bool isPhysical) {
+    dump_mem_scale_reg(Mnemonic_MOVZX, size, base_reg, isBasePhysical,
+                 disp, index_reg, isIndexPhysical, scale,
+                 reg, isPhysical, LowOpndRegType_gp);
+}
+void moves_mem_disp_scale_to_reg(OpndSize size,
+                  int base_reg, bool isBasePhysical,
+                  int disp, int index_reg, bool isIndexPhysical, int scale,
+                  int reg, bool isPhysical) {
+    dump_mem_scale_reg(Mnemonic_MOVSX, size, base_reg, isBasePhysical,
+                  disp, index_reg, isIndexPhysical, scale,
+                  reg, isPhysical, LowOpndRegType_gp);
+}
+//!movsx from memory to reg
+
+//!
+void moves_mem_to_reg(LowOp* op, OpndSize size,
+               int disp, int base_reg, bool isBasePhysical,
+               int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_MOVSX;
+    dump_moves_mem_reg(m, size, disp, base_reg, isBasePhysical, reg, isPhysical);
+}
+//!mov from one reg to another reg
+
+//!
+void move_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical,
+                      int reg2, bool isPhysical2) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_reg_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg2, isPhysical2, getTypeFromIntSize(size));
+}
+//!mov from one reg to another reg
+
+//!Sign extends the value. Only 32-bit support.
+void moves_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical,
+                      int reg2, bool isPhysical2) {
+    Mnemonic m = Mnemonic_MOVSX;
+    dump_reg_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg2, isPhysical2, getTypeFromIntSize(size));
+}
+
+//!mov from one reg to another reg
+
+//!Operands are already allocated
+void move_reg_to_reg_noalloc(OpndSize size,
+                  int reg, bool isPhysical,
+                  int reg2, bool isPhysical2) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_reg_reg_noalloc(m, size, reg, isPhysical, reg2, isPhysical2, getTypeFromIntSize(size));
+}
+//!move from memory to reg
+
+//!
+void move_mem_scale_to_reg(OpndSize size,
+                int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
+                int reg, bool isPhysical) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_mem_scale_reg(m, size, base_reg, isBasePhysical, 0/*disp*/, index_reg, isIndexPhysical, scale,
+                              reg, isPhysical, getTypeFromIntSize(size));
+}
+void move_mem_disp_scale_to_reg(OpndSize size,
+                int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                int reg, bool isPhysical) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_mem_scale_reg(m, size, base_reg, isBasePhysical, disp, index_reg, isIndexPhysical, scale,
+                              reg, isPhysical, getTypeFromIntSize(size));
+}
+//!move from reg to memory
+
+//!
+void move_reg_to_mem_scale(OpndSize size,
+                int reg, bool isPhysical,
+                int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_reg_mem_scale(m, size, reg, isPhysical,
+                              base_reg, isBasePhysical, 0/*disp*/, index_reg, isIndexPhysical, scale,
+                              getTypeFromIntSize(size));
+}
+void move_reg_to_mem_disp_scale(OpndSize size,
+                int reg, bool isPhysical,
+                int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_reg_mem_scale(m, size, reg, isPhysical,
+                              base_reg, isBasePhysical, disp, index_reg, isIndexPhysical, scale,
+                              getTypeFromIntSize(size));
+}
+
+void move_chain_to_mem(OpndSize size, int imm,
+                        int disp, int base_reg, bool isBasePhysical) {
+    dump_imm_mem(Mnemonic_MOV, ATOM_NORMAL, size, imm, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, true);
+}
+
+//!move an immediate to memory
+
+//!
+void move_imm_to_mem(OpndSize size, int imm,
+                      int disp, int base_reg, bool isBasePhysical) {
+    assert(size != OpndSize_64);
+    if(size == OpndSize_64) {
+        ALOGI("JIT_INFO: Trying to move 64-bit imm to memory");
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return;
+    }
+    dump_imm_mem(Mnemonic_MOV, ATOM_NORMAL, size, imm, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, false);
+}
+//! set a VR to an immediate
+
+//!
+void set_VR_to_imm(int vA, OpndSize size, int imm) {
+    assert(size != OpndSize_64);
+    if(size == OpndSize_64) {
+        ALOGI("JIT_INFO: Trying to set VR with 64-bit imm");
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return;
+    }
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        int regAll = checkVirtualReg(vA, getTypeFromIntSize(size), 0);
+        if(regAll != PhysicalReg_Null) {
+            dump_imm_reg_noalloc(m, size, imm, regAll, true, LowOpndRegType_gp);
+            updateRefCount(vA, getTypeFromIntSize(size));
+            updateVirtualReg(vA, getTypeFromIntSize(size));
+            return;
+        }
+        //will call freeReg
+        freeReg(false);
+        regAll = registerAlloc(LowOpndRegType_virtual | getTypeFromIntSize(size), vA, false/*dummy*/, true, true);
+        if(regAll == PhysicalReg_Null) {
+            dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
+            return;
+        }
+
+        dump_imm_reg_noalloc(m, size, imm, regAll, true, LowOpndRegType_gp);
+        updateVirtualReg(vA, getTypeFromIntSize(size));
+    }
+    else {
+        dump_imm_mem(m, ATOM_NORMAL, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, false);
+    }
+}
+void set_VR_to_imm_noupdateref(LowOp* op, int vA, OpndSize size, int imm) {
+    return;
+}
+//! set a VR to an immediate
+
+//! Do not allocate a physical register for the VR
+void set_VR_to_imm_noalloc(int vA, OpndSize size, int imm) {
+    assert(size != OpndSize_64);
+    if(size == OpndSize_64) {
+        ALOGI("JIT_INFO: Trying to move 64-bit imm to memory (noalloc)");
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return;
+    }
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
+}
+
+void move_chain_to_reg(OpndSize size, int imm, int reg, bool isPhysical) {
+    dump_imm_reg(Mnemonic_MOV, ATOM_NORMAL, size, imm, reg, isPhysical, LowOpndRegType_gp, true);
+}
+
+//! move an immediate to reg
+
+//!
+void move_imm_to_reg(OpndSize size, int imm, int reg, bool isPhysical) {
+    assert(size != OpndSize_64);
+    if(size == OpndSize_64) {
+        ALOGI("JIT_INFO: Trying to move 64-bit imm to register");
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return;
+    }
+    Mnemonic m = Mnemonic_MOV;
+    dump_imm_reg(m, ATOM_NORMAL, size, imm, reg, isPhysical, LowOpndRegType_gp, false);
+}
+//! move an immediate to reg
+
+//! The operand is already allocated
+void move_imm_to_reg_noalloc(OpndSize size, int imm, int reg, bool isPhysical) {
+    assert(size != OpndSize_64);
+    if(size == OpndSize_64) {
+        ALOGI("JIT_INFO: Trying to move 64-bit imm to register (noalloc)");
+        SET_JIT_ERROR(kJitErrorRegAllocFailed);
+        return;
+    }
+    Mnemonic m = Mnemonic_MOV;
+    dump_imm_reg_noalloc(m, size, imm, reg, isPhysical, LowOpndRegType_gp);
+}
+//!cmov from reg to reg
+
+//!
+void conditional_move_reg_to_reg(OpndSize size, ConditionCode cc, int reg1, bool isPhysical1, int reg, bool isPhysical) {
+    Mnemonic m = (Mnemonic)(Mnemonic_CMOVcc+cc);
+    dump_reg_reg(m, ATOM_NORMAL, size, reg1, isPhysical1, reg, isPhysical, LowOpndRegType_gp);
+}
+//!movss from memory to reg
+
+//!
+void move_ss_mem_to_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
+                         int reg, bool isPhysical) {
+    dump_mem_reg(Mnemonic_MOVSS, ATOM_NORMAL, OpndSize_32, disp, base_reg, isBasePhysical,
+        MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_xmm, NULL);
+}
+//!movss from reg to memory
+
+//!
+void move_ss_reg_to_mem(LowOp* op, int reg, bool isPhysical,
+                         int disp, int base_reg, bool isBasePhysical) {
+    dump_reg_mem(Mnemonic_MOVSS, ATOM_NORMAL, OpndSize_32, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, LowOpndRegType_xmm);
+}
+//!movsd from memory to reg
+
+//!
+void move_sd_mem_to_reg(int disp, int base_reg, bool isBasePhysical,
+                         int reg, bool isPhysical) {
+    dump_mem_reg(Mnemonic_MOVSD, ATOM_NORMAL, OpndSize_64, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical, LowOpndRegType_xmm, NULL);
+}
+//!movsd from reg to memory
+
+//!
+void move_sd_reg_to_mem(LowOp* op, int reg, bool isPhysical,
+                         int disp, int base_reg, bool isBasePhysical) {
+    dump_reg_mem(Mnemonic_MOVSD, ATOM_NORMAL, OpndSize_64, reg, isPhysical,
+                        disp, base_reg, isBasePhysical,
+                        MemoryAccess_Unknown, -1, LowOpndRegType_xmm);
+}
+//!load from VR to a temporary
+
+//!
+void get_virtual_reg_all(int vR, OpndSize size, int reg, bool isPhysical, Mnemonic m) {
+    LowOpndRegType type = getTypeFromIntSize(size);
+    LowOpndRegType pType = type;//gp or xmm
+    OpndSize size2 = size;
+    Mnemonic m2 = m;
+    if(m == Mnemonic_MOVSS) {
+        size = OpndSize_32;
+        size2 = OpndSize_64;
+        type = LowOpndRegType_ss;
+        pType = LowOpndRegType_xmm;
+        m2 = Mnemonic_MOVQ; //to move from one xmm register to another
+    }
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        int tmpValue[2];
+        int isConst;
+        isConst = isVirtualRegConstant(vR, type, tmpValue, true/*updateRefCount*/);
+        if(isConst == 3) {
+            if(m == Mnemonic_MOVSS) { //load 32 bits from VR
+                bool storedAddr = false;
+
+                if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
+                    tmpValue[1] = 0;// set higher 32 bits to zero
+                    // create a new record of a constant
+                    addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vR, false);
+
+                    // save mem access location in constList
+                    const int offset = 4; // offset is 4 for MOVSS operations
+                    storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vR, stream, offset);
+
+                    ConstInfo* tmpPtr = gCompilationUnit->constListHead;
+                    if (storedAddr == true){ // creating constant record and saving address to constant list was successful
+#ifdef DEBUG_CONST
+                        ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
+                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+#endif
+                    } else {
+                        ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
+                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+                    }
+                }
+                // Lower mem_reg instruction with constant to be accessed from constant data section
+                if (storedAddr == true){
+                    int dispAddr =  getGlobalDataAddr("64bits");
+                    dump_mem_reg(m, ATOM_NORMAL, size, dispAddr, PhysicalReg_Null, true,
+                                       MemoryAccess_Constants, vR, reg, isPhysical, pType,
+                                       &(gCompilationUnit->constListHead));
+                } else {
+                    //VR is not mapped to a register but in memory
+                    writeBackConstVR(vR, tmpValue[0]);
+                    //temporary reg has "pType" (which is xmm)
+                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
+                                       MemoryAccess_VR, vR, reg, isPhysical, pType, NULL);
+                }
+                return;
+            }
+            else if(m == Mnemonic_MOVSD || size == OpndSize_64) {
+                bool storedAddr = false;
+
+                if((gDvmJit.disableOpt & (1 << kElimConstInitOpt)) == false){
+                    // create a new record of a constant
+                    addNewToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vR, false);
+
+                    // save mem access location in constList
+                    const int offset = 4; // offset is 4 for MOVSD operations
+                    storedAddr = saveAddrToConstList(&(gCompilationUnit->constListHead), tmpValue[0], tmpValue[1], vR, stream, offset);
+
+                    ConstInfo* tmpPtr = gCompilationUnit->constListHead;
+                    if (storedAddr == true){ // creating constant record and saving address to constant list was successful
+#ifdef DEBUG_CONST
+                        ALOGD("constVRList regnum %d, valueL %d(%x) valueH %d(%x)",
+                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+#endif
+                    } else {
+                        ALOGI("JIT_INFO: Error creating constant failed for VR %d, valueL %d(%x) valueH %d(%x)",
+                                tmpPtr->regNum, tmpPtr->valueL, tmpPtr->valueL, tmpPtr->valueH, tmpPtr->valueH);
+                    }
+                }
+                // Lower mem_reg instruction with constant to be accessed from constant data section
+                if (storedAddr == true){
+                    int dispAddr =  getGlobalDataAddr("64bits");
+                    dump_mem_reg(m, ATOM_NORMAL, size, dispAddr, PhysicalReg_Null, true,
+                                       MemoryAccess_Constants, vR, reg, isPhysical, pType,
+                                       &(gCompilationUnit->constListHead));
+                } else {
+                    //VR is not mapped to a register but in memory
+                    writeBackConstVR(vR, tmpValue[0]);
+                    writeBackConstVR(vR+1, tmpValue[1]);
+                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
+                                       MemoryAccess_VR, vR, reg, isPhysical, pType, NULL);
+                }
+                return;
+            }
+            else if(size != OpndSize_64) {
+                //VR is not mapped to a register
+                dump_imm_reg(m, ATOM_NORMAL, size, tmpValue[0], reg, isPhysical, pType, false);
+                return;
+            }
+        }
+        if(isConst == 1) writeBackConstVR(vR, tmpValue[0]);
+        if(isConst == 2) writeBackConstVR(vR+1, tmpValue[1]);
+
+        // We want to free any variables no longer in use
+        freeReg(false);
+
+        // Do we have a physical register associated for this VR?
+        int physRegForVR = checkVirtualReg(vR, type, 0);
+
+        // If we do, then let register allocator decide if a new physical
+        // register needs allocated for the temp
+        if(physRegForVR != PhysicalReg_Null) {
+            startNativeCode(vR, type);
+
+            //Do not spill physRegForVR
+            gCompilationUnit->setCanSpillRegister (physRegForVR, false);
+
+            //check XFER_MEM_TO_XMM
+            updateVRAtUse(vR, type, physRegForVR);
+            //temporary reg has "pType"
+            dump_reg_reg_noalloc_src(m2, ATOM_NORMAL, size2, physRegForVR, true, reg, isPhysical, pType);
+            endNativeCode();
+            updateRefCount(vR, type);
+            return;
+        }
+
+        // When we get to this point, we know that we have no physical register
+        // associated with the VR
+        physRegForVR = registerAlloc(LowOpndRegType_virtual | type, vR, false/*dummy*/, false);
+
+        // If we still have no physical register for the VR, then use it as
+        // a memory operand
+        if(physRegForVR == PhysicalReg_Null) {
+            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
+                MemoryAccess_VR, vR, reg, isPhysical, pType);
+            return;
+        }
+
+        // At this point we definitely have a physical register for the VR.
+        // Check to see if the temp can share same physical register.
+        if(checkTempReg2(reg, pType, isPhysical, physRegForVR, vR)) {
+            registerAllocMove(reg, pType, isPhysical, physRegForVR);
+            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
+                MemoryAccess_VR, vR, physRegForVR, true, pType);
+            updateRefCount(vR, type);
+            return;
+        }
+        else {
+            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
+                MemoryAccess_VR, vR, physRegForVR, true, pType);
+            //xmm with 32 bits
+            startNativeCode(vR, type);
+
+            //Do not spill physRegForVR
+            gCompilationUnit->setCanSpillRegister (physRegForVR, false);
+
+            dump_reg_reg_noalloc_src(m2, ATOM_NORMAL, size2, physRegForVR, true, reg, isPhysical, pType);
+            endNativeCode();
+            updateRefCount(vR, type);
+            return;
+        }
+    }
+    else {
+        dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
+            MemoryAccess_VR, vR, reg, isPhysical, pType, NULL);
+    }
+}
+void get_virtual_reg(int vB, OpndSize size, int reg, bool isPhysical) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    return get_virtual_reg_all(vB, size, reg, isPhysical, m);
+}
+void get_virtual_reg_noalloc(int vB, OpndSize size, int reg, bool isPhysical) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_mem_reg_noalloc(m, size, 4*vB, PhysicalReg_FP, true,
+        MemoryAccess_VR, vB, reg, isPhysical, getTypeFromIntSize(size));
+}
+//3 cases: gp, xmm, ss
+//ss: the temporary register is xmm
+//!load from a temporary to a VR
+
+//!
+void set_virtual_reg_all(int vA, OpndSize size, int reg, bool isPhysical, Mnemonic m) {
+    LowOpndRegType type = getTypeFromIntSize(size);
+    LowOpndRegType pType = type;//gp or xmm
+    OpndSize size2 = size;
+    Mnemonic m2 = m;
+    if(m == Mnemonic_MOVSS) {
+        size = OpndSize_32;
+        size2 = OpndSize_64;
+        type = LowOpndRegType_ss;
+        pType = LowOpndRegType_xmm;
+        m2 = Mnemonic_MOVQ;
+    }
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        //3 cases
+        //1: virtual register is already allocated to a physical register
+        //   call dump_reg_reg_noalloc_dst
+        //2: src reg is already allocated, VR is not yet allocated
+        //   allocate VR to the same physical register used by src reg
+        //   [call registerAllocMove]
+        //3: both not yet allocated
+        //   allocate a physical register for the VR
+        //   then call dump_reg_reg_noalloc_dst
+        //may need to convert from gp to xmm or the other way
+        freeReg(false);
+        int regAll = checkVirtualReg(vA, type, 0);
+        if(regAll != PhysicalReg_Null)  { //case 1
+            startNativeCode(-1, -1);
+
+            //Do not spill regAll
+            gCompilationUnit->setCanSpillRegister (regAll, false);
+
+            dump_reg_reg_noalloc_dst(m2, size2, reg, isPhysical, regAll, true, pType); //temporary reg is "pType"
+            endNativeCode();
+            updateRefCount(vA, type);
+            updateVirtualReg(vA, type); //will dump VR to memory, should happen afterwards
+            return;
+        }
+        regAll = checkTempReg(reg, pType, isPhysical, vA); //vA is not used inside
+        if(regAll != PhysicalReg_Null) { //case 2
+            registerAllocMove(vA, LowOpndRegType_virtual | type, false, regAll, true);
+            updateVirtualReg(vA, type); //will dump VR to memory, should happen afterwards
+            return; //next native instruction starts at op
+        }
+        //case 3
+        regAll = registerAlloc(LowOpndRegType_virtual | type, vA, false/*dummy*/, false, true);
+        if(regAll == PhysicalReg_Null) {
+            dump_reg_mem_noalloc(m, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
+                MemoryAccess_VR, vA, pType);
+            return;
+        }
+
+        startNativeCode(-1, -1);
+
+        //Do not spill regAll
+        gCompilationUnit->setCanSpillRegister (regAll, false);
+
+        dump_reg_reg_noalloc_dst(m2, size2, reg, isPhysical, regAll, true, pType);
+        endNativeCode();
+        updateRefCount(vA, type);
+        updateVirtualReg(vA, type);
+    }
+    else {
+        dump_reg_mem(m, ATOM_NORMAL, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
+            MemoryAccess_VR, vA, pType);
+    }
+}
+void set_virtual_reg(int vA, OpndSize size, int reg, bool isPhysical) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    return set_virtual_reg_all(vA, size, reg, isPhysical, m);
+}
+void set_virtual_reg_noalloc(int vA, OpndSize size, int reg, bool isPhysical) {
+    Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
+    dump_reg_mem_noalloc(m, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
+        MemoryAccess_VR, vA, getTypeFromIntSize(size));
+}
+void get_VR_ss(int vB, int reg, bool isPhysical) {
+    return get_virtual_reg_all(vB, OpndSize_64, reg, isPhysical, Mnemonic_MOVSS);
+}
+void set_VR_ss(int vA, int reg, bool isPhysical) {
+    return set_virtual_reg_all(vA, OpndSize_64, reg, isPhysical, Mnemonic_MOVSS);
+}
+void get_VR_sd(int vB, int reg, bool isPhysical) {
+    return get_virtual_reg_all(vB, OpndSize_64, reg, isPhysical, Mnemonic_MOVSD);
+}
+void set_VR_sd(int vA, int reg, bool isPhysical) {
+    return set_virtual_reg_all(vA, OpndSize_64, reg, isPhysical, Mnemonic_MOVSD);
+}
+////////////////////////////////// END: IA32 native instructions //////////////
+
+//! \brief generate native code to perform null check
+//!
+//! \details This function does not export PC
+//! \param reg
+//! \param isPhysical is the reg is physical
+//! \param vr the vr corresponding to reg
+//!
+//! \return -1 if error happened, 0 otherwise
+int simpleNullCheck(int reg, bool isPhysical, int vr) {
+    if(isVRNullCheck(vr, OpndSize_32)) {
+        updateRefCount2(reg, LowOpndRegType_gp, isPhysical);
+        num_removed_nullCheck++;
+        return 0;
+    }
+    compare_imm_reg(OpndSize_32, 0, reg, isPhysical);
+    conditional_jump_global_API(Condition_E, "common_errNullObject", false);
+    int retCode = setVRNullCheck(vr, OpndSize_32);
+    if (retCode < 0)
+        return retCode;
+    return 0;
+}
+
+/* only for O1 code generator */
+int boundCheck(int vr_array, int reg_array, bool isPhysical_array,
+               int vr_index, int reg_index, bool isPhysical_index,
+               int exceptionNum) {
+#ifdef BOUNDCHECK_OPT
+    if(isVRBoundCheck(vr_array, vr_index)) {
+        updateRefCount2(reg_array, LowOpndRegType_gp, isPhysical_array);
+        updateRefCount2(reg_index, LowOpndRegType_gp, isPhysical_index);
+        return 0;
+    }
+#endif
+    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length),
+                    reg_array, isPhysical_array,
+                    reg_index, isPhysical_index);
+
+    char errName[256];
+    sprintf(errName, "common_errArrayIndex");
+    handlePotentialException(
+                                       Condition_NC, Condition_C,
+                                       exceptionNum, errName);
+#ifdef BOUNDCHECK_OPT
+    setVRBoundCheck(vr_array, vr_index);
+#endif
+    return 0;
+}
+
+/**
+ * @brief Generates native code to perform null check
+ * @param reg temporary or physical register to test
+ * @param isPhysical flag to indicate whether parameter reg is physical
+ * register
+ * @param exceptionNum
+ * @param vr virtual register for which the null check is being done
+ * @return >= 0 on success
+ */
+int nullCheck(int reg, bool isPhysical, int exceptionNum, int vr) {
+    const char * errorName = "common_errNullObject";
+    int retCode = 0;
+
+    //nullCheck optimization is available in O1 mode only
+    if(gDvm.executionMode == kExecutionModeNcgO1 && isVRNullCheck(vr, OpndSize_32)) {
+        updateRefCount2(reg, LowOpndRegType_gp, isPhysical);
+        if(exceptionNum <= 1) {
+            updateRefCount2(PhysicalReg_EDX, LowOpndRegType_gp, true);
+            updateRefCount2(PhysicalReg_EDX, LowOpndRegType_gp, true);
+        }
+        num_removed_nullCheck++;
+        return 0;
+    }
+
+    compare_imm_reg(OpndSize_32, 0, reg, isPhysical);
+
+    // Get a label for exception handling restore state
+    char * newStreamLabel =
+            singletonPtr<ExceptionHandlingRestoreState>()->getUniqueLabel();
+
+    // Since we are not doing the exception handling restore state inline, in case of
+    // ZF=1 we must jump to the BB that restores the state
+    conditional_jump(Condition_E, newStreamLabel, true);
+
+    // We can save stream pointer now since this follows a jump and ensures that
+    // scheduler already flushed stream
+    char * originalStream = stream;
+
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        rememberState(exceptionNum);
+        if (exceptionNum > 1) {
+            nextVersionOfHardReg(PhysicalReg_EDX, 2); //next version has 2 ref count
+        }
+    }
+
+    export_pc(); //use %edx
+
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("exception"); //dump GG, GL VRs
+    }
+
+    // We must flush scheduler queue now before we copy to exception handling
+    // stream.
+    if(gDvmJit.scheduling)
+        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+
+    // Move all instructions to a deferred stream that will be dumped later
+    singletonPtr<ExceptionHandlingRestoreState>()->createExceptionHandlingStream(
+            originalStream, stream, errorName);
+
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        goToState(exceptionNum);
+        retCode = setVRNullCheck(vr, OpndSize_32);
+        if (retCode < 0)
+            return retCode;
+    }
+
+    return 0;
+}
+
+/**
+ * @brief Generates code to handle potential exception
+ * @param code_excep Condition code to take exception path
+ * @param code_okay Condition code to skip exception
+ * @param exceptionNum
+ * @param errName Name of exception to handle
+ * @return >= 0 on success
+ */
+int handlePotentialException(
+                             ConditionCode code_excep, ConditionCode code_okay,
+                             int exceptionNum, const char* errName) {
+    // Get a label for exception handling restore state
+    char * newStreamLabel =
+            singletonPtr<ExceptionHandlingRestoreState>()->getUniqueLabel();
+
+    // Since we are not doing the exception handling restore state inline, in case of
+    // code_excep we must jump to the BB that restores the state
+    conditional_jump(code_excep, newStreamLabel, true);
+
+    // We can save stream pointer now since this follows a jump and ensures that
+    // scheduler already flushed stream
+    char * originalStream = stream;
+
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
+        rememberState(exceptionNum);
+        if (exceptionNum > 1) {
+            nextVersionOfHardReg(PhysicalReg_EDX, 2); //next version has 2 ref count
+        }
+    }
+
+    export_pc(); //use %edx
+
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("exception"); //dump GG, GL VRs
+    }
+
+    if(!strcmp(errName, "common_throw_message")) {
+        move_imm_to_reg(OpndSize_32, LstrInstantiationErrorPtr, PhysicalReg_ECX, true);
+    }
+
+    // We must flush scheduler queue now before we copy to exception handling
+    // stream.
+    if(gDvmJit.scheduling)
+        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+
+    // Move all instructions to a deferred stream that will be dumped later
+    singletonPtr<ExceptionHandlingRestoreState>()->createExceptionHandlingStream(
+            originalStream, stream, errName);
+
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        goToState(exceptionNum);
+    }
+
+    return 0;
+}
+
+//!generate native code to get the self pointer from glue
+
+//!It uses one scratch register
+int get_self_pointer(int reg, bool isPhysical) {
+    move_mem_to_reg(OpndSize_32, offEBP_self, PhysicalReg_EBP, true, reg, isPhysical);
+    return 0;
+}
+
+int get_res_classes(int reg, bool isPhysical)
+{
+    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
+
+    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.methodClassDex), C_SCRATCH_1, isScratchPhysical, C_SCRATCH_2, isScratchPhysical);
+
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(DvmDex, pResClasses), C_SCRATCH_2, isScratchPhysical, reg, isPhysical);
+
+    return 0;
+}
+
+//!generate native code to get the current class object from glue
+
+//!It uses two scratch registers
+int get_glue_method_class(int reg, bool isPhysical) {
+    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
+    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.method), C_SCRATCH_1, isScratchPhysical, C_SCRATCH_2, isScratchPhysical);
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Method, clazz), C_SCRATCH_2, isScratchPhysical, reg, isPhysical);
+    return 0;
+}
+//!generate native code to get the current method from glue
+
+//!It uses one scratch register
+int get_glue_method(int reg, bool isPhysical) {
+    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
+    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.method), C_SCRATCH_1, isScratchPhysical, reg, isPhysical);
+    return 0;
+}
+
+//!generate native code to get SuspendCount from glue
+
+//!It uses one scratch register
+int get_suspendCount(int reg, bool isPhysical) {
+    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
+    move_mem_to_reg(OpndSize_32, offsetof(Thread, suspendCount), C_SCRATCH_1, isScratchPhysical, reg, isPhysical);
+    return 0;
+}
+
+//!generate native code to get retval from glue
+
+//!It uses one scratch register
+int get_return_value(OpndSize size, int reg, bool isPhysical) {
+    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
+    move_mem_to_reg(size, offsetof(Thread, interpSave.retval), C_SCRATCH_1, isScratchPhysical, reg, isPhysical);
+    return 0;
+}
+
+//!generate native code to set retval in glue
+
+//!It uses one scratch register
+int set_return_value(OpndSize size, int reg, bool isPhysical) {
+    get_self_pointer(C_SCRATCH_1, isScratchPhysical);
+    move_reg_to_mem(size, reg, isPhysical, offsetof(Thread, interpSave.retval), C_SCRATCH_1, isScratchPhysical);
+    return 0;
+}
+
+/**
+ * @brief Sets self Thread's retval.
+ * @details This needs a scratch register to hold pointer to self.
+ * @param size Size of return value
+ * @param sourceReg Register that holds the return value.
+ * @param isSourcePhysical Flag that determines if the source register is
+ * physical or not. For example, the source register can be a temporary.
+ * @param scratchRegForSelfThread Scratch register to use for self pointer
+ * @param isScratchPhysical Marks whether the scratch register is physical
+ * or not.
+ * @todo Is retval set as expected for 64-bit? If retval is set as 64 bit
+ * but read as 32-bit, is this correct?
+ */
+void set_return_value(OpndSize size, int sourceReg, bool isSourcePhysical,
+        int scratchRegForSelfThread, int isScratchPhysical) {
+    // Get self pointer
+    get_self_pointer(scratchRegForSelfThread, isScratchPhysical);
+
+    // Now set Thread.retval with the source register's value
+    move_reg_to_mem(size, sourceReg, isSourcePhysical,
+            offsetof(Thread, interpSave.retval), scratchRegForSelfThread, isScratchPhysical);
+}
+//!generate native code to clear exception object in glue
+
+//!It uses two scratch registers
+int clear_exception() {
+    get_self_pointer(C_SCRATCH_2, isScratchPhysical);
+    move_imm_to_mem(OpndSize_32, 0, offsetof(Thread, exception), C_SCRATCH_2, isScratchPhysical);
+    return 0;
+}
+//!generate native code to get exception object in glue
+
+//!It uses two scratch registers
+int get_exception(int reg, bool isPhysical) {
+    get_self_pointer(C_SCRATCH_2, isScratchPhysical);
+    move_mem_to_reg(OpndSize_32, offsetof(Thread, exception), C_SCRATCH_2, isScratchPhysical, reg, isPhysical);
+    return 0;
+}
+//!generate native code to set exception object in glue
+
+//!It uses two scratch registers
+int set_exception(int reg, bool isPhysical) {
+    get_self_pointer(C_SCRATCH_2, isScratchPhysical);
+    move_reg_to_mem(OpndSize_32, reg, isPhysical, offsetof(Thread, exception), C_SCRATCH_2, isScratchPhysical);
+    return 0;
+}
+
+//! get SaveArea pointer
+
+//!
+int savearea_from_fp(int reg, bool isPhysical) {
+    int sizeofStackSaveArea = sizeof(StackSaveArea);
+    load_effective_addr(-sizeofStackSaveArea, PhysicalReg_FP, true, reg, isPhysical);
+    return 0;
+}
+
+#ifdef DEBUG_CALL_STACK3
+int call_debug_dumpSwitch() {
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = debug_dumpSwitch;
+    callFuncPtr((int)funcPtr, "debug_dumpSwitch");
+    return 0;
+}
+#endif
+
+int call_dvmQuasiAtomicSwap64() {
+    typedef int64_t (*vmHelper)(int64_t, volatile int64_t*);
+    vmHelper funcPtr = dvmQuasiAtomicSwap64;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmQuasiAtomicSwap64");
+        callFuncPtr((int)funcPtr, "dvmQuasiAtomicSwap64");
+        afterCall("dvmQuasiAtomicSwap64");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmQuasiAtomicSwap64");
+    }
+    return 0;
+}
+
+int call_dvmQuasiAtomicRead64() {
+    typedef int64_t (*vmHelper)(volatile const int64_t*);
+    vmHelper funcPtr = dvmQuasiAtomicRead64;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmQuasiAtomiRead64");
+        callFuncPtr((int)funcPtr, "dvmQuasiAtomicRead64");
+        afterCall("dvmQuasiAtomicRead64");
+        touchEax(); //for return value
+        touchEdx();
+    } else {
+        callFuncPtr((int)funcPtr, "dvmQuasiAtomicRead64");
+    }
+    return 0;
+}
+
+int call_dvmJitToInterpPunt() {
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmJitToInterpPunt;
+    callFuncPtr((int)funcPtr, "dvmJitToInterpPunt");
+    return 0;
+}
+
+void call_dvmJitToInterpNormal() {
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmJitToInterpNormal;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmJitToInterpNormal");
+        callFuncPtrImm((int)funcPtr);
+        afterCall("dvmJitToInterpNormal");
+        touchEbx();
+    } else {
+        callFuncPtrImm((int)funcPtr);
+    }
+    return;
+}
+
+/*
+ * helper function for generating the call to dvmJitToInterpBackwardBranch
+ * This transition to the interpreter is also required for
+ * self-verification, in particular, in order to check
+ * for control or data divergence for each loop iteration.
+ */
+void call_dvmJitToInterpBackwardBranch() {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmJitToInterpBackwardBranch");
+    }
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmJitToInterpBackwardBranch;
+    callFuncPtrImm((int)funcPtr);
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        afterCall("dvmJitToInterpBackwardBranch");
+   }
+   return;
+ }
+
+int call_dvmJitToInterpTraceSelectNoChain() {
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmJitToInterpTraceSelectNoChain;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmJitToInterpTraceSelectNoChain");
+        callFuncPtr((int)funcPtr, "dvmJitToInterpTraceSelectNoChain");
+        afterCall("dvmJitToInterpTraceSelectNoChain");
+        touchEbx();
+    } else {
+        callFuncPtr((int)funcPtr, "dvmJitToInterpTraceSelectNoChain");
+    }
+    return 0;
+}
+
+void call_dvmJitToInterpTraceSelect() {
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmJitToInterpTraceSelect;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmJitToInterpTraceSelect");
+        callFuncPtrImm((int)funcPtr);
+        afterCall("dvmJitToInterpTraceSelect");
+        touchEbx();
+    } else {
+        callFuncPtrImm((int)funcPtr);
+    }
+    return;
+}
+
+int call_dvmJitToPatchPredictedChain() {
+    typedef const Method * (*vmHelper)(const Method *method,
+                                       Thread *self,
+                                       PredictedChainingCell *cell,
+                                       const ClassObject *clazz);
+    vmHelper funcPtr = dvmJitToPatchPredictedChain;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmJitToPatchPredictedChain");
+        callFuncPtr((int)funcPtr, "dvmJitToPatchPredictedChain");
+        afterCall("dvmJitToPatchPredictedChain");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmJitToPatchPredictedChain");
+    }
+    return 0;
+}
+
+//!generate native code to call __moddi3
+
+//!
+int call_moddi3() {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("moddi3");
+        callFuncPtr((intptr_t)__moddi3, "__moddi3");
+        afterCall("moddi3");
+    } else {
+        callFuncPtr((intptr_t)__moddi3, "__moddi3");
+    }
+    return 0;
+}
+//!generate native code to call __divdi3
+
+//!
+int call_divdi3() {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("divdi3");
+        callFuncPtr((intptr_t)__divdi3, "__divdi3");
+        afterCall("divdi3");
+    } else {
+        callFuncPtr((intptr_t)__divdi3, "__divdi3");
+    }
+    return 0;
+}
+
+//!generate native code to call fmod
+
+//!
+int call_fmod() {
+    typedef double (*libHelper)(double, double);
+    libHelper funcPtr = fmod;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("fmod");
+        callFuncPtr((int)funcPtr, "fmod");
+        afterCall("fmod");
+    } else {
+        callFuncPtr((int)funcPtr, "fmod");
+    }
+    return 0;
+}
+//!generate native code to call fmodf
+
+//!
+int call_fmodf() {
+    typedef float (*libHelper)(float, float);
+    libHelper funcPtr = fmodf;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("fmodf");
+        callFuncPtr((int)funcPtr, "fmodf");
+        afterCall("fmodf");
+    } else {
+        callFuncPtr((int)funcPtr, "fmodf");
+    }
+    return 0;
+}
+//!generate native code to call dvmFindCatchBlock
+
+//!
+int call_dvmFindCatchBlock() {
+    //int dvmFindCatchBlock(Thread* self, int relPc, Object* exception,
+    //bool doUnroll, void** newFrame)
+    typedef int (*vmHelper)(Thread*, int, Object*, int, void**);
+    vmHelper funcPtr = dvmFindCatchBlock;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmFindCatchBlock");
+        callFuncPtr((int)funcPtr, "dvmFindCatchBlock");
+        afterCall("dvmFindCatchBlock");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmFindCatchBlock");
+    }
+    return 0;
+}
+//!generate native code to call dvmThrowVerificationError
+
+//!
+int call_dvmThrowVerificationError() {
+    typedef void (*vmHelper)(const Method*, int, int);
+    vmHelper funcPtr = dvmThrowVerificationError;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmThrowVerificationError");
+        callFuncPtr((int)funcPtr, "dvmThrowVerificationError");
+        afterCall("dvmThrowVerificationError");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmThrowVerificationError");
+    }
+    return 0;
+}
+
+//!generate native code to call dvmResolveMethod
+
+//!
+int call_dvmResolveMethod() {
+    //Method* dvmResolveMethod(const ClassObject* referrer, u4 methodIdx, MethodType methodType);
+    typedef Method* (*vmHelper)(const ClassObject*, u4, MethodType);
+    vmHelper funcPtr = dvmResolveMethod;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmResolveMethod");
+        callFuncPtr((int)funcPtr, "dvmResolveMethod");
+        afterCall("dvmResolveMethod");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmResolveMethod");
+    }
+    return 0;
+}
+//!generate native code to call dvmResolveClass
+
+//!
+int call_dvmResolveClass() {
+    //ClassObject* dvmResolveClass(const ClassObject* referrer, u4 classIdx, bool fromUnverifiedConstant)
+    typedef ClassObject* (*vmHelper)(const ClassObject*, u4, bool);
+    vmHelper funcPtr = dvmResolveClass;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmResolveClass");
+        callFuncPtr((int)funcPtr, "dvmResolveClass");
+        afterCall("dvmResolveClass");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmResolveClass");
+    }
+    return 0;
+}
+
+//!generate native code to call dvmInstanceofNonTrivial
+
+//!
+int call_dvmInstanceofNonTrivial() {
+    typedef int (*vmHelper)(const ClassObject*, const ClassObject*);
+    vmHelper funcPtr = dvmInstanceofNonTrivial;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmInstanceofNonTrivial");
+        callFuncPtr((int)funcPtr, "dvmInstanceofNonTrivial");
+        afterCall("dvmInstanceofNonTrivial");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmInstanceofNonTrivial");
+    }
+    return 0;
+}
+//!generate native code to call dvmThrowException
+
+//!
+int call_dvmThrow() {
+    typedef void (*vmHelper)(ClassObject* exceptionClass, const char*);
+    vmHelper funcPtr = dvmThrowException;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmThrowException");
+        callFuncPtr((int)funcPtr, "dvmThrowException");
+        afterCall("dvmThrowException");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmThrowException");
+    }
+    return 0;
+}
+//!generate native code to call dvmThrowExceptionWithClassMessage
+
+//!
+int call_dvmThrowWithMessage() {
+    typedef void (*vmHelper)(ClassObject* exceptionClass, const char*);
+    vmHelper funcPtr = dvmThrowExceptionWithClassMessage;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmThrowExceptionWithClassMessage");
+        callFuncPtr((int)funcPtr, "dvmThrowExceptionWithClassMessage");
+        afterCall("dvmThrowExceptionWithClassMessage");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmThrowExceptionWithClassMessage");
+    }
+    return 0;
+}
+//!generate native code to call dvmCheckSuspendPending
+
+//!
+int call_dvmCheckSuspendPending() {
+    typedef bool (*vmHelper)(Thread*);
+    vmHelper funcPtr = dvmCheckSuspendPending;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmCheckSuspendPending");
+        callFuncPtr((int)funcPtr, "dvmCheckSuspendPending");
+        afterCall("dvmCheckSuspendPending");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmCheckSuspendPending");
+    }
+    return 0;
+}
+//!generate native code to call dvmLockObject
+
+//!
+int call_dvmLockObject() {
+    typedef void (*vmHelper)(struct Thread*, struct Object*);
+    vmHelper funcPtr = dvmLockObject;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmLockObject");
+        callFuncPtr((int)funcPtr, "dvmLockObject");
+        afterCall("dvmLockObject");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmLockObject");
+    }
+    return 0;
+}
+//!generate native code to call dvmUnlockObject
+
+//!
+int call_dvmUnlockObject() {
+    typedef bool (*vmHelper)(Thread*, Object*);
+    vmHelper funcPtr = dvmUnlockObject;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmUnlockObject");
+        callFuncPtr((int)funcPtr, "dvmUnlockObject");
+        afterCall("dvmUnlockObject");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmUnlockObject");
+    }
+    return 0;
+}
+//!generate native code to call dvmInitClass
+
+//!
+int call_dvmInitClass() {
+    typedef bool (*vmHelper)(ClassObject*);
+    vmHelper funcPtr = dvmInitClass;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmInitClass");
+        callFuncPtr((int)funcPtr, "dvmInitClass");
+        afterCall("dvmInitClass");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmInitClass");
+    }
+    return 0;
+}
+//!generate native code to call dvmAllocObject
+
+//!
+int call_dvmAllocObject() {
+    typedef Object* (*vmHelper)(ClassObject*, int);
+    vmHelper funcPtr = dvmAllocObject;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmAllocObject");
+        callFuncPtr((int)funcPtr, "dvmAllocObject");
+        afterCall("dvmAllocObject");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmAllocObject");
+    }
+    return 0;
+}
+//!generate native code to call dvmAllocArrayByClass
+
+//!
+int call_dvmAllocArrayByClass() {
+    typedef ArrayObject* (*vmHelper)(ClassObject*, size_t, int);
+    vmHelper funcPtr = dvmAllocArrayByClass;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmAllocArrayByClass");
+        callFuncPtr((int)funcPtr, "dvmAllocArrayByClass");
+        afterCall("dvmAllocArrayByClass");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmAllocArrayByClass");
+    }
+    return 0;
+}
+//!generate native code to call dvmAllocPrimitiveArray
+
+//!
+int call_dvmAllocPrimitiveArray() {
+    typedef ArrayObject* (*vmHelper)(char, size_t, int);
+    vmHelper funcPtr = dvmAllocPrimitiveArray;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmAllocPrimitiveArray");
+        callFuncPtr((int)funcPtr, "dvmAllocPrimitiveArray");
+        afterCall("dvmAllocPrimitiveArray");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmAllocPrimitiveArray");
+    }
+    return 0;
+}
+//!generate native code to call dvmInterpHandleFillArrayData
+
+//!
+int call_dvmInterpHandleFillArrayData() {
+    typedef bool (*vmHelper)(ArrayObject*, const u2*);
+    vmHelper funcPtr = dvmInterpHandleFillArrayData;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmInterpHandleFillArrayData"); //before move_imm_to_reg to avoid spilling C_SCRATCH_1
+        callFuncPtr((int)funcPtr, "dvmInterpHandleFillArrayData");
+        afterCall("dvmInterpHandleFillArrayData");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmInterpHandleFillArrayData");
+    }
+    return 0;
+}
+
+//!generate native code to call dvmNcgHandlePackedSwitch
+
+//!
+int call_dvmNcgHandlePackedSwitch() {
+    typedef s4 (*vmHelper)(const s4*, s4, u2, s4);
+    vmHelper funcPtr = dvmNcgHandlePackedSwitch;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmNcgHandlePackedSwitch");
+        callFuncPtr((int)funcPtr, "dvmNcgHandlePackedSwitch");
+        afterCall("dvmNcgHandlePackedSwitch");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmNcgHandlePackedSwitch");
+    }
+    return 0;
+}
+
+int call_dvmJitHandlePackedSwitch() {
+    typedef s4 (*vmHelper)(const s4*, s4, u2, s4);
+    vmHelper funcPtr = dvmJitHandlePackedSwitch;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmJitHandlePackedSwitch");
+        callFuncPtr((int)funcPtr, "dvmJitHandlePackedSwitch");
+        afterCall("dvmJitHandlePackedSwitch");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmJitHandlePackedSwitch");
+    }
+    return 0;
+}
+
+//!generate native code to call dvmNcgHandleSparseSwitch
+
+//!
+int call_dvmNcgHandleSparseSwitch() {
+    typedef s4 (*vmHelper)(const s4*, u2, s4);
+    vmHelper funcPtr = dvmNcgHandleSparseSwitch;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmNcgHandleSparseSwitch");
+        callFuncPtr((int)funcPtr, "dvmNcgHandleSparseSwitch");
+        afterCall("dvmNcgHandleSparseSwitch");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmNcgHandleSparseSwitch");
+    }
+    return 0;
+}
+
+int call_dvmJitHandleSparseSwitch() {
+    typedef s4 (*vmHelper)(const s4*, u2, s4);
+    vmHelper funcPtr = dvmJitHandleSparseSwitch;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmJitHandleSparseSwitch");
+        callFuncPtr((int)funcPtr, "dvmJitHandleSparseSwitch");
+        afterCall("dvmJitHandleSparseSwitch");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmJitHandleSparseSwitch");
+    }
+    return 0;
+}
+
+//!generate native code to call dvmCanPutArrayElement
+
+//!
+int call_dvmCanPutArrayElement() {
+    typedef bool (*vmHelper)(const ClassObject*, const ClassObject*);
+    vmHelper funcPtr = dvmCanPutArrayElement;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmCanPutArrayElement");
+        callFuncPtr((int)funcPtr, "dvmCanPutArrayElement");
+        afterCall("dvmCanPutArrayElement");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmCanPutArrayElement");
+    }
+    return 0;
+}
+
+//!generate native code to call dvmFindInterfaceMethodInCache
+
+//!
+int call_dvmFindInterfaceMethodInCache() {
+    typedef Method* (*vmHelper)(ClassObject*, u4, const Method*, DvmDex*);
+    vmHelper funcPtr = dvmFindInterfaceMethodInCache;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmFindInterfaceMethodInCache");
+        callFuncPtr((int)funcPtr, "dvmFindInterfaceMethodInCache");
+        afterCall("dvmFindInterfaceMethodInCache");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmFindInterfaceMethodInCache");
+    }
+    return 0;
+}
+
+//!generate native code to call dvmHandleStackOverflow
+
+//!
+int call_dvmHandleStackOverflow() {
+    typedef void (*vmHelper)(Thread*, const Method*);
+    vmHelper funcPtr = dvmHandleStackOverflow;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmHandleStackOverflow");
+        callFuncPtr((int)funcPtr, "dvmHandleStackOverflow");
+        afterCall("dvmHandleStackOverflow");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmHandleStackOverflow");
+    }
+    return 0;
+}
+//!generate native code to call dvmResolveString
+
+//!
+int call_dvmResolveString() {
+    //StringObject* dvmResolveString(const ClassObject* referrer, u4 stringIdx)
+    typedef StringObject* (*vmHelper)(const ClassObject*, u4);
+    vmHelper funcPtr = dvmResolveString;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmResolveString");
+        callFuncPtr((int)funcPtr, "dvmResolveString");
+        afterCall("dvmResolveString");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmResolveString");
+    }
+    return 0;
+}
+//!generate native code to call dvmResolveInstField
+
+//!
+int call_dvmResolveInstField() {
+    //InstField* dvmResolveInstField(const ClassObject* referrer, u4 ifieldIdx)
+    typedef InstField* (*vmHelper)(const ClassObject*, u4);
+    vmHelper funcPtr = dvmResolveInstField;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmResolveInstField");
+        callFuncPtr((int)funcPtr, "dvmResolveInstField");
+        afterCall("dvmResolveInstField");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmResolveInstField");
+    }
+    return 0;
+}
+//!generate native code to call dvmResolveStaticField
+
+//!
+int call_dvmResolveStaticField() {
+    //StaticField* dvmResolveStaticField(const ClassObject* referrer, u4 sfieldIdx)
+    typedef StaticField* (*vmHelper)(const ClassObject*, u4);
+    vmHelper funcPtr = dvmResolveStaticField;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("dvmResolveStaticField");
+        callFuncPtr((int)funcPtr, "dvmResolveStaticField");
+        afterCall("dvmResolveStaticField");
+    } else {
+        callFuncPtr((int)funcPtr, "dvmResolveStaticField");
+    }
+    return 0;
+}
+
+#define P_GPR_2 PhysicalReg_ECX
+/*!
+\brief This function is used to resolve a string reference
+
+INPUT: const pool index in %eax
+
+OUTPUT: resolved string in %eax
+
+The registers are hard-coded, 2 physical registers %esi and %edx are used as scratch registers;
+It calls a C function dvmResolveString;
+The only register that is still live after this function is ebx
+*/
+int const_string_resolve() {
+    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    if (insertLabel(".const_string_resolve", false) == -1)
+        return -1;
+    //method stored in glue structure as well as on the interpreted stack
+    get_glue_method_class(P_GPR_2, true);
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 4, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, P_GPR_2, true, 0, PhysicalReg_ESP, true);
+    call_dvmResolveString();
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    compare_imm_reg( OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_E, "common_exceptionThrown", false);
+    x86_return();
+    return 0;
+}
+#undef P_GPR_2
+/*!
+\brief This function is used to resolve a class
+
+INPUT: const pool index in argument "indexReg" (%eax)
+
+OUTPUT: resolved class in %eax
+
+The registers are hard-coded, 3 physical registers (%esi, %edx, startLR:%eax) are used as scratch registers.
+It calls a C function dvmResolveClass;
+The only register that is still live after this function is ebx
+*/
+int resolve_class2(
+           int startLR/*scratch register*/, bool isPhysical, int indexReg/*const pool index*/,
+           bool indexPhysical, int thirdArg) {
+    if (insertLabel(".class_resolve", false) == -1)
+        return -1;
+
+    //Get call back
+    void (*backEndSymbolCreationCallback) (const char *, void *) =
+        gDvmJit.jitFramework.backEndSymbolCreationCallback;
+
+    //Call it if we have one
+    if (backEndSymbolCreationCallback != 0)
+    {
+        backEndSymbolCreationCallback (".class_resolve", (void*) stream);
+    }
+
+    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+
+    //push index to stack first, to free indexReg
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, indexReg, indexPhysical, 4, PhysicalReg_ESP, true);
+    get_glue_method_class(startLR, isPhysical);
+    move_imm_to_mem(OpndSize_32, thirdArg, 8, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, startLR, isPhysical, 0, PhysicalReg_ESP, true);
+    call_dvmResolveClass();
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_E, "common_exceptionThrown", false);
+
+    x86_return();
+    return 0;
+}
+/*!
+\brief This function is used to resolve a method, and it is called once with %eax for both indexReg and startLR
+
+INPUT: const pool index in argument "indexReg" (%eax)
+
+OUTPUT: resolved method in %eax
+
+The registers are hard-coded, 3 physical registers (%esi, %edx, startLR:%eax) are used as scratch registers.
+It calls a C function dvmResolveMethod;
+The only register that is still live after this function is ebx
+*/
+int resolve_method2(
+            int startLR/*logical register index*/, bool isPhysical, int indexReg/*const pool index*/,
+            bool indexPhysical,
+            int thirdArg/*VIRTUAL*/) {
+    if(thirdArg == METHOD_VIRTUAL) {
+        if (insertLabel(".virtual_method_resolve", false) == -1)
+            return -1;
+    }
+    else if(thirdArg == METHOD_DIRECT) {
+        if (insertLabel(".direct_method_resolve", false) == -1)
+            return -1;
+    }
+    else if(thirdArg == METHOD_STATIC) {
+        if (insertLabel(".static_method_resolve", false) == -1)
+            return -1;
+    }
+
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, indexReg, indexPhysical, 4, PhysicalReg_ESP, true);
+
+    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    get_glue_method_class(startLR, isPhysical);
+
+    move_imm_to_mem(OpndSize_32, thirdArg, 8, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, startLR, isPhysical, 0, PhysicalReg_ESP, true);
+    call_dvmResolveMethod();
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_E, "common_exceptionThrown", false);
+
+    x86_return();
+    return 0;
+}
+/*!
+\brief This function is used to resolve an instance field
+
+INPUT: const pool index in argument "indexReg" (%eax)
+
+OUTPUT: resolved field in %eax
+
+The registers are hard-coded, 3 physical registers (%esi, %edx, startLR:%eax) are used as scratch registers.
+It calls a C function dvmResolveInstField;
+The only register that is still live after this function is ebx
+*/
+int resolve_inst_field2(
+            int startLR/*logical register index*/, bool isPhysical,
+            int indexReg/*const pool index*/, bool indexPhysical) {
+    if (insertLabel(".inst_field_resolve", false) == -1)
+        return -1;
+    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, indexReg, indexPhysical, 4, PhysicalReg_ESP, true);
+    //method stored in glue structure as well as interpreted stack
+    get_glue_method_class(startLR, isPhysical);
+    move_reg_to_mem(OpndSize_32, startLR, isPhysical, 0, PhysicalReg_ESP, true);
+    call_dvmResolveInstField();
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_E, "common_exceptionThrown", false);
+
+    x86_return();
+    return 0;
+}
+/*!
+\brief This function is used to resolve a static field
+
+INPUT: const pool index in argument "indexReg" (%eax)
+
+OUTPUT: resolved field in %eax
+
+The registers are hard-coded, 3 physical registers (%esi, %edx, startLR:%eax) are used as scratch registers.
+It calls a C function dvmResolveStaticField;
+The only register that is still live after this function is ebx
+*/
+int resolve_static_field2(
+              int startLR/*logical register index*/, bool isPhysical, int indexReg/*const pool index*/,
+              bool indexPhysical) {
+    if (insertLabel(".static_field_resolve", false) == -1)
+        return -1;
+    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, indexReg, indexPhysical, 4, PhysicalReg_ESP, true);
+    get_glue_method_class(startLR, isPhysical);
+    move_reg_to_mem(OpndSize_32, startLR, isPhysical, 0, PhysicalReg_ESP, true);
+    call_dvmResolveStaticField();
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_E, "common_exceptionThrown", false);
+
+    x86_return();
+    return 0;
+}
+
+int pushAllRegs() {
+    load_effective_addr(-28, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EAX, true, 24, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
+    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EBX, true, 20, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
+    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_ECX, true, 16, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
+    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EDX, true, 12, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
+    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_ESI, true, 8, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
+    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EDI, true, 4, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
+    move_reg_to_mem_noalloc(OpndSize_32, PhysicalReg_EBP, true, 0, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1);
+    return 0;
+}
+int popAllRegs() {
+    move_mem_to_reg_noalloc(OpndSize_32, 24, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EAX, true);
+    move_mem_to_reg_noalloc(OpndSize_32, 20, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EBX, true);
+    move_mem_to_reg_noalloc(OpndSize_32, 16, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_ECX, true);
+    move_mem_to_reg_noalloc(OpndSize_32, 12, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EDX, true);
+    move_mem_to_reg_noalloc(OpndSize_32, 8, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_ESI, true);
+    move_mem_to_reg_noalloc(OpndSize_32, 4, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EDI, true);
+    move_mem_to_reg_noalloc(OpndSize_32, 0, PhysicalReg_ESP, true, MemoryAccess_Unknown, -1, PhysicalReg_EBP, true);
+    load_effective_addr(28, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    return 0;
+}
+
+/* align the relative offset of jmp/jcc and movl within 16B */
+void alignOffset(int offset) {
+    int rem, nop_size;
+
+    if ((uint)(stream + offset) % 16 > 12) {
+        rem = (uint)(stream + offset) % 16;
+        nop_size = (16 - rem) % 16;
+        stream = encoder_nops(nop_size, stream);
+    }
+}
+
+/**
+  * @brief align a pointer to n-bytes aligned
+  * @param addr the pointer need to be aligned
+  * @param n n-bytes aligned
+  * @return aligned address
+  */
+char* align(char* addr, int n) {
+    char* alignedAddr = reinterpret_cast<char*>((reinterpret_cast<unsigned int>(addr) + (n-1)) & ~(n-1));
+    return alignedAddr;
+}
+/**
+ * @brief Returns whether the jump to BB needs alignment
+ * because it might be patched later on.
+ * @param bb Basic Block to look at
+ * @return Returns true for all chaining cells and also for
+ * the prebackward block.
+ */
+bool doesJumpToBBNeedAlignment(BasicBlock * bb) {
+    // Get type for this BB
+    int type = static_cast<int>(bb->blockType);
+
+    if ((type >= static_cast<int> (kChainingCellNormal)
+            && type < static_cast<int> (kChainingCellLast))
+            && type != static_cast<int> (kChainingCellBackwardBranch))
+    {
+        //We always return true if BB is a chaining cell except if it is
+        //backward branch chaining cell. The reason we make exception for
+        //BBCC is because we always patch the jump to preBackwardBlock and
+        //not the jump to the chaining cell
+        return true;
+    }
+    else if (type == static_cast<int> (kPreBackwardBlock))
+    {
+        //Since the prebackward block is always used in front of
+        //backward branch chaining cell and the jump to it is
+        //the one being patched, we also return true.
+        return true;
+    }
+    else
+    {
+        return false;
+    }
+}
+
+#ifdef WITH_SELF_VERIFICATION
+int selfVerificationLoad(int addr, int opndSize) {
+    assert (opndSize != OpndSize_64);
+    assert(addr != 0);
+
+    Thread *self = dvmThreadSelf();
+    ShadowSpace *shadowSpace = self->shadowSpace;
+    ShadowHeap *heapSpacePtr;
+
+    assert(shadowSpace != 0);
+    assert(shadowSpace->heapSpace != 0);
+    int data = 0;
+
+    for (heapSpacePtr = shadowSpace->heapSpace;
+        heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
+        if (heapSpacePtr->addr == addr) {
+            addr = (unsigned int)(&(heapSpacePtr->data));
+            break;
+        }
+    }
+
+    /* load addr from the shadow heap, native addr-> shadow heap addr
+     * if not found load the data from the native heap
+     */
+    switch (opndSize) {
+        case OpndSize_8:
+            data = *(reinterpret_cast<u1*> (addr));
+            break;
+        case OpndSize_16:
+            data = *(reinterpret_cast<u2*> (addr));
+            break;
+        //signed versions
+        case 0x11:  //signed OpndSize_8
+            data = *(reinterpret_cast<s1*> (addr));
+            break;
+        case 0x22:  //signed OpndSize_16
+            data = *(reinterpret_cast<s2*> (addr));
+            break;
+        case OpndSize_32:
+            data = *(reinterpret_cast<u4*> (addr));
+            break;
+        default:
+            ALOGE("*** ERROR: BAD SIZE IN selfVerificationLoad: %d", opndSize);
+            data = 0;
+            dvmAbort();
+            break;
+    }
+
+#if defined(SELF_VERIFICATION_LOG)
+    ALOGD("*** HEAP LOAD: Addr: %#x Data: %d Size: %d", addr, data, opndSize);
+#endif
+    return data;
+}
+
+void selfVerificationStore(int addr, int data, int opndSize)
+{
+    assert(addr != 0);
+    Thread *self = dvmThreadSelf();
+    ShadowSpace *shadowSpace = self->shadowSpace;
+    ShadowHeap *heapSpacePtr;
+
+    assert(shadowSpace != 0);
+    assert(shadowSpace->heapSpace != 0);
+#if defined(SELF_VERIFICATION_LOG)
+    ALOGD("*** HEAP STORE: Addr: %#x Data: %d Size: %d", addr, data, opndSize);
+#endif
+    for (heapSpacePtr = shadowSpace->heapSpace;
+         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
+        if (heapSpacePtr->addr == addr) {
+            break;
+        }
+    }
+
+    //If the store addr is requested for the first time, its not present in the
+    //heap so add it to the shadow heap.
+    if (heapSpacePtr == shadowSpace->heapSpaceTail) {
+        heapSpacePtr->addr = addr;
+        shadowSpace->heapSpaceTail++;
+        // shadow heap can contain HEAP_SPACE(JIT_MAX_TRACE_LEN) number of entries
+        if(shadowSpace->heapSpaceTail > &(shadowSpace->heapSpace[HEAP_SPACE])) {
+            ALOGD("*** Shadow HEAP store ran out of space, aborting VM");
+            dvmAbort();
+        }
+    }
+
+    addr = ((unsigned int) &(heapSpacePtr->data));
+    switch (opndSize) {
+        case OpndSize_8:
+            *(reinterpret_cast<u1*>(addr)) = data;
+            break;
+        case OpndSize_16:
+            *(reinterpret_cast<u2*>(addr)) = data;
+            break;
+        case OpndSize_32:
+            *(reinterpret_cast<u4*>(addr)) = data;
+            break;
+        default:
+            ALOGE("*** ERROR: BAD SIZE IN selfVerificationSave: %d", opndSize);
+            dvmAbort();
+            break;
+    }
+}
+
+void selfVerificationLoadDoubleword(int addr)
+{
+    assert(addr != 0);
+    Thread *self = dvmThreadSelf();
+    ShadowSpace* shadowSpace = self->shadowSpace;
+    ShadowHeap* heapSpacePtr;
+    int byte_count = 0;
+
+    assert(shadowSpace != 0);
+    assert(shadowSpace->heapSpace != 0);
+    //TODO: do a volatile GET_WIDE implementation
+
+    int addr2 = addr+4;
+    /* load data and data2 from the native heap
+     * so in case this address is not stored in the shadow heap
+     * the value loaded from the native heap is used, else
+     * it is overwritten with the value from the shadow stack
+     */
+    unsigned int data = *(reinterpret_cast<unsigned int*> (addr));
+    unsigned int data2 = *(reinterpret_cast<unsigned int*> (addr2));
+
+    for (heapSpacePtr = shadowSpace->heapSpace;
+         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
+        if (heapSpacePtr->addr == addr) {
+            data = heapSpacePtr->data;
+            byte_count++;
+        } else if (heapSpacePtr->addr == addr2) {
+            data2 = heapSpacePtr->data;
+            byte_count++;
+        }
+        if(byte_count == 2) break;
+    }
+
+#if defined(SELF_VERIFICATION_LOG)
+    ALOGD("*** HEAP LOAD DOUBLEWORD: Addr: %#x Data: %#x Data2: %#x",
+        addr, data, data2);
+#endif
+
+    // xmm6 is scratch; passing value back to aget_common_nohelper in xmm7
+    asm volatile (
+            "movd %0, %%xmm6\n\t"
+            "movd %1, %%xmm7\n\t"
+            "psllq $32, %%xmm6\n\t"
+            "paddq %%xmm6, %%xmm7"
+            :
+            : "rm" (data2), "rm" (data)
+            : "xmm6", "xmm7");
+}
+
+void selfVerificationStoreDoubleword(int addr, s8 double_data)
+{
+    assert(addr != 0);
+
+    Thread *self = dvmThreadSelf();
+    ShadowSpace *shadowSpace = self->shadowSpace;
+    ShadowHeap *heapSpacePtr;
+
+    assert(shadowSpace != 0);
+    assert(shadowSpace->heapSpace != 0);
+
+    int addr2 = addr+4;
+    int data = double_data;
+    int data2 = double_data >> 32;
+    bool store1 = false, store2 = false;
+
+#if defined(SELF_VERIFICATION_LOG)
+    ALOGD("*** HEAP STORE DOUBLEWORD: Addr: %#x Data: %#x, Data2: %#x",
+        addr, data, data2);
+#endif
+
+    //data++; data2++;  // test case for SV detection
+
+    for (heapSpacePtr = shadowSpace->heapSpace;
+         heapSpacePtr != shadowSpace->heapSpaceTail; heapSpacePtr++) {
+        if (heapSpacePtr->addr == addr) {
+            heapSpacePtr->data = data;
+            store1 = true;
+        } else if (heapSpacePtr->addr == addr2) {
+            heapSpacePtr->data = data2;
+            store2 = true;
+        }
+        if(store1 && store2) {
+            break;
+        }
+    }
+
+    // shadow heap can contain HEAP_SPACE(JIT_MAX_TRACE_LEN) number of entries
+    int additions = store1 ? 1 : 0;
+    additions += store2 ? 1 : 0;
+    if((shadowSpace->heapSpaceTail + additions) >= &(shadowSpace->heapSpace[HEAP_SPACE])) {
+        ALOGD("*** Shadow HEAP store ran out of space, aborting VM");
+        dvmAbort();
+    }
+
+    if (store1 == false) {
+        shadowSpace->heapSpaceTail->addr = addr;
+        shadowSpace->heapSpaceTail->data = data;
+        shadowSpace->heapSpaceTail++;
+    }
+    if (store2 == false) {
+        shadowSpace->heapSpaceTail->addr = addr2;
+        shadowSpace->heapSpaceTail->data = data2;
+        shadowSpace->heapSpaceTail++;
+    }
+}
+
+int call_selfVerificationLoad(void) {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("selfVerificationLoad");
+    }
+    typedef int (*vmHelper)(int, int);
+    vmHelper funcPtr = selfVerificationLoad;
+    callFuncPtr((int)funcPtr, "selfVerificationLoad");
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        afterCall("selfVerificationLoad");
+    }
+    return 0;
+}
+
+int call_selfVerificationLoadDoubleword(void) {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("selfVerificationLoadDoubleword");
+    }
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = selfVerificationLoadDoubleword;
+    callFuncPtr((int)funcPtr, "selfVerificationLoadDoubleword");
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        afterCall("selfVerificationLoadDoubleword");
+    }
+    return 0;
+}
+
+int call_selfVerificationStore(void) {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("selfVerificationStore");
+    }
+    typedef void (*vmHelper)(int, int, int);
+    vmHelper funcPtr = selfVerificationStore;
+    callFuncPtr((int)funcPtr, "selfVerificationStore");
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        afterCall("selfVerificationStore");
+    }
+    return 0;
+}
+
+int call_selfVerificationStoreDoubleword(void) {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall("selfVerificationStoreDoubleword");
+    }
+    typedef void (*vmHelper)(int, s8);
+    vmHelper funcPtr = selfVerificationStoreDoubleword;
+    callFuncPtr((int)funcPtr, "selfVerificationStoreDoubleword");
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        afterCall("selfVerificationStoreDoubleword");
+    }
+    return 0;
+}
+#endif
+
+void pushCallerSavedRegs(void) {
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 8, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 4, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, 0, PhysicalReg_ESP, true);
+}
+
+void popCallerSavedRegs(void) {
+    move_mem_to_reg(OpndSize_32, 8, PhysicalReg_ESP, true,  PhysicalReg_EAX, true);
+    move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true,  PhysicalReg_ECX, true);
+    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true,  PhysicalReg_EDX, true);
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+}
+
+//! \brief compareAndExchange with one reg operand and one mem operand
+//! used for implementing monitor-enter
+//! \param size operand size
+//! \param reg src register
+//! \param isPhysical if reg is a physical register
+//! \param disp displacement offset
+//! \param base_reg physical register (PhysicalReg type) or a logical register
+//! \param isBasePhysical if base_reg is a physical register
+void compareAndExchange(OpndSize size,
+             int reg, bool isPhysical,
+             int disp, int base_reg, bool isBasePhysical) {
+    dump_reg_mem(Mnemonic_CMPXCHG, ATOM_NORMAL, size, reg, isPhysical, disp, base_reg, isBasePhysical, MemoryAccess_Unknown, -1, getTypeFromIntSize(size));
+}
+
diff --git a/vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp b/vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp
new file mode 100644
index 0000000..d2143ba
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp
@@ -0,0 +1,2090 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file LowerInvoke.cpp
+    \brief This file lowers the following bytecodes: INVOKE_XXX
+*/
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "mterp/Mterp.h"
+#include "Lower.h"
+#include "NcgAot.h"
+#include "enc_wrapper.h"
+
+#if defined VTUNE_DALVIK
+#include "compiler/JitProfiling.h"
+#endif
+
+char* streamMisPred = NULL;
+
+/* according to callee, decide the ArgsDoneType*/
+ArgsDoneType convertCalleeToType(const Method* calleeMethod) {
+    if(calleeMethod == NULL)
+        return ArgsDone_Full;
+    if(dvmIsNativeMethod(calleeMethod))
+        return ArgsDone_Native;
+    return ArgsDone_Normal;
+}
+int common_invokeMethodRange(ArgsDoneType,
+        const DecodedInstruction &decodedInst);
+int common_invokeMethodNoRange(ArgsDoneType,
+        const DecodedInstruction &decodedInst);
+void gen_predicted_chain(bool isRange, u2 tmp, int IMMC, bool isInterface,
+        int inputReg, const DecodedInstruction &decodedInst);
+
+//inputs to common_invokeMethodRange: %ecx
+//          common_errNoSuchMethod: %edx
+#define P_GPR_1 PhysicalReg_ESI
+#define P_GPR_2 PhysicalReg_EBX
+#define P_GPR_3 PhysicalReg_ECX
+#define P_SCRATCH_1 PhysicalReg_EDX
+#define PP_GPR_1 PhysicalReg_EBX
+#define PP_GPR_2 PhysicalReg_ESI
+#define PP_GPR_3 PhysicalReg_EAX
+#define PP_GPR_4 PhysicalReg_EDX
+
+#ifdef WITH_JIT_INLINING_PHASE2
+/*
+ * The function here takes care the
+ * branch over if prediction is correct and the misprediction target for misPredBranchOver.
+ */
+static void genLandingPadForMispredictedCallee(MIR* mir) {
+    BasicBlock *fallThrough = traceCurrentBB->fallThrough;
+    /* Bypass the move-result block if there is one */
+    if (fallThrough->firstMIRInsn) {
+        assert(fallThrough->firstMIRInsn->OptimizationFlags & MIR_INLINED_PRED);
+        fallThrough = fallThrough->fallThrough;
+    }
+    /* Generate a branch over if the predicted inlining is correct */
+    jumpToBasicBlock(stream, fallThrough->id);
+    /* Hook up the target to the verification branch */
+    int relativeNCG = stream - streamMisPred;
+    unsigned instSize = encoder_get_inst_size(streamMisPred);
+    relativeNCG -= instSize; //size of the instruction
+    updateJumpInst(streamMisPred, OpndSize_8, relativeNCG);
+}
+#endif
+
+//! LOWER bytecode INVOKE_VIRTUAL without usage of helper function
+
+//!
+int common_invoke_virtual_nohelper(bool isRange, u2 tmp, int vD, const MIR *mir)
+{
+    const DecodedInstruction &decodedInst = mir->dalvikInsn;
+
+#ifdef WITH_JIT_INLINING_PHASE2
+    /*
+     * If the invoke has non-null misPredBranchOver, we need to generate
+     * the non-inlined version of the invoke here to handle the
+     * mispredicted case.
+     */
+    if (mir->meta.callsiteInfo->misPredBranchOver) {
+        genLandingPadForMispredictedCallee (mir);
+    }
+#endif
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    export_pc();
+    beforeCall("exception"); //dump GG, GL VRs
+
+    get_virtual_reg(vD, OpndSize_32, 5, false);
+
+    if ( (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        simpleNullCheck(5, false, vD);
+    }
+#ifndef PREDICTED_CHAINING
+    move_mem_to_reg(OpndSize_32, offObject_clazz, 5, false, 6, false); //clazz of "this"
+    move_mem_to_reg(OpndSize_32, offClassObject_vtable, 6, false, 7, false); //vtable
+    /* method is already resolved in trace-based JIT */
+    int methodIndex =
+                currentMethod->clazz->pDvmDex->pResMethods[tmp]->methodIndex;
+    move_mem_to_reg(OpndSize_32, methodIndex*4, 7, false, PhysicalReg_ECX, true);
+    if(isRange) {
+        common_invokeMethodRange(ArgsDone_Full);
+    }
+    else {
+        common_invokeMethodNoRange(ArgsDone_Full);
+    }
+#else
+    int methodIndex =
+                currentMethod->clazz->pDvmDex->pResMethods[tmp]->methodIndex;
+    gen_predicted_chain(isRange, tmp, methodIndex * 4, false, 5/*tmp5*/,
+            decodedInst);
+#endif
+    ///////////////////////////////////
+    return 0;
+}
+
+#if 0 /* Code is deprecated. If reenabling, must add parameter for decoded instruction */
+//! wrapper to call either common_invoke_virtual_helper or common_invoke_virtual_nohelper
+
+//!
+int common_invoke_virtual(bool isRange, u2 tmp, int vD) {
+    return common_invoke_virtual_nohelper(isRange, tmp, vD);
+}
+#endif
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef P_SCRATCH_1
+#undef PP_GPR_1
+#undef PP_GPR_2
+#undef PP_GPR_3
+#undef PP_GPR_4
+
+#define P_GPR_1 PhysicalReg_ESI
+#define P_GPR_2 PhysicalReg_EBX
+#define P_GPR_3 PhysicalReg_EDX
+#define PP_GPR_1 PhysicalReg_EBX
+#define PP_GPR_2 PhysicalReg_ESI
+#define PP_GPR_3 PhysicalReg_EAX
+#define PP_GPR_4 PhysicalReg_EDX
+//! common section to lower INVOKE_SUPER
+
+//! It will use helper function if the switch is on
+int common_invoke_super(bool isRange, u2 tmp,
+        const DecodedInstruction &decodedInst) {
+    export_pc();
+    beforeCall("exception"); //dump GG, GL VRs
+        ///////////////////////
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+#if !defined(WITH_JIT)
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        get_res_methods(3, false);
+        //LR[4] = vB*4(LR[3])
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
+        //cmp $0, LR[4]
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+
+        conditional_jump(Condition_NE, ".LinvokeSuper_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call(".virtual_method_resolve"); //in %eax
+        transferToState(1);
+        if (insertLabel(".LinvokeSuper_resolved", true) == -1)
+            return -1;
+        scratchRegs[0] = PhysicalReg_SCRATCH_3; scratchRegs[1] = PhysicalReg_SCRATCH_4;
+        get_glue_method_class(6, false);
+        move_mem_to_reg(OpndSize_32, offClassObject_super, 6, false, 7, false);
+        movez_mem_to_reg(OpndSize_16, offMethod_methodIndex, PhysicalReg_EAX, true, 8, false);
+        compare_mem_reg(OpndSize_32, offClassObject_vtableCount, 7, false, 8, false);
+
+        conditional_jump_global_API(Condition_NC, ".invoke_super_nsm", false);
+        move_mem_to_reg(OpndSize_32, offClassObject_vtable, 7, false, 9, false);
+        move_mem_scale_to_reg(OpndSize_32, 9, false, 8, false, 4, PhysicalReg_ECX, true);
+        const Method *calleeMethod = NULL;
+#else
+        /* method is already resolved in trace-based JIT */
+        int mIndex = currentMethod->clazz->pDvmDex->
+                pResMethods[tmp]->methodIndex;
+        const Method *calleeMethod =
+                currentMethod->clazz->super->vtable[mIndex];
+        move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
+#endif
+        if(isRange) {
+            common_invokeMethodRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+        else {
+            common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+ return 0;
+}
+#undef PP_GPR_1
+#undef PP_GPR_2
+#undef PP_GPR_3
+#undef PP_GPR_4
+
+//! \brief helper function to handle no such method error
+//! \return -1 if error, 0 otherwise
+int invoke_super_nsm(void) {
+    if (insertLabel(".invoke_super_nsm", false) == -1)
+        return -1;
+    //NOTE: it seems that the name in %edx is not used in common_errNoSuchMethod
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Method, name), PhysicalReg_EAX, true, PhysicalReg_EDX, true); //method name
+    unconditional_jump("common_errNoSuchMethod", false);
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ESI
+#define P_GPR_3 PhysicalReg_ECX
+#define PP_GPR_1 PhysicalReg_EBX
+#define PP_GPR_2 PhysicalReg_ESI
+#define PP_GPR_3 PhysicalReg_EAX
+#define PP_GPR_4 PhysicalReg_EDX
+//! common section to lower INVOKE_DIRECT
+
+//! It will use helper function if the switch is on
+int common_invoke_direct(bool isRange, u2 tmp, int vD, const MIR *mir)
+{
+    const DecodedInstruction &decodedInst = mir->dalvikInsn;
+    //%ecx can be used as scratch when calling export_pc, get_res_methods and resolve_method
+    export_pc();
+    beforeCall("exception"); //dump GG, GL VRs
+#if !defined(WITH_JIT)
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+        get_res_methods(3, false);
+        //LR[4] = vB*4(LR[3])
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_ECX, true);
+#endif
+        get_virtual_reg(vD, OpndSize_32, 5, false);
+        if ( (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+        {
+            simpleNullCheck(5, false, vD);
+        }
+#if !defined(WITH_JIT)
+        //cmp $0, LR[4]
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_ECX, true);
+        conditional_jump(Condition_NE, ".LinvokeDirect_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call_helper_API(".direct_method_resolve"); //in %eax
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
+        transferToState(1);
+        if (insertLabel(".LinvokeDirect_resolved", true) == -1)
+            return -1;
+        const Method *calleeMethod = NULL;
+#else
+        /* method is already resolved in trace-based JIT */
+        const Method *calleeMethod =
+                currentMethod->clazz->pDvmDex->pResMethods[tmp];
+        move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
+#endif
+        //%ecx passed to common_invokeMethod...
+        if(isRange) {
+            common_invokeMethodRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+        else {
+            common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef PP_GPR_1
+#undef PP_GPR_2
+#undef PP_GPR_3
+#undef PP_GPR_4
+
+#define P_GPR_1  PhysicalReg_EBX
+#define P_GPR_3  PhysicalReg_ECX
+#define PP_GPR_1 PhysicalReg_EBX
+#define PP_GPR_2 PhysicalReg_ESI
+#define PP_GPR_3 PhysicalReg_EAX
+#define PP_GPR_4 PhysicalReg_EDX
+//! common section to lower INVOKE_STATIC
+
+//! It will use helper function if the switch is on
+int common_invoke_static(bool isRange, u2 tmp,
+        const DecodedInstruction &decodedInst) {
+    //%ecx can be used as scratch when calling export_pc, get_res_methods and resolve_method
+    export_pc();
+    beforeCall("exception"); //dump GG, GL VRs
+#if !defined(WITH_JIT)
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null;      scratchRegs[3] = PhysicalReg_Null;
+        get_res_methods(3, false);
+        //LR[4] = vB*4(LR[3])
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_ECX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_ECX, true);
+
+        conditional_jump(Condition_NE, ".LinvokeStatic_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call(".static_method_resolve"); //in %eax
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
+        transferToState(1);
+        if (insertLabel(".LinvokeStatic_resolved", true) == -1)
+            return -1;
+        const Method *calleeMethod = NULL;
+#else
+        /* method is already resolved in trace-based JIT */
+        const Method *calleeMethod =
+                currentMethod->clazz->pDvmDex->pResMethods[tmp];
+        move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
+#endif
+        //%ecx passed to common_invokeMethod...
+        if(isRange) {
+            common_invokeMethodRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+        else {
+            common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
+                    decodedInst);
+        }
+    return 0;
+}
+#undef P_GPR_1
+#undef PP_GPR_1
+#undef PP_GPR_2
+#undef PP_GPR_3
+#undef PP_GPR_4
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_EAX //scratch
+#define P_GPR_3 PhysicalReg_ECX
+#define P_SCRATCH_1 PhysicalReg_ESI //clazz of object
+#define PP_GPR_1 PhysicalReg_EBX
+#define PP_GPR_2 PhysicalReg_ESI
+#define PP_GPR_3 PhysicalReg_EAX
+#define PP_GPR_4 PhysicalReg_EDX
+//! common section to lower INVOKE_INTERFACE
+
+//! It will use helper function if the switch is on
+int common_invoke_interface(bool isRange, u2 tmp, int vD, const MIR *mir) {
+
+    const DecodedInstruction &decodedInst = mir->dalvikInsn;
+
+#ifdef WITH_JIT_INLINING_PHASE2
+    /*
+     * If the invoke has non-null misPredBranchOver, we need to generate
+     * the non-inlined version of the invoke here to handle the
+     * mispredicted case.
+     */
+    if (mir->meta.callsiteInfo->misPredBranchOver) {
+        genLandingPadForMispredictedCallee (mir);
+    }
+#endif
+    export_pc(); //use %edx
+    beforeCall("exception"); //dump GG, GL VRs
+    ///////////////////////
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    get_virtual_reg(vD, OpndSize_32, 1, false);
+
+    if ( (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        simpleNullCheck(1, false, vD);
+    }
+
+#ifndef PREDICTED_CHAINING
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, tmp, 4, PhysicalReg_ESP, true);
+    /* for trace-based JIT, pDvmDex is a constant at JIT time
+       4th argument to dvmFindInterfaceMethodInCache at -4(%esp) */
+    move_imm_to_mem(OpndSize_32, (int) currentMethod->clazz->pDvmDex, 12, PhysicalReg_ESP, true);
+    move_mem_to_reg(OpndSize_32, offObject_clazz, 1, false, 5, false);
+    /* for trace-based JIT, method is a constant at JIT time
+       3rd argument to dvmFindInterfaceMethodInCache at 8(%esp) */
+    move_imm_to_mem(OpndSize_32, (int) currentMethod, 8, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_3; scratchRegs[1] = PhysicalReg_Null;
+    call_dvmFindInterfaceMethodInCache();
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+
+    conditional_jump_global_API(Condition_E, "common_exceptionThrown", false);
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
+    if(isRange) {
+        common_invokeMethodRange(ArgsDone_Full);
+    }
+    else {
+        common_invokeMethodNoRange(ArgsDone_Full);
+    }
+#else
+        gen_predicted_chain(isRange, tmp, -1, true /*interface*/, 1/*tmp1*/,
+                decodedInst);
+#endif
+    ///////////////////////
+    return 0;
+}
+#undef PP_GPR_1
+#undef PP_GPR_2
+#undef PP_GPR_3
+#undef PP_GPR_4
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef P_SCRATCH_1
+//! lower bytecode INVOKE_VIRTUAL by calling common_invoke_virtual
+
+//!
+int op_invoke_virtual(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vC */
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_virtual_nohelper(false/*not range*/, tmp, vD, mir);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_SUPER by calling common_invoke_super
+
+//!
+int op_invoke_super(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_super(false/*not range*/, tmp, mir->dalvikInsn);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_DIRECT by calling common_invoke_direct
+
+//!
+int op_invoke_direct(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_DIRECT);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vC */
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_direct(false/*not range*/, tmp, vD, mir);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_STATIC by calling common_invoke_static
+
+//!
+int op_invoke_static(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_STATIC);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_static(false/*not range*/, tmp, mir->dalvikInsn);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_INTERFACE by calling common_invoke_interface
+
+//!
+int op_invoke_interface(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_INTERFACE);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    // A|G|op BBBB F|E|D|C
+    // C: the first argument, which is the "this" pointer
+    // A: argument count
+    // C, D, E, F, G: arguments
+    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vC */
+    u2 tmp = mir->dalvikInsn.vB;
+    int retval = common_invoke_interface(false/*not range*/, tmp, vD, mir);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_VIRTUAL_RANGE by calling common_invoke_virtual
+
+//!
+int op_invoke_virtual_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL_RANGE);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    //AA|op BBBB CCCC
+    //CCCC: the first argument, which is the "this" pointer
+    //AA: argument count
+    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vCCCC */
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_virtual_nohelper(true/*range*/, tmp, vD, mir);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_SUPER_RANGE by calling common_invoke_super
+
+//!
+int op_invoke_super_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER_RANGE);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_super(true/*range*/, tmp, mir->dalvikInsn);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_DIRECT_RANGE by calling common_invoke_direct
+
+//!
+int op_invoke_direct_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_DIRECT_RANGE);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vCCCC */
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_direct(true/*range*/, tmp, vD, mir);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_STATIC_RANGE by calling common_invoke_static
+
+//!
+int op_invoke_static_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_STATIC_RANGE);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_static(true/*range*/, tmp, mir->dalvikInsn);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_INTERFACE_RANGE by calling common_invoke_interface
+
+//!
+int op_invoke_interface_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_INTERFACE_RANGE);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    int vD = mir->dalvikInsn.vC; /* Note: variable is still named vD because
+                                               of historical reasons. In reality, first
+                                               argument is in vCCCC */
+    u2 tmp = mir->dalvikInsn.vB; //BBBB, method index
+    int retval = common_invoke_interface(true/*range*/, tmp, vD, mir);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart1); //check when helper switch is on
+#endif
+    return retval;
+}
+
+//used %ecx, %edi, %esp %ebp
+#define P_GPR_1 PhysicalReg_EBX
+#define P_SCRATCH_1 PhysicalReg_ESI
+#define P_SCRATCH_2 PhysicalReg_EAX
+#define P_SCRATCH_3 PhysicalReg_EDX
+#define P_SCRATCH_4 PhysicalReg_ESI
+#define P_SCRATCH_5 PhysicalReg_EAX
+
+/**
+ * @brief Pass the arguments for invoking method without range
+ * @details Use both XMM and gp registers for INVOKE_(VIRTUAL, DIRECT, STATIC, INTERFACE, SUPER)
+ * @param decodedInst the decoded Insturction
+ * @return 0 always
+ */
+int common_invokeMethodNoRange_noJmp(const DecodedInstruction &decodedInst) {
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+    u2 count = decodedInst.vA;
+    int sizeofStackSaveArea = sizeof(StackSaveArea);
+    int offsetFromSaveArea = -4 * count;
+    int numQuad = 0; // keeping track of xmm moves
+    int numMov = 0; // keeping track of gp moves
+    for (int vrNum = 0; vrNum < count; vrNum++) {
+        if (vrNum != 0 && (vrNum + 1 < count) && decodedInst.arg[vrNum] + 1 == decodedInst.arg[vrNum + 1]) {
+            // move 64 bit values from VR to memory if consecutive VRs are to be copied to memory
+            get_virtual_reg(decodedInst.arg[vrNum], OpndSize_64, 22, false);
+            move_reg_to_mem(OpndSize_64, 22, false, offsetFromSaveArea - sizeofStackSaveArea,
+                            PhysicalReg_FP, true);
+            vrNum++;
+            numQuad++;               // keep track of number of 64 bit moves
+            offsetFromSaveArea += 8; // double the offset for 64 bits
+        } else {
+            // move 32 bit values from VR to memory.
+            // We need to use separate temp reg for each case.
+            if (numMov == 4){
+                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 27, false);
+                move_reg_to_mem(OpndSize_32, 27, false, offsetFromSaveArea - sizeofStackSaveArea,
+                                PhysicalReg_FP, true);
+                offsetFromSaveArea += 4;
+                numMov++;
+            }
+            if (numMov == 3){
+                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 26, false);
+                move_reg_to_mem(OpndSize_32, 26, false, offsetFromSaveArea - sizeofStackSaveArea,
+                                PhysicalReg_FP, true);
+                offsetFromSaveArea += 4;
+                numMov++;
+            }
+            if (numMov == 2){
+                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 25, false);
+                move_reg_to_mem(OpndSize_32, 25, false, offsetFromSaveArea - sizeofStackSaveArea,
+                                PhysicalReg_FP, true);
+                offsetFromSaveArea += 4;
+                numMov++;
+            }
+            if (numMov == 1){
+                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 24, false);
+                move_reg_to_mem(OpndSize_32, 24, false, offsetFromSaveArea - sizeofStackSaveArea,
+                                PhysicalReg_FP, true);
+                offsetFromSaveArea += 4;
+                numMov++;
+            }
+            if (numMov == 0){
+                get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 23, false);
+                move_reg_to_mem(OpndSize_32, 23, false, offsetFromSaveArea - sizeofStackSaveArea,
+                                PhysicalReg_FP, true);
+                offsetFromSaveArea += 4;
+                numMov++;
+            }
+        }
+    }
+    while(numQuad > 0 && numMov < count){ // refcount update for gp registers not used due to xmm moves
+        updateRefCount2(23+numMov, LowOpndRegType_gp, false);
+        updateRefCount2(23+numMov, LowOpndRegType_gp, false);
+        numMov++;
+    }
+    while(numQuad < 2) { //Max number of arguments is 5. Reading max of 5 VRs needed i.e. upto 2 64 bit moves.
+        updateRefCount2(22, LowOpndRegType_xmm, false);
+        updateRefCount2(22, LowOpndRegType_xmm, false);
+        numQuad++;
+    }
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_invokeMethodNoRange_noJmp");
+    }
+#endif
+    return 0;
+}
+
+int common_invokeMethod_Jmp(ArgsDoneType form) {
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+    nextVersionOfHardReg(PhysicalReg_EDX, 1);
+    move_imm_to_reg(OpndSize_32, (int)rPC, PhysicalReg_EDX, true);
+    //arguments needed in ArgsDone:
+    //    start of HotChainingCell for next bytecode: -4(%esp)
+    //    start of InvokeSingletonChainingCell for callee: -8(%esp)
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    if(!gDvmJit.scheduling) {
+        alignOffset(4); // 4 is (the instruction size of "mov imm32, 4(esp)" - sizeof(imm32))
+        insertChainingWorklist(traceCurrentBB->fallThrough->id, stream);
+    }
+    move_chain_to_mem(OpndSize_32, traceCurrentBB->fallThrough->id, 4, PhysicalReg_ESP, true);
+    // for honeycomb: JNI call doesn't need a chaining cell, so the taken branch is null
+    if(!gDvmJit.scheduling && traceCurrentBB->taken) {
+        alignOffset(3); // 3 is (the instruction size of "mov imm32, 0(esp)" - sizeof(imm32))
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    }
+    int takenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
+    move_chain_to_mem(OpndSize_32, takenId, 0, PhysicalReg_ESP, true);
+    if(form == ArgsDone_Full)
+        unconditional_jump_global_API(".invokeArgsDone_jit", false);
+    else if(form == ArgsDone_Native)
+        unconditional_jump_global_API(".invokeArgsDone_native", false);
+    else
+        unconditional_jump_global_API(".invokeArgsDone_normal", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_invokeMethod_Jmp");
+    }
+#endif
+    return 0;
+}
+
+int common_invokeMethodNoRange(ArgsDoneType form, const DecodedInstruction &decodedInst) {
+    common_invokeMethodNoRange_noJmp(decodedInst);
+    common_invokeMethod_Jmp(form);
+    return 0;
+}
+
+#undef P_GPR_1
+#undef P_SCRATCH_1
+#undef P_SCRATCH_2
+#undef P_SCRATCH_3
+#undef P_SCRATCH_4
+#undef P_SCRATCH_5
+
+//input: %ecx (method to call)
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ESI
+#define P_GPR_3 PhysicalReg_EDX //not used with P_SCRATCH_2
+#define P_SCRATCH_1 PhysicalReg_EAX
+#define P_SCRATCH_2 PhysicalReg_EDX
+#define P_SCRATCH_3 PhysicalReg_EAX
+#define P_SCRATCH_4 PhysicalReg_EDX
+#define P_SCRATCH_5 PhysicalReg_EAX
+#define P_SCRATCH_6 PhysicalReg_EDX
+#define P_SCRATCH_7 PhysicalReg_EAX
+#define P_SCRATCH_8 PhysicalReg_EDX
+#define P_SCRATCH_9 PhysicalReg_EAX
+#define P_SCRATCH_10 PhysicalReg_EDX
+//! pass the arguments for invoking method with range
+
+//! loop is unrolled when count <= 10
+int common_invokeMethodRange_noJmp(const DecodedInstruction &decodedInst) {
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+    u2 count = decodedInst.vA;
+    int vD = decodedInst.vC; //the first argument
+
+    savearea_from_fp(21, false);
+    //vD to rFP-4*count-20
+    //vD+1 to rFP-4*count-20+4 = rFP-20-4*(count-1)
+    if(count >= 1 && count <= 10) {
+        get_virtual_reg(vD, OpndSize_32, 22, false);
+        move_reg_to_mem(OpndSize_32, 22, false, -4*count, 21, false);
+    }
+    if(count >= 2 && count <= 10) {
+        get_virtual_reg(vD+1, OpndSize_32, 23, false);
+        move_reg_to_mem(OpndSize_32, 23, false, -4*(count-1), 21, false);
+    }
+    if(count >= 3 && count <= 10) {
+        get_virtual_reg(vD+2, OpndSize_32, 24, false);
+        move_reg_to_mem(OpndSize_32, 24, false, -4*(count-2), 21, false);
+    }
+    if(count >= 4 && count <= 10) {
+        get_virtual_reg(vD+3, OpndSize_32, 25, false);
+        move_reg_to_mem(OpndSize_32, 25, false, -4*(count-3), 21, false);
+    }
+    if(count >= 5 && count <= 10) {
+        get_virtual_reg(vD+4, OpndSize_32, 26, false);
+        move_reg_to_mem(OpndSize_32, 26, false, -4*(count-4), 21, false);
+    }
+    if(count >= 6 && count <= 10) {
+        get_virtual_reg(vD+5, OpndSize_32, 27, false);
+        move_reg_to_mem(OpndSize_32, 27, false, -4*(count-5), 21, false);
+    }
+    if(count >= 7 && count <= 10) {
+        get_virtual_reg(vD+6, OpndSize_32, 28, false);
+        move_reg_to_mem(OpndSize_32, 28, false, -4*(count-6), 21, false);
+    }
+    if(count >= 8 && count <= 10) {
+        get_virtual_reg(vD+7, OpndSize_32, 29, false);
+        move_reg_to_mem(OpndSize_32, 29, false, -4*(count-7), 21, false);
+    }
+    if(count >= 9 && count <= 10) {
+        get_virtual_reg(vD+8, OpndSize_32, 30, false);
+        move_reg_to_mem(OpndSize_32, 30, false, -4*(count-8), 21, false);
+    }
+    if(count == 10) {
+        get_virtual_reg(vD+9, OpndSize_32, 31, false);
+        move_reg_to_mem(OpndSize_32, 31, false, -4*(count-9), 21, false);
+    }
+    if(count > 10) {
+        //dump to memory first, should we set physicalReg to Null?
+        //this bytecodes uses a set of virtual registers (update getVirtualInfo)
+        //this is necessary to correctly insert transfer points
+        int k;
+        for(k = 0; k < count; k++) {
+            spillVirtualReg(vD+k, LowOpndRegType_gp, true); //will update refCount
+        }
+        load_effective_addr(4*vD, PhysicalReg_FP, true, 12, false);
+        alu_binary_imm_reg(OpndSize_32, sub_opc, 4*count, 21, false);
+        move_imm_to_reg(OpndSize_32, count, 13, false);
+        if (insertLabel(".invokeMethod_1", true) == -1) //if checkDup: will perform work from ShortWorklist
+            return -1;
+        rememberState(1);
+        move_mem_to_reg(OpndSize_32, 0, 12, false, 14, false);
+        move_reg_to_mem(OpndSize_32, 14, false, 0, 21, false);
+        load_effective_addr(4, 12, false, 12, false);
+        alu_binary_imm_reg(OpndSize_32, sub_opc, 1, 13, false);
+        load_effective_addr(4, 21, false, 21, false);
+        transferToState(1);
+        conditional_jump(Condition_NE, ".invokeMethod_1", true); //backward branch
+    }
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_invokeMethodRange_noJmp");
+    }
+#endif
+    return 0;
+}
+
+int common_invokeMethodRange(ArgsDoneType form, const DecodedInstruction &decodedInst) {
+    common_invokeMethodRange_noJmp(decodedInst);
+    common_invokeMethod_Jmp(form);
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef P_SCRATCH_1
+#undef P_SCRATCH_2
+#undef P_SCRATCH_3
+#undef P_SCRATCH_4
+#undef P_SCRATCH_5
+#undef P_SCRATCH_6
+#undef P_SCRATCH_7
+#undef P_SCRATCH_8
+#undef P_SCRATCH_9
+#undef P_SCRATCH_10
+
+//! spill a register to native stack
+
+//! decrease %esp by 4, then store a register at 0(%esp)
+int spill_reg(int reg, bool isPhysical) {
+    load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, reg, isPhysical, 0, PhysicalReg_ESP, true);
+    return 0;
+}
+//! get a register from native stack
+
+//! load a register from 0(%esp), then increase %esp by 4
+int unspill_reg(int reg, bool isPhysical) {
+    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, reg, isPhysical);
+    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    return 0;
+}
+
+int generate_invokeNative(void); //forward declaration
+int generate_stackOverflow(void); //forward declaration
+
+const char *dvmCompilerHandleInvokeArgsHeader (int value)
+{
+    ArgsDoneType form = static_cast<ArgsDoneType> (value);
+
+    // Insert different labels for the various forms
+    const char * sectionLabel = 0;
+
+    //Look at the form
+    switch (form)
+    {
+        case ArgsDone_Full:
+            sectionLabel = ".invokeArgsDone_jit";
+            break;
+        case ArgsDone_Normal:
+            sectionLabel = ".invokeArgsDone_normal";
+            break;
+        default:
+            sectionLabel = ".invokeArgsDone_native";
+            break;
+    }
+
+    return sectionLabel;
+}
+
+/**
+ * @brief Common code to invoke a method after all of the arguments
+ * are handled.
+ * @details Requires that ECX holds the method to be called.
+ * @param form Used to decide which variant to generate which may contain
+ * fewer instructions than a full implementation. For invokeNativeSingle
+ * use ArgsDone_Native. For invokeNonNativeSingle use ArgsDone_Normal.
+ * To dynamically determine which one to choose (the full implementation)
+ * use ArgsDone_Full.
+ * @return value >= 0 on success
+ * @todo Since this section is static code that is dynamically generated,
+ * it can be written directly in assembly and built at compile time.
+ */
+int common_invokeArgsDone(ArgsDoneType form) {
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+    int sizeofStackSaveArea = sizeof(StackSaveArea);
+
+    // Define scratch registers
+    scratchRegs[0] = PhysicalReg_EBX;
+    scratchRegs[1] = PhysicalReg_ESI;
+    scratchRegs[2] = PhysicalReg_EDX;
+    scratchRegs[3] = PhysicalReg_Null;
+
+    //Get the callback
+    const char* (*backEndInvokeArgsDone) (int) = gDvmJit.jitFramework.backEndInvokeArgsDone;
+
+    //We actually need this call back for this backend
+    assert (backEndInvokeArgsDone != 0);
+
+    const char *sectionLabel = 0;
+
+    //If we don't have a backend handler, bail
+    if (backEndInvokeArgsDone == 0)
+    {
+        SET_JIT_ERROR (kJitErrorPlugin);
+        return -1;
+    }
+
+    //Get the section label
+    sectionLabel = backEndInvokeArgsDone (form);
+
+    //If we don't have a section label, bail
+    if (sectionLabel == 0)
+    {
+        SET_JIT_ERROR (kJitErrorTraceFormation);
+        return -1;
+    }
+
+    //If we can't insert a label, bail
+    if (insertLabel(sectionLabel, false) == -1)
+    {
+        return -1;
+    }
+
+    // Determine how many ins+locals we have
+    movez_mem_to_reg(OpndSize_16, OFFSETOF_MEMBER(Method,registersSize),
+            PhysicalReg_ECX, true, PhysicalReg_EAX, true);
+
+    // Determine the offset by multiplying size of 4 with how many ins+locals we have
+    alu_binary_imm_reg(OpndSize_32, shl_opc, 2, PhysicalReg_EAX, true);
+
+    // Load save area
+    savearea_from_fp(PhysicalReg_ESI, true);
+
+    // Computer the new FP (old save area - regsSize)
+    alu_binary_reg_reg(OpndSize_32, sub_opc, PhysicalReg_EAX, true,
+            PhysicalReg_ESI, true);
+
+    // Get pointer to self Thread
+    get_self_pointer(PhysicalReg_EAX, true);
+
+    // Make a copy of the new FP
+    move_reg_to_reg(OpndSize_32, PhysicalReg_ESI, true, PhysicalReg_EBX, true);
+
+    // Set newSaveArea->savedPc
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true,
+            OFFSETOF_MEMBER(StackSaveArea,savedPc) - sizeofStackSaveArea,
+            PhysicalReg_ESI, true);
+
+    // Load the size of stack save area into register
+    alu_binary_imm_reg(OpndSize_32, sub_opc, sizeofStackSaveArea,
+            PhysicalReg_ESI, true);
+
+    // Determine how many outs we have
+    movez_mem_to_reg(OpndSize_16, OFFSETOF_MEMBER(Method,outsSize),
+            PhysicalReg_ECX, true, PhysicalReg_EDX, true);
+
+    // Determine the offset by multiplying size of 4 with how many outs we have
+    alu_binary_imm_reg(OpndSize_32, shl_opc, 2, PhysicalReg_EDX, true);
+
+    // Calculate the bottom, namely newSaveArea - outsSize
+    alu_binary_reg_reg(OpndSize_32, sub_opc, PhysicalReg_EDX, true,
+            PhysicalReg_ESI, true);
+
+    // Set newSaveArea->prevFrame
+    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true,
+            OFFSETOF_MEMBER(StackSaveArea,prevFrame) - sizeofStackSaveArea,
+            PhysicalReg_EBX, true);
+
+    // Compare self->interpStackEnd and bottom
+    compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(Thread, interpStackEnd),
+            PhysicalReg_EAX, true, PhysicalReg_ESI, true);
+
+    // Handle frame overflow
+    conditional_jump(Condition_B, ".stackOverflow", true);
+
+    if (form == ArgsDone_Full) {
+        // Check for a native call
+        test_imm_mem(OpndSize_32, ACC_NATIVE,
+                OFFSETOF_MEMBER(Method,accessFlags), PhysicalReg_ECX, true);
+    }
+
+    // Set newSaveArea->method
+    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true,
+            OFFSETOF_MEMBER(StackSaveArea,method) - sizeofStackSaveArea,
+            PhysicalReg_EBX, true);
+
+    if (form == ArgsDone_Native || form == ArgsDone_Full) {
+        // to correctly handle code cache reset:
+        //  update returnAddr and check returnAddr after done with the native method
+        //  if returnAddr is set to NULL during code cache reset,
+        //  the execution will correctly continue with interpreter
+        // get returnAddr from 4(%esp) and update the save area with it
+        move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true, PhysicalReg_EDX,
+                true);
+        move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true,
+                OFFSETOF_MEMBER(StackSaveArea,returnAddr) - sizeofStackSaveArea,
+                PhysicalReg_EBX, true);
+    }
+
+    if (form == ArgsDone_Native) {
+        // Since we know we are invoking native method, generate code for the
+        // native invoke and the invoke implementation is done.
+        if (generate_invokeNative() == -1)
+            return -1;
+
+#if defined VTUNE_DALVIK
+        if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+            int endStreamPtr = (int)stream;
+            sendLabelInfoToVTune(startStreamPtr, endStreamPtr, sectionLabel);
+        }
+#endif
+        return 0;
+    }
+
+    if (form == ArgsDone_Full) {
+        // Since we are generating the full implementation, we just did the
+        // check for native method and can now go do the native invoke
+        conditional_jump(Condition_NE, ".invokeNative", true);
+    }
+
+    // Get method->clazz
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Method,clazz), PhysicalReg_ECX,
+            true, PhysicalReg_EDX, true);
+
+    // Update frame pointer with the new FP
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EBX, true, PhysicalReg_FP, true);
+
+    // Get pointer to self Thread
+    get_self_pointer(PhysicalReg_EBX, true);
+
+    // Get method->clazz->pDvmDex
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ClassObject,pDvmDex),
+            PhysicalReg_EDX, true, PhysicalReg_EDX, true);
+
+    // Set self->methodClassDex with method->clazz->pDvmDex
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true,
+            OFFSETOF_MEMBER(Thread, interpSave.methodClassDex), PhysicalReg_EBX,
+            true);
+
+    // Set self->curFrame to the new FP
+    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true,
+            OFFSETOF_MEMBER(Thread,interpSave.curFrame), PhysicalReg_EBX, true);
+
+    // returnAddr updated already for Full. Get returnAddr from 4(%esp)
+    if (form == ArgsDone_Normal) {
+        move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true, PhysicalReg_EDX,
+                true);
+    }
+
+    // Set self->method with method to call
+    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true,
+            OFFSETOF_MEMBER(Thread, interpSave.method), PhysicalReg_EBX, true);
+
+    // Place starting bytecode in EBX for dvmJitToInterp
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Method,insns), PhysicalReg_ECX,
+            true, PhysicalReg_EBX, true);
+
+    if (form == ArgsDone_Normal) {
+        // We have obtained the return address and now we can actually update it
+        move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true,
+                OFFSETOF_MEMBER(StackSaveArea,returnAddr) - sizeofStackSaveArea,
+                PhysicalReg_FP, true);
+    }
+
+    if (insertLabel(".invokeInterp", true) == -1)
+        return -1;
+
+    bool callNoChain = false;
+#ifdef PREDICTED_CHAINING
+    if (form == ArgsDone_Full)
+        callNoChain = true;
+#endif
+
+    if (callNoChain) {
+        scratchRegs[0] = PhysicalReg_EAX;
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+#if defined(WITH_JIT_TUNING)
+        // Predicted chaining failed. Fall back to interpreter and indicated
+        // inline cache miss.
+        move_imm_to_reg(OpndSize_32, kInlineCacheMiss, PhysicalReg_EDX, true);
+#endif
+        call_dvmJitToInterpTraceSelectNoChain(); //input: rPC in %ebx
+    } else {
+        // Jump to the stub at (%esp)
+        move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EDX,
+                true);
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        unconditional_jump_reg(PhysicalReg_EDX, true);
+    }
+
+    if (form == ArgsDone_Full) {
+        // Generate code for handling native invoke
+        if (generate_invokeNative() == -1)
+            return -1;
+    }
+
+    // Generate code for handling stack overflow
+    if (generate_stackOverflow() == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, sectionLabel);
+    }
+#endif
+    return 0;
+}
+
+/* when WITH_JIT is true,
+     JIT'ed code invokes native method, after invoke, execution will continue
+     with the interpreter or with JIT'ed code if chained
+*/
+int generate_invokeNative() {
+    int sizeofStackSaveArea = sizeof(StackSaveArea);
+
+    if (insertLabel(".invokeNative", true) == -1)
+        return -1;
+
+    //if(!generateForNcg)
+    //    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    load_effective_addr(-28, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EBX, true, 0, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EBX, true, 20, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_EDX;
+    get_self_pointer(PhysicalReg_EAX, true); //glue->self
+    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 8, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 12, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 24, PhysicalReg_ESP, true);
+    move_mem_to_reg(OpndSize_32, offThread_jniLocal_nextEntry, PhysicalReg_EAX, true, PhysicalReg_EDX, true); //get self->local_next
+    scratchRegs[1] = PhysicalReg_EAX;
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, OFFSETOF_MEMBER(StackSaveArea, xtra.currentPc) - sizeofStackSaveArea, PhysicalReg_EBX, true); //update jniLocalRef of stack
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EBX, true, OFFSETOF_MEMBER(Thread, interpSave.curFrame), PhysicalReg_EAX, true); //set self->curFrame
+    move_imm_to_mem(OpndSize_32, 0, OFFSETOF_MEMBER(Thread, inJitCodeCache), PhysicalReg_EAX, true); //clear self->inJitCodeCache
+    load_effective_addr(OFFSETOF_MEMBER(Thread, interpSave.retval), PhysicalReg_EAX, true, PhysicalReg_EAX, true); //self->retval
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 4, PhysicalReg_ESP, true);
+    //NOTE: native method checks the interpreted stack for arguments
+    //      The immediate arguments on native stack: address of return value, new FP, self
+    call_mem(40, PhysicalReg_ECX, true);//*40(%ecx)
+    //we can't assume the argument stack is unmodified after the function call
+    //duplicate newFP & glue->self on stack: newFP (-28 & -8) glue->self (-16 & -4)
+    move_mem_to_reg(OpndSize_32, 20, PhysicalReg_ESP, true, PhysicalReg_ESI, true); //new FP
+    move_mem_to_reg(OpndSize_32, 24, PhysicalReg_ESP, true, PhysicalReg_EBX, true); //glue->self
+    load_effective_addr(28, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(StackSaveArea, xtra.currentPc) - sizeofStackSaveArea, PhysicalReg_ESI, true, PhysicalReg_EAX, true); //newSaveArea->jniLocal
+    compare_imm_mem(OpndSize_32, 0, OFFSETOF_MEMBER(Thread, exception), PhysicalReg_EBX, true); //self->exception
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    //NOTE: PhysicalReg_FP should be callee-saved register
+    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true, OFFSETOF_MEMBER(Thread, interpSave.curFrame), PhysicalReg_EBX, true); //set self->curFrame
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, offThread_jniLocal_nextEntry, PhysicalReg_EBX, true); //set self->jniLocal
+    conditional_jump(Condition_NE, "common_exceptionThrown", false);
+
+    //get returnAddr, if it is not NULL,
+    //    return to JIT'ed returnAddr after executing the native method
+    /* to correctly handle code cache reset:
+       update returnAddr and check returnAddr after done with the native method
+       if returnAddr is set to NULL during code cache reset,
+       the execution will correctly continue with interpreter */
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(StackSaveArea, returnAddr)-sizeofStackSaveArea, PhysicalReg_ESI, true, PhysicalReg_EDX, true);
+    //set self->inJitCodeCache to returnAddr (PhysicalReg_EBX is in %ebx)
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, OFFSETOF_MEMBER(Thread, inJitCodeCache), PhysicalReg_EBX, true);
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(StackSaveArea, savedPc) - sizeofStackSaveArea, PhysicalReg_ESI, true, PhysicalReg_EBX, true); //savedPc
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EDX, true);
+    conditional_jump(Condition_E, ".nativeToInterp", true);
+    unconditional_jump_reg(PhysicalReg_EDX, true);
+    //if returnAddr is NULL, return to interpreter after executing the native method
+    if (insertLabel(".nativeToInterp", true) == -1) {
+        return -1;
+    }
+    //move rPC by 6 (3 bytecode units for INVOKE)
+    alu_binary_imm_reg(OpndSize_32, add_opc, 6, PhysicalReg_EBX, true);
+    scratchRegs[0] = PhysicalReg_EAX;
+#if defined(WITH_JIT_TUNING)
+        /* Return address not in code cache. Indicate that continuing with interpreter
+         */
+        move_imm_to_reg(OpndSize_32, kCallsiteInterpreted, PhysicalReg_EDX, true);
+#endif
+    call_dvmJitToInterpTraceSelectNoChain(); //rPC in %ebx
+    return 0;
+}
+
+int generate_stackOverflow() {
+    if (insertLabel(".stackOverflow", true) == -1)
+        return -1;
+    //load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 4, PhysicalReg_ESP, true);
+    get_self_pointer(PhysicalReg_EBX, true); //glue->self
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EBX, true, 0, PhysicalReg_ESP, true);
+    call_dvmHandleStackOverflow();
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    unconditional_jump("common_exceptionThrown", false);
+    return 0;
+}
+
+/////////////////////////////////////////////
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+#define P_SCRATCH_1 PhysicalReg_ESI
+#define P_SCRATCH_2 PhysicalReg_EDX
+#define P_SCRATCH_3 PhysicalReg_ESI
+#define P_SCRATCH_4 PhysicalReg_EDX
+//! lower bytecode EXECUTE_INLINE
+
+//!
+int op_execute_inline(const MIR * mir, bool isRange) {
+    assert(mir->dalvikInsn.opcode == OP_EXECUTE_INLINE
+            || mir->dalvikInsn.opcode == OP_EXECUTE_INLINE_RANGE);
+    int num = mir->dalvikInsn.vA;
+    u2 tmp = mir->dalvikInsn.vB;
+    int vC, vD, vE, vF;
+    if(!isRange) {
+        // Note that vC, vD, vE, and vF might have bad values
+        // depending on count. The variable "num" should be
+        // checked before using any of these.
+        vC = mir->dalvikInsn.arg[0];
+        vD = mir->dalvikInsn.arg[1];
+        vE = mir->dalvikInsn.arg[2];
+        vF = mir->dalvikInsn.arg[3];
+    } else {
+        vC = mir->dalvikInsn.vC;
+        vD = vC + 1;
+        vE = vC + 2;
+        vF = vC + 3;
+    }
+    export_pc();
+    switch (tmp) {
+        case INLINE_EMPTYINLINEMETHOD:
+            return 0;  /* Nop */
+        case INLINE_STRING_LENGTH:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            compare_imm_reg(OpndSize_32, 0, 1, false);
+            conditional_jump(Condition_NE, ".do_inlined_string_length", true);
+            scratchRegs[0] = PhysicalReg_SCRATCH_1;
+            rememberState(1);
+            beforeCall("exception"); //dump GG, GL VRs
+            unconditional_jump("common_errNullObject", false);
+            goToState(1);
+            if (insertLabel(".do_inlined_string_length", true) == -1)
+                return -1;
+            move_mem_to_reg(OpndSize_32, 0x14, 1, false, 2, false);
+            get_self_pointer(3, false);
+            move_reg_to_mem(OpndSize_32, 2, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
+            return 0;
+        case INLINE_STRING_IS_EMPTY:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            compare_imm_reg(OpndSize_32, 0, 1, false);
+            conditional_jump(Condition_NE, ".do_inlined_string_length", true);
+            scratchRegs[0] = PhysicalReg_SCRATCH_1;
+            rememberState(1);
+            beforeCall("exception"); //dump GG, GL VRs
+            unconditional_jump("common_errNullObject", false);
+            goToState(1);
+            if (insertLabel(".do_inlined_string_length", true) == -1)
+                return -1;
+            compare_imm_mem(OpndSize_32, 0, 0x14, 1, false);
+            conditional_jump(Condition_E, ".inlined_string_length_return_true",
+                             true);
+            get_self_pointer(2, false);
+            move_imm_to_mem(OpndSize_32, 0, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
+            unconditional_jump(".inlined_string_length_done", true);
+            if (insertLabel(".inlined_string_length_return_true", true) == -1)
+                return -1;
+            get_self_pointer(2, false);
+            move_imm_to_mem(OpndSize_32, 1, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
+            if (insertLabel(".inlined_string_length_done", true) == -1)
+                return -1;
+            return 0;
+        case INLINE_MATH_ABS_INT:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            move_reg_to_reg(OpndSize_32, 1, false, 2, false);
+            alu_binary_imm_reg(OpndSize_32, sar_opc, 0x1f, 2, false);
+            alu_binary_reg_reg(OpndSize_32, xor_opc, 2, false, 1, false);
+            alu_binary_reg_reg(OpndSize_32, sub_opc, 2, false, 1, false);
+            get_self_pointer(3, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
+            return 0;
+        case INLINE_MATH_ABS_LONG:
+            get_virtual_reg(vD, OpndSize_32, 1, false);
+            move_reg_to_reg(OpndSize_32, 1, false, 2, false);
+            alu_binary_imm_reg(OpndSize_32, sar_opc, 0x1f, 1, false);
+            move_reg_to_reg(OpndSize_32, 1, false, 3, false);
+            move_reg_to_reg(OpndSize_32, 1, false, 4, false);
+            get_virtual_reg(vC, OpndSize_32, 5, false);
+            alu_binary_reg_reg(OpndSize_32, xor_opc, 5, false, 1, false);
+            get_self_pointer(6, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 6, false);
+            alu_binary_reg_reg(OpndSize_32, xor_opc, 2, false, 3, false);
+            move_reg_to_mem(OpndSize_32, 3, false, 4 + OFFSETOF_MEMBER(Thread, interpSave.retval), 6, false);
+            alu_binary_reg_mem(OpndSize_32, sub_opc, 4, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 6, false);
+            alu_binary_reg_mem(OpndSize_32, sbb_opc, 4, false, 4 + OFFSETOF_MEMBER(Thread, interpSave.retval), 6, false);
+            return 0;
+        case INLINE_MATH_MAX_INT:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            get_virtual_reg(vD, OpndSize_32, 2, false);
+            compare_reg_reg(1, false, 2, false);
+            conditional_move_reg_to_reg(OpndSize_32, Condition_GE, 2,
+                                        false/*src*/, 1, false/*dst*/);
+            get_self_pointer(3, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
+            return 0;
+        case INLINE_MATH_MIN_INT:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            get_virtual_reg(vD, OpndSize_32, 2, false);
+            compare_reg_reg(1, false, 2, false);
+            conditional_move_reg_to_reg(OpndSize_32, Condition_LE, 2,
+                                        false/*src*/, 1, false/*dst*/);
+            get_self_pointer(3, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
+            return 0;
+        case INLINE_MATH_ABS_FLOAT:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            alu_binary_imm_reg(OpndSize_32, and_opc, 0x7fffffff, 1, false);
+            get_self_pointer(2, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
+            return 0;
+        case INLINE_MATH_ABS_DOUBLE:
+            get_virtual_reg(vC, OpndSize_64, 1, false);
+            alu_binary_mem_reg(OpndSize_64, and_opc, LvaluePosInfLong, PhysicalReg_Null, true, 1, false);
+            get_self_pointer(2, false);
+            move_reg_to_mem(OpndSize_64, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
+            return 0;
+        case INLINE_STRING_CHARAT:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            compare_imm_reg(OpndSize_32, 0, 1, false);
+            conditional_jump(Condition_NE, ".inlined_string_CharAt_arg_validate_1", true);
+            rememberState(1);
+            beforeCall("exception");
+            unconditional_jump("common_errNullObject", false);
+            goToState(1);
+            if (insertLabel(".inlined_string_CharAt_arg_validate_1", true) == -1)
+                return -1;
+            get_virtual_reg(vD, OpndSize_32, 2, false);
+            compare_mem_reg(OpndSize_32, 0x14, 1, false, 2, false);
+            conditional_jump(Condition_L, ".inlined_string_CharAt_arg_validate_2", true);
+            rememberState(2);
+            beforeCall("exception");
+            unconditional_jump("common_errStringIndexOutOfBounds", false);
+            goToState(2);
+            if (insertLabel(".inlined_string_CharAt_arg_validate_2", true) == -1)
+                return -1;
+            compare_imm_reg(OpndSize_32, 0, 2, false);
+            conditional_jump(Condition_NS, ".do_inlined_string_CharAt", true);
+            rememberState(3);
+            beforeCall("exception");
+            unconditional_jump("common_errStringIndexOutOfBounds", false);
+            goToState(3);
+            if (insertLabel(".do_inlined_string_CharAt", true) == -1)
+                return -1;
+            alu_binary_mem_reg(OpndSize_32, add_opc, 0x10, 1, false, 2, false);
+            move_mem_to_reg(OpndSize_32, 0x8, 1, false, 1, false);
+            movez_mem_disp_scale_to_reg(OpndSize_16, 1, false, OFFSETOF_MEMBER(ArrayObject, contents)/*disp*/, 2, false, 2, 2, false);
+            get_self_pointer(3, false);
+            move_reg_to_mem(OpndSize_32, 2, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
+            return 0;
+        case INLINE_STRING_FASTINDEXOF_II:
+#if defined(USE_GLOBAL_STRING_DEFS)
+            break;
+#else
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            compare_imm_reg(OpndSize_32, 0, 1, false);
+            get_virtual_reg(vD, OpndSize_32, 2, false);
+            get_virtual_reg(vE, OpndSize_32, 3, false);
+            conditional_jump(Condition_NE, ".do_inlined_string_fastIndexof",
+                             true);
+            scratchRegs[0] = PhysicalReg_SCRATCH_1;
+            rememberState(1);
+            beforeCall("exception"); //dump GG, GL VRs
+            unconditional_jump("common_errNullObject", false);
+            goToState(1);
+            if (insertLabel(".do_inlined_string_fastIndexof", true) == -1)
+                return -1;
+            move_mem_to_reg(OpndSize_32, 0x14, 1, false, 4, false);
+            move_mem_to_reg(OpndSize_32, 0x8, 1, false, 5, false);
+            move_mem_to_reg(OpndSize_32, 0x10, 1, false, 6, false);
+            alu_binary_reg_reg(OpndSize_32, xor_opc, 1, false, 1, false);
+            compare_imm_reg(OpndSize_32, 0, 3, false);
+            conditional_move_reg_to_reg(OpndSize_32, Condition_NS, 3, false, 1,
+                                        false);
+            compare_reg_reg(4, false, 1, false);
+            conditional_jump(Condition_GE,
+                             ".do_inlined_string_fastIndexof_exitfalse", true);
+            dump_mem_scale_reg(Mnemonic_LEA, OpndSize_32, 5, false, OFFSETOF_MEMBER(ArrayObject, contents)/*disp*/,
+                               6, false, 2, 5, false, LowOpndRegType_gp);
+            movez_mem_disp_scale_to_reg(OpndSize_16, 5, false, 0, 1, false, 2,
+                                        3, false);
+            compare_reg_reg(3, false, 2, false);
+            conditional_jump(Condition_E, ".do_inlined_string_fastIndexof_exit",
+                             true);
+            load_effective_addr(0x1, 1, false, 3, false);
+            load_effective_addr_scale(5, false, 3, false, 2, 5, false);
+            unconditional_jump(".do_inlined_string_fastIndexof_iter", true);
+            if (insertLabel(".do_inlined_string_fastIndexof_ch_cmp", true) == -1)
+                return -1;
+            if(gDvm.executionMode == kExecutionModeNcgO1) {
+                rememberState(1);
+            }
+            movez_mem_to_reg(OpndSize_16, 0, 5, false, 6, false);
+            load_effective_addr(0x2, 5, false, 5, false);
+            compare_reg_reg(6, false, 2, false);
+            conditional_jump(Condition_E, ".do_inlined_string_fastIndexof_exit",
+                             true);
+            load_effective_addr(0x1, 3, false, 3, false);
+            if (insertLabel(".do_inlined_string_fastIndexof_iter", true) == -1)
+                return -1;
+            compare_reg_reg(4, false, 3, false);
+            move_reg_to_reg(OpndSize_32, 3, false, 1, false);
+            if(gDvm.executionMode == kExecutionModeNcgO1) {
+                transferToState(1);
+            }
+            conditional_jump(Condition_NE,
+                             ".do_inlined_string_fastIndexof_ch_cmp", true);
+            if (insertLabel(".do_inlined_string_fastIndexof_exitfalse", true) == -1)
+                return -1;
+            move_imm_to_reg(OpndSize_32, 0xffffffff, 1,  false);
+            if (insertLabel(".do_inlined_string_fastIndexof_exit", true) == -1)
+                return -1;
+            get_self_pointer(7, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 7, false);
+            return 0;
+        case INLINE_FLOAT_TO_RAW_INT_BITS:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            get_self_pointer(2, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
+            return 0;
+        case INLINE_INT_BITS_TO_FLOAT:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            get_self_pointer(2, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 2, false);
+            return 0;
+        case INLINE_DOUBLE_TO_RAW_LONG_BITS:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            get_self_pointer(3, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
+            get_virtual_reg(vD, OpndSize_32, 2, false);
+            move_reg_to_mem(OpndSize_32, 2, false, 4 + OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
+            return 0;
+        case INLINE_LONG_BITS_TO_DOUBLE:
+            get_virtual_reg(vC, OpndSize_32, 1, false);
+            get_virtual_reg(vD, OpndSize_32, 2, false);
+            get_self_pointer(3, false);
+            move_reg_to_mem(OpndSize_32, 2, false, 4 + OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
+            move_reg_to_mem(OpndSize_32, 1, false, OFFSETOF_MEMBER(Thread, interpSave.retval), 3, false);
+            return 0;
+        default:
+                break;
+    }
+#endif
+    get_self_pointer(PhysicalReg_SCRATCH_1, false);
+    load_effective_addr(OFFSETOF_MEMBER(Thread, interpSave.retval), PhysicalReg_SCRATCH_1, false, 1, false);
+    load_effective_addr(-24, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 1, false, 16, PhysicalReg_ESP, true);
+    if(num >= 1) {
+        get_virtual_reg(vC, OpndSize_32, 2, false);
+        move_reg_to_mem(OpndSize_32, 2, false, 0, PhysicalReg_ESP, true);
+    }
+    if(num >= 2) {
+        get_virtual_reg(vD, OpndSize_32, 3, false);
+        move_reg_to_mem(OpndSize_32, 3, false, 4, PhysicalReg_ESP, true);
+    }
+    if(num >= 3) {
+        get_virtual_reg(vE, OpndSize_32, 4, false);
+        move_reg_to_mem(OpndSize_32, 4, false, 8, PhysicalReg_ESP, true);
+    }
+    if(num >= 4) {
+        get_virtual_reg(vF, OpndSize_32, 5, false);
+        move_reg_to_mem(OpndSize_32, 5, false, 12, PhysicalReg_ESP, true);
+    }
+    beforeCall("execute_inline");
+    load_imm_global_data_API("gDvmInlineOpsTable", OpndSize_32, 6, false);
+    call_mem(16*tmp, 6, false);//
+    afterCall("execute_inline");
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+
+    load_effective_addr(24, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    conditional_jump(Condition_NE, ".execute_inline_done", true);
+    //jump to dvmJitToExceptionThrown
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    jumpToExceptionThrown(1/*exception number*/);
+    if (insertLabel(".execute_inline_done", true) == -1)
+        return -1;
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_SCRATCH_1
+#undef P_SCRATCH_2
+#undef P_SCRATCH_3
+#undef P_SCRATCH_4
+
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_SCRATCH_1 PhysicalReg_ESI
+#define P_SCRATCH_2 PhysicalReg_EDX
+#define PP_GPR_1 PhysicalReg_EBX
+#define PP_GPR_2 PhysicalReg_ESI
+#define PP_GPR_3 PhysicalReg_EAX
+#define PP_GPR_4 PhysicalReg_EDX
+//! common code for INVOKE_VIRTUAL_QUICK
+
+//! It uses helper function if the switch is on
+int common_invoke_virtual_quick(bool hasRange, int vD, u2 IMMC, const MIR *mir) {
+
+    const DecodedInstruction &decodedInst = mir->dalvikInsn;
+
+#ifdef WITH_JIT_INLINING_PHASE2
+    /*
+     * If the invoke has non-null misPredBranchOver, we need to generate
+     * the non-inlined version of the invoke here to handle the
+     * mispredicted case.
+     */
+    if (mir->meta.callsiteInfo->misPredBranchOver) {
+        genLandingPadForMispredictedCallee (mir);
+    }
+#endif
+    export_pc();
+    beforeCall("exception"); //dump GG, GL VRs
+    /////////////////////////////////////////////////
+    get_virtual_reg(vD, OpndSize_32, 1, false);
+    if ( (mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        simpleNullCheck(1, false, vD);
+    }
+#ifndef PREDICTED_CHAINING
+    move_mem_to_reg(OpndSize_32, 0, 1, false, 2, false);
+    move_mem_to_reg(OpndSize_32, offClassObject_vtable, 2, false, 3, false);
+    move_mem_to_reg(OpndSize_32, IMMC, 3, false, PhysicalReg_ECX, true);
+
+    if(hasRange) {
+        common_invokeMethodRange(ArgsDone_Full);
+    }
+    else {
+        common_invokeMethodNoRange(ArgsDone_Full);
+    }
+#else
+    gen_predicted_chain(hasRange, -1, IMMC, false, 1/*tmp1*/, decodedInst);
+#endif
+    ////////////////////////
+    return 0;
+}
+#undef P_GPR_1
+#undef P_SCRATCH_1
+#undef P_SCRATCH_2
+#undef PP_GPR_1
+#undef PP_GPR_2
+#undef PP_GPR_3
+#undef PP_GPR_4
+//! lower bytecode INVOKE_VIRTUAL_QUICK by calling common_invoke_virtual_quick
+
+//!
+int op_invoke_virtual_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL_QUICK);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    int vD = mir->dalvikInsn.vC;
+    u2 IMMC = 4 * mir->dalvikInsn.vB;
+    int retval = common_invoke_virtual_quick(false, vD, IMMC, mir);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_VIRTUAL_QUICK_RANGE by calling common_invoke_virtual_quick
+
+//!
+int op_invoke_virtual_quick_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_VIRTUAL_QUICK_RANGE);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    int vD = mir->dalvikInsn.vC;
+    u2 IMMC = 4 * mir->dalvikInsn.vB;
+    int retval = common_invoke_virtual_quick(true, vD, IMMC, mir);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ESI
+#define P_SCRATCH_1 PhysicalReg_EDX
+//! common code to lower INVOKE_SUPER_QUICK
+
+//!
+int common_invoke_super_quick(bool hasRange, int vD, u2 IMMC,
+        const DecodedInstruction &decodedInst) {
+    export_pc();
+    beforeCall("exception"); //dump GG, GL VRs
+    compare_imm_VR(OpndSize_32, 0, vD);
+
+    conditional_jump_global_API(Condition_E, "common_errNullObject", false);
+    /* for trace-based JIT, callee is already resolved */
+    int mIndex = IMMC/4;
+    const Method *calleeMethod = currentMethod->clazz->super->vtable[mIndex];
+    move_imm_to_reg(OpndSize_32, (int) calleeMethod, PhysicalReg_ECX, true);
+    if(hasRange) {
+        common_invokeMethodRange(convertCalleeToType(calleeMethod),
+                decodedInst);
+    }
+    else {
+        common_invokeMethodNoRange(convertCalleeToType(calleeMethod),
+                decodedInst);
+    }
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_SCRATCH_1
+//! lower bytecode INVOKE_SUPER_QUICK by calling common_invoke_super_quick
+
+//!
+int op_invoke_super_quick(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER_QUICK);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    int vD = mir->dalvikInsn.vC;
+    u2 IMMC = 4 * mir->dalvikInsn.vB;
+    int retval = common_invoke_super_quick(false, vD, IMMC, mir->dalvikInsn);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+//! lower bytecode INVOKE_SUPER_QUICK_RANGE by calling common_invoke_super_quick
+
+//!
+int op_invoke_super_quick_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INVOKE_SUPER_QUICK_RANGE);
+
+    /* An invoke with the MIR_INLINED is effectively a no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    int vD = mir->dalvikInsn.vC;
+    u2 IMMC = 4 * mir->dalvikInsn.vB;
+    int retval = common_invoke_super_quick(true, vD, IMMC, mir->dalvikInsn);
+#if defined(ENABLE_TRACING) && !defined(TRACING_OPTION2)
+    insertMapWorklist(offsetPC+3, stream - streamMethodStart, 1); //check when helper switch is on
+#endif
+    return retval;
+}
+/////// code to predict the callee method for invoke_virtual & invoke_interface
+#define offChainingCell_clazz 8
+#define offChainingCell_method 12
+#define offChainingCell_counter 16
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_EAX
+#define P_GPR_3 PhysicalReg_ESI
+#define P_SCRATCH_2 PhysicalReg_EDX
+/* TODO gingerbread: implemented for O1, but not for O0:
+   valid input to JitToPatch & use icRechainCount */
+/* update predicted method for invoke interface */
+// 2 inputs: ChainingCell in P_GPR_1, current class object in P_GPR_3
+void predicted_chain_interface_O0(u2 tmp) {
+    ALOGI("TODO chain_interface_O0");
+
+    /* set up arguments to dvmFindInterfaceMethodInCache */
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, tmp, 4, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, (int) currentMethod->clazz->pDvmDex, 12, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, (int) currentMethod, 8, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, P_GPR_3, true, 0, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_EDX;
+    call_dvmFindInterfaceMethodInCache();
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+    /* if dvmFindInterfaceMethodInCache returns NULL, throw exception
+       otherwise, jump to .find_interface_done */
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_NE, ".find_interface_done", true);
+    scratchRegs[0] = PhysicalReg_EAX;
+    jumpToExceptionThrown(1/*exception number*/);
+
+    /* the interface method is found */
+    if (insertLabel(".find_interface_done", true) == -1)
+        return;
+    /* reduce counter in chaining cell by 1 */
+    move_mem_to_reg(OpndSize_32, offChainingCell_counter, P_GPR_1, true, P_SCRATCH_2, true); //counter
+    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, P_SCRATCH_2, true);
+    move_reg_to_mem(OpndSize_32, P_SCRATCH_2, true, offChainingCell_counter, P_GPR_1, true);
+
+    /* if counter is still greater than zero, skip prediction
+       if it is zero, update predicted method */
+    compare_imm_reg(OpndSize_32, 0, P_SCRATCH_2, true);
+    conditional_jump(Condition_G, ".skipPrediction", true);
+
+    /* call dvmJitToPatchPredictedChain to update predicted method */
+    //%ecx has callee method for virtual, %eax has callee for interface
+    /* set up arguments for dvmJitToPatchPredictedChain */
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    move_chain_to_mem(OpndSize_32, traceCurrentBB->taken->id, 8, PhysicalReg_ESP, true); //predictedChainCell
+    move_reg_to_mem(OpndSize_32, P_GPR_3, true, 12, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_EAX;
+    call_dvmJitToPatchPredictedChain(); //inputs: method, unused, predictedChainCell, clazz
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    if (insertLabel(".skipPrediction", true) == -1)
+        return;
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
+}
+
+// 2 inputs: ChainingCell in temp 41, current class object in temp 40
+void predicted_chain_interface_O1(u2 tmp) {
+
+    /* set up arguments to dvmFindInterfaceMethodInCache */
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, tmp, 4, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, (int) currentMethod->clazz->pDvmDex, 12, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, (int) currentMethod, 8, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 40, false, 0, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_10;
+    call_dvmFindInterfaceMethodInCache();
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+    /* if dvmFindInterfaceMethodInCache returns NULL, throw exception
+       otherwise, jump to .find_interface_done */
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_NE, ".find_interface_done", true);
+    rememberState(3);
+    scratchRegs[0] = PhysicalReg_SCRATCH_9;
+    jumpToExceptionThrown(1/*exception number*/);
+
+    goToState(3);
+    /* the interface method is found */
+    if (insertLabel(".find_interface_done", true) == -1)
+        return;
+#if 1 //
+    /* for gingerbread, counter is stored in glue structure
+       if clazz is not initialized, set icRechainCount to 0, otherwise, reduce it by 1 */
+    /* for gingerbread: t43 = 0 t44 = t33 t33-- cmov_ne t43 = t33 cmov_ne t44 = t33 */
+    move_mem_to_reg(OpndSize_32, offChainingCell_clazz, 41, false, 45, false);
+    move_imm_to_reg(OpndSize_32, 0, 43, false);
+    get_self_pointer(PhysicalReg_SCRATCH_7, isScratchPhysical);
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Thread, icRechainCount), PhysicalReg_SCRATCH_7, isScratchPhysical, 33, false); //counter
+    move_reg_to_reg(OpndSize_32, 33, false, 44, false);
+    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, 33, false);
+    /* sub_opc will update control flags, so compare_imm_reg must happen after */
+    compare_imm_reg(OpndSize_32, 0, 45, false);
+    conditional_move_reg_to_reg(OpndSize_32, Condition_NZ, 33, false/*src*/, 43, false/*dst*/);
+    conditional_move_reg_to_reg(OpndSize_32, Condition_NZ, 33, false/*src*/, 44, false/*dst*/);
+    move_reg_to_mem(OpndSize_32, 44, false, OFFSETOF_MEMBER(Thread, icRechainCount), PhysicalReg_SCRATCH_7, isScratchPhysical);
+#else
+    /* reduce counter in chaining cell by 1 */
+    move_mem_to_reg(OpndSize_32, offChainingCell_counter, 41, false, 33, false); //counter
+    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, 33, false);
+    move_reg_to_mem(OpndSize_32, 33, false, offChainingCell_counter, 41, false);
+#endif
+
+    /* if counter is still greater than zero, skip prediction
+       if it is zero, update predicted method */
+    compare_imm_reg(OpndSize_32, 0, 43, false);
+    conditional_jump(Condition_G, ".skipPrediction", true);
+
+    rememberState(4);
+    /* call dvmJitToPatchPredictedChain to update predicted method */
+    //%ecx has callee method for virtual, %eax has callee for interface
+    /* set up arguments for dvmJitToPatchPredictedChain */
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_SCRATCH_7, isScratchPhysical, 4, PhysicalReg_ESP, true);
+   if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    move_chain_to_mem(OpndSize_32, traceCurrentBB->taken->id, 8, PhysicalReg_ESP, true); //predictedChainCell
+    move_reg_to_mem(OpndSize_32, 40, false, 12, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_8;
+    call_dvmJitToPatchPredictedChain(); //inputs: method, unused, predictedChainCell, clazz
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    transferToState(4);
+
+    if (insertLabel(".skipPrediction", true) == -1)
+        return;
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
+}
+
+/* update predicted method for invoke virtual */
+// 2 inputs: ChainingCell in P_GPR_1, current class object in P_GPR_3
+void predicted_chain_virtual_O0(u2 IMMC) {
+    ALOGI("TODO chain_virtual_O0");
+
+    /* reduce counter in chaining cell by 1 */
+    move_mem_to_reg(OpndSize_32, offChainingCell_counter, P_GPR_1, true, P_GPR_2, true); //counter
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ClassObject, vtable), P_GPR_3, true, P_SCRATCH_2, true);
+    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, P_GPR_2, true);
+    move_mem_to_reg(OpndSize_32, IMMC, P_SCRATCH_2, true, PhysicalReg_ECX, true);
+    move_reg_to_mem(OpndSize_32, P_GPR_2, true, offChainingCell_counter, P_GPR_1, true);
+
+    /* if counter is still greater than zero, skip prediction
+       if it is zero, update predicted method */
+    compare_imm_reg(OpndSize_32, 0, P_GPR_2, true);
+    conditional_jump(Condition_G, ".skipPrediction", true);
+
+    /* call dvmJitToPatchPredictedChain to update predicted method */
+    //%ecx has callee method for virtual, %eax has callee for interface
+    /* set up arguments for dvmJitToPatchPredictedChain */
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 0,  PhysicalReg_ESP, true);
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    move_chain_to_mem(OpndSize_32, traceCurrentBB->taken->id, 8, PhysicalReg_ESP, true); //predictedChainCell
+    move_reg_to_mem(OpndSize_32, P_GPR_3, true, 12, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_EAX;
+    call_dvmJitToPatchPredictedChain(); //inputs: method, unused, predictedChainCell, clazz
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+    //callee method in %ecx for invoke virtual
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
+    if (insertLabel(".skipPrediction", true) == -1)
+        return;
+}
+
+// 2 inputs: ChainingCell in temp 41, current class object in temp 40
+// extra input: predicted clazz in temp 32
+void predicted_chain_virtual_O1(u2 IMMC) {
+
+    /* reduce counter in chaining cell by 1 */
+    /* for gingerbread: t43 = 0 t44 = t33 t33-- cmov_ne t43 = t33 cmov_ne t44 = t33 */
+    get_self_pointer(PhysicalReg_SCRATCH_7, isScratchPhysical);
+    move_imm_to_reg(OpndSize_32, 0, 43, false);
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Thread, icRechainCount), PhysicalReg_SCRATCH_7, isScratchPhysical, 33, false); //counter
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ClassObject, vtable), 40, false, 34, false);
+    move_reg_to_reg(OpndSize_32, 33, false, 44, false);
+    alu_binary_imm_reg(OpndSize_32, sub_opc, 0x1, 33, false);
+    compare_imm_reg(OpndSize_32, 0, 32, false); // after sub_opc
+    move_mem_to_reg(OpndSize_32, IMMC, 34, false, PhysicalReg_ECX, true);
+    conditional_move_reg_to_reg(OpndSize_32, Condition_NZ, 33, false/*src*/, 43, false/*dst*/);
+    conditional_move_reg_to_reg(OpndSize_32, Condition_NZ, 33, false/*src*/, 44, false/*dst*/);
+    move_reg_to_mem(OpndSize_32, 44, false, OFFSETOF_MEMBER(Thread, icRechainCount), PhysicalReg_SCRATCH_7, isScratchPhysical);
+
+    /* if counter is still greater than zero, skip prediction
+       if it is zero, update predicted method */
+    compare_imm_reg(OpndSize_32, 0, 43, false);
+    conditional_jump(Condition_G, ".skipPrediction", true);
+
+    rememberState(2);
+    /* call dvmJitToPatchPredictedChain to update predicted method */
+    //%ecx has callee method for virtual, %eax has callee for interface
+    /* set up arguments for dvmJitToPatchPredictedChain */
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 0, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_SCRATCH_7, isScratchPhysical, 4, PhysicalReg_ESP, true);
+    if(!gDvmJit.scheduling && traceCurrentBB->taken)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    int traceTakenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
+    move_chain_to_mem(OpndSize_32, traceTakenId, 8, PhysicalReg_ESP, true); //predictedChainCell
+    move_reg_to_mem(OpndSize_32, 40, false, 12, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_10;
+    call_dvmJitToPatchPredictedChain(); //inputs: method, unused, predictedChainCell, clazz
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+    //callee method in %ecx for invoke virtual
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, PhysicalReg_ECX, true);
+    transferToState(2);
+
+    if (insertLabel(".skipPrediction", true) == -1)
+        return;
+}
+
+static int invokeChain_inst = 0;
+/* object "this" is in %ebx */
+void gen_predicted_chain_O0(bool isRange, u2 tmp, int IMMC, bool isInterface,
+        int inputReg, const DecodedInstruction &decodedInst) {
+    ALOGI("TODO predicted_chain_O0");
+
+    /* get current class object */
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), PhysicalReg_EBX, true,
+             P_GPR_3, true);
+#ifdef DEBUG_CALL_STACK3
+    scratchRegs[0] = PhysicalReg_EAX;
+    call_debug_dumpSwitch(); //%ebx, %eax, %edx
+    move_imm_to_reg(OpndSize_32, 0xdd11, PhysicalReg_EBX, true);
+    call_debug_dumpSwitch();
+#endif
+
+    /* get predicted clazz
+       get predicted method
+    */
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    move_chain_to_reg(OpndSize_32, traceCurrentBB->taken->id, P_GPR_1, true); //predictedChainCell
+    move_mem_to_reg(OpndSize_32, offChainingCell_clazz, P_GPR_1, true, P_SCRATCH_2, true);//predicted clazz
+    move_mem_to_reg(OpndSize_32, offChainingCell_method, P_GPR_1, true, PhysicalReg_ECX, true);//predicted method
+
+#ifdef DEBUG_CALL_STACK3
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, P_GPR_1, true, 8, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, P_SCRATCH_2, true, 4, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, P_GPR_3, true, 0, PhysicalReg_ESP, true);
+
+    move_reg_to_reg(OpndSize_32, P_SCRATCH_2, true, PhysicalReg_EBX, true);
+    call_debug_dumpSwitch();
+    move_imm_to_reg(OpndSize_32, 0xdd22, PhysicalReg_EBX, true);
+    scratchRegs[0] = PhysicalReg_EAX;
+    call_debug_dumpSwitch(); //%ebx, %eax, %edx
+    move_reg_to_reg(OpndSize_32, P_GPR_3, true, PhysicalReg_EBX, true);
+    call_debug_dumpSwitch();
+    move_reg_to_reg(OpndSize_32, PhysicalReg_ECX, true, PhysicalReg_EBX, true);
+    call_debug_dumpSwitch();
+
+    move_mem_to_reg(OpndSize_32, 8, PhysicalReg_ESP, true, P_GPR_1, true);
+    move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true, P_SCRATCH_2, true);
+    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, P_GPR_3, true);
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+#endif
+
+    /* compare current class object against predicted clazz
+       if equal, prediction is still valid, jump to .invokeChain */
+    //live registers: P_GPR_1, P_GPR_3, P_SCRATCH_2
+    compare_reg_reg(P_GPR_3, true, P_SCRATCH_2, true);
+    conditional_jump(Condition_E, ".invokeChain", true);
+    invokeChain_inst++;
+
+    //get callee method and update predicted method if necessary
+    if(isInterface) {
+        predicted_chain_interface_O0(tmp);
+    } else {
+        predicted_chain_virtual_O0(IMMC);
+    }
+
+#ifdef DEBUG_CALL_STACK3
+    move_imm_to_reg(OpndSize_32, 0xeeee, PhysicalReg_EBX, true);
+    scratchRegs[0] = PhysicalReg_EAX;
+    call_debug_dumpSwitch(); //%ebx, %eax, %edx
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    move_chain_to_reg(OpndSize_32, traceCurrentBB->taken->id, PhysicalReg_EBX, true);
+    call_debug_dumpSwitch();
+#endif
+
+    if(isRange) {
+        common_invokeMethodRange(ArgsDone_Full, decodedInst);
+    }
+    else {
+        common_invokeMethodNoRange(ArgsDone_Full, decodedInst);
+    }
+
+    if (insertLabel(".invokeChain", true) == -1)
+        return;
+#ifdef DEBUG_CALL_STACK3
+    move_imm_to_reg(OpndSize_32, 0xdddd, PhysicalReg_EBX, true);
+    scratchRegs[0] = PhysicalReg_EAX;
+    call_debug_dumpSwitch(); //%ebx, %eax, %edx
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    move_chain_to_reg(OpndSize_32, traceCurrentBB->taken->id, PhysicalReg_EBX, true);
+    call_debug_dumpSwitch();
+    move_reg_to_reg(OpndSize_32, PhysicalReg_ECX, true, PhysicalReg_EBX, true);
+    call_debug_dumpSwitch();
+#endif
+
+    if(isRange) {
+        common_invokeMethodRange(ArgsDone_Normal, decodedInst);
+    }
+    else {
+        common_invokeMethodNoRange(ArgsDone_Normal, decodedInst);
+    }
+}
+
+/* object "this" is in inputReg: 5 for virtual, 1 for interface, 1 for virtual_quick */
+void gen_predicted_chain_O1(bool isRange, u2 tmp, int IMMC, bool isInterface,
+        int inputReg, const DecodedInstruction &decodedInst) {
+
+    /* get current class object */
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), inputReg, false,
+             40, false);
+
+    /* get predicted clazz
+       get predicted method
+    */
+    if(!gDvmJit.scheduling && traceCurrentBB->taken)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    int traceTakenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
+    move_chain_to_reg(OpndSize_32, traceTakenId, 41, false); //predictedChainCell
+    move_mem_to_reg(OpndSize_32, offChainingCell_clazz, 41, false, 32, false);//predicted clazz
+    move_mem_to_reg(OpndSize_32, offChainingCell_method, 41, false, PhysicalReg_ECX, true);//predicted method
+
+    /* update stack with parameters first, then decide the callee */
+    if(isRange) common_invokeMethodRange_noJmp(decodedInst);
+    else common_invokeMethodNoRange_noJmp(decodedInst);
+
+    /* compare current class object against predicted clazz
+       if equal, prediction is still valid, jump to .invokeChain */
+    compare_reg_reg(40, false, 32, false);
+    conditional_jump(Condition_E, ".invokeChain", true);
+    rememberState(1);
+    invokeChain_inst++;
+
+    //get callee method and update predicted method if necessary
+    if(isInterface) {
+        predicted_chain_interface_O1(tmp);
+    } else {
+        predicted_chain_virtual_O1(IMMC);
+    }
+
+    common_invokeMethod_Jmp(ArgsDone_Full); //will touch %ecx
+
+    if (insertLabel(".invokeChain", true) == -1)
+        return;
+    goToState(1);
+    common_invokeMethod_Jmp(ArgsDone_Normal);
+}
+
+void gen_predicted_chain(bool isRange, u2 tmp, int IMMC, bool isInterface,
+        int inputReg, const DecodedInstruction &decodedInst) {
+    return gen_predicted_chain_O1(isRange, tmp, IMMC, isInterface, inputReg,
+            decodedInst);
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef P_SCRATCH_2
diff --git a/vm/compiler/codegen/x86/lightcg/LowerJump.cpp b/vm/compiler/codegen/x86/lightcg/LowerJump.cpp
new file mode 100644
index 0000000..b932a4b
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/LowerJump.cpp
@@ -0,0 +1,2208 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/**
+ * @file vm/compiler/codegen/x86/LowerJump.cpp
+ * @brief This file lowers the following bytecodes: IF_XXX, GOTO
+ */
+#include <math.h>
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "NcgAot.h"
+#include "enc_wrapper.h"
+#include "interp/InterpDefs.h"
+#include "NcgHelper.h"
+#include "RegisterizationBE.h"
+#include "Scheduler.h"
+#include "Singleton.h"
+
+#if defined VTUNE_DALVIK
+#include "compiler/JitProfiling.h"
+#endif
+
+LabelMap* globalMap;
+LabelMap* globalShortMap;//make sure for each bytecode, there is no duplicated label
+LabelMap* globalWorklist = NULL;
+LabelMap* globalShortWorklist;
+
+int globalMapNum;
+int globalWorklistNum;
+int globalDataWorklistNum;
+int VMAPIWorklistNum;
+int globalPCWorklistNum;
+int chainingWorklistNum;
+
+LabelMap* globalDataWorklist = NULL;
+LabelMap* globalPCWorklist = NULL;
+LabelMap* chainingWorklist = NULL;
+LabelMap* VMAPIWorklist = NULL;
+
+char* ncgClassData;
+char* ncgClassDataPtr;
+char* ncgMethodData;
+char* ncgMethodDataPtr;
+int   ncgClassNum;
+int   ncgMethodNum;
+
+NCGWorklist* globalNCGWorklist;
+DataWorklist* methodDataWorklist;
+#ifdef ENABLE_TRACING
+MapWorklist* methodMapWorklist;
+#endif
+/*!
+\brief search globalShortMap to find the entry for the given label
+
+*/
+LabelMap* findItemForShortLabel(const char* label) {
+    LabelMap* ptr = globalShortMap;
+    while(ptr != NULL) {
+        if(!strcmp(label, ptr->label)) {
+            return ptr;
+        }
+        ptr = ptr->nextItem;
+    }
+    return NULL;
+}
+//assume size of "jump reg" is 2
+#define JUMP_REG_SIZE 2
+#define ADD_REG_REG_SIZE 3
+/*!
+\brief update value of the immediate in the given jump instruction
+
+check whether the immediate is out of range for the pre-set size
+*/
+int updateJumpInst(char* jumpInst, OpndSize immSize, int relativeNCG) {
+#ifdef DEBUG_NCG_JUMP
+    ALOGI("update jump inst @ %p with %d", jumpInst, relativeNCG);
+#endif
+    if(immSize == OpndSize_8) { //-128 to 127
+        if(relativeNCG >= 128 || relativeNCG < -128) {
+            ALOGI("JIT_INFO: Pre-allocated space for a forward jump is not big enough\n");
+            SET_JIT_ERROR(kJitErrorShortJumpOffset);
+            return -1;
+        }
+    }
+    if(immSize == OpndSize_16) { //-2^16 to 2^16-1
+        if(relativeNCG >= 32768 || relativeNCG < -32768) {
+            ALOGI("JIT_INFO: Pre-allocated space (16-bit) for a forward jump is not big enough\n");
+            SET_JIT_ERROR(kJitErrorShortJumpOffset);
+            return -1;
+        }
+    }
+    dump_imm_update(relativeNCG, jumpInst, false);
+    return 0;
+}
+
+/*!
+\brief insert a label
+
+It takes argument checkDup, if checkDup is true, an entry is created in globalShortMap, entries in globalShortWorklist are checked, if there exists a match, the immediate in the jump instruction is updated and the entry is removed from globalShortWorklist;
+otherwise, an entry is created in globalMap.
+*/
+int insertLabel(const char* label, bool checkDup) {
+    LabelMap* item = NULL;
+
+    // We are inserting a label. Someone might want to jump to it
+    // so flush scheduler's queue
+    if (gDvmJit.scheduling)
+        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+
+    if(!checkDup) {
+        item = (LabelMap*)malloc(sizeof(LabelMap));
+        if(item == NULL) {
+            ALOGI("JIT_INFO: Memory allocation failed at insertLabel with checkDup false");
+            SET_JIT_ERROR(kJitErrorMallocFailed);
+            return -1;
+        }
+        snprintf(item->label, LABEL_SIZE, "%s", label);
+        item->codePtr = stream;
+        item->nextItem = globalMap;
+        globalMap = item;
+#ifdef DEBUG_NCG_CODE_SIZE
+        ALOGI("insert global label %s %p", label, stream);
+#endif
+        globalMapNum++;
+        return 0;
+    }
+
+    item = (LabelMap*)malloc(sizeof(LabelMap));
+    if(item == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertLabel with checkDup true");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    snprintf(item->label, LABEL_SIZE, "%s", label);
+    item->codePtr = stream;
+    item->nextItem = globalShortMap;
+    globalShortMap = item;
+#ifdef DEBUG_NCG
+    ALOGI("Insert short-term label %s %p", label, stream);
+#endif
+    LabelMap* ptr = globalShortWorklist;
+    LabelMap* ptr_prevItem = NULL;
+    while(ptr != NULL) {
+        if(!strcmp(ptr->label, label)) {
+            //perform work
+            int relativeNCG = stream - ptr->codePtr;
+            unsigned instSize = encoder_get_inst_size(ptr->codePtr);
+            relativeNCG -= instSize; //size of the instruction
+#ifdef DEBUG_NCG
+            ALOGI("Perform work short-term %p for label %s relative %d\n", ptr->codePtr, label, relativeNCG);
+#endif
+            int retval = updateJumpInst(ptr->codePtr, ptr->size, relativeNCG);
+            //If this fails, the jump offset was not big enough. Raise the corresponding error flag
+            //We may decide to re-compiler the trace with a large jump offset later
+            if (retval == -1){
+                ALOGI("JIT_INFO: Label \"%s\" too far away from jump location", label);
+                SET_JIT_ERROR(kJitErrorShortJumpOffset);
+                return retval;
+            }
+
+            //remove work
+            if(ptr_prevItem == NULL) {
+                globalShortWorklist = ptr->nextItem;
+                free(ptr);
+                ptr = globalShortWorklist; //ptr_prevItem is still NULL
+            }
+            else {
+                ptr_prevItem->nextItem = ptr->nextItem;
+                free(ptr);
+                ptr = ptr_prevItem->nextItem;
+            }
+        }
+        else {
+            ptr_prevItem = ptr;
+            ptr = ptr->nextItem;
+        }
+    } //while
+    return 0;
+}
+/*!
+\brief search globalMap to find the entry for the given label
+
+*/
+char* findCodeForLabel(const char* label) {
+    LabelMap* ptr = globalMap;
+    while(ptr != NULL) {
+        if(!strcmp(label, ptr->label)) {
+            return ptr->codePtr;
+        }
+        ptr = ptr->nextItem;
+    }
+    return NULL;
+}
+/*!
+\brief search globalShortMap to find the entry for the given label
+
+*/
+char* findCodeForShortLabel(const char* label) {
+    LabelMap* ptr = globalShortMap;
+    while(ptr != NULL) {
+        if(!strcmp(label, ptr->label)) {
+            return ptr->codePtr;
+        }
+        ptr = ptr->nextItem;
+    }
+    return NULL;
+}
+int insertLabelWorklist(const char* label, OpndSize immSize) {
+    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
+    if(item == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertLabelWorklist");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    snprintf(item->label, LABEL_SIZE, "%s", label);
+    item->codePtr = stream;
+    item->size = immSize;
+    item->nextItem = globalWorklist;
+    globalWorklist = item;
+#ifdef DEBUG_NCG
+    ALOGI("Insert globalWorklist: %s %p", label, stream);
+#endif
+    return 0;
+}
+
+int insertShortWorklist(const char* label, OpndSize immSize) {
+    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
+    if(item == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertShortWorklist");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    snprintf(item->label, LABEL_SIZE, "%s", label);
+    item->codePtr = stream;
+    item->size = immSize;
+    item->nextItem = globalShortWorklist;
+    globalShortWorklist = item;
+#ifdef DEBUG_NCG
+    ALOGI("Insert globalShortWorklist: %s %p", label, stream);
+#endif
+    return 0;
+}
+/*!
+\brief free memory allocated for globalMap
+
+*/
+void freeLabelMap() {
+    LabelMap* ptr = globalMap;
+    while(ptr != NULL) {
+        globalMap = ptr->nextItem;
+        free(ptr);
+        ptr = globalMap;
+    }
+}
+/*!
+\brief free memory allocated for globalShortMap
+
+*/
+void freeShortMap() {
+    LabelMap* ptr = globalShortMap;
+    while(ptr != NULL) {
+        globalShortMap = ptr->nextItem;
+        free(ptr);
+        ptr = globalShortMap;
+    }
+    globalShortMap = NULL;
+}
+
+int insertGlobalPCWorklist(char * offset, char * codeStart)
+{
+    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
+    if(item == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertGlobalPCWorklist");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    snprintf(item->label, LABEL_SIZE, "%s", "export_pc");
+    item->size = OpndSize_32;
+    item->codePtr = offset; //points to the immediate operand
+    item->addend = codeStart - streamMethodStart; //relative code pointer
+    item->nextItem = globalPCWorklist;
+    globalPCWorklist = item;
+    globalPCWorklistNum ++;
+
+#ifdef DEBUG_NCG
+    ALOGI("Insert globalPCWorklist: %p %p %p %x %p", globalDvmNcg->streamCode,  codeStart, streamCode, item->addend, item->codePtr);
+#endif
+    return 0;
+}
+
+/*
+ * search chainingWorklist to return instruction offset address in move instruction
+ */
+char* searchChainingWorklist(unsigned int blockId) {
+    LabelMap* ptr = chainingWorklist;
+    unsigned instSize;
+
+    while (ptr != NULL) {
+       if (blockId == ptr->addend) {
+           instSize = encoder_get_inst_size(ptr->codePtr);
+           assert((uint)(ptr->codePtr + instSize - 4) % 16 <= 12);
+           return (ptr->codePtr + instSize - 4); // 32bit relative offset
+       }
+       ptr = ptr->nextItem;
+    }
+#ifdef DEBUG_NCG
+    ALOGI("can't find item for blockId %d in searchChainingWorklist\n", blockId);
+#endif
+    return NULL;
+}
+
+int insertChainingWorklist(int bbId, char * codeStart)
+{
+    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
+    if(item == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertChainingWorklist");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    item->size = OpndSize_32;
+    item->codePtr = codeStart; //points to the move instruction
+    item->addend = bbId; //relative code pointer
+    item->nextItem = chainingWorklist;
+    chainingWorklist = item;
+
+#ifdef DEBUG_NCG
+    ALOGI("InsertChainingWorklist: %p basic block %d", codeStart, bbId);
+#endif
+    return 0;
+}
+
+int insertGlobalDataWorklist(char * offset, const char* label)
+{
+    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
+    if(item == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertGlobalDataWorklist");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    snprintf(item->label, LABEL_SIZE, "%s", label);
+    item->codePtr = offset;
+    item->size = OpndSize_32;
+    item->nextItem = globalDataWorklist;
+    globalDataWorklist = item;
+    globalDataWorklistNum ++;
+
+#ifdef DEBUG_NCG
+    ALOGI("Insert globalDataWorklist: %s %p", label, offset);
+#endif
+
+    return 0;
+}
+
+int insertVMAPIWorklist(char * offset, const char* label)
+{
+    LabelMap* item = (LabelMap*)malloc(sizeof(LabelMap));
+    if(item == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertVMAPIWorklist");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    snprintf(item->label, LABEL_SIZE, "%s", label);
+    item->codePtr = offset;
+    item->size = OpndSize_32;
+
+    item->nextItem = VMAPIWorklist;
+    VMAPIWorklist = item;
+
+    VMAPIWorklistNum ++;
+
+#ifdef DEBUG_NCG
+    ALOGI("Insert VMAPIWorklist: %s %p", label, offset);
+#endif
+    return 0;
+}
+////////////////////////////////////////////////
+
+
+int updateImmRMInst(char* moveInst, const char* label, int relativeNCG); //forward declaration
+//////////////////// performLabelWorklist is defined differently for code cache
+void performChainingWorklist() {
+    LabelMap* ptr = chainingWorklist;
+    while(ptr != NULL) {
+        int tmpNCG = getLabelOffset (ptr->addend);
+        char* NCGaddr = streamMethodStart + tmpNCG;
+        updateImmRMInst(ptr->codePtr, "", (int)NCGaddr);
+        chainingWorklist = ptr->nextItem;
+        free(ptr);
+        ptr = chainingWorklist;
+    }
+}
+void freeChainingWorklist() {
+    LabelMap* ptr = chainingWorklist;
+    while(ptr != NULL) {
+        chainingWorklist = ptr->nextItem;
+        free(ptr);
+        ptr = chainingWorklist;
+    }
+}
+
+/*
+ *search globalWorklist to find the jmp/jcc offset address
+ */
+char* searchLabelWorklist(char* label) {
+    LabelMap* ptr = globalWorklist;
+    unsigned instSize;
+
+    while(ptr != NULL) {
+        if(!strcmp(label, ptr->label)) {
+            instSize = encoder_get_inst_size(ptr->codePtr);
+            assert((uint)(ptr->codePtr + instSize - 4) % 16 <= 12);
+            return (ptr->codePtr + instSize - 4); // 32bit relative offset
+        }
+        ptr = ptr->nextItem;
+   }
+#ifdef DEBUG_NCG
+    ALOGI("can't find item for label %s in searchLabelWorklist\n", label);
+#endif
+    return NULL;
+}
+
+// delete the node with label "vr_store_at_loop_back" from globalMap
+static void deleteVRStoreLabelGlobalMap()
+{
+    LabelMap * ptr = globalMap;
+    LabelMap * prePtr = NULL;
+
+    while(ptr != NULL) {
+        if (strstr(ptr->label, ".vr_store_at_loop_back") != 0) {
+            if (prePtr == NULL)
+                globalMap = ptr->nextItem;
+            else
+                prePtr->nextItem = ptr->nextItem;
+            free(ptr);
+            return;
+        }
+        prePtr = ptr;
+        ptr = ptr->nextItem;
+    }
+}
+
+//Work only for initNCG
+void performLabelWorklist() {
+    LabelMap* ptr = globalWorklist;
+    while(ptr != NULL) {
+#ifdef DEBUG_NCG
+        ALOGI("Perform work global %p for label %s", ptr->codePtr, ptr->label);
+#endif
+        char* targetCode = findCodeForLabel(ptr->label);
+        assert(targetCode != NULL);
+        int relativeNCG = targetCode - ptr->codePtr;
+        unsigned instSize = encoder_get_inst_size(ptr->codePtr);
+        relativeNCG -= instSize; //size of the instruction
+        updateJumpInst(ptr->codePtr, ptr->size, relativeNCG);
+        globalWorklist = ptr->nextItem;
+        free(ptr);
+        ptr = globalWorklist;
+    }
+    deleteVRStoreLabelGlobalMap();
+}
+
+void freeLabelWorklist() {
+    LabelMap* ptr = globalWorklist;
+    while(ptr != NULL) {
+        globalWorklist = ptr->nextItem;
+        free(ptr);
+        ptr = globalWorklist;
+    }
+}
+
+///////////////////////////////////////////////////
+/*!
+\brief update value of the immediate in the given move instruction
+
+*/
+int updateImmRMInst(char* moveInst, const char* label, int relativeNCG) {
+#ifdef DEBUG_NCG
+    ALOGI("Perform work ImmRM inst @ %p for label %s with %d", moveInst, label, relativeNCG);
+#endif
+    dump_imm_update(relativeNCG, moveInst, true);
+    return 0;
+}
+//! maximum instruction size for jump,jcc,call: 6 for jcc rel32
+#define MAX_JCC_SIZE 6
+//! minimum instruction size for jump,jcc,call: 2
+#define MIN_JCC_SIZE 2
+/*!
+\brief estimate size of the immediate
+
+Somehow, 16 bit jump does not work. This function will return either 8 bit or 32 bit
+EXAMPLE:
+  native code at A: ...
+  native code at B: jump relOffset (target is A)
+  native code at B':
+  --> relOffset = A - B' = A - B - size of the jump instruction
+  Argument "target" is equal to A - B. To determine size of the immediate, we check tha value of "target - size of the jump instructoin"
+*/
+OpndSize estOpndSizeFromImm(int target) {
+    if(target-MIN_JCC_SIZE < 128 && target-MAX_JCC_SIZE >= -128) return OpndSize_8;
+#ifdef SUPPORT_IMM_16
+    if(target-MIN_JCC_SIZE < 32768 && target-MAX_JCC_SIZE >= -32768) return OpndSize_16;
+#endif
+    return OpndSize_32;
+}
+
+/*!
+\brief return size of a jump or call instruction
+*/
+unsigned getJmpCallInstSize(OpndSize size, JmpCall_type type) {
+    if(type == JmpCall_uncond) {
+        if(size == OpndSize_8) return 2;
+        if(size == OpndSize_16) return 4;
+        return 5;
+    }
+    if(type == JmpCall_cond) {
+        if(size == OpndSize_8) return 2;
+        if(size == OpndSize_16) return 5;
+        return 6;
+    }
+    if(type == JmpCall_reg) {
+        assert(size == OpndSize_32);
+        return JUMP_REG_SIZE;
+    }
+    if(type == JmpCall_call) {
+        assert(size != OpndSize_8);
+        if(size == OpndSize_16) return 4;
+        return 5;
+    }
+    return 0;
+}
+
+//! \brief Get the offset given a jump target
+//!
+//! \details check whether a branch target is already handled if yes, return the
+//! size of the immediate; otherwise, call insertShortWorklist or insertLabelWorklist.
+//!
+//! If the branch target is not handled, call insertShortWorklist or insertLabelWorklist
+//! depending on isShortTerm, unknown is set to true, immSize is set to 32 if isShortTerm
+//! is false, set to 32 if isShortTerm is true and target is check_cast_null, set to 8 otherwise.
+//!
+//! If the branch target is handled, call estOpndSizeFromImm to set immSize for jump
+//! instruction, returns the value of the immediate
+//!
+//! \param target the target of the jump
+//! \param isShortTerm whether this is a short term jump
+//! \param type Call or Jmp
+//! \param unknown target known or not
+//! \param immSize size of the jump offset
+//!
+//! \return jump offset (can also return error value, but caller cannot distinguish)
+int getRelativeOffset(const char* target, bool isShortTerm, JmpCall_type type, bool* unknown, OpndSize* immSize) {
+    char* targetPtrInStream = NULL;
+    if(isShortTerm) targetPtrInStream = findCodeForShortLabel(target);
+    else targetPtrInStream = findCodeForLabel(target);
+
+    int relOffset;
+    int retCode = 0;
+    *unknown = false;
+    if(targetPtrInStream == NULL) {
+        //branch target is not handled yet
+        relOffset = 0;
+        *unknown = true;
+        if(isShortTerm) {
+            /* for backward jump, at this point, we don't know how far the target is from this jump
+               since the lable is only used within a single bytecode, we assume OpndSize_8 is big enough
+               but there are special cases where we should use 32 bit offset
+            */
+            //Check if we have failed with 8-bit offset previously. Use 32-bit offsets if so.
+            if (gDvmJit.disableOpt & (1 << kShortJumpOffset)){
+                *immSize = OpndSize_32;
+            }
+            //Check if it is a special case:
+            //These labels are known to be far off from the jump location
+            //Safe to set them to large offset by default
+            else if(!strcmp(target, ".stackOverflow") ||
+                    !strcmp(target, ".invokeChain") ||
+                    !strcmp(target, "after_exception_1") ||
+                    !strncmp(target, "exception_restore_state_", 24)) {
+#ifdef SUPPORT_IMM_16
+                *immSize = OpndSize_16;
+#else
+                *immSize = OpndSize_32;
+#endif
+            } else {
+                *immSize = OpndSize_8;
+            }
+#ifdef WITH_SELF_VERIFICATION
+            if(!strcmp(target, ".aput_object_skip_check") ||
+               !strcmp(target, ".aput_object_after_check") ) {
+                *immSize = OpndSize_32;
+            }
+#endif
+#ifdef DEBUG_NCG_JUMP
+            ALOGI("Insert to short worklist %s %d", target, *immSize);
+#endif
+            retCode = insertShortWorklist(target, *immSize);
+            //NOTE: Returning negative value here cannot indicate an error
+            //The caller accepts any value as correct. Only the premature
+            //return matters here.
+            if (retCode < 0)
+                return retCode;
+        }
+        else {
+#ifdef SUPPORT_IMM_16
+            *immSize = OpndSize_16;
+#else
+            *immSize = OpndSize_32;
+#endif
+            retCode = insertLabelWorklist(target, *immSize);
+            //NOTE: Returning negative value here cannot indicate an error
+            //The caller accepts any value as correct. Only the premature
+            //return matters here.
+            if (retCode < 0) {
+                return retCode;
+            }
+        }
+        if(type == JmpCall_call) { //call sz16 does not work in gdb
+            *immSize = OpndSize_32;
+        }
+        return 0;
+    }
+    else if (!isShortTerm) {
+#ifdef SUPPORT_IMM_16
+        *immSize = OpndSize_16;
+#else
+        *immSize = OpndSize_32;
+#endif
+        retCode = insertLabelWorklist(target, *immSize);
+        if (retCode < 0) {
+            return retCode;
+        }
+    }
+
+#ifdef DEBUG_NCG
+    ALOGI("Backward branch @ %p for label %s", stream, target);
+#endif
+    relOffset = targetPtrInStream - stream;
+    if (type == JmpCall_call) {
+        *immSize = OpndSize_32;
+    }
+    else {
+        *immSize = estOpndSizeFromImm(relOffset);
+    }
+    relOffset -= getJmpCallInstSize(*immSize, type);
+    return relOffset;
+}
+
+/*!
+\brief generate a single native instruction "jcc imm" to jump to a label
+
+*/
+void conditional_jump(ConditionCode cc, const char* target, bool isShortTerm) {
+    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
+        condJumpToBasicBlock(stream, cc, currentExceptionBlockIdx);
+        return;
+    }
+    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
+    bool unknown;
+    OpndSize size = OpndSize_Null;
+    int imm = 0;
+    if(!gDvmJit.scheduling)
+        imm = getRelativeOffset(target, isShortTerm, JmpCall_cond, &unknown, &size);
+    dump_label(m, size, imm, target, isShortTerm);
+}
+
+/*!
+\brief generate a single native instruction "jmp imm" to jump to a label
+
+If the target is ".invokeArgsDone" and mode is NCG O1, extra work is performed to dump content of virtual registers to memory.
+*/
+void unconditional_jump(const char* target, bool isShortTerm) {
+    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
+        jumpToBasicBlock(stream, currentExceptionBlockIdx);
+        return;
+    }
+    Mnemonic m = Mnemonic_JMP;
+    bool unknown;
+    OpndSize size = OpndSize_Null;
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        //for other three labels used by JIT: invokeArgsDone_formal, _native, _jit
+        if(!strncmp(target, ".invokeArgsDone", 15)) {
+            touchEcx(); //keep ecx live, if ecx was spilled, it is loaded here
+            beforeCall(target); //
+        }
+        if(!strcmp(target, ".invokeArgsDone")) {
+            nextVersionOfHardReg(PhysicalReg_EDX, 1); //edx will be used in a function
+            call("ncgGetEIP"); //must be immediately before JMP
+        }
+    }
+    int imm = 0;
+    if(!gDvmJit.scheduling)
+        imm = getRelativeOffset(target, isShortTerm, JmpCall_uncond, &unknown, &size);
+    dump_label(m, size, imm, target, isShortTerm);
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        if(!strncmp(target, ".invokeArgsDone", 15)) {
+            afterCall(target); //un-spill before executing the next bytecode
+        }
+    }
+}
+/*!
+\brief generate a single native instruction "jcc imm"
+
+*/
+void conditional_jump_int(ConditionCode cc, int target, OpndSize size) {
+    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
+    dump_imm(m, size, target);
+}
+/*!
+\brief generate a single native instruction "jmp imm"
+
+*/
+void unconditional_jump_int(int target, OpndSize size) {
+    Mnemonic m = Mnemonic_JMP;
+    dump_imm(m, size, target);
+}
+
+//! Used to generate a single native instruction for conditionally
+//! jumping to a block when the immediate is not yet known.
+//! This should only be used when instruction scheduling is enabled.
+//! \param cc type of conditional jump
+//! \param targetBlockId id of MIR basic block
+//! \param immediateNeedsAligned Whether the immediate needs to be aligned
+//! within 16-bytes
+void conditional_jump_block(ConditionCode cc, int targetBlockId,
+        bool immediateNeedsAligned) {
+    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
+    dump_blockid_imm(m, targetBlockId, immediateNeedsAligned);
+}
+
+//! Used to generate a single native instruction for unconditionally
+//! jumping to a block when the immediate is not yet known.
+//! This should only be used when instruction scheduling is enabled.
+//! \param targetBlockId id of MIR basic block
+//! \param immediateNeedsAligned Whether the immediate needs to be aligned
+//! within 16-bytes
+void unconditional_jump_block(int targetBlockId, bool immediateNeedsAligned) {
+    Mnemonic m = Mnemonic_JMP;
+    dump_blockid_imm(m, targetBlockId, immediateNeedsAligned);
+}
+
+/*!
+\brief generate a single native instruction "jmp reg"
+
+*/
+void unconditional_jump_reg(int reg, bool isPhysical) {
+    dump_reg(Mnemonic_JMP, ATOM_NORMAL, OpndSize_32, reg, isPhysical, LowOpndRegType_gp);
+}
+
+/*!
+\brief generate a single native instruction to call a function
+
+If mode is NCG O1, extra work is performed to dump content of virtual registers to memory.
+*/
+void call(const char* target) {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        beforeCall(target);
+    }
+    Mnemonic m = Mnemonic_CALL;
+    bool dummy;
+    OpndSize size = OpndSize_Null;
+    int relOffset = 0;
+    if(!gDvmJit.scheduling)
+        relOffset = getRelativeOffset(target, false, JmpCall_call, &dummy, &size);
+    dump_label(m, size, relOffset, target, false);
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        afterCall(target);
+    }
+}
+/*!
+\brief generate a single native instruction to call a function
+
+*/
+void call_reg(int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_CALL;
+    dump_reg(m, ATOM_NORMAL, OpndSize_32, reg, isPhysical, LowOpndRegType_gp);
+}
+void call_reg_noalloc(int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_CALL;
+    dump_reg_noalloc(m, OpndSize_32, reg, isPhysical, LowOpndRegType_gp);
+}
+
+/*!
+\brief generate a single native instruction to call a function
+
+*/
+void call_mem(int disp, int reg, bool isPhysical) {
+    Mnemonic m = Mnemonic_CALL;
+    dump_mem(m, ATOM_NORMAL, OpndSize_32, disp, reg, isPhysical);
+}
+
+/*!
+\brief insert an entry to globalNCGWorklist
+
+*/
+int insertNCGWorklist(s4 relativePC, OpndSize immSize) {
+    int offsetNCG2 = stream - streamMethodStart;
+#ifdef DEBUG_NCG
+    ALOGI("Insert NCGWorklist (goto forward) @ %p offsetPC %x relativePC %x offsetNCG %x", stream, offsetPC, relativePC, offsetNCG2);
+#endif
+    NCGWorklist* item = (NCGWorklist*)malloc(sizeof(NCGWorklist));
+    if(item == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertNCGWorklist");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    item->relativePC = relativePC;
+    item->offsetPC = offsetPC;
+    item->offsetNCG = offsetNCG2;
+    item->codePtr = stream;
+    item->size = immSize;
+    item->nextItem = globalNCGWorklist;
+    globalNCGWorklist = item;
+    return 0;
+}
+
+
+/*
+ *search globalNCGWorklist to find the jmp/jcc offset address
+ */
+char* searchNCGWorklist(int blockId) {
+    NCGWorklist* ptr = globalNCGWorklist;
+    unsigned instSize;
+
+    while (ptr != NULL) {
+      if (blockId == ptr->relativePC) {
+           instSize = encoder_get_inst_size(ptr->codePtr);
+           assert((uint)(ptr->codePtr + instSize - 4) % 16 <= 12);
+           return (ptr->codePtr + instSize - 4); // 32bit relative offset
+       }
+       ptr = ptr->nextItem;
+    }
+#ifdef DEBUG_NCG
+    ALOGI("can't find item for blockId %d in searchNCGWorklist\n", blockId);
+#endif
+    return NULL;
+}
+
+#ifdef ENABLE_TRACING
+int insertMapWorklist(s4 BCOffset, s4 NCGOffset, int isStartOfPC) {
+    return 0;
+}
+#endif
+/*!
+\brief insert an entry to methodDataWorklist
+
+This function is used by bytecode FILL_ARRAY_DATA, PACKED_SWITCH, SPARSE_SWITCH
+*/
+int insertDataWorklist(s4 relativePC, char* codePtr1) {
+    //insert according to offsetPC+relativePC, smallest at the head
+    DataWorklist* item = (DataWorklist*)malloc(sizeof(DataWorklist));
+    if(item == NULL) {
+        ALOGI("JIT_INFO: Memory allocation failed at insertDataWorklist");
+        SET_JIT_ERROR(kJitErrorMallocFailed);
+        return -1;
+    }
+    item->relativePC = relativePC;
+    item->offsetPC = offsetPC;
+    item->codePtr = codePtr1;
+    item->codePtr2 = stream; //jump_reg for switch
+    DataWorklist* ptr = methodDataWorklist;
+    DataWorklist* prev_ptr = NULL;
+    while(ptr != NULL) {
+        int tmpPC = ptr->offsetPC + ptr->relativePC;
+        int tmpPC2 = relativePC + offsetPC;
+        if(tmpPC2 < tmpPC) {
+            break;
+        }
+        prev_ptr = ptr;
+        ptr = ptr->nextItem;
+    }
+    //insert item before ptr
+    if(prev_ptr != NULL) {
+        prev_ptr->nextItem = item;
+    }
+    else methodDataWorklist = item;
+    item->nextItem = ptr;
+    return 0;
+}
+
+/*!
+\brief work on globalNCGWorklist
+
+*/
+int performNCGWorklist() {
+    NCGWorklist* ptr = globalNCGWorklist;
+    while(ptr != NULL) {
+        int tmpNCG = getLabelOffset (ptr->relativePC);
+        ALOGV("Perform NCG worklist: @ %p target block %d target NCG %x",
+             ptr->codePtr, ptr->relativePC, tmpNCG);
+        assert(tmpNCG >= 0);
+        int relativeNCG = tmpNCG - ptr->offsetNCG;
+        unsigned instSize = encoder_get_inst_size(ptr->codePtr);
+        relativeNCG -= instSize;
+        updateJumpInst(ptr->codePtr, ptr->size, relativeNCG);
+        globalNCGWorklist = ptr->nextItem;
+        free(ptr);
+        ptr = globalNCGWorklist;
+    }
+    return 0;
+}
+void freeNCGWorklist() {
+    NCGWorklist* ptr = globalNCGWorklist;
+    while(ptr != NULL) {
+        globalNCGWorklist = ptr->nextItem;
+        free(ptr);
+        ptr = globalNCGWorklist;
+    }
+}
+
+/*!
+\brief used by bytecode SWITCH
+\param targetPC points to start of the data section
+\param codeInst the code instruction pointer
+\return the offset in native code between add_reg_reg and the data section
+
+Code sequence for SWITCH
+  call ncgGetEIP
+  codeInst: add_reg_reg %eax, %edx
+  jump_reg %edx
+This function returns the offset in native code between add_reg_reg and the data section
+*/
+int getRelativeNCGForSwitch(int targetPC, char* codeInst) {
+    int tmpNCG = mapFromBCtoNCG[targetPC];
+    int offsetNCG2 = codeInst - streamMethodStart;
+    int relativeOff = tmpNCG - offsetNCG2;
+    return relativeOff;
+}
+
+/*!
+\brief work on methodDataWorklist
+*/
+int performDataWorklist(void) {
+    DataWorklist* ptr = methodDataWorklist;
+    if(ptr == NULL) return 0;
+
+    char* codeCacheEnd = ((char *) gDvmJit.codeCache) + gDvmJit.codeCacheSize - CODE_CACHE_PADDING;
+    u2 insnsSize = dvmGetMethodInsnsSize(currentMethod); //bytecode
+    //align stream to multiple of 4
+    int alignBytes = (int)stream & 3;
+    if(alignBytes != 0) alignBytes = 4-alignBytes;
+    stream += alignBytes;
+
+    while(ptr != NULL) {
+        int tmpPC = ptr->offsetPC + ptr->relativePC;
+        int endPC = insnsSize;
+        if(ptr->nextItem != NULL) endPC = ptr->nextItem->offsetPC + ptr->nextItem->relativePC;
+        mapFromBCtoNCG[tmpPC] = stream - streamMethodStart; //offsetNCG in byte
+
+        //handle fill_array_data, packed switch & sparse switch
+        u2 tmpInst = *(currentMethod->insns + ptr->offsetPC);
+        u2* sizePtr;
+        s4* entryPtr_bytecode;
+        u2 tSize, iVer;
+        u4 sz;
+
+        if (gDvmJit.codeCacheFull == true) {
+            // We are out of code cache space. Skip writing data/code to
+            //   code cache. Simply free the item.
+            methodDataWorklist = ptr->nextItem;
+            free(ptr);
+            ptr = methodDataWorklist;
+        }
+
+        switch (INST_INST(tmpInst)) {
+        case OP_FILL_ARRAY_DATA:
+            sz = (endPC-tmpPC)*sizeof(u2);
+            if ((stream + sz) < codeCacheEnd) {
+                memcpy(stream, (u2*)currentMethod->insns+tmpPC, sz);
+#ifdef DEBUG_NCG_CODE_SIZE
+                ALOGI("Copy data section to stream %p: start at %d, %d bytes", stream, tmpPC, sz);
+#endif
+#ifdef DEBUG_NCG
+                ALOGI("Update data section at %p with %d", ptr->codePtr, stream-ptr->codePtr);
+#endif
+                updateImmRMInst(ptr->codePtr, "", stream - ptr->codePtr);
+                stream += sz;
+            } else {
+                gDvmJit.codeCacheFull = true;
+            }
+            break;
+        case OP_PACKED_SWITCH:
+            updateImmRMInst(ptr->codePtr, "", stream-ptr->codePtr);
+            sizePtr = (u2*)currentMethod->insns+tmpPC + 1 /*signature*/;
+            entryPtr_bytecode = (s4*)(sizePtr + 1 /*size*/ + 2 /*firstKey*/);
+            tSize = *(sizePtr);
+            sz = tSize * 4;     /* expected size needed in stream */
+            if ((stream + sz) < codeCacheEnd) {
+                for(iVer = 0; iVer < tSize; iVer++) {
+                    //update entries
+                    s4 relativePC = *entryPtr_bytecode; //relative to ptr->offsetPC
+                    //need stream, offsetPC,
+                    int relativeNCG = getRelativeNCGForSwitch(relativePC+ptr->offsetPC, ptr->codePtr2);
+#ifdef DEBUG_NCG_CODE_SIZE
+                    ALOGI("Convert target from %d to %d", relativePC+ptr->offsetPC, relativeNCG);
+#endif
+                    *((s4*)stream) = relativeNCG;
+                    stream += 4;
+                    entryPtr_bytecode++;
+                }
+            } else {
+                gDvmJit.codeCacheFull = true;
+            }
+            break;
+        case OP_SPARSE_SWITCH:
+            updateImmRMInst(ptr->codePtr, "", stream-ptr->codePtr);
+            sizePtr = (u2*)currentMethod->insns+tmpPC + 1 /*signature*/;
+            s4* keyPtr_bytecode = (s4*)(sizePtr + 1 /*size*/);
+            tSize = *(sizePtr);
+            entryPtr_bytecode = (s4*)(keyPtr_bytecode + tSize);
+            sz = tSize * (sizeof(s4) + 4); /* expected size needed in stream */
+            if ((stream + sz) < codeCacheEnd) {
+                memcpy(stream, keyPtr_bytecode, tSize*sizeof(s4));
+                stream += tSize*sizeof(s4);
+                for(iVer = 0; iVer < tSize; iVer++) {
+                    //update entries
+                    s4 relativePC = *entryPtr_bytecode; //relative to ptr->offsetPC
+                    //need stream, offsetPC,
+                    int relativeNCG = getRelativeNCGForSwitch(relativePC+ptr->offsetPC, ptr->codePtr2);
+                    *((s4*)stream) = relativeNCG;
+                    stream += 4;
+                    entryPtr_bytecode++;
+                }
+            } else {
+                gDvmJit.codeCacheFull = true;
+            }
+            break;
+        }
+
+        //remove the item
+        methodDataWorklist = ptr->nextItem;
+        free(ptr);
+        ptr = methodDataWorklist;
+    }
+    return 0;
+}
+void freeDataWorklist() {
+    DataWorklist* ptr = methodDataWorklist;
+    while(ptr != NULL) {
+        methodDataWorklist = ptr->nextItem;
+        free(ptr);
+        ptr = methodDataWorklist;
+    }
+}
+
+//////////////////////////
+/*!
+\brief check whether a branch target (specified by relative offset in bytecode) is already handled, if yes, return the size of the immediate; otherwise, call insertNCGWorklist.
+
+If the branch target is not handled, call insertNCGWorklist, unknown is set to true, immSize is set to 32.
+
+If the branch target is handled, call estOpndSizeFromImm to set immSize for jump instruction, returns the value of the immediate
+*/
+int getRelativeNCG(s4 tmp, JmpCall_type type, bool* unknown, OpndSize* size) {//tmp: relativePC
+    int tmpNCG = getLabelOffset (tmp);
+
+    *unknown = false;
+    if(tmpNCG <0) {
+        *unknown = true;
+#ifdef SUPPORT_IMM_16
+        *size = OpndSize_16;
+#else
+        *size = OpndSize_32;
+#endif
+        insertNCGWorklist(tmp, *size);
+        return 0;
+    }
+    int offsetNCG2 = stream - streamMethodStart;
+#ifdef DEBUG_NCG
+    ALOGI("Goto backward @ %p offsetPC %d relativePC %d offsetNCG %d relativeNCG %d", stream, offsetPC, tmp, offsetNCG2, tmpNCG-offsetNCG2);
+#endif
+    int relativeOff = tmpNCG - offsetNCG2;
+    *size = estOpndSizeFromImm(relativeOff);
+    return relativeOff - getJmpCallInstSize(*size, type);
+}
+/*!
+\brief a helper function to handle backward branch
+
+input: jump target in %eax; at end of the function, jump to %eax
+*/
+int common_backwardBranch() {
+    if (insertLabel("common_backwardBranch", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+     int startStreamPtr = (int)stream;
+#endif
+
+    spill_reg(PhysicalReg_EAX, true);
+    call("common_periodicChecks_entry");
+    unspill_reg(PhysicalReg_EAX, true);
+    unconditional_jump_reg(PhysicalReg_EAX, true);
+
+#if defined(VTUNE_DALVIK)
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_backwardBranch");
+    }
+#endif
+    return 0;
+}
+#if !defined(WITH_JIT)
+/*!
+\brief common code to handle GOTO
+
+If it is a backward branch, call common_periodicChecks4 to handle GC request.
+Since this is the end of a basic block, globalVREndOfBB are called right before the jump instruction.
+*/
+int common_goto(s4 tmp) { //tmp: relativePC
+    int retCode = 0;
+    if(tmp < 0) {
+#ifdef ENABLE_TRACING
+#if !defined(TRACING_OPTION2)
+        insertMapWorklist(offsetPC + tmp, mapFromBCtoNCG[offsetPC+tmp], 1);
+#endif
+        //(target offsetPC * 2)
+        move_imm_to_reg(OpndSize_32, 2*(offsetPC+tmp), PhysicalReg_EDX, true);
+#endif
+        //call( ... ) will dump VRs to memory first
+        //potential garbage collection will work as designed
+        call_helper_API("common_periodicChecks4");
+    }
+    retCode = handleRegistersEndOfBB(true);
+    if (retCode < 0)
+        return retCode;
+    bool unknown;
+    OpndSize size;
+    int relativeNCG = tmp;
+    if(!gDvmJit.scheduling)
+        relativeNCG = getRelativeNCG(tmp, JmpCall_uncond, &unknown, &size);
+    unconditional_jump_int(relativeNCG, size);
+    return 0;
+}
+//the common code to lower a if bytecode
+int common_if(s4 tmp, ConditionCode cc_next, ConditionCode cc_taken) {
+    if(tmp < 0) { //backward
+        conditional_jump(cc_next, ".if_next", true);
+        common_goto(tmp);
+        if (insertLabel(".if_next", true) == -1)
+            return -1;
+    }
+    else {
+        //if(tmp < 0) ALOGI("skip periodicCheck for if");
+        bool unknown;
+        OpndSize size;
+        int relativeNCG = tmp;
+        if(!gDvmJit.scheduling)
+            relativeNCG = getRelativeNCG(tmp, JmpCall_cond, &unknown, &size); //must be known
+        conditional_jump_int(cc_taken, relativeNCG, size); //CHECK
+    }
+    return 0;
+}
+#else
+
+//! \brief common code to handle GOTO
+//!
+//! \details If it is a backward branch, call common_periodicChecks4
+//! to handle GC request.
+//! Since this is the end of a basic block,
+//! globalVREndOfBB is called right before the jump instruction.
+//! when this is called from JIT, there is no need to check GC
+//!
+//! \param targetBlockId
+//!
+//! \return -1 if error
+int common_goto(s4 targetBlockId) {
+    bool unknown;
+    int retCode = 0;
+    OpndSize size;
+    bool needAlignment = doesJumpToBBNeedAlignment(traceCurrentBB->taken);
+
+    // We call it with true because we want to actually want to update
+    // association tables of children and handle ME spill requests
+    retCode = handleRegistersEndOfBB (true);
+    if (retCode < 0)
+    {
+        //Just pass along error information
+        return retCode;
+    }
+
+    if(gDvmJit.scheduling) {
+        // Assuming that gotos never go to chaining cells because they are not
+        // part of the bytecode and are just for trace transitions
+        unconditional_jump_block((int)targetBlockId, needAlignment);
+    } else {
+        if (needAlignment == true) {
+            alignOffset(1);
+        }
+        int relativeNCG = getRelativeNCG(targetBlockId, JmpCall_uncond, &unknown, &size);
+        unconditional_jump_int(relativeNCG, size);
+    }
+    return 1;
+}
+
+int generateConditionalJumpToTakenBlock (ConditionCode takenCondition)
+{
+    // A basic block whose last bytecode is "if" must have two children
+    assert (traceCurrentBB->taken != NULL);
+    assert (traceCurrentBB->fallThrough != NULL);
+
+    BasicBlock_O1 * takenBB =
+            reinterpret_cast<BasicBlock_O1 *>(traceCurrentBB->taken);
+    BasicBlock_O1 * fallThroughBB =
+            reinterpret_cast<BasicBlock_O1 *>(traceCurrentBB->fallThrough);
+
+    // When assert version is disabled, fallthroughBB is not used
+    (void) fallThroughBB;
+
+    // We should always have a pre backward block before backward chaining cell
+    // so we can assert that here.
+    if (takenBB->blockType == kChainingCellBackwardBranch)
+    {
+        ALOGI("JIT_INFO: No pre-backward on taken branch");
+        SET_JIT_ERROR(kJitErrorTraceFormation);
+        return -1;
+    }
+
+    if (fallThroughBB->blockType == kChainingCellBackwardBranch)
+    {
+        ALOGI("JIT_INFO: No pre-backward on fallThrough branch");
+        SET_JIT_ERROR(kJitErrorTraceFormation);
+        return -1;
+    }
+
+    // The prebackward block should always be the taken branch
+    if (fallThroughBB->blockType == kPreBackwardBlock)
+    {
+        ALOGI("JIT_INFO: Pre-backward branch is fallThrough");
+        SET_JIT_ERROR(kJitErrorTraceFormation);
+        return -1;
+    }
+
+    // Since we have reached the end of basic block, let's handle registers at
+    // end of BB without actually syncing the state. We sync the state below
+    // when we handle each child
+    handleRegistersEndOfBB (false);
+
+    // So if we have a Prebackward block, we need to satisfy associations
+    // of loop entry
+    if (takenBB->blockType == kPreBackwardBlock)
+    {
+        // The child of the prebackward block should always be backward
+        // chaining cell so it should never be null.
+        assert (takenBB->fallThrough != 0);
+
+        BasicBlock_O1 * backward =
+                reinterpret_cast<BasicBlock_O1 *> (takenBB->fallThrough);
+
+        //This must be a backward branch chaining cell
+        assert (backward->blockType == kChainingCellBackwardBranch);
+
+        //Backward CC must always have as child the loop entry
+        assert (backward->fallThrough != 0);
+
+        //Get the child
+        BasicBlock_O1 * loopEntry =
+                reinterpret_cast<BasicBlock_O1 *> (backward->fallThrough);
+
+        //Paranoid. We want to make sure that the loop entry has been
+        //already handled.
+        if (loopEntry->associationTable.hasBeenFinalized() == false)
+        {
+            ALOGI("JIT_INFO: Loop entry still not finalized at common_if");
+            SET_JIT_ERROR(kJitErrorTraceFormation);
+            return -1;
+        }
+
+        //Just in case the current BB has any spill requests, let's handle them
+        //before we satisfy BB associations
+        if (AssociationTable::handleSpillRequestsFromME (currentBB) == false)
+        {
+            return -1;
+        }
+
+        //Now we want to satisfy the associations of the loop entry.
+        //We also inform satisfyBBAssociations that this is a backward branch.
+        if (AssociationTable::satisfyBBAssociations (backward, loopEntry,
+                true) == false)
+        {
+            return -1;
+        }
+    }
+
+    // First sync with the taken child
+    if (AssociationTable::createOrSyncTable (currentBB, false) == false)
+    {
+        return -1;
+    }
+
+    if (gDvmJit.scheduling)
+    {
+        conditional_jump_block (takenCondition, takenBB->id, doesJumpToBBNeedAlignment (takenBB));
+    }
+    else
+    {
+        //Conditional jumps in x86 are 2 bytes
+        alignOffset (2);
+
+        bool unknown;
+        OpndSize size = OpndSize_Null;
+        int relativeNCG = getRelativeNCG (takenBB->id, JmpCall_cond, &unknown, &size);
+        conditional_jump_int (takenCondition, relativeNCG, size);
+    }
+
+    // Now sync with the fallthrough child
+    if (AssociationTable::createOrSyncTable (currentBB, true) == false)
+    {
+        return -1;
+    }
+
+    // Return success
+    return 1;
+}
+#endif
+
+/*!
+\brief helper function to handle null object error
+
+*/
+int common_errNullObject() {
+    if (insertLabel("common_errNullObject", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    move_imm_to_reg(OpndSize_32, (int) gDvm.exNullPointerException, PhysicalReg_ECX, true);
+    unconditional_jump("common_throw", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errNullObject");
+    }
+#endif
+
+    return 0;
+}
+/*!
+\brief helper function to handle string index error
+
+*/
+int common_errStringIndexOutOfBounds() {
+    if (insertLabel("common_errStringIndexOutOfBounds", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    move_imm_to_reg(OpndSize_32, (int)gDvm.exStringIndexOutOfBoundsException, PhysicalReg_ECX, true);
+    unconditional_jump("common_throw", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errStringIndexOutOfBounds");
+    }
+#endif
+    return 0;
+}
+
+/*!
+\brief helper function to handle array index error
+
+*/
+int common_errArrayIndex() {
+    if (insertLabel("common_errArrayIndex", false) == -1)
+        return -1;
+
+    //Get call back
+    void (*backEndSymbolCreationCallback) (const char *, void *) =
+        gDvmJit.jitFramework.backEndSymbolCreationCallback;
+
+    if (backEndSymbolCreationCallback != 0)
+    {
+        backEndSymbolCreationCallback ("common_errArrayIndex", (void*) stream);
+    }
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    move_imm_to_reg(OpndSize_32, LstrArrayIndexException, PhysicalReg_ECX, true);
+    unconditional_jump("common_throw", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errArrayIndex");
+    }
+#endif
+    return 0;
+}
+/*!
+\brief helper function to handle array store error
+
+*/
+int common_errArrayStore() {
+    if (insertLabel("common_errArrayStore", false) == -1)
+        return -1;
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    move_imm_to_reg(OpndSize_32, LstrArrayStoreException, PhysicalReg_ECX, true);
+    unconditional_jump("common_throw", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errArrayStore");
+    }
+#endif
+    return 0;
+}
+/*!
+\brief helper function to handle negative array size error
+
+*/
+int common_errNegArraySize() {
+    if (insertLabel("common_errNegArraySize", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    move_imm_to_reg(OpndSize_32, LstrNegativeArraySizeException, PhysicalReg_ECX, true);
+    unconditional_jump("common_throw", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errNegArraySize");
+    }
+#endif
+    return 0;
+}
+/*!
+\brief helper function to handle divide-by-zero error
+
+*/
+int common_errDivideByZero() {
+    if (insertLabel("common_errDivideByZero", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+    move_imm_to_reg(OpndSize_32, LstrDivideByZero, PhysicalReg_EAX, true);
+    move_imm_to_reg(OpndSize_32, LstrArithmeticException, PhysicalReg_ECX, true);
+    unconditional_jump("common_throw", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errDivideByZero");
+    }
+#endif
+    return 0;
+}
+/*!
+\brief helper function to handle no such method error
+
+*/
+int common_errNoSuchMethod() {
+    if (insertLabel("common_errNoSuchMethod", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    move_imm_to_reg(OpndSize_32, LstrNoSuchMethodError, PhysicalReg_ECX, true);
+    unconditional_jump("common_throw", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_errNoSuchMethod");
+    }
+#endif
+    return 0;
+}
+int call_dvmFindCatchBlock();
+
+#define P_GPR_1 PhysicalReg_ESI //self callee-saved
+#define P_GPR_2 PhysicalReg_EBX //exception callee-saved
+#define P_GPR_3 PhysicalReg_EAX //method that caused exception
+/*!
+\brief helper function common_exceptionThrown
+
+*/
+int common_exceptionThrown() {
+    if (insertLabel("common_exceptionThrown", false) == -1)
+        return -1;
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmJitToExceptionThrown;
+    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
+    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_exceptionThrown");
+    }
+#endif
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+
+/*!
+\brief helper function to throw an exception with message
+
+INPUT: obj_reg(%eax), exceptionPtrReg(%ecx)
+SCRATCH: C_SCRATCH_1(%esi) & C_SCRATCH_2(%edx)
+OUTPUT: no
+*/
+int throw_exception_message(int exceptionPtrReg, int obj_reg, bool isPhysical,
+                            int startLR/*logical register index*/, bool startPhysical) {
+    if (insertLabel("common_throw_message", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+ #endif
+    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EDX;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), obj_reg, isPhysical, C_SCRATCH_1, isScratchPhysical);
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ClassObject, descriptor), C_SCRATCH_1, isScratchPhysical, C_SCRATCH_2, isScratchPhysical);
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, C_SCRATCH_2, isScratchPhysical, 4, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, exceptionPtrReg, true, 0, PhysicalReg_ESP, true);
+    call_dvmThrowWithMessage();
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    unconditional_jump("common_exceptionThrown", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_throw_message");
+    }
+#endif
+    return 0;
+}
+/*!
+\brief helper function to throw an exception
+
+scratch: C_SCRATCH_1(%edx)
+*/
+int throw_exception(int exceptionPtrReg, int immReg,
+                    int startLR/*logical register index*/, bool startPhysical) {
+    if (insertLabel("common_throw", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+    scratchRegs[0] = PhysicalReg_EDX; scratchRegs[1] = PhysicalReg_Null;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, immReg, true, 4, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, exceptionPtrReg, true, 0, PhysicalReg_ESP, true);
+    call_dvmThrow();
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    unconditional_jump("common_exceptionThrown", false);
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_throw");
+    }
+#endif
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode goto
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_goto(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_GOTO);
+#if !defined(WITH_JIT)
+    u2 tt = INST_AA(inst);
+    s2 tmp = (s2)((s2)tt << 8) >> 8; //00AA --> AA00 --> xxAA
+#else
+    s2 tmp = traceCurrentBB->taken->id;
+#endif
+    int retval = common_goto(tmp);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode goto/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_goto_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_GOTO_16);
+#if !defined(WITH_JIT)
+    s2 tmp = (s2)FETCH(1);
+#else
+    s2 tmp = traceCurrentBB->taken->id;
+#endif
+    int retval = common_goto(tmp);
+    return retval;
+}
+
+/**
+ * @brief Generate native code for bytecode goto/32
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_goto_32(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_GOTO_32);
+#if !defined(WITH_JIT)
+    u4 tmp = (u4)FETCH(1);
+    tmp |= (u4)FETCH(2) << 16;
+#else
+    s2 tmp = traceCurrentBB->taken->id;
+#endif
+    int retval = common_goto((s4)tmp);
+    return retval;
+}
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecode packed-switch
+ * @param mir bytecode representation
+ * @param dalvikPC program counter for Dalvik bytecode
+ * @return value >= 0 when handled
+ */
+int op_packed_switch(const MIR * mir, const u2 * dalvikPC) {
+    int retCode = 0;
+    assert(mir->dalvikInsn.opcode == OP_PACKED_SWITCH);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+
+#ifdef DEBUG_EACH_BYTECODE
+    u2 tSize = 0;
+    s4 firstKey = 0;
+    s4* entries = NULL;
+#else
+    u2* switchData = const_cast<u2 *>(dalvikPC) + (s4)tmp;
+    if (*switchData++ != kPackedSwitchSignature) {
+        /* should have been caught by verifier */
+        dvmThrowInternalError(
+                          "bad packed switch magic");
+        return 0; //no_op
+    }
+    u2 tSize = *switchData++;
+    assert(tSize > 0);
+    s4 firstKey = *switchData++;
+    firstKey |= (*switchData++) << 16;
+    s4* entries = (s4*) switchData;
+    assert(((u4)entries & 0x3) == 0);
+#endif
+
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    //dvmNcgHandlePackedSwitch: testVal, size, first_key, targets
+    load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, tSize, 8, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, firstKey, 4, PhysicalReg_ESP, true);
+
+    /* "entries" is constant for JIT
+       it is the 1st argument to dvmJitHandlePackedSwitch */
+    move_imm_to_mem(OpndSize_32, (int)entries, 0, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 1, false, 12, PhysicalReg_ESP, true);
+
+    //We are done using the VRs and it is end of BB, so we handle it right now
+    retCode = handleRegistersEndOfBB (true);
+    if (retCode < 0)
+    {
+        //Just pass along error information
+        return retCode;
+    }
+
+    //if value out of range, fall through (no_op)
+    //return targets[testVal - first_key]
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    call_dvmJitHandlePackedSwitch();
+    load_effective_addr(16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+    //get rPC, %eax has the relative PC offset
+    alu_binary_imm_reg(OpndSize_32, add_opc, (int)dalvikPC, PhysicalReg_EAX, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_2;
+#if defined(WITH_JIT_TUNING)
+    /* Fall back to interpreter after resolving address of switch target.
+     * Indicate a kSwitchOverflow. Note: This is not an "overflow". But it helps
+     * count the times we return from a Switch
+     */
+    move_imm_to_mem(OpndSize_32, kSwitchOverflow, 0, PhysicalReg_ESP, true);
+#endif
+    jumpToInterpNoChain();
+    return 0;
+}
+#undef P_GPR_1
+
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecode sparse-switch
+ * @param mir bytecode representation
+ * @param dalvikPC program counter for Dalvik bytecode
+ * @return value >= 0 when handled
+ */
+int op_sparse_switch(const MIR * mir, const u2 * dalvikPC) {
+    int retCode = 0;
+    assert(mir->dalvikInsn.opcode == OP_SPARSE_SWITCH);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+#ifdef DEBUG_EACH_BYTECODE
+    u2 tSize = 0;
+    const s4* keys = NULL;
+    s4* entries = NULL;
+#else
+    u2* switchData = const_cast<u2 *>(dalvikPC) + (s4)tmp;
+
+    if (*switchData++ != kSparseSwitchSignature) {
+        /* should have been caught by verifier */
+        dvmThrowInternalError(
+                          "bad sparse switch magic");
+        return 0; //no_op
+    }
+    u2 tSize = *switchData++;
+    assert(tSize > 0);
+    const s4* keys = (const s4*) switchData;
+    assert(((u4)keys & 0x3) == 0);
+    assert((((u4) ((s4*) switchData + tSize)) & 0x3) == 0);
+#endif
+
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    //dvmNcgHandleSparseSwitch: keys, size, testVal
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, tSize, 4, PhysicalReg_ESP, true);
+
+    /* "keys" is constant for JIT
+       it is the 1st argument to dvmJitHandleSparseSwitch */
+    move_imm_to_mem(OpndSize_32, (int)keys, 0, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 1, false, 8, PhysicalReg_ESP, true);
+
+    //We are done using the VRs and it is end of BB, so we handle it right now
+    retCode = handleRegistersEndOfBB (true);
+    if (retCode < 0)
+    {
+        //Just pass along error information
+        return retCode;
+    }
+
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    //if testVal is in keys, return the corresponding target
+    //otherwise, fall through (no_op)
+    call_dvmJitHandleSparseSwitch();
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+    //get rPC, %eax has the relative PC offset
+    alu_binary_imm_reg(OpndSize_32, add_opc, (int)dalvikPC, PhysicalReg_EAX, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_2;
+#if defined(WITH_JIT_TUNING)
+    /* Fall back to interpreter after resolving address of switch target.
+     * Indicate a kSwitchOverflow. Note: This is not an "overflow". But it helps
+     * count the times we return from a Switch
+     */
+    move_imm_to_mem(OpndSize_32, kSwitchOverflow, 0, PhysicalReg_ESP, true);
+#endif
+    jumpToInterpNoChain();
+    return 0;
+}
+
+#undef P_GPR_1
+
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecode if-eq
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_eq(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_EQ);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    compare_VR_reg(OpndSize_32, vB, 1, false);
+
+    return generateConditionalJumpToTakenBlock (Condition_E);
+}
+
+/**
+ * @brief Generate native code for bytecode if-ne
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_ne(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_NE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    compare_VR_reg(OpndSize_32, vB, 1, false);
+
+    return generateConditionalJumpToTakenBlock (Condition_NE);
+}
+
+/**
+ * @brief Generate native code for bytecode if-lt
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_lt(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_LT);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    compare_VR_reg(OpndSize_32, vB, 1, false);
+
+    return generateConditionalJumpToTakenBlock (Condition_L);
+}
+
+/**
+ * @brief Generate native code for bytecode if-ge
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_ge(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_GE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    compare_VR_reg(OpndSize_32, vB, 1, false);
+
+    return generateConditionalJumpToTakenBlock (Condition_GE);
+}
+
+/**
+ * @brief Generate native code for bytecode if-gt
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_gt(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_GT);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    compare_VR_reg(OpndSize_32, vB, 1, false);
+
+    return generateConditionalJumpToTakenBlock (Condition_G);
+}
+
+/**
+ * @brief Generate native code for bytecode if-le
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_le(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_LE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    compare_VR_reg(OpndSize_32, vB, 1, false);
+
+    return generateConditionalJumpToTakenBlock (Condition_LE);
+}
+#undef P_GPR_1
+
+/**
+ * @brief Generate native code for bytecode if-eqz
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_eqz(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_EQZ);
+    int vA = mir->dalvikInsn.vA;
+
+    compare_imm_VR(OpndSize_32, 0, vA);
+
+    return generateConditionalJumpToTakenBlock (Condition_E);
+}
+
+/**
+ * @brief Generate native code for bytecode if-nez
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_nez(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_NEZ);
+    int vA = mir->dalvikInsn.vA;
+
+    compare_imm_VR(OpndSize_32, 0, vA);
+
+    return generateConditionalJumpToTakenBlock (Condition_NE);
+}
+
+/**
+ * @brief Generate native code for bytecode if-ltz
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_ltz(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_LTZ);
+    int vA = mir->dalvikInsn.vA;
+
+    compare_imm_VR(OpndSize_32, 0, vA);
+
+    return generateConditionalJumpToTakenBlock (Condition_L);
+}
+
+/**
+ * @brief Generate native code for bytecode if-gez
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_gez(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_GEZ);
+    int vA = mir->dalvikInsn.vA;
+
+    compare_imm_VR(OpndSize_32, 0, vA);
+
+    return generateConditionalJumpToTakenBlock (Condition_GE);
+}
+
+/**
+ * @brief Generate native code for bytecode if-gtz
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_gtz(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_GTZ);
+    int vA = mir->dalvikInsn.vA;
+
+    compare_imm_VR(OpndSize_32, 0, vA);
+
+    return generateConditionalJumpToTakenBlock (Condition_G);
+}
+
+/**
+ * @brief Generate native code for bytecode if-lez
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_if_lez(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_IF_LEZ);
+    int vA = mir->dalvikInsn.vA;
+
+    compare_imm_VR(OpndSize_32, 0, vA);
+
+    return generateConditionalJumpToTakenBlock (Condition_LE);
+}
+
+#define P_GPR_1 PhysicalReg_ECX
+#define P_GPR_2 PhysicalReg_EBX
+/*!
+\brief helper function common_periodicChecks4 to check GC request
+BCOffset in %edx
+*/
+int common_periodicChecks4() {
+    if (insertLabel("common_periodicChecks4", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+#if (!defined(ENABLE_TRACING))
+    get_self_pointer(PhysicalReg_ECX, true);
+    move_mem_to_reg(OpndSize_32, offsetof(Thread, suspendCount), PhysicalReg_ECX, true, PhysicalReg_EAX, true);
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //suspendCount
+    conditional_jump(Condition_NE, "common_handleSuspend4", true); //called once
+    x86_return();
+
+    if (insertLabel("common_handleSuspend4", true) == -1)
+        return -1;
+    push_reg_to_stack(OpndSize_32, PhysicalReg_ECX, true);
+    call_dvmCheckSuspendPending();
+    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    x86_return();
+
+#else
+    ///////////////////
+    //get debuggerActive: 3 memory accesses, and $7
+    move_mem_to_reg(OpndSize_32, offGlue_pSelfSuspendCount, PhysicalReg_Glue, true, P_GPR_1, true);
+    move_mem_to_reg(OpndSize_32, offGlue_pIntoDebugger, PhysicalReg_Glue, true, P_GPR_2, true);
+
+    compare_imm_mem(OpndSize_32, 0, 0, P_GPR_1, true); //suspendCount
+    conditional_jump(Condition_NE, "common_handleSuspend4_1", true); //called once
+
+    compare_imm_mem(OpndSize_32, 0, 0, P_GPR_2, true); //debugger active
+
+    conditional_jump(Condition_NE, "common_debuggerActive4", true);
+
+    //recover registers and return
+    x86_return();
+
+    if (insertLabel("common_handleSuspend4_1", true) == -1)
+        return -1;
+    push_mem_to_stack(OpndSize_32, offGlue_self, PhysicalReg_Glue, true);
+    call_dvmCheckSuspendPending();
+    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    x86_return();
+
+    if (insertLabel("common_debuggerActive4", true) == -1)
+        return -1;
+    //%edx: offsetBC (at run time, get method->insns_bytecode, then calculate BCPointer)
+    move_mem_to_reg(OpndSize_32, offGlue_method, PhysicalReg_Glue, true, P_GPR_1, true);
+    move_mem_to_reg(OpndSize_32, offMethod_insns_bytecode, P_GPR_1, true, P_GPR_2, true);
+    alu_binary_reg_reg(OpndSize_32, add_opc, P_GPR_2, true, PhysicalReg_EDX, true);
+    move_imm_to_mem(OpndSize_32, 0, offGlue_entryPoint, PhysicalReg_Glue, true);
+    unconditional_jump("common_gotoBail", false); //update glue->rPC with edx
+#endif
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_periodicChecks4");
+    }
+#endif
+    return 0;
+}
+//input: %edx PC adjustment
+//CHECK: should %edx be saved before calling dvmCheckSuspendPending?
+/*!
+\brief helper function common_periodicChecks_entry to check GC request
+
+*/
+int common_periodicChecks_entry() {
+    if (insertLabel("common_periodicChecks_entry", false) == -1)
+        return -1;
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+    scratchRegs[0] = PhysicalReg_ESI; scratchRegs[1] = PhysicalReg_EAX;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    get_suspendCount(P_GPR_1, true);
+
+    //get debuggerActive: 3 memory accesses, and $7
+#if 0 //defined(WITH_DEBUGGER)
+    get_debuggerActive(P_GPR_2, true);
+#endif
+
+    compare_imm_reg(OpndSize_32, 0, P_GPR_1, true); //suspendCount
+    conditional_jump(Condition_NE, "common_handleSuspend", true); //called once
+
+#if 0 //defined(WITH_DEBUGGER)
+#ifdef NCG_DEBUG
+    compare_imm_reg(OpndSize_32, 0, P_GPR_2, true); //debugger active
+    conditional_jump(Condition_NE, "common_debuggerActive", true);
+#endif
+#endif
+
+    //recover registers and return
+    x86_return();
+    if (insertLabel("common_handleSuspend", true) == -1)
+        return -1;
+    get_self_pointer(P_GPR_1, true);
+    load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, P_GPR_1, true, 0, PhysicalReg_ESP, true);
+    call_dvmCheckSuspendPending();
+    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    x86_return();
+#ifdef NCG_DEBUG
+    if (insertLabel("common_debuggerActive", true) == -1)
+        return -1;
+    //adjust PC!!! use 0(%esp) TODO
+    set_glue_entryPoint_imm(0); //kInterpEntryInstr);
+    unconditional_jump("common_gotoBail", false);
+#endif
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_periodicChecks_entry");
+    }
+#endif
+
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+/*!
+\brief helper function common_gotoBail
+  input: %edx: BCPointer %esi: Glue
+  set %eax to 1 (switch interpreter = true), recover the callee-saved registers and return
+*/
+int common_gotoBail(void) {
+    if (insertLabel("common_gotoBail", false) == -1)
+        return -1;
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+    //scratchRegs[0] = PhysicalReg_EDX; scratchRegs[1] = PhysicalReg_ESI;
+    //scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    get_self_pointer(PhysicalReg_EAX, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true, offsetof(Thread, interpSave.curFrame), PhysicalReg_EAX, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, offsetof(Thread, interpSave.pc), PhysicalReg_EAX, true);
+
+    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.bailPtr), PhysicalReg_EAX, true, PhysicalReg_ESP, true);
+    move_reg_to_reg(OpndSize_32, PhysicalReg_ESP, true, PhysicalReg_EBP, true);
+    load_effective_addr(FRAME_SIZE-4, PhysicalReg_EBP, true, PhysicalReg_EBP, true);
+    move_imm_to_reg(OpndSize_32, 1, PhysicalReg_EAX, true); //return value
+    move_mem_to_reg(OpndSize_32, -4, PhysicalReg_EBP, true, PhysicalReg_EDI, true);
+    move_mem_to_reg(OpndSize_32, -8, PhysicalReg_EBP, true, PhysicalReg_ESI, true);
+    move_mem_to_reg(OpndSize_32, -12, PhysicalReg_EBP, true, PhysicalReg_EBX, true);
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EBP, true, PhysicalReg_ESP, true);
+    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EBP, true);
+    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    x86_return();
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_gotoBail");
+    }
+#endif
+    return 0;
+}
+/*!
+\brief helper function common_gotoBail_0
+
+  set %eax to 0, recover the callee-saved registers and return
+*/
+int common_gotoBail_0(void) {
+    if (insertLabel("common_gotoBail_0", false) == -1)
+        return -1;
+
+    //Get call back
+    void (*backEndSymbolCreationCallback) (const char *, void *) =
+        gDvmJit.jitFramework.backEndSymbolCreationCallback;
+
+    if (backEndSymbolCreationCallback != 0)
+    {
+        backEndSymbolCreationCallback ("common_gotoBail_0", (void*) stream);
+    }
+
+#if defined VTUNE_DALVIK
+    int startStreamPtr = (int)stream;
+#endif
+
+    get_self_pointer(PhysicalReg_EAX, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_FP, true, offsetof(Thread, interpSave.curFrame), PhysicalReg_EAX, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EDX, true, offsetof(Thread, interpSave.pc), PhysicalReg_EAX, true);
+
+    /*
+    movl    offThread_bailPtr(%ecx),%esp # Restore "setjmp" esp
+    movl    %esp,%ebp
+    addl    $(FRAME_SIZE-4), %ebp       # Restore %ebp at point of setjmp
+    movl    EDI_SPILL(%ebp),%edi
+    movl    ESI_SPILL(%ebp),%esi
+    movl    EBX_SPILL(%ebp),%ebx
+    movl    %ebp, %esp                   # strip frame
+    pop     %ebp                         # restore caller's ebp
+    ret                                  # return to dvmMterpStdRun's caller
+    */
+    move_mem_to_reg(OpndSize_32, offsetof(Thread, interpSave.bailPtr), PhysicalReg_EAX, true, PhysicalReg_ESP, true);
+    move_reg_to_reg(OpndSize_32, PhysicalReg_ESP, true, PhysicalReg_EBP, true);
+    load_effective_addr(FRAME_SIZE-4, PhysicalReg_EBP, true, PhysicalReg_EBP, true);
+    move_imm_to_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //return value
+    move_mem_to_reg(OpndSize_32, -4, PhysicalReg_EBP, true, PhysicalReg_EDI, true);
+    move_mem_to_reg(OpndSize_32, -8, PhysicalReg_EBP, true, PhysicalReg_ESI, true);
+    move_mem_to_reg(OpndSize_32, -12, PhysicalReg_EBP, true, PhysicalReg_EBX, true);
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EBP, true, PhysicalReg_ESP, true);
+    move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EBP, true);
+    load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    x86_return();
+
+#if defined VTUNE_DALVIK
+    if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
+        int endStreamPtr = (int)stream;
+        sendLabelInfoToVTune(startStreamPtr, endStreamPtr, "common_gotoBail_0");
+    }
+#endif
+    return 0;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/LowerMove.cpp b/vm/compiler/codegen/x86/lightcg/LowerMove.cpp
new file mode 100644
index 0000000..df3d07c
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/LowerMove.cpp
@@ -0,0 +1,182 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file LowerMove.cpp
+    \brief This file lowers the following bytecodes: MOVE_XXX
+*/
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "enc_wrapper.h"
+
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecodes move and
+ * move-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE
+            || mir->dalvikInsn.opcode == OP_MOVE_OBJECT);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false/*isPhysical*/);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 2;
+}
+
+/**
+ * @brief Generate native code for bytecodes move/from16
+ * and move-object/from16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_from16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_FROM16
+            || mir->dalvikInsn.opcode == OP_MOVE_OBJECT_FROM16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 2;
+}
+
+/**
+ * @brief Generate native code for bytecodes move/16 and
+ * move-object/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_16
+            || mir->dalvikInsn.opcode == OP_MOVE_OBJECT_16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_32, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 2;
+}
+#undef P_GPR_1
+
+/**
+ * @brief Generate native code for bytecode move-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_WIDE);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_64, 1, false);
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 2;
+}
+
+/**
+ * @brief Generate native code for bytecode move-wide/from16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_wide_from16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_WIDE_FROM16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_64, 1, false);
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 2;
+}
+
+/**
+ * @brief Generate native code for bytecode move-wide/16
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_wide_16(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_WIDE_16);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    get_virtual_reg(vB, OpndSize_64, 1, false);
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 2;
+}
+
+/**
+ * @brief Generate native code for bytecodes move-result
+ * and move-result-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_result(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_RESULT
+            || mir->dalvikInsn.opcode == OP_MOVE_RESULT_OBJECT);
+
+    /* An inlined move result is effectively no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    int vA = mir->dalvikInsn.vA;
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    get_return_value(OpndSize_32, 1, false);
+    set_virtual_reg(vA, OpndSize_32, 1, false);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode move-result-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_result_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_RESULT_WIDE);
+
+    /* An inlined move result is effectively no-op */
+    if (mir->OptimizationFlags & MIR_INLINED)
+        return 0;
+
+    int vA = mir->dalvikInsn.vA;
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    get_return_value(OpndSize_64, 1, false);
+    set_virtual_reg(vA, OpndSize_64, 1, false);
+    return 0;
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+
+/**
+ * @brief Generate native code for bytecode move-exception
+ * @details Updates virtual register with exception from Thread and then
+ * clear the exception from Thread.
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_move_exception(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MOVE_EXCEPTION);
+    int vA = mir->dalvikInsn.vA;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_Null;
+    get_self_pointer(2, false);
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Thread, exception), 2, false, 3, false);
+    move_imm_to_mem(OpndSize_32, 0, OFFSETOF_MEMBER(Thread, exception), 2, false);
+    set_virtual_reg(vA, OpndSize_32, 3, false);
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+
diff --git a/vm/compiler/codegen/x86/lightcg/LowerObject.cpp b/vm/compiler/codegen/x86/lightcg/LowerObject.cpp
new file mode 100644
index 0000000..81f0402
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/LowerObject.cpp
@@ -0,0 +1,1035 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*! \file LowerObject.cpp
+    \brief This file lowers the following bytecodes: CHECK_CAST,
+*/
+#include "libdex/DexOpcodes.h"
+#include "libdex/DexFile.h"
+#include "Lower.h"
+#include "NcgAot.h"
+#include "enc_wrapper.h"
+
+extern void markCard_filled(int tgtAddrReg, bool isTgtPhysical, int scratchReg, bool isScratchPhysical);
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_ESI
+//! LOWER bytecode CHECK_CAST and INSTANCE_OF
+//!   CALL class_resolve (%ebx is live across the call)
+//!        dvmInstanceofNonTrivial
+//!   NO register is live through function check_cast_helper
+int check_cast_nohelper(int vA, u4 tmp, bool instance, int vDest) {
+    get_virtual_reg(vA, OpndSize_32, 1, false); //object
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    /* for trace-based JIT, it is likely that the class is already resolved */
+    bool needToResolve = true;
+    ClassObject *classPtr =
+                (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
+    ALOGV("In check_cast, class is resolved to %p", classPtr);
+    if(classPtr != NULL) {
+        needToResolve = false;
+        ALOGV("check_cast class %s", classPtr->descriptor);
+    }
+    if(needToResolve) {
+        //get_res_classes is moved here for NCG O1 to improve performance of GLUE optimization
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        get_res_classes(4, false);
+    }
+    compare_imm_reg(OpndSize_32, 0, 1, false);
+
+    rememberState(1);
+    //for private code cache, previously it jumped to .instance_of_okay_1
+    //if object reference is null, jump to the handler for this special case
+    if(instance) {
+        conditional_jump(Condition_E, ".instance_of_null", true);
+    }
+    else {
+        conditional_jump(Condition_E, ".check_cast_null", true);
+    }
+    //check whether the class is already resolved
+    //if yes, jump to check_cast_resolved
+    //if not, call class_resolve
+    if(needToResolve) {
+        move_mem_to_reg(OpndSize_32, tmp*4, 4, false, PhysicalReg_EAX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+        if(instance)
+            conditional_jump(Condition_NE, ".instance_of_resolved", true);
+        else
+            conditional_jump(Condition_NE, ".check_cast_resolved", true);
+        //try to resolve the class
+        rememberState(2);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        export_pc(); //trying to resolve the class
+        call_helper_API(".class_resolve");
+        transferToState(2);
+    } //needToResolve
+    else {
+        /* the class is already resolved and is constant */
+        move_imm_to_reg(OpndSize_32, (int)classPtr, PhysicalReg_EAX, true);
+    }
+    //class is resolved, and it is in %eax
+    if(!instance) {
+        if (insertLabel(".check_cast_resolved", true) == -1){
+            return -1;
+        }
+    }
+    else {
+        if (insertLabel(".instance_of_resolved", true) == -1) {
+            return -1;
+        }
+    }
+
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(Object, clazz), 1, false, 6, false); //object->clazz
+
+    //%eax: resolved class
+    //compare resolved class and object->clazz
+    //if the same, jump to the handler for this special case
+    compare_reg_reg(PhysicalReg_EAX, true, 6, false);
+    rememberState(3);
+    if(instance) {
+        conditional_jump(Condition_E, ".instance_of_equal", true);
+    } else {
+        conditional_jump(Condition_E, ".check_cast_equal", true);
+    }
+
+    //prepare to call dvmInstanceofNonTrivial
+    //INPUT: the resolved class & object reference
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 6, false, 0, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 4, PhysicalReg_ESP, true); //resolved class
+    scratchRegs[0] = PhysicalReg_SCRATCH_3;
+    nextVersionOfHardReg(PhysicalReg_EAX, 2); //next version has 2 refs
+    call_dvmInstanceofNonTrivial();
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    //
+    if(instance) {
+        //move return value to P_GPR_2
+        move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 3, false);
+        rememberState(4);
+        unconditional_jump(".instance_of_okay", true);
+    } else {
+        //if return value of dvmInstanceofNonTrivial is zero, throw exception
+        compare_imm_reg(OpndSize_32, 0,  PhysicalReg_EAX, true);
+        rememberState(4);
+        conditional_jump(Condition_NE, ".check_cast_okay", true);
+        //two inputs for common_throw_message: object reference in eax, exception pointer in ecx
+        nextVersionOfHardReg(PhysicalReg_EAX, 1); //next version has 1 ref
+        move_reg_to_reg(OpndSize_32, 1, false, PhysicalReg_EAX, true);
+
+        load_imm_global_data_API("strClassCastExceptionPtr", OpndSize_32, PhysicalReg_ECX, true);
+
+        nextVersionOfHardReg(PhysicalReg_EDX, 2); //next version has 2 ref count
+        export_pc();
+
+        unconditional_jump_global_API("common_throw_message", false);
+    }
+    //handler for speical case where object reference is null
+    if(instance) {
+        if (insertLabel(".instance_of_null", true) == -1) {
+            return -1;
+        }
+    }
+    else {
+        if (insertLabel(".check_cast_null", true) == -1) {
+            return -1;
+        }
+    }
+    goToState(1);
+    if(instance) {
+        move_imm_to_reg(OpndSize_32, 0, 3, false);
+    }
+    transferToState(4);
+    if(instance)
+        unconditional_jump(".instance_of_okay", true);
+    else
+        unconditional_jump(".check_cast_okay", true);
+
+    //handler for special case where class of object is the same as the resolved class
+    if(instance) {
+        if (insertLabel(".instance_of_equal", true) == -1){
+            return -1;
+        }
+    }
+    else {
+        if (insertLabel(".check_cast_equal", true) == -1)
+            return -1;
+    }
+    goToState(3);
+    if(instance) {
+        move_imm_to_reg(OpndSize_32, 1, 3, false);
+    }
+    transferToState(4);
+    if(instance) {
+        if (insertLabel(".instance_of_okay", true) == -1) {
+            return -1;
+        }
+    }
+    else {
+        if (insertLabel(".check_cast_okay", true) == -1) {
+            return -1;
+        }
+    }
+    //all cases merge here and the value is put to virtual register
+    if(instance) {
+        set_virtual_reg(vDest, OpndSize_32, 3, false);
+    }
+    return 0;
+}
+//! common code to lower CHECK_CAST & INSTANCE_OF
+
+//!
+int common_check_cast_instance_of(int vA, u4 tmp, bool instance, int vDest) {
+    return check_cast_nohelper(vA, tmp, instance, vDest);
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+
+/**
+ * @brief Generate native code for bytecode check-cast
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_check_cast(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_CHECK_CAST);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+    return common_check_cast_instance_of(vA, tmp, false, 0);
+}
+
+/**
+ * @brief Generate native code for bytecode instance-of
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_instance_of(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_INSTANCE_OF);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+    u4 tmp = mir->dalvikInsn.vC;
+    return common_check_cast_instance_of(vB, tmp, true, vA);
+}
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+//! LOWER bytecode MONITOR_ENTER without usage of helper function
+
+//!   CALL dvmLockObject
+int monitor_enter_nohelper(int vA, const MIR *mir) {
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        requestVRFreeDelay(vA,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
+    }
+
+    //get_self_pointer is separated
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    //to optimize redundant null check, NCG O1 wraps up null check in a function: nullCheck
+    get_self_pointer(3, false);
+    //If we can't ignore the NULL check
+    if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+    {
+        nullCheck(1, false, 1, vA); //maybe optimized away
+        cancelVRFreeDelayRequest(vA,VRDELAY_NULLCHECK);
+    }
+
+    /////////////////////////////
+    //inline the simple case with JIT
+    //simple case is thin lock, held by no-one.
+
+    //backup the self pointer and Oject for native implementation
+    //which will be passed to dvmLockObject() as parameter
+    move_reg_to_reg(OpndSize_32, 1, false, 4, false);
+    move_reg_to_reg(OpndSize_32, 3, false, 5, false);
+
+    //get the Obj->lock
+    move_mem_to_reg(OpndSize_32, offsetof(Object, lock), 1, false, 2, false);
+
+    //if it is the simple case, the object lock should contain all 0s except the hash_state bits
+    //save the value to EAX, which will be used for CMPXCHG
+    alu_binary_imm_reg(OpndSize_32, and_opc, (LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT), 2, false);
+    move_reg_to_reg(OpndSize_32, 2, false, PhysicalReg_EAX, true);
+
+    //get self->threadId
+    move_mem_to_reg(OpndSize_32, offsetof(Thread, threadId), 3, false, 3, false);
+
+    //generate the new lock
+    alu_binary_imm_reg(OpndSize_32, shl_opc, LW_LOCK_OWNER_SHIFT, 3, false);
+    alu_binary_reg_reg(OpndSize_32, or_opc, 2, false, 3, false);
+
+    //add the lock to Object using cmpxchg, if it is simple case, EAX value should be same as Object->lock
+    compareAndExchange(OpndSize_32, 3, false, offsetof(Object, lock), 1, false);
+
+    //remember the state of register before comditional_jump
+    rememberState(1);
+
+    //if successful added lock, jump to the end of this function
+    conditional_jump(Condition_Z, ".call_monitor_native_done", true);
+
+    /////////////////////////////
+    //prepare to call dvmLockObject, inputs: object reference and self
+    // TODO: Should reset inJitCodeCache before calling dvmLockObject
+    //       so that code cache can be reset if needed when locking object
+    //       taking a long time. Not resetting inJitCodeCache may delay
+    //       code cache reset when code cache is full, preventing traces from
+    //       JIT compilation. This has performance implication.
+    //       However, after resetting inJitCodeCache, the code should be
+    //       wrapped in a helper instead of directly inlined in code cache.
+    //       If the code after dvmLockObject call is in code cache and the code
+    //       cache is reset during dvmLockObject call, execution after
+    //       dvmLockObject will return to a cleared code cache region,
+    //       resulting in seg fault.
+    if (insertLabel(".call_monitor_native_implementation", true) == -1) {
+       return -1;
+    }
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 4, false, 4, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 5, false, 0, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_2;
+    call_dvmLockObject();
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    //now we restore the register state for later use of VR
+    transferToState(1);
+    if (insertLabel(".call_monitor_native_done", true) == -1) {
+       return -1;
+    }
+    /////////////////////////////
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode monitor-enter
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_monitor_enter(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MONITOR_ENTER);
+    int vA = mir->dalvikInsn.vA;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[11]) {
+        // .monitor_enter_helper
+        //   INPUT: P_GPR_1 (virtual register for object)
+        //   OUTPUT: none
+        //   %esi is live through function monitor_enter_helper
+        export_pc(); //use %edx
+        move_imm_to_reg(OpndSize_32, vA, P_GPR_1, true);
+        spillVirtualReg(vA, LowOpndRegType_gp, true);
+        call_helper_API(".monitor_enter_helper");
+    }
+    else
+#endif
+    {
+        export_pc();
+        monitor_enter_nohelper(vA, mir);
+    }
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+
+/**
+ * @brief Generate native code for bytecode monitor-exit
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_monitor_exit(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_MONITOR_EXIT);
+    int vA = mir->dalvikInsn.vA;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[11]) {
+        export_pc();
+        // .moniter_exit_helper
+        //   INPUT: in P_GPR_1 (virtual register for object)
+        //   OUTPUT: none
+        //   %esi is live through function moniter_exit_helper
+        move_imm_to_reg(OpndSize_32, vA, P_GPR_1, true);
+        spillVirtualReg(vA, LowOpndRegType_gp, true);
+        call_helper_API(".monitor_exit_helper");
+    }
+    else
+#endif
+    {
+        ////////////////////
+        //LOWER bytecode MONITOR_EXIT without helper function
+        //inline simple case with JIT,
+        //simple case is thin lock held by unlocking thread with recursive count 0
+        //other case will CALL dvmUnlockObject
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+
+        if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+        {
+            requestVRFreeDelay(vA,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
+        }
+
+        get_virtual_reg(vA, OpndSize_32, 1, false);
+
+        if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+        {
+            nullCheck(1, false, 1, vA); //maybe optimized away
+            cancelVRFreeDelayRequest(vA,VRDELAY_NULLCHECK);
+        }
+
+        //get the self pointer
+        get_self_pointer(3,false);
+
+        //get self->threadid
+        move_mem_to_reg(OpndSize_32, offsetof(Thread, threadId), 3, false, 4, false);
+
+        //threadid << 3, for comparison with obj->lock
+        alu_binary_imm_reg(OpndSize_32, shl_opc, 3, 4,false);
+
+        //get obj->lock
+        move_reg_to_reg(OpndSize_32, 1, false, 7, false);
+
+        //get Obj->lock
+        move_mem_to_reg(OpndSize_32, offsetof(Object, lock), 7, false, 5, false);
+        move_reg_to_reg(OpndSize_32, 5, false, 6, false);
+
+        //test whether obj->lock is thin lock and object is locked by current thread
+        alu_binary_imm_reg(OpndSize_32, and_opc,  ~(LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT), 5, false);
+        compare_reg_reg( 4, false, 5, false);
+
+        //In native implementation, dvmUnlockObject() invokes beforeCall() that spill VRs which maybe used later
+        //However, after inlining the JIT code,
+        //there will be a chance that dvmUnlockObject() will not be called,
+        //but the spill in beforecall() marked the VR as in memory,
+        //so later use of VR may incorrectly unspill the value from memory.
+        //To avoid this issue, we remember the state.
+        rememberState(1);
+
+        //locked by other thread or fat lock or recursive lock, jump to call the native functions
+        conditional_jump(Condition_NE, "j_call_dvmUnlockObject", true);
+
+        //create the new words(32bit) for obj->lock, it only contains the hash bits of original obj->lock
+        alu_binary_imm_reg(OpndSize_32, and_opc, (LW_HASH_STATE_MASK << LW_HASH_STATE_SHIFT), 6, false);
+
+        //release the lock
+        move_reg_to_mem(OpndSize_32, 6, false, offsetof(Object, lock),7, false);
+
+        //jump to the end of the function
+        unconditional_jump(".unlock_object_done", true);
+        if (insertLabel("j_call_dvmUnlockObject", true) == -1) {
+           return -1;
+        }
+
+        /////////////////////////////
+        //prepare to call dvmUnlockObject, inputs: object reference and self
+        push_reg_to_stack(OpndSize_32, 1, false);
+        push_mem_to_stack(OpndSize_32, offEBP_self, PhysicalReg_EBP, true);
+        scratchRegs[0] = PhysicalReg_SCRATCH_2;
+        call_dvmUnlockObject();
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        //transfor the register state for later use of VR
+        transferToState(1);
+
+#if defined(WITH_JIT)
+        conditional_jump(Condition_NE, ".unlock_object_done", true);
+        //jump to dvmJitToExceptionThrown
+        scratchRegs[0] = PhysicalReg_SCRATCH_3;
+        jumpToExceptionThrown(2/*exception number*/);
+#else
+        //throw exception if dvmUnlockObject returns 0
+        char errName[256];
+        sprintf(errName, "common_exceptionThrown");
+        handlePotentialException(
+                                           Condition_E, Condition_NE,
+                                           2, errName);
+#endif
+        if (insertLabel(".unlock_object_done", true) == -1) {
+            return -1;
+        }
+        ///////////////////////////
+    }
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_EDX /*vA*/
+
+/**
+ * @brief Generate native code for bytecode array-length
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_array_length(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_ARRAY_LENGTH);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[14]) {
+        // .array_length_helper
+        //   INPUT: P_GPR_1 (virtual register for array object)
+        //          P_GPR_3 (virtual register for length)
+        //   OUTPUT: none
+        //   %eax, %esi, %ebx: live through function array_length_helper
+        export_pc(); //use %edx
+        move_imm_to_reg(OpndSize_32, vA, P_GPR_3, true);
+        move_imm_to_reg(OpndSize_32, vB, P_GPR_1, true);
+        call_helper_API(".array_length_helper");
+    }
+    else
+#endif
+    {
+        ////////////////////
+        //no usage of helper function
+        if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+        {
+            requestVRFreeDelay(vB,VRDELAY_NULLCHECK); // Request VR delay before transfer to temporary
+        }
+
+        get_virtual_reg(vB, OpndSize_32, 1, false);
+
+        if((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
+        {
+            nullCheck(1, false, 1, vB); //maybe optimized away
+            cancelVRFreeDelayRequest(vB,VRDELAY_NULLCHECK);
+        }
+
+        move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), 1, false, 2, false);
+        set_virtual_reg(vA, OpndSize_32, 2, false);
+        ///////////////////////
+    }
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_ESI
+
+/**
+ * @brief Generate native code for bytecode new-instance
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_new_instance(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEW_INSTANCE);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[4]) {
+        // .new_instance_helper
+        //   INPUT: P_GPR_3 (const pool index)
+        //   OUTPUT: %eax
+        //   no register is live through function array_length_helper
+        export_pc();
+        move_imm_to_reg(OpndSize_32, tmp, P_GPR_3, true);
+        call_helper_API(".new_instance_helper");
+    }
+    else
+#endif
+    {
+        export_pc();
+#if defined(WITH_JIT)
+        /* for trace-based JIT, class is already resolved */
+        ClassObject *classPtr =
+              (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
+        assert(classPtr != NULL);
+        assert(classPtr->status & CLASS_INITIALIZED);
+        /*
+         * If it is going to throw, it should not make to the trace to begin
+         * with.  However, Alloc might throw, so we need to genExportPC()
+        */
+        assert((classPtr->accessFlags & (ACC_INTERFACE|ACC_ABSTRACT)) == 0);
+#else
+        //////////////////////////////////////////
+        //resolve class, check whether it has been resolved
+        //if yes, jump to resolved
+        //if no, call class_resolve
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+        get_res_classes(3, false);
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //resolved class
+        conditional_jump(Condition_NE, ".new_instance_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call_helper_API(".class_resolve");
+        transferToState(1);
+
+        //here, class is resolved
+        if (insertLabel(".new_instance_resolved", true) == -1)
+            return -1;
+        //check whether the class is initialized
+        //if yes, jump to initialized
+        //if no, call new_instance_needinit
+        movez_mem_to_reg(OpndSize_8, offClassObject_status, PhysicalReg_EAX, true, 5, false);
+        compare_imm_reg(OpndSize_32, CLASS_INITIALIZED, 5, false);
+        conditional_jump(Condition_E, ".new_instance_initialized", true);
+        rememberState(2);
+        call_helper_API(".new_instance_needinit");
+        transferToState(2);
+        //here, class is already initialized
+        if (insertLabel(".new_instance_initialized", true) == -1)
+            return -1;
+        //check whether the class is an interface or abstract, if yes, throw exception
+        move_mem_to_reg(OpndSize_32, offClassObject_accessFlags, PhysicalReg_EAX, true, 6, false);
+        test_imm_reg(OpndSize_32, ACC_INTERFACE|ACC_ABSTRACT, 6, false); //access flags
+
+        //two inputs for common_throw_message: object reference in eax, exception pointer in ecx
+        handlePotentialException(
+                                           Condition_NE, Condition_E,
+                                           2, "common_throw_message");
+#endif
+        //prepare to call dvmAllocObject, inputs: resolved class & flag ALLOC_DONT_TRACK
+        load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+#if defined(WITH_JIT)
+        /* 1st argument to dvmAllocObject at -8(%esp) */
+        move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
+#else
+        move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true); //resolved class
+#endif
+        move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 4, PhysicalReg_ESP, true);
+        scratchRegs[0] = PhysicalReg_SCRATCH_3;
+        nextVersionOfHardReg(PhysicalReg_EAX, 3); //next version has 3 refs
+        call_dvmAllocObject();
+        load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+        //return value of dvmAllocObject is in %eax
+        //if return value is null, throw exception
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+#if defined(WITH_JIT)
+        conditional_jump(Condition_NE, ".new_instance_done", true);
+        //jump to dvmJitToExceptionThrown
+        scratchRegs[0] = PhysicalReg_SCRATCH_4;
+        jumpToExceptionThrown(3/*exception number*/);
+#else
+        handlePotentialException(
+                                           Condition_E, Condition_NE,
+                                           3, "common_exceptionThrown");
+#endif
+    }
+    if (insertLabel(".new_instance_done", true) == -1)
+        return -1;
+    set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+    return 0;
+}
+
+//! function to initialize a class
+
+//!INPUT: %eax (class object) %eax is recovered before return
+//!OUTPUT: none
+//!CALL: dvmInitClass
+//!%eax, %esi, %ebx are live through function new_instance_needinit
+int new_instance_needinit() {
+    if (insertLabel(".new_instance_needinit", false) == -1)
+        return -1;
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 4, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_ECX;
+    call_dvmInitClass();
+    //if return value of dvmInitClass is zero, throw exception
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    //recover EAX with the class object
+    move_mem_to_reg(OpndSize_32, 4, PhysicalReg_ESP, true, PhysicalReg_EAX, true);
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    conditional_jump(Condition_E, "common_exceptionThrown", false);
+    x86_return();
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+
+#define P_GPR_1 PhysicalReg_EBX //live through C function, must in callee-saved reg
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_EDX
+
+/**
+ * @brief Generate native code for bytecode new-array
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_new_array(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_NEW_ARRAY);
+    int vA = mir->dalvikInsn.vA; //destination
+    int vB = mir->dalvikInsn.vB; //length
+    u4 tmp = mir->dalvikInsn.vC;
+#ifdef INC_NCG_O0
+    if(gDvm.helper_switch[17]) {
+        // .new_array_helper
+        //   INPUT: P_GPR_3 (const pool index)
+        //          P_GPR_1 (virtual register with size of the array)
+        //   OUTPUT: %eax
+        //   no reg is live through function new_array_helper
+        export_pc(); //use %edx
+        move_imm_to_reg(OpndSize_32, tmp, P_GPR_3, true);
+        move_imm_to_reg(OpndSize_32, vB, P_GPR_1, true);
+        spillVirtualReg(vB, LowOpndRegType_gp, true);
+        call_helper_API(".new_array_helper");
+        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+    }
+    else
+#endif
+    {
+        /////////////////////////
+        //   REGS used: %esi, %eax, P_GPR_1, P_GPR_2
+        //   CALL class_resolve, dvmAllocArrayByClass
+        export_pc(); //use %edx
+        //check size of the array, if negative, throw exception
+        get_virtual_reg(vB, OpndSize_32, 5, false);
+        compare_imm_reg(OpndSize_32, 0, 5, false);
+        handlePotentialException(
+                                           Condition_S, Condition_NS,
+                                           1, "common_errNegArraySize");
+#if defined(WITH_JIT)
+       void *classPtr = (void*)
+            (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
+       assert(classPtr != NULL);
+#else
+        //try to resolve class, if already resolved, jump to resolved
+        //if not, call class_resolve
+        scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+        scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+        get_res_classes(3, false);
+        move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+        conditional_jump(Condition_NE, ".new_array_resolved", true);
+        rememberState(1);
+        move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+        call_helper_API(".class_resolve");
+        transferToState(1);
+#endif
+        //here, class is already resolved, the class object is in %eax
+        //prepare to call dvmAllocArrayByClass with inputs: resolved class, array length, flag ALLOC_DONT_TRACK
+        if (insertLabel(".new_array_resolved", true) == -1)
+                return -1;
+        load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+#if defined(WITH_JIT)
+        /* 1st argument to dvmAllocArrayByClass at 0(%esp) */
+        move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
+#else
+        move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
+#endif
+        move_reg_to_mem(OpndSize_32, 5, false, 4, PhysicalReg_ESP, true);
+        move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 8, PhysicalReg_ESP, true);
+        scratchRegs[0] = PhysicalReg_SCRATCH_3;
+        nextVersionOfHardReg(PhysicalReg_EAX, 3); //next version has 3 refs
+        call_dvmAllocArrayByClass();
+        load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+        //the allocated object is in %eax
+        //check whether it is null, throw exception if null
+        compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+#if defined(WITH_JIT)
+        conditional_jump(Condition_NE, ".new_array_done", true);
+        //jump to dvmJitToExceptionThrown
+        scratchRegs[0] = PhysicalReg_SCRATCH_4;
+        jumpToExceptionThrown(2/*exception number*/);
+#else
+        handlePotentialException(
+                                           Condition_E, Condition_NE,
+                                           2, "common_exceptionThrown");
+#endif
+        if (insertLabel(".new_array_done", true) == -1)
+            return -1;
+        set_virtual_reg(vA, OpndSize_32, PhysicalReg_EAX, true);
+        //////////////////////////////////////
+    }
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+
+#define P_GPR_1 PhysicalReg_EBX
+#define P_GPR_2 PhysicalReg_ECX
+#define P_GPR_3 PhysicalReg_ESI
+//! common code to lower FILLED_NEW_ARRAY
+
+//! call: class_resolve call_dvmAllocPrimitiveArray
+//! exception: filled_new_array_notimpl common_exceptionThrown
+int common_filled_new_array(int length, u4 tmp, bool hasRange) {
+    ClassObject *classPtr =
+              (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
+    if(classPtr != NULL) ALOGI("FILLED_NEW_ARRAY class %s", classPtr->descriptor);
+    //check whether class is resolved, if yes, jump to resolved
+    //if not, call class_resolve
+    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    get_res_classes(3, false);
+    move_mem_to_reg(OpndSize_32, tmp*4, 3, false, PhysicalReg_EAX, true);
+    export_pc();
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true); //resolved class
+    conditional_jump(Condition_NE, ".filled_new_array_resolved", true);
+    rememberState(1);
+    move_imm_to_reg(OpndSize_32, tmp, PhysicalReg_EAX, true);
+    call_helper_API(".class_resolve");
+    transferToState(1);
+    //here, class is already resolved
+    if (insertLabel(".filled_new_array_resolved", true) == -1)
+        return -1;
+    //check descriptor of the class object, if not implemented, throws exception
+    move_mem_to_reg(OpndSize_32, 24, PhysicalReg_EAX, true, 5, false);
+    //load a single byte of the descriptor
+    movez_mem_to_reg(OpndSize_8, 1, 5, false, 6, false);
+    compare_imm_reg(OpndSize_32, 'I', 6, false);
+    conditional_jump(Condition_E, ".filled_new_array_impl", true);
+    compare_imm_reg(OpndSize_32, 'L', 6, false);
+    conditional_jump(Condition_E, ".filled_new_array_impl", true);
+    compare_imm_reg(OpndSize_32, '[', 6, false);
+    conditional_jump(Condition_NE, ".filled_new_array_notimpl", false);
+
+    if (insertLabel(".filled_new_array_impl", true) == -1)
+        return -1;
+    //prepare to call dvmAllocArrayByClass with inputs: classObject, length, flag ALLOC_DONT_TRACK
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, (int)classPtr, 0, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, length, 4, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, ALLOC_DONT_TRACK, 8, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_3; scratchRegs[1] = PhysicalReg_Null;
+    if(hasRange) {
+        nextVersionOfHardReg(PhysicalReg_EAX, 5+(length >= 1 ? LOOP_COUNT : 0)); //next version
+    }
+    else {
+        nextVersionOfHardReg(PhysicalReg_EAX, 5+length); //next version
+    }
+    call_dvmAllocArrayByClass();
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    //return value of dvmAllocPrimitiveArray is in %eax
+    //if the return value is null, throw exception
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    handlePotentialException(
+                                       Condition_E, Condition_NE,
+                                       3, "common_exceptionThrown");
+
+    /* we need to mark the card of the new array, if it's not an int */
+    compare_imm_reg(OpndSize_32, 'I', 6, false);
+    conditional_jump(Condition_E, ".dont_mark_filled_new_array", true);
+
+    // Need to make copy of EAX, because it's used later in op_filled_new_array()
+    move_reg_to_reg(OpndSize_32, PhysicalReg_EAX, true, 6, false);
+
+    markCard_filled(6, false, PhysicalReg_SCRATCH_4, false);
+
+    if (insertLabel(".dont_mark_filled_new_array", true) == -1)
+        return -1;
+
+    //return value of bytecode FILLED_NEW_ARRAY is in GLUE structure
+    scratchRegs[0] = PhysicalReg_SCRATCH_4; scratchRegs[1] = PhysicalReg_Null;
+    set_return_value(OpndSize_32, PhysicalReg_EAX, true);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecode filled-new-array
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_filled_new_array(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FILLED_NEW_ARRAY);
+    u4 length = mir->dalvikInsn.vA;
+    u4 classIdx = mir->dalvikInsn.vB;
+    int v1, v2, v3, v4, v5;
+
+    // Note that v1, v2, v3, v4, and/or v5 may not be valid.
+    // Always check "length" before using any of them.
+    v5 = mir->dalvikInsn.arg[4];
+    v4 = mir->dalvikInsn.arg[3];
+    v3 = mir->dalvikInsn.arg[2];
+    v2 = mir->dalvikInsn.arg[1];
+    v1 = mir->dalvikInsn.arg[0];
+
+    if (common_filled_new_array(length, classIdx, false) == -1)
+        return -1;
+    if(length >= 1) {
+        //move from virtual register to contents of array object
+        get_virtual_reg(v1, OpndSize_32, 7, false);
+        move_reg_to_mem(OpndSize_32, 7, false, OFFSETOF_MEMBER(ArrayObject, contents), PhysicalReg_EAX, true);
+    }
+    if(length >= 2) {
+        //move from virtual register to contents of array object
+        get_virtual_reg(v2, OpndSize_32, 8, false);
+        move_reg_to_mem(OpndSize_32, 8, false, OFFSETOF_MEMBER(ArrayObject, contents)+4, PhysicalReg_EAX, true);
+    }
+    if(length >= 3) {
+        //move from virtual register to contents of array object
+        get_virtual_reg(v3, OpndSize_32, 9, false);
+        move_reg_to_mem(OpndSize_32, 9, false, OFFSETOF_MEMBER(ArrayObject, contents)+8, PhysicalReg_EAX, true);
+    }
+    if(length >= 4) {
+        //move from virtual register to contents of array object
+        get_virtual_reg(v4, OpndSize_32, 10, false);
+        move_reg_to_mem(OpndSize_32, 10, false, OFFSETOF_MEMBER(ArrayObject, contents)+12, PhysicalReg_EAX, true);
+    }
+    if(length >= 5) {
+        //move from virtual register to contents of array object
+        get_virtual_reg(v5, OpndSize_32, 11, false);
+        move_reg_to_mem(OpndSize_32, 11, false, OFFSETOF_MEMBER(ArrayObject, contents)+16, PhysicalReg_EAX, true);
+    }
+    return 0;
+}
+//! function to handle the error of array not implemented
+
+//!
+int filled_new_array_notimpl() {
+    //two inputs for common_throw:
+    if (insertLabel(".filled_new_array_notimpl", false) == -1)
+        return -1;
+    move_imm_to_reg(OpndSize_32, LstrFilledNewArrayNotImpl, PhysicalReg_EAX, true);
+    move_imm_to_reg(OpndSize_32, (int) gDvm.exInternalError, PhysicalReg_ECX, true);
+    unconditional_jump("common_throw", false);
+    return 0;
+}
+
+#define P_SCRATCH_1 PhysicalReg_EDX
+
+/**
+ * @brief Generate native code for bytecode filled-new-array/range
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_filled_new_array_range(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_FILLED_NEW_ARRAY_RANGE);
+    int length = mir->dalvikInsn.vA;
+    u4 classIdx = mir->dalvikInsn.vB;
+    int vC = mir->dalvikInsn.vC;
+    if (common_filled_new_array(length, classIdx, true/*hasRange*/) == -1)
+        return -1;
+    //here, %eax points to the array object
+    if(length >= 1) {
+        //dump all virtual registers used by this bytecode to stack, for NCG O1
+        int k;
+        for(k = 0; k < length; k++) {
+            spillVirtualReg(vC+k, LowOpndRegType_gp, true); //will update refCount
+        }
+        //address of the first virtual register that will be moved to the array object
+        load_effective_addr(vC*4, PhysicalReg_FP, true, 7, false); //addr
+        //start address for contents of the array object
+        load_effective_addr(OFFSETOF_MEMBER(ArrayObject, contents), PhysicalReg_EAX, true, 8, false); //addr
+        //loop counter
+        move_imm_to_reg(OpndSize_32, length-1, 9, false); //counter
+        //start of the loop
+        if (insertLabel(".filled_new_array_range_loop1", true) == -1)
+            return -1;
+        rememberState(1);
+        move_mem_to_reg(OpndSize_32, 0, 7, false, 10, false);
+        load_effective_addr(4, 7, false, 7, false);
+        move_reg_to_mem(OpndSize_32, 10, false, 0, 8, false);
+        load_effective_addr(4, 8, false, 8, false);
+        alu_binary_imm_reg(OpndSize_32, sub_opc, 1, 9, false);
+        transferToState(1);
+        //jump back to the loop start
+        conditional_jump(Condition_NS, ".filled_new_array_range_loop1", true);
+    }
+    return 0;
+}
+#undef P_GPR_1
+#undef P_GPR_2
+#undef P_GPR_3
+#undef P_SCRATCH_1
+
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecode fill-array-data
+ * @details Calls dvmInterpHandleFillArrayData
+ * @param mir bytecode representation
+ * @param dalvikPC program counter for Dalvik bytecode
+ * @return value >= 0 when handled
+ */
+int op_fill_array_data(const MIR * mir, const u2 * dalvikPC) {
+    assert(mir->dalvikInsn.opcode == OP_FILL_ARRAY_DATA);
+    int vA = mir->dalvikInsn.vA;
+    u4 tmp = mir->dalvikInsn.vB;
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    scratchRegs[1] = PhysicalReg_Null;
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    //prepare to call dvmInterpHandleFillArrayData, input: array object, address of the data
+    load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 1, false, 0, PhysicalReg_ESP, true);
+    /* 2nd argument to dvmInterpHandleFillArrayData at 4(%esp) */
+    move_imm_to_mem(OpndSize_32, (int)(dalvikPC+tmp), 4, PhysicalReg_ESP, true);
+    call_dvmInterpHandleFillArrayData();
+    load_effective_addr(8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+    //check return value of dvmInterpHandleFillArrayData, if zero, throw exception
+    compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
+    conditional_jump(Condition_NE, ".fill_array_data_done", true);
+    //jump to dvmJitToExceptionThrown
+    scratchRegs[0] = PhysicalReg_SCRATCH_2;
+    jumpToExceptionThrown(2/*exception number*/);
+    if (insertLabel(".fill_array_data_done", true) == -1)
+        return -1;
+    return 0;
+}
+#undef P_GPR_1
+
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecode throw
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_throw(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_THROW);
+    int vA = mir->dalvikInsn.vA;
+    export_pc();
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+    //null check
+    compare_imm_reg(OpndSize_32, 0, 1, false);
+    conditional_jump(Condition_E, "common_errNullObject", false);
+    //set glue->exception & throw exception
+    scratchRegs[2] = PhysicalReg_Null; scratchRegs[3] = PhysicalReg_Null;
+    scratchRegs[0] = PhysicalReg_SCRATCH_1; scratchRegs[1] = PhysicalReg_SCRATCH_2;
+    set_exception(1, false);
+    unconditional_jump("common_exceptionThrown", false);
+    return 0;
+}
+#undef P_GPR_1
+#define P_GPR_1 PhysicalReg_EBX
+
+/**
+ * @brief Generate native code for bytecode throw-verification-error
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_throw_verification_error(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_THROW_VERIFICATION_ERROR);
+    int vA = mir->dalvikInsn.vA;
+    int vB = mir->dalvikInsn.vB;
+
+    export_pc();
+    scratchRegs[0] = PhysicalReg_SCRATCH_1;
+    get_glue_method(1, false);
+
+    load_effective_addr(-12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, vB, 8, PhysicalReg_ESP, true);
+    move_imm_to_mem(OpndSize_32, vA, 4, PhysicalReg_ESP, true);
+    move_reg_to_mem(OpndSize_32, 1, false, 0, PhysicalReg_ESP, true);
+    scratchRegs[0] = PhysicalReg_SCRATCH_2;
+    call_dvmThrowVerificationError();
+    load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+    unconditional_jump("common_exceptionThrown", false);
+    return 0;
+}
+#undef P_GPR_1
diff --git a/vm/compiler/codegen/x86/lightcg/LowerReturn.cpp b/vm/compiler/codegen/x86/lightcg/LowerReturn.cpp
new file mode 100644
index 0000000..e82772b
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/LowerReturn.cpp
@@ -0,0 +1,98 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*! \file LowerReturn.cpp
+    \brief This file lowers the following bytecodes: RETURN
+
+*/
+
+#include "Lower.h"
+#include "NcgHelper.h"
+
+/**
+ * @brief Generates jump to dvmJitHelper_returnFromMethod.
+ * @details Uses one scratch register to make the jump
+ * @return value 0 when successful
+ */
+inline int jumpTocommon_returnFromMethod() {
+    int sizeofStackSaveArea = sizeof(StackSaveArea);
+
+    void * funcPtr = reinterpret_cast<void *>(dvmJitHelper_returnFromMethod);
+
+    // Load save area into EDX
+    load_effective_addr(-sizeofStackSaveArea, PhysicalReg_FP, true, PhysicalReg_EDX, true);
+
+    // We may suffer from agen stall here due if edx is not ready
+    // So instead of doing:
+    //   movl offStackSaveArea_prevFrame(%edx), rFP
+    // We can just compute directly
+    //   movl (offStackSaveArea_prevFrame - sizeofStackSaveArea)(rFP), rFP
+    move_mem_to_reg(OpndSize_32,
+            OFFSETOF_MEMBER(StackSaveArea, prevFrame) - sizeofStackSaveArea,
+            PhysicalReg_FP, true, PhysicalReg_FP, true);
+
+    unconditional_jump_rel32(funcPtr);
+    return 0;
+}
+
+/**
+ * @brief Generate native code for bytecodes return-void
+ * and return-void-barrier
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_return_void(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RETURN_VOID
+            || mir->dalvikInsn.opcode == OP_RETURN_VOID_BARRIER);
+
+    // Put self pointer in ecx
+    get_self_pointer(PhysicalReg_ECX, true);
+
+    return jumpTocommon_returnFromMethod();
+}
+
+/**
+ * @brief Generate native code for bytecodes return
+ * and return-object
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_return(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RETURN
+            || mir->dalvikInsn.opcode == OP_RETURN_OBJECT);
+    int vA = mir->dalvikInsn.vA;
+    get_virtual_reg(vA, OpndSize_32, 1, false);
+
+    set_return_value(OpndSize_32, 1, false, PhysicalReg_ECX, true);
+
+    return jumpTocommon_returnFromMethod();
+}
+
+/**
+ * @brief Generate native code for bytecode return-wide
+ * @param mir bytecode representation
+ * @return value >= 0 when handled
+ */
+int op_return_wide(const MIR * mir) {
+    assert(mir->dalvikInsn.opcode == OP_RETURN_WIDE);
+    int vA = mir->dalvikInsn.vA;
+    get_virtual_reg(vA, OpndSize_64, 1, false);
+
+    set_return_value(OpndSize_64, 1, false, PhysicalReg_ECX, true);
+
+    return jumpTocommon_returnFromMethod();
+}
diff --git a/vm/compiler/codegen/x86/lightcg/NcgAot.cpp b/vm/compiler/codegen/x86/lightcg/NcgAot.cpp
new file mode 100644
index 0000000..75cd822
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/NcgAot.cpp
@@ -0,0 +1,226 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#include "Lower.h"
+#include "NcgAot.h"
+#include "NcgHelper.h"
+#include "Scheduler.h"
+#include "Singleton.h"
+
+//returns # of ops generated by this function
+//entries relocatable: eip + relativePC
+int get_eip_API() {
+    call("ncgGetEIP");//%edx //will push eip to stack
+    return 1;
+}
+#define NEW_EXPORT_PC
+//!update current PC in the stack frame with %eip
+
+//!
+int export_pc() {
+    /* for trace-based JIT, pc points to bytecode
+       for NCG, pc points to native code */
+    int sizeofStackSaveArea = sizeof(StackSaveArea);
+
+    move_imm_to_mem(OpndSize_32, (int)rPC,
+                    -sizeofStackSaveArea+OFFSETOF_MEMBER(StackSaveArea, xtra.currentPc), PhysicalReg_FP, true);
+    return 1; //return number of ops
+}
+
+/* jump from JIT'ed code to interpreter without chaining */
+int jumpToInterpNoChain() {
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmJitToInterpNoChain;
+    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
+
+    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
+    if(gDvm.executionMode == kExecutionModeNcgO1) touchEax();
+    return 0;
+}
+
+/* jump from JIT'ed code to interpreter becaues of exception */
+int jumpToInterpPunt() {
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmJitToInterpPunt;
+    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
+
+    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
+    //if(gDvm.executionMode == kExecutionModeNcgO1) touchEax();
+    return 0;
+}
+
+/* jump to common_exceptionThrown from JIT'ed code */
+int jumpToExceptionThrown(int exceptionNum) {
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        rememberState(exceptionNum);
+        export_pc();
+        beforeCall("exception"); //dump GG, GL VRs
+    }
+
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmJitToExceptionThrown;
+    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
+    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
+
+    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        goToState(exceptionNum);
+    }
+    return 0;
+}
+
+//! generate native code to call dvmNcgInvokeInterpreter
+
+//!the interpreter will start execution from %eax
+int invokeInterpreter(bool fromApp)
+{
+    typedef void (*vmHelper)(int);
+    vmHelper funcPtr = dvmNcgInvokeInterpreter;
+
+    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
+
+    unconditional_jump_reg(C_SCRATCH_1, isScratchPhysical);
+    if(gDvm.executionMode == kExecutionModeNcgO1) touchEax();
+    return 0;
+}
+
+//!work to do before calling a function pointer with code cache enabled
+
+//!
+void callFuncPtr(int funcPtr, const char* funcName) {
+
+    move_imm_to_reg(OpndSize_32, (int)funcPtr, C_SCRATCH_1, isScratchPhysical);
+    call_reg(C_SCRATCH_1, isScratchPhysical);
+}
+
+/* generate a "call imm32" */
+void callFuncPtrImm(int funcPtr) {
+    Mnemonic m = Mnemonic_CALL;
+    if(gDvmJit.scheduling)
+        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+    int relOffset = funcPtr - (int)stream - 5; // 5: Bytes of "call imm32"
+    dump_imm(m, OpndSize_32, relOffset);
+}
+
+//.const_string_resolve: input in %eax, output in %eax
+//.const_string_helper:
+//.class_resolve: input in %eax, output in %eax
+int call_helper_API(const char* helperName) {
+    call(helperName);
+    return 1;
+}
+
+/* check whether we are throwing an exception */
+bool jumpToException(const char* target) {
+    bool isException = false;
+    if(!strncmp(target, "common_err", 10)) isException = true;
+    if(!strncmp(target, "common_throw", 12)) isException = true;
+    if(!strncmp(target, "common_exception", 16)) isException = true;
+    return isException;
+}
+
+int conditional_jump_global_API(
+                                ConditionCode cc, const char* target,
+                                bool isShortTerm) {
+    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
+        condJumpToBasicBlock(stream, cc, currentExceptionBlockIdx);
+        return 1; //return number of ops
+    }
+    conditional_jump(cc, target, isShortTerm);
+    return 1;
+}
+int unconditional_jump_global_API(
+                                  const char* target, bool isShortTerm) {
+    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
+        jumpToBasicBlock(stream, currentExceptionBlockIdx);
+        return 1; //return number of ops
+    }
+    unconditional_jump(target, isShortTerm);
+    return 1;
+}
+
+/* @brief Provides address to global constants
+ * @details Essentially provides an associative
+ * array of global data values indexed by name
+ * @param dataName the string for the data
+ * @return pointer to the global data
+ */
+int getGlobalDataAddr(const char* dataName) {
+    int dataAddr = 0;
+    if(!strcmp(dataName, "doubNeg")) dataAddr = LdoubNeg;
+    else if(!strcmp(dataName, "intMax")) dataAddr = LintMax;
+    else if(!strcmp(dataName, "intMin")) dataAddr = LintMin;
+    else if(!strcmp(dataName, "valueNanLong")) dataAddr = LvalueNanLong;
+    else if(!strcmp(dataName, "valuePosInfLong")) dataAddr = LvaluePosInfLong;
+    else if(!strcmp(dataName, "valueNegInfLong")) dataAddr = LvalueNegInfLong;
+    else if(!strcmp(dataName, "shiftMask")) dataAddr = LshiftMask;
+    else if(!strcmp(dataName, "value64")) dataAddr = Lvalue64;
+    else if(!strcmp(dataName, "64bits")) dataAddr = L64bits;
+    else if(!strcmp(dataName, "strClassCastExceptionPtr")) dataAddr = LstrClassCastExceptionPtr;
+    else if(!strcmp(dataName, "strInstantiationError")) dataAddr = LstrInstantiationErrorPtr;
+    else if(!strcmp(dataName, "gDvmInlineOpsTable")) dataAddr = (int)gDvmInlineOpsTable;
+    else {
+        ALOGI("JIT_INFO: global data %s not supported\n", dataName);
+        SET_JIT_ERROR(kJitErrorGlobalData);
+    }
+    return dataAddr;
+}
+
+//for shared code cache, we use scratchRegs[0] & [1]
+int load_imm_global_data_API(const char* dataName,
+                         OpndSize size,
+                         int reg, bool isPhysical) {
+
+    //find the address from name
+    int dataAddr = getGlobalDataAddr(dataName);
+    if (dataAddr == 0)
+        return -1;
+    move_imm_to_reg(size, dataAddr, reg, isPhysical);
+    return 0;
+}
+//for shared code cache, we use scratchRegs[0] & [1] & [2]
+//FIXME: [2] is assumed to be hard-coded register
+int load_global_data_API(const char* dataName,
+                         OpndSize size,
+                         int reg, bool isPhysical) {
+
+    //find the address from name
+    int dataAddr = getGlobalDataAddr(dataName);
+    if (dataAddr == 0)
+        return -1;
+    move_mem_to_reg(size, dataAddr, PhysicalReg_Null, true, reg, isPhysical);
+    return 0;
+}
+int load_sd_global_data_API(const char* dataName,
+                            int reg, bool isPhysical) {
+
+    //find the address from name
+    int dataAddr = getGlobalDataAddr(dataName);
+    if (dataAddr == 0)
+        return -1;
+    move_sd_mem_to_reg(dataAddr, PhysicalReg_Null, true, reg, isPhysical);
+    return 0;
+}
+
+int load_fp_stack_global_data_API(const char* dataName,
+                                  OpndSize size) {
+
+    int dataAddr = getGlobalDataAddr(dataName);
+    if (dataAddr == 0)
+        return -1;
+    load_int_fp_stack_imm(size, dataAddr); //fildl
+    return 0;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/NcgAot.h b/vm/compiler/codegen/x86/lightcg/NcgAot.h
new file mode 100644
index 0000000..4d3d978
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/NcgAot.h
@@ -0,0 +1,48 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+
+#ifndef _DALVIK_NCG_AOT
+#define _DALVIK_NCG_AOT
+int ncgAppGetEIP();
+int get_eip_API();
+int invokeInterpreter(bool fromApp);
+int invokeNcg(bool fromApp);
+int jumpToInterpNoChain();
+int jumpToInterpPunt();
+int jumpToExceptionThrown(int exceptionNum);
+void callFuncPtr(int funcPtr, const char* funcName);
+/** @brief generate a "call imm32"*/
+void callFuncPtrImm(int funcPtr);
+int call_helper_API(const char* helperName);
+int conditional_jump_global_API(
+                                ConditionCode cc, const char* target,
+                                bool isShortTerm);
+int unconditional_jump_global_API(
+                                  const char* target, bool isShortTerm);
+int load_imm_global_data_API(const char* dataName,
+                             OpndSize size,
+                             int reg, bool isPhysical);
+int load_global_data_API(const char* dataName,
+                         OpndSize size,
+                         int reg, bool isPhysical);
+int load_sd_global_data_API(const char* dataName,
+                            int reg, bool isPhysical);
+int load_fp_stack_global_data_API(const char* dataName,
+                                  OpndSize size);
+#endif
+
diff --git a/vm/compiler/codegen/x86/lightcg/NcgHelper.cpp b/vm/compiler/codegen/x86/lightcg/NcgHelper.cpp
new file mode 100644
index 0000000..739dc2a
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/NcgHelper.cpp
@@ -0,0 +1,104 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#include "Dalvik.h"
+#include "NcgHelper.h"
+#include "interp/InterpDefs.h"
+
+
+/*
+ * Find the matching case.  Returns the offset to the handler instructions.
+ *
+ * Returns 3 if we don't find a match (it's the size of the packed-switch
+ * instruction).
+ */
+s4 dvmNcgHandlePackedSwitch(const s4* entries, s4 firstKey, u2 size, s4 testVal)
+{
+    //skip add_reg_reg (ADD_REG_REG_SIZE) and jump_reg (JUMP_REG_SIZE)
+    const int kInstrLen = 4; //default to next bytecode
+    if (testVal < firstKey || testVal >= firstKey + size) {
+        LOGVV("Value %d not found in switch (%d-%d)",
+            testVal, firstKey, firstKey+size-1);
+        return kInstrLen;
+    }
+
+    assert(testVal - firstKey >= 0 && testVal - firstKey < size);
+    LOGVV("Value %d found in slot %d (goto 0x%02x)",
+        testVal, testVal - firstKey,
+        s4FromSwitchData(&entries[testVal - firstKey]));
+    return s4FromSwitchData(&entries[testVal - firstKey]);
+
+}
+/* return the number of bytes to increase the bytecode pointer by */
+s4 dvmJitHandlePackedSwitch(const s4* entries, s4 firstKey, u2 size, s4 testVal)
+{
+    if (testVal < firstKey || testVal >= firstKey + size) {
+        LOGVV("Value %d not found in switch (%d-%d)",
+            testVal, firstKey, firstKey+size-1);
+        return 2*3;//bytecode packed_switch is 6(2*3) bytes long
+    }
+
+    LOGVV("Value %d found in slot %d (goto 0x%02x)",
+        testVal, testVal - firstKey,
+        s4FromSwitchData(&entries[testVal - firstKey]));
+    return 2*s4FromSwitchData(&entries[testVal - firstKey]); //convert from u2 to byte
+
+}
+/*
+ * Find the matching case.  Returns the offset to the handler instructions.
+ *
+ * Returns 3 if we don't find a match (it's the size of the sparse-switch
+ * instruction).
+ */
+s4 dvmNcgHandleSparseSwitch(const s4* keys, u2 size, s4 testVal)
+{
+    const int kInstrLen = 4; //CHECK
+    const s4* entries = keys + size;
+    int i;
+    for (i = 0; i < size; i++) {
+        s4 k = s4FromSwitchData(&keys[i]);
+        if (k == testVal) {
+            LOGVV("Value %d found in entry %d (goto 0x%02x)",
+                testVal, i, s4FromSwitchData(&entries[i]));
+            return s4FromSwitchData(&entries[i]);
+        } else if (k > testVal) {
+            break;
+        }
+    }
+
+    LOGVV("Value %d not found in switch", testVal);
+    return kInstrLen;
+}
+/* return the number of bytes to increase the bytecode pointer by */
+s4 dvmJitHandleSparseSwitch(const s4* keys, u2 size, s4 testVal)
+{
+    const s4* entries = keys + size;
+    int i;
+    for (i = 0; i < size; i++) {
+        s4 k = s4FromSwitchData(&keys[i]);
+        if (k == testVal) {
+            LOGVV("Value %d found in entry %d (goto 0x%02x)",
+                testVal, i, s4FromSwitchData(&entries[i]));
+            return 2*s4FromSwitchData(&entries[i]); //convert from u2 to byte
+        } else if (k > testVal) {
+            break;
+        }
+    }
+
+    LOGVV("Value %d not found in switch", testVal);
+    return 2*3; //bytecode sparse_switch is 6(2*3) bytes long
+}
diff --git a/vm/compiler/codegen/x86/lightcg/NcgHelper.h b/vm/compiler/codegen/x86/lightcg/NcgHelper.h
new file mode 100644
index 0000000..ec56c85
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/NcgHelper.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+#ifndef _DALVIK_NCG_HELPER
+#define _DALVIK_NCG_HELPER
+#include "mterp/Mterp.h"
+s4 dvmNcgHandlePackedSwitch(const s4*, s4, u2, s4);
+s4 dvmNcgHandleSparseSwitch(const s4*, u2, s4);
+extern "C" s4 dvmJitHandlePackedSwitch(const s4*, s4, u2, s4);
+s4 dvmJitHandleSparseSwitch(const s4*, u2, s4);
+extern "C" void dvmNcgInvokeInterpreter(int pc); //interpreter to execute at pc
+extern "C" void dvmNcgInvokeNcg(int pc);
+
+#if defined(WITH_JIT)
+extern "C" void dvmJitHelper_returnFromMethod();
+extern "C" void dvmJitToInterpNormal(int targetpc); //in %ebx
+/** @brief interface function between FI and JIT for backward chaining cell */
+extern "C" void dvmJitToInterpBackwardBranch(int targetpc);
+extern "C" void dvmJitToInterpTraceSelect(int targetpc); //in %ebx
+extern "C" void dvmJitToInterpTraceSelectNoChain(int targetpc); //in %ebx
+extern "C" void dvmJitToInterpNoChain(int targetpc); //in %eax
+extern "C" void dvmJitToInterpNoChainNoProfile(int targetpc); //in %eax
+extern "C" void dvmJitToInterpPunt(int targetpc); //in currentPc
+extern "C" void dvmJitToExceptionThrown(int targetpc); //in currentPc
+#endif
+
+extern "C" const Method *dvmJitToPatchPredictedChain(const Method *method,
+                                          Thread *self,
+                                          PredictedChainingCell *cell,
+                                          const ClassObject *clazz);
+#endif /*_DALVIK_NCG_HELPER*/
diff --git a/vm/compiler/codegen/x86/lightcg/Profile.cpp b/vm/compiler/codegen/x86/lightcg/Profile.cpp
new file mode 100644
index 0000000..076b524
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/Profile.cpp
@@ -0,0 +1,492 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "Dalvik.h"
+#include "libdex/DexOpcodes.h"
+
+#include "compiler/CompilerInternals.h"
+#include <sys/mman.h>           /* for protection change */
+#include "Profile.h"
+#include "Singleton.h"
+#include "Scheduler.h"
+
+#if defined(WITH_JIT_TPROFILE)
+/*
+ * Translation layout in the code cache.  Note that the codeAddress pointer
+ * in JitTable will point directly to the code body (field codeAddress).  The
+ * chain cell offset codeAddress - 4, the address of the trace profile counter
+ * is at codeAddress - 8, and the loop counter address is codeAddress - 12.
+ *
+ *      +----------------------------+
+ *      | Trace Loop Counter addr    |  -> 4 bytes (EXTRA_BYTES_FOR_LOOP_COUNT_ADDR)
+ *      +----------------------------+
+ *      | Trace Profile Counter addr |  -> 4 bytes (EXTRA_BYTES_FOR_PROF_ADDR)
+ *      +----------------------------+
+ *   +--| Offset to chain cell counts|  -> 2 bytes (CHAIN_CELL_COUNT_OFFSET)
+ *   |  +----------------------------+
+ *   |  | Offset to chain cell       |  -> 2 bytes (CHAIN_CELL_OFFSET)
+ *   |  +----------------------------+
+ *   |  | Trace profile code         |  <- entry point when profiling (16 bytes)
+ *   |  .  -   -   -   -   -   -   - .
+ *   |  | Code body                  |  <- entry point when not profiling
+ *   |  .                            .
+ *   |  |                            |
+ *   |  +----------------------------+
+ *   |  | Chaining Cells             |  -> 16/20 bytes, 4 byte aligned
+ *   |  .                            .
+ *   |  .                            .
+ *   |  |                            |
+ *   |  +----------------------------+
+ *   |  | Gap for large switch stmt  |  -> # cases >= MAX_CHAINED_SWITCH_CASES
+ *   |  +----------------------------+
+ *   +->| Chaining cell counts       |  -> 8 bytes, chain cell counts by type
+ *      +----------------------------+
+ *      | Trace description          |  -> variable sized
+ *      .                            .
+ *      |                            |
+ *      +----------------------------+
+ *      | # Class pointer pool size  |  -> 4 bytes
+ *      +----------------------------+
+ *      | Class pointer pool         |  -> 4-byte aligned, variable size
+ *      .                            .
+ *      .                            .
+ *      |                            |
+ *      +----------------------------+
+ *      | Literal pool               |  -> 4-byte aligned, variable size
+ *      .                            .
+ *      .                            .
+ *      |                            |
+ *      +----------------------------+
+ *
+ */
+
+/**
+ * @brief A map between bytecode offset and source code line number
+ */
+typedef struct jitProfileAddrToLine {
+    u4 lineNum;              /**< @brief The source code line number */
+    u4 bytecodeOffset;       /**< @brief The bytecode offset */
+} jitProfileAddrToLine;
+
+/**
+ * @brief Get the loop counter's address
+ * @param p The JitEntry of the trace
+ * @return The address of the loop counter
+ */
+static inline char *getLoopCounterBase(const JitEntry *p)
+{
+    return (char*)p->codeAddress -
+        (EXTRA_BYTES_FOR_PROF_ADDR + EXTRA_BYTES_FOR_CHAINING + EXTRA_BYTES_FOR_LOOP_COUNT_ADDR);
+}
+
+/**
+ * @brief Get the trace counter's address
+ * @param p The JitEntry of the trace
+ * @return The address of the trace counter
+ */
+static inline char *getTraceCounterBase(const JitEntry *p)
+{
+    return (char*)p->codeAddress -
+        (EXTRA_BYTES_FOR_PROF_ADDR + EXTRA_BYTES_FOR_CHAINING);
+}
+
+/**
+ * @brief Check the trace's loop info
+ * @param entry The JitEntry of the trace
+ * @return 0 for non-loop, -1 for nested loop, otherwise non-nested loop
+ */
+static inline int checkLoopInfo(const JitEntry *entry)
+{
+    if (entry->dPC == 0 || entry->codeAddress == 0) {
+        return 0;
+    }
+
+    JitTraceCounter_t **addr = (JitTraceCounter_t **) getLoopCounterBase(entry);
+    return (int) *addr;
+}
+
+/**
+ * @brief Retrieve the profile loop count for a loop trace
+ * @param entry The JitEntry of the trace
+ * @return The loop count value
+ */
+static inline JitTraceCounter_t getProfileLoopCount(const JitEntry *entry)
+{
+    if (entry->dPC == 0 || entry->codeAddress == 0) {
+        return 0;
+    }
+
+    JitTraceCounter_t **p = (JitTraceCounter_t **) getLoopCounterBase(entry);
+
+    return **p;
+}
+
+/**
+ * @brief Callback function to track the bytecode offset/line number relationiship
+ * @param cnxt A point of jitProfileAddrToLine
+ * @param bytecodeOffset The offset of the bytecode
+ * @param lineNum The line number
+ * @return 0 for success
+ */
+static int addrToLineCb (void *cnxt, u4 bytecodeOffset, u4 lineNum)
+{
+    jitProfileAddrToLine *addrToLine = (jitProfileAddrToLine *) cnxt;
+
+    /* Best match so far for this offset */
+    if (addrToLine->bytecodeOffset >= bytecodeOffset) {
+        addrToLine->lineNum = lineNum;
+    }
+    return 0;
+}
+
+/**
+ * @brief Reset the trace profile count
+ * @param entry The JitEntry of the trace
+ */
+static inline void resetProfileCount(const JitEntry *entry)
+{
+    if (entry->dPC == 0 || entry->codeAddress == 0) {
+        return;
+    }
+
+    JitTraceCounter_t **p = (JitTraceCounter_t **) getTraceCounterBase(entry);
+
+    **p = 0;
+}
+
+/**
+ * @brief Get the pointer of the chain cell count
+ * @param base The pointer point to trace counter
+ * @return The pointer of Chain Cell Counts
+ */
+static inline ChainCellCounts* getChainCellCountsPointer(const char *base)
+{
+    /* 4 is the size of the profile count */
+    u2 *chainCellOffsetP = (u2 *) (base + EXTRA_BYTES_FOR_PROF_ADDR);
+    u2 chainCellOffset = *chainCellOffsetP;
+    return (ChainCellCounts *) ((char *) chainCellOffsetP + chainCellOffset + EXTRA_BYTES_FOR_CHAINING);
+}
+
+/**
+ * @brief Get the starting pointer of the trace description section
+ * @param base The pointer point to trace counter
+ * @return The pointer of Trace Description
+ */
+static JitTraceDescription* getTraceDescriptionPointer(const char *base)
+{
+    ChainCellCounts* pCellCounts = getChainCellCountsPointer(base);
+    return (JitTraceDescription*) ((char*)pCellCounts + sizeof(*pCellCounts));
+}
+
+/**
+ * @brief Retrieve the trace profile count
+ * @param entry The JitEntry of the trace
+ * @return The trace profile count
+ */
+static inline JitTraceCounter_t getProfileCount(const JitEntry *entry)
+{
+    if (entry->dPC == 0 || entry->codeAddress == 0) {
+        return 0;
+    }
+
+    JitTraceCounter_t **p = (JitTraceCounter_t **) getTraceCounterBase(entry);
+
+    return **p;
+}
+
+/**
+ * @brief Qsort callback function
+ * @param entry1 The JitEntry compared
+ * @param entry2 The JitEntry compared
+ * @return 0 if count is equal, -1 for entry1's counter greater than entry2's, otherwise return 1
+ */
+static int sortTraceProfileCount(const void *entry1, const void *entry2)
+{
+    const JitEntry *jitEntry1 = (const JitEntry *)entry1;
+    const JitEntry *jitEntry2 = (const JitEntry *)entry2;
+
+    JitTraceCounter_t count1 = getProfileCount(jitEntry1);
+    JitTraceCounter_t count2 = getProfileCount(jitEntry2);
+
+    return (count1 == count2) ? 0 : ((count1 > count2) ? -1 : 1);
+}
+
+/**
+ * @brief Dumps profile info for a single trace
+ * @param p The JitEntry of the trace
+ * @param silent Wheter to dump the trace count info
+ * @param reset Whether to reset the counter
+ * @param sum The total count of all the trace
+ * @return The trace count
+ */
+static int dumpTraceProfile(JitEntry *p, bool silent, bool reset,
+                            unsigned long sum)
+{
+    int idx;
+
+    if (p->codeAddress == 0) {
+        if (silent == false) {
+            ALOGD("TRACEPROFILE NULL");
+        }
+        return 0;
+    }
+
+    JitTraceCounter_t count = getProfileCount(p);
+
+    if (reset == true) {
+        resetProfileCount(p);
+    }
+    if (silent == true) {
+        return count;
+    }
+
+    JitTraceDescription *desc = getTraceDescriptionPointer(getTraceCounterBase(p));
+    const Method *method = desc->method;
+    char *methodDesc = dexProtoCopyMethodDescriptor(&method->prototype);
+    jitProfileAddrToLine addrToLine = {0, desc->trace[0].info.frag.startOffset};
+
+    /*
+     * We may end up decoding the debug information for the same method
+     * multiple times, but the tradeoff is we don't need to allocate extra
+     * space to store the addr/line mapping. Since this is a debugging feature
+     * and done infrequently so the slower but simpler mechanism should work
+     * just fine.
+     */
+    dexDecodeDebugInfo(method->clazz->pDvmDex->pDexFile,
+                       dvmGetMethodCode(method),
+                       method->clazz->descriptor,
+                       method->prototype.protoIdx,
+                       method->accessFlags,
+                       addrToLineCb, 0, &addrToLine);
+
+    ALOGD("TRACEPROFILE 0x%08x % 10d %5.2f%% [%#x(+%d), %d] %s%s;%s",
+         (int) getTraceCounterBase(p),
+         count,
+         ((float ) count) / sum * 100.0,
+         desc->trace[0].info.frag.startOffset,
+         desc->trace[0].info.frag.numInsts,
+         addrToLine.lineNum,
+         method->clazz->descriptor, method->name, methodDesc);
+    free(methodDesc);
+    methodDesc = 0;
+
+    if (checkLoopInfo(p) != 0 && checkLoopInfo(p) != -1) {
+        ALOGD("++++++++++ Loop Trace, loop executed: %d ++++++++++", (int) getProfileLoopCount(p));
+    } else if (checkLoopInfo(p) == -1) {
+        ALOGD("++++++++++ Loop Trace with Nested Loop, can't handle the loop counter for this currently ++++++++++");
+    }
+
+    /* Find the last fragment (ie runEnd is set) */
+    for (idx = 0;
+         (desc->trace[idx].isCode == true) && (desc->trace[idx].info.frag.runEnd == false);
+         idx++) {
+    }
+
+    /*
+     * runEnd must comes with a JitCodeDesc frag. If isCode is false it must
+     * be a meta info field (only used by callsite info for now).
+     */
+    if (desc->trace[idx].isCode == false) {
+        const Method *method = (const Method *)
+            desc->trace[idx+JIT_TRACE_CUR_METHOD-1].info.meta;
+        char *methodDesc = dexProtoCopyMethodDescriptor(&method->prototype);
+        /* Print the callee info in the trace */
+        ALOGD("    -> %s%s;%s", method->clazz->descriptor, method->name,
+             methodDesc);
+        free(methodDesc);
+        methodDesc = 0;
+    }
+    return count;
+}
+
+/**
+ * @brief Get the size of a jit trace description
+ * @param desc the point of jit trace description we want check
+ * @return The size of the jit trace description
+ */
+int getTraceDescriptionSize(const JitTraceDescription *desc)
+{
+    int runCount;
+    /* Trace end is always of non-meta type (ie isCode == true) */
+    for (runCount = 0; ; runCount++) {
+        if (desc->trace[runCount].isCode &&
+            desc->trace[runCount].info.frag.runEnd)
+           break;
+    }
+    return sizeof(JitTraceDescription) + ((runCount+1) * sizeof(JitTraceRun));
+}
+
+/**
+ * @brief Generate the loop counter profile code for loop trace
+ *   Currently only handle the loop trace without nested loops, so just add code to bump up the loop counter before the loop entry basic block
+ *   For loop trace with nested loops, set the loop counter's addr to -1
+ * @param cUnit The compilation unit of the trace
+ * @param bb The basic block is processing
+ * @param bbO1 The processing basic block's BasicBlock_O1 version
+ * @return the size (in bytes) of the generated code
+ */
+int genLoopCounterProfileCode(CompilationUnit *cUnit, BasicBlock_O1 *bbO1)
+{
+    //If the trace is loop trace without nested loop, and the bb processing is the loop entry basic block,
+    //      add loop counter before the trace stream and profile code before the bb
+    //else if the trace is loop trace with nested loop, and the bb processing is the loop entry basic block,
+    //      set the loop counter to -1, so that we can dump the infomation later
+    LoopInformation *info = cUnit->loopInformation;
+    if (info != 0 && bbO1->lastMIRInsn != 0 && info->getLoopInformationByEntry(bbO1) != 0) {
+        if (info->getLoopInformationByEntry(bbO1) != 0) {
+            int nesting = info->getNestedNbr();
+            if (nesting == 0) {
+                if ((gDvmJit.profileMode == kTraceProfilingContinuous) ||
+                    (gDvmJit.profileMode == kTraceProfilingDisabled)) {
+                        //Set the loop counter address
+                        intptr_t addr = (intptr_t)dvmJitNextTraceCounter();
+                        unsigned int *intaddr = reinterpret_cast<unsigned int *>(streamMethodStart
+                                    - EXTRA_BYTES_FOR_LOOP_COUNT_ADDR - EXTRA_BYTES_FOR_PROF_ADDR - EXTRA_BYTES_FOR_CHAINING);
+                        *intaddr = addr;
+
+                        //Add the code before loop entry basic block to bump up the loop counter, the generated code may looks like (19 bytes):
+                        //  LEA -4(ESP), ESP
+                        //  MOV EAX, 0(ESP)
+                        //  MOV #80049734, EAX
+                        //  ADD #1, 0(EAX)
+                        //  MOV 0(ESP), EAX
+                        //  LEA 4(ESP), ESP
+                        load_effective_addr(-4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+                        move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
+                        move_imm_to_reg(OpndSize_32, (int)addr, PhysicalReg_EAX, true);
+                        alu_binary_imm_mem(OpndSize_32, add_opc, 1, 0, PhysicalReg_EAX, true);
+                        move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EAX, true);
+                        load_effective_addr(4, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
+
+                        return 19;
+                }
+            } else {
+                // TODO: we should refine the nested loop handle when nested loop enabled
+                // For nested loop, currently we just set the loop counter's addr to -1
+                ALOGD("This trace contains nested loops, cann't handle this currently");
+                unsigned int *intaddr = reinterpret_cast<unsigned int *>(streamMethodStart
+                                    - EXTRA_BYTES_FOR_LOOP_COUNT_ADDR - EXTRA_BYTES_FOR_PROF_ADDR - EXTRA_BYTES_FOR_CHAINING);
+                *intaddr = -1;
+
+                return 0;
+            }
+        }
+    }
+    return 0;
+}
+#endif /* WITH_JIT_TPROFILE */
+
+/**
+ * @brief Sort the trace profile counts and dump them
+ */
+void dvmCompilerSortAndPrintTraceProfiles(void)
+{
+#if defined(WITH_JIT_TPROFILE)
+    JitEntry *sortedEntries;
+    int numTraces = 0;
+    unsigned long sum = 0;
+    unsigned int i;
+
+    /* Make sure that the table is not changing */
+    dvmLockMutex(&gDvmJit.tableLock);
+
+    /* Sort the entries by descending order */
+    sortedEntries = (JitEntry *)alloca(sizeof(JitEntry) * gDvmJit.jitTableSize);
+    memcpy(sortedEntries, gDvmJit.pJitEntryTable,
+           sizeof(JitEntry) * gDvmJit.jitTableSize);
+    qsort(sortedEntries, gDvmJit.jitTableSize, sizeof(JitEntry),
+          sortTraceProfileCount);
+
+    /* Dump the sorted entries */
+    for (i=0; i < gDvmJit.jitTableSize; i++) {
+        if (sortedEntries[i].dPC != 0) {
+            sum += dumpTraceProfile(&sortedEntries[i],
+                                    true,
+                                    false,
+                                    0);
+            numTraces++;
+        }
+    }
+
+    if (numTraces == 0) {
+        numTraces = 1;
+    }
+    if (sum == 0) {
+        sum = 1;
+    }
+
+    ALOGI("JIT: Average execution count -> %d",(int)(sum / numTraces));
+
+    /* Dump the sorted entries. The count of each trace will be reset to 0. */
+    for (i=0; i < gDvmJit.jitTableSize; i++) {
+        if (sortedEntries[i].dPC != 0) {
+                dumpTraceProfile(&sortedEntries[i],
+                             false /* silent */,
+                             true /* reset */,
+                             sum);
+        }
+    }
+
+done:
+    dvmUnlockMutex(&gDvmJit.tableLock);
+#endif
+    return;
+}
+
+/**
+ *@brief Generate the trace count profile code before the begin of trace code
+ *@details Reserve 12 bytes at the beginning of the trace
+ *        +----------------------------+
+ *        | loop counter addr (4 bytes)|
+ *        +----------------------------+
+ *        | prof counter addr (4 bytes)|
+ *        +----------------------------+
+ *        | chain cell offset (4 bytes)|
+ *        +----------------------------+
+ *
+ * ...and then code to increment the execution
+ *
+ * For continuous profiling (16 bytes)
+ *       MOV   EAX, addr     @ get prof count addr    [5 bytes]
+ *       ADD   #1, 0(EAX)    @ increment counter      [6 bytes]
+ *       NOPS                                         [5 bytes]
+ *
+ *@param cUnit the compilation unit
+ *@return the size (in bytes) of the generated code.
+ */
+int genTraceProfileEntry(CompilationUnit *cUnit)
+{
+#if defined(WITH_JIT_TPROFILE)
+    intptr_t addr = (intptr_t)dvmJitNextTraceCounter();
+    assert(__BYTE_ORDER == __LITTLE_ENDIAN);
+    unsigned int *intaddr = reinterpret_cast<unsigned int *>(streamMethodStart - EXTRA_BYTES_FOR_PROF_ADDR - EXTRA_BYTES_FOR_CHAINING);
+    *intaddr = addr;
+
+    cUnit->headerSize = EXTRA_BYTES_FOR_PROF_ADDR + EXTRA_BYTES_FOR_CHAINING + EXTRA_BYTES_FOR_LOOP_COUNT_ADDR;
+    if ((gDvmJit.profileMode == kTraceProfilingContinuous) ||
+        (gDvmJit.profileMode == kTraceProfilingDisabled)) {
+        move_imm_to_reg(OpndSize_32, (int)addr, PhysicalReg_EAX, true);
+        alu_binary_imm_mem(OpndSize_32, add_opc, 1, 0, PhysicalReg_EAX, true);
+        if(gDvmJit.scheduling == true) {
+            singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
+        }
+        /*Add 5 nops to the end to make sure trace can align with 16B*/
+        stream = encoder_nops(5, stream);
+        return 16;
+    }
+#endif
+    return 0;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/Profile.h b/vm/compiler/codegen/x86/lightcg/Profile.h
new file mode 100644
index 0000000..9d58a43
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/Profile.h
@@ -0,0 +1,61 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef H_ASSEMBLE
+#define H_ASSEMBLE
+
+struct BasicBlock_O1;
+struct JitTraceDescription;
+struct CompilationUnit;
+
+/* 4 is the number  f additional bytes needed for chaining information for trace:
+ * 2 bytes for chaining cell count offset and 2 bytes for chaining cell offset */
+#define EXTRA_BYTES_FOR_CHAINING 4
+
+#ifdef WITH_JIT_TPROFILE
+
+/* 4 is the number  f additional bytes needed for loop count addr */
+#define EXTRA_BYTES_FOR_LOOP_COUNT_ADDR 4
+/* 4 is the number  f additional bytes needed for execution count addr */
+#define EXTRA_BYTES_FOR_PROF_ADDR 4
+
+/**
+ * @brief Get the size of a jit trace description
+ * @param desc the point of jit trace description we want check
+ * @return The size of the jit trace description
+ */
+int getTraceDescriptionSize(const JitTraceDescription *desc);
+
+/**
+ * @brief Generate the loop counter profile code for loop trace
+ *   Currently only handle the loop trace without nested loops, so just add code to bump up the loop counter before the loop entry basic block
+ *   For loop trace with nested loops, set the loop counter's addr to -1
+ * @param cUnit The compilation unit of the trace
+ * @param bbO1 The current basic block being processed
+ * @return the size (in bytes) of the generated code
+ */
+int genLoopCounterProfileCode(CompilationUnit *cUnit, BasicBlock_O1 *bbO1);
+
+#endif /* WITH_JIT_TPROFILE */
+
+/**
+ * @brief Generate the trace counter profile code for each trace
+ * @param cUnit The compilation unit of the trace
+ * @return the size (in bytes) of the generated code
+ */
+int genTraceProfileEntry(CompilationUnit *cUnit);
+
+#endif
diff --git a/vm/compiler/codegen/x86/lightcg/RegisterizationBE.cpp b/vm/compiler/codegen/x86/lightcg/RegisterizationBE.cpp
new file mode 100644
index 0000000..77f86e1
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/RegisterizationBE.cpp
@@ -0,0 +1,1569 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <map>
+#include <set>
+#include <algorithm>
+#include "Dalvik.h"
+#include "Lower.h"
+#include "AnalysisO1.h"
+#include "CodegenErrors.h"
+#include "RegisterizationBE.h"
+
+//#define DEBUG_REGISTERIZATION
+
+#ifdef DEBUG_REGISTERIZATION
+#define DEBUG_ASSOCIATION(X) X
+#define DEBUG_SPILLING(X) X
+#define DEBUG_ASSOCIATION_MERGE(X) X
+#define DEBUG_COMPILETABLE_UPDATE(X) X
+#else
+#define DEBUG_ASSOCIATION(X)
+#define DEBUG_SPILLING(X)
+#define DEBUG_ASSOCIATION_MERGE(X)
+#define DEBUG_COMPILETABLE_UPDATE(X)
+#endif
+
+AssociationTable::AssociationTable(void) {
+    //Call clear function, it will reset everything
+    clear();
+}
+
+AssociationTable::~AssociationTable(void) {
+    //Call clear function, it will reset everything
+    clear();
+}
+
+void AssociationTable::clear(void) {
+    DEBUG_ASSOCIATION (ALOGD ("Clearing association table\n"));
+
+    //Clear maps
+    associations.clear();
+    inMemoryTracker.clear();
+    constTracker.clear();
+
+    //Have we finalized the table
+    isFinal = false;
+}
+
+bool AssociationTable::copy(AssociationTable &source) {
+    //We cannot copy anything if we are finalized
+    assert (hasBeenFinalized() == false);
+
+    //Insert all associations from source
+    associations.insert(source.associations.begin(), source.associations.end());
+
+    //Insert all memory trackers
+    inMemoryTracker.insert(source.inMemoryTracker.begin(),
+            source.inMemoryTracker.end());
+
+    //Insert all constants
+    constTracker.insert(source.constTracker.begin(), source.constTracker.end());
+
+    //Finalize the current table and return success
+    finalize();
+    return true;
+}
+
+bool AssociationTable::associate(const CompileTableEntry &compileEntry)
+{
+    // We cannot update once the association table has been finalized
+    assert (hasBeenFinalized() == false);
+
+    // Paranoid: this must be a virtual register
+    assert (compileEntry.isVirtualReg () == true);
+
+    //Get local versions of the compileEntry
+    int VR = compileEntry.regNum;
+    int physicalReg = compileEntry.physicalReg;
+
+    bool safeToUpdate = true;
+
+    // Check if we are overwriting an existing association
+    iterator assocEntry = associations.find(VR);
+    if (assocEntry != associations.end())
+    {
+        int oldPhysicalReg = assocEntry->second.physicalReg;
+
+        //If the new physical register is null, then we don't want to update the
+        //association that we saved already.
+        if (physicalReg == PhysicalReg_Null)
+        {
+            safeToUpdate = false;
+        }
+
+        // We might be saving VRs even when they don't have physical register
+        // associated and thus we don't care for overwriting unless one has
+        // physical register
+        if (oldPhysicalReg != PhysicalReg_Null && physicalReg != PhysicalReg_Null) {
+            // Overwriting an association must mean that we are reading from a source
+            // that has the duplicate entries for the same VR. Most likely this can
+            // happen when a VR is associated with XMM and GP in same trace
+            ALOGI ("JIT_INFO: Overwriting association of v%d:%s with %s\n", VR,
+                    physicalRegToString(static_cast<PhysicalReg>(oldPhysicalReg)),
+                    physicalRegToString(static_cast<PhysicalReg>(physicalReg)));
+            SET_JIT_ERROR(kJitErrorBERegisterization);
+            return false;
+        }
+    }
+
+    //We only do the update if it is safe
+    if (safeToUpdate == true)
+    {
+        if (assocEntry != associations.end())
+        {
+            //If we already have an entry for this VR then simply update its compile entry
+            assocEntry->second = compileEntry;
+        }
+        else
+        {
+            //Otherwise we insert it into our associations
+            associations.insert (std::make_pair (VR, compileEntry));
+        }
+
+        DEBUG_ASSOCIATION (ALOGD ("Associating v%d with %s\n", VR,
+                physicalRegToString(static_cast<PhysicalReg> (physicalReg))));
+    }
+
+    //Report success
+    return true;
+}
+
+bool AssociationTable::associate(const MemoryVRInfo &memVRInfo) {
+    // We cannot update once the association table has been finalized
+    assert (hasBeenFinalized() == false);
+
+    int VR = memVRInfo.regNum;
+
+    // Make a copy of the in memory information
+    inMemoryTracker[VR] = memVRInfo;
+
+    return true;
+}
+
+bool AssociationTable::associate(const ConstVRInfo &constVRInfo) {
+    // We cannot update once the association table has been finalized
+    assert (hasBeenFinalized() == false);
+
+    int VR = constVRInfo.regNum;
+
+    // Make a copy of the in memory information
+    constTracker[VR] = constVRInfo;
+
+    return true;
+}
+
+bool AssociationTable::wasVRInMemory(int VR) const
+{
+    //Find the VR in the inMemoryTracker map
+    inMemoryTrackerConstIterator entry = inMemoryTracker.find(VR);
+
+    //If we cannot find then it must be in memory. Our parent would have kept track of it
+    //if it used it. Since it did not use it, it must be in memory.
+    if (entry == inMemoryTracker.end())
+    {
+        return true;
+    }
+    else
+    {
+        //Return what the entry tells us
+        return entry->second.inMemory;
+    }
+}
+
+bool AssociationTable::wasVRConstant(int VR) const
+{
+    //Find the VR in the constTracker map
+    constantTrackerConstIterator entry = constTracker.find(VR);
+
+    //Return whether we found it
+    return (entry != constTracker.end());
+}
+
+int AssociationTable::getVRConstValue(int VR) const
+{
+    //Find the VR in the constTracker map
+    constantTrackerConstIterator entry = constTracker.find(VR);
+
+    //Paranoid: this function should not be called if wasVRConstant returns false
+    assert(entry != constTracker.end());
+
+    //Return value
+    return entry->second.value;
+}
+
+void AssociationTable::finalize() {
+    //Set to final
+    isFinal = true;
+}
+
+void AssociationTable::findUsedRegisters (std::set<PhysicalReg> & outUsedRegisters) const
+{
+    // Go through all association table entries and find used registers
+    for (const_iterator iter = begin(); iter != end(); iter++)
+    {
+        //Get a local version of the register used
+        PhysicalReg regUsed = iter->second.getPhysicalReg ();
+
+        //If not the Null register, insert it
+        if (regUsed != PhysicalReg_Null)
+        {
+            outUsedRegisters.insert(regUsed);
+        }
+    }
+}
+
+void AssociationTable::printToDot(FILE * file) {
+    DEBUG_ASSOCIATION(ALOGD("Printing association table to dot file"));
+
+    if (associations.size() == 0) {
+        fprintf(file, " {Association table is empty} |\\\n");
+    }
+    else
+    {
+        fprintf(file, " {Association table at entry:}|\\\n");
+
+        //Now go through the iteration
+        for (const_iterator iter = associations.begin(); iter != associations.end(); iter++) {
+            //If it's a constant, print it out using %d
+            if (wasVRConstant(iter->second.regNum)) {
+                fprintf(file, "{v%d : %d} | \\\n", iter->first,
+                        getVRConstValue(iter->second.regNum));
+            } else {
+                //Otherwise, use the physicalRegToString function
+                fprintf(file, "{v%d : %s} | \\\n", iter->first,
+                        physicalRegToString(
+                            static_cast<PhysicalReg>(iter->second.physicalReg)));
+            }
+        }
+    }
+}
+
+static bool shouldSaveAssociation(const CompileTableEntry &compileEntry)
+{
+    int vR = compileEntry.regNum;
+    LowOpndRegType type = compileEntry.getPhysicalType ();
+
+    // We want to save association if the VR is either in a physical register or is a constant
+    bool res = compileEntry.inPhysicalRegister () || isVirtualRegConstant (vR, type, 0, false) != VR_IS_NOT_CONSTANT;
+
+    return res;
+}
+
+bool AssociationTable::syncAssociationsWithCompileTable (AssociationTable & associationsToUpdate)
+{
+    if (associationsToUpdate.hasBeenFinalized ())
+    {
+        ALOGI ("JIT_INFO: Association table has been finalized but we want to update it.");
+        SET_JIT_ERROR(kJitErrorBERegisterization);
+        return false;
+    }
+
+    // Go through each entry of the compile table
+    for (int entry = 0; entry < compileTable.size (); entry++)
+    {
+        // Update associations for every VR entry we find
+        if (compileTable[entry].isVirtualReg () == true && shouldSaveAssociation (compileTable[entry]) == true)
+        {
+            if (associationsToUpdate.associate (compileTable[entry]) == false)
+            {
+                return false;
+            }
+        }
+    }
+
+    // Go through each entry in memVRTable to save whether VR is in memory
+    for (int entry = 0; entry < num_memory_vr; entry++)
+    {
+        if (associationsToUpdate.associate (memVRTable[entry]) == false)
+        {
+            return false;
+        }
+    }
+
+    // Go through each entry in constVRTable
+    for (int entry = 0; entry < num_const_vr; entry++)
+    {
+        // Only save entry if it is actually a constant
+        if (constVRTable[entry].isConst == true)
+        {
+            if (associationsToUpdate.associate (constVRTable[entry]) == false)
+            {
+                return false;
+            }
+        }
+    }
+
+    //Finalize the table and report success
+    associationsToUpdate.finalize();
+
+    return true;
+}
+
+bool AssociationTable::syncCompileTableWithAssociations(AssociationTable & associationsToUse) {
+    DEBUG_COMPILETABLE_UPDATE(ALOGD("There are %d associations to merge",
+            associationsToUse.size()));
+
+    // Go through every association we saved
+    for (AssociationTable::const_iterator assocIter = associationsToUse.begin();
+            assocIter != associationsToUse.end(); assocIter++)
+    {
+        //Suppose we will not find the entry
+        bool foundCompileTableEntry = false;
+
+        DEBUG_COMPILETABLE_UPDATE(ALOGD("Starting to search through compile "
+                "table that has %d entries", compileTable.size ()));
+
+        int vR = assocIter->first;
+        const CompileTableEntry &associationEntry = assocIter->second;
+
+        // Now search the compile table for an appropriate entry
+        for (int entry = 0; entry < compileTable.size (); entry++)
+        {
+            //If it is a virtual register and the right register
+            if (compileTable[entry].isVirtualReg () == true
+                    && compileTable[entry].getPhysicalType () == associationEntry.getPhysicalType ()
+                    && compileTable[entry].getRegisterNumber () == vR)
+            {
+                DEBUG_COMPILETABLE_UPDATE(ALOGD("Found that v%d is in compile "
+                        "table already.", assocIter->first));
+
+                // The only relevant part we care about updating is the physical register
+                compileTable[entry].setPhysicalReg (associationEntry.getPhysicalReg ());
+
+                //Mark that we found it
+                foundCompileTableEntry = true;
+                break;
+            }
+        }
+
+        // If we did not find an entry, we must insert it
+        if (foundCompileTableEntry == false)
+        {
+            DEBUG_COMPILETABLE_UPDATE(ALOGD("We have not found v%d in compile "
+                        "table so we will make a new entry.", assocIter->first));
+
+            CompileTableEntry newEntry = assocIter->second;
+
+            //Since we added it ourselves and it wasn't there before, lets reset it
+            newEntry.reset ();
+
+            //Now set its physical register correctly
+            newEntry.setPhysicalReg (assocIter->second.getPhysicalReg ());
+
+            //Add it to the global compileTable
+            compileTable.insert (newEntry);
+        }
+    }
+
+    // In case we have updated the compile table, we must also update the
+    // state of registers to match what compile table believes
+    if (associationsToUse.size() > 0) {
+        syncAllRegs();
+    }
+
+    DEBUG_COMPILETABLE_UPDATE (ALOGD ("Finished merging associations into compile table"));
+
+    //Report success
+    return true;
+}
+
+/**
+ * @brief Used to represent the possibilities of the state of a virtual register.
+ */
+enum VirtualRegisterState
+{
+    VRState_InMemory = 0, //!< In memory.
+    VRState_InGP,         //!< In general purpose register.
+    VRState_Constant,     //!< Constant value.
+    VRState_NonWideInXmm, //!< Non-wide VR in xmm register.
+    VRState_WideInXmm,    //!< Wide VR in xmm register.
+    VRState_HighOfWideVR, //!< The high bits when we have wide VR.
+};
+
+#ifdef DEBUG_REGISTERIZATION
+/**
+ * @brief Provides a mapping between the virtual register state and representative string.
+ * @param state The virtual register state.
+ * @return Returns string representation of virtual register state.
+ */
+static const char* convertVirtualRegisterStateToString (VirtualRegisterState state)
+{
+    switch (state)
+    {
+        case VRState_InMemory:
+            return "in memory";
+        case VRState_InGP:
+            return "in GP";
+        case VRState_Constant:
+            return "constant";
+        case VRState_NonWideInXmm:
+            return "non-wide in xmm";
+        case VRState_WideInXmm:
+            return "wide in xmm";
+        case VRState_HighOfWideVR:
+            return "high of wide";
+        default:
+            break;
+    }
+
+    return "invalid state";
+}
+#endif
+
+/**
+ * @brief Container for keeping track of actions attributed with a VR when state mismatch
+ * is found between two basic blocks.
+ */
+struct VirtualRegisterStateActions
+{
+    std::set<int> virtualRegistersToStore;        //!< Set of VRs to store on stack.
+    std::set<int> virtualRegistersToLoad;         //!< Set of VRs to load into registers.
+    std::set<int> virtualRegistersRegToReg;       //!< Set of VRs that must be moved to different registers.
+    std::set<int> virtualRegistersCheckConstants; //!< Set of VRs that are constants but must be checked for consistency.
+    std::set<int> virtualRegistersImmToReg;       //!< Set of VRs that are constant but must be moved to register.
+};
+
+/**
+ * @brief Fills the set of virtual registers with the union of all VRs used in both parent and child.
+ * @param parentAssociations The association table of parent.
+ * @param childAssociations The association table of child.
+ * @param virtualRegisters Updated by function to contain the set of all VRs used in both parent and child.
+ */
+static void filterVirtualRegisters (const AssociationTable &parentAssociations,
+                                    const AssociationTable &childAssociations,
+                                    std::set<int> &virtualRegisters)
+{
+    AssociationTable::const_iterator assocIter;
+
+    //Simply look through all of parent's associations and save all those VRs
+    for (assocIter = parentAssociations.begin (); assocIter != parentAssociations.end (); assocIter++)
+    {
+        virtualRegisters.insert (assocIter->first);
+    }
+
+    //Now look through all of child's associations and save all those VRs
+    for (assocIter = childAssociations.begin (); assocIter != childAssociations.end (); assocIter++)
+    {
+        virtualRegisters.insert (assocIter->first);
+    }
+}
+
+/**
+ * @brief Looks through the association table to determine the state of each VR of interest.
+ * @param associations The association table to look at.
+ * @param virtualRegisters The virtual registers to determine state for.
+ * @param vrState Updated by function
+ * @return True if we can determine state of all VRs of interest. Otherwise error is set and false is returned.
+ */
+static bool determineVirtualRegisterState (const AssociationTable &associations,
+                                           const std::set<int> &virtualRegisters,
+                                           std::map<int, VirtualRegisterState> &vrState)
+{
+    //We iterate through every VR of interest
+    for (std::set<int>::const_iterator iter = virtualRegisters.begin (); iter != virtualRegisters.end (); iter++)
+    {
+        int vR = *iter;
+
+        //We are iterating over a set which is sorted container. So if we are dealing with
+        //a wide VR, then we have already set the mapping for the low bits to contain information
+        //about the wideness.
+        std::map<int, VirtualRegisterState>::const_iterator wideIter = vrState.find (vR - 1);
+
+        //Do we have an entry for the low VR?
+        if (wideIter != vrState.end())
+        {
+            VirtualRegisterState lowState = wideIter->second;
+
+            //If we have a wide VR, then set the high bits correspondingly
+            if (lowState == VRState_WideInXmm)
+            {
+                vrState[vR] = VRState_HighOfWideVR;
+                continue;
+            }
+        }
+
+        //Look for the compile table entry for this VR
+        AssociationTable::const_iterator assocIter = associations.find (vR);
+
+        if (assocIter != associations.end ())
+        {
+            const CompileTableEntry &compileEntry = assocIter->second;
+
+            bool inPhysicalReg = compileEntry.inPhysicalRegister ();
+            bool inGP = compileEntry.inGeneralPurposeRegister ();
+            bool inXMM = compileEntry.inXMMRegister ();
+
+            //In order to have saved it, it must have been in either GP or XMM
+            //It also can be in a contant, which doesn't associated with physical reg
+            assert (inPhysicalReg == false || (inGP || inXMM) == true);
+
+            if (inGP == true)
+            {
+                vrState[vR] = VRState_InGP;
+                continue;
+            }
+            else if (inXMM == true)
+            {
+                //If it is in XMM, let's figure out if the VR is wide or not
+                OpndSize size = compileEntry.getSize ();
+
+                if (size == OpndSize_64)
+                {
+                    vrState[vR] = VRState_WideInXmm;
+                    continue;
+                }
+                else if (size == OpndSize_32)
+                {
+                    vrState[vR] = VRState_NonWideInXmm;
+                    continue;
+                }
+            }
+            else if (inPhysicalReg == true)
+            {
+                ALOGI ("JIT_INFO: We failed to satisfy BB associations because we found a VR that "
+                        "is in physical register but not in GP or XMM.");
+                SET_JIT_ERROR(kJitErrorBERegisterization);
+                return false;
+            }
+        }
+
+        //Let's figure out if it is believed that this VR is constant.
+        //We do this before checking if it was in memory because even if it was in memory,
+        //a child generated code using the assumptions of constant.
+        if (associations.wasVRConstant (vR) == true)
+        {
+            vrState[vR] = VRState_Constant;
+            continue;
+        }
+
+        //When we get here, we have tried our best to determine what physical register was used
+        //for this VR or if it was a constant. Only thing left is to see if this VR was marked as
+        //in memory
+        if (associations.wasVRInMemory (vR) == true)
+        {
+            vrState[vR] = VRState_InMemory;
+            continue;
+        }
+
+        //If we make it here it means we have not figured out the state of the VR
+        ALOGI ("JIT_INFO: We failed to satisfy BB associations because we couldn't figure "
+                "out state of virtual register v%d.", vR);
+        SET_JIT_ERROR(kJitErrorBERegisterization);
+        return false;
+    }
+
+    //If we make it here we are all good
+    return true;
+}
+
+/**
+ * @brief For every virtual register, it compares state in parent and child and then makes a decision on action to take.
+ * @param parentState Map of virtual register to its state in the parent association table.
+ * @param childState Map of virtual register to its state in the child association table.
+ * @param virtualRegisters List of virtual registers to make a decision for.
+ * @param actions Updated by function to contain the actions to take in order to merge the two states.
+ * @return Returns true if all state merging can be handled. Return false if mismatch of state is detected which
+ * cannot be handled safely.
+ */
+static bool decideOnMismatchAction (const std::map<int, VirtualRegisterState> &parentState,
+                                    const std::map<int, VirtualRegisterState> &childState,
+                                    const std::set<int> &virtualRegisters,
+                                    VirtualRegisterStateActions &actions)
+{
+    //We iterate through every VR of interest
+    for (std::set<int>::const_iterator iter = virtualRegisters.begin (); iter != virtualRegisters.end (); iter++)
+    {
+        //Get the VR
+        int vR = *iter;
+
+        //Create an iterator so we can look through child state and parent state for the VR
+        std::map<int, VirtualRegisterState>::const_iterator vrStateIter;
+
+        //Get the state of this VR in parent
+        vrStateIter = parentState.find (vR);
+
+        //Paranoid because parentState should contain all VRs in set of virtualRegisters
+        assert (vrStateIter != parentState.end());
+
+        //Save state of this VR in parent
+        VirtualRegisterState vrStateInParent = vrStateIter->second;
+
+        //Get the state of this VR in child
+        vrStateIter = childState.find (vR);
+
+        //Paranoid because childState should contain all VRs in set of virtualRegisters
+        assert (vrStateIter != childState.end());
+
+        //Save state of this VR in child
+        VirtualRegisterState vrStateInChild = vrStateIter->second;
+
+        DEBUG_ASSOCIATION_MERGE (ALOGD ("We are looking at v%d that is %s for parent and "
+                "%s for child", vR, convertVirtualRegisterStateToString (vrStateInParent),
+                convertVirtualRegisterStateToString (vrStateInChild)));
+
+        bool mismatched = (vrStateInParent != vrStateInChild);
+
+        if (mismatched == true)
+        {
+            //First let's check to see if child believes VR is constant
+            if (vrStateInChild == VRState_Constant)
+            {
+                //So we have a state mismatch and child believes that VR is a constant
+                ALOGI ("JIT_INFO: Child believes VR is constant but we don't. Without a runtime check "
+                        "we cannot confirm.");
+                SET_JIT_ERROR(kJitErrorBERegisterization);
+                return false;
+            }
+            //Now check if parent has it in memory
+            else if (vrStateInParent == VRState_InMemory)
+            {
+                //The high bits of this VR will be taken care of along with the low bits since
+                //we know we have a wide VR.
+                if (vrStateInChild == VRState_HighOfWideVR)
+                {
+                    continue;
+                }
+
+                //Paranoid because we are expecting to load it into register
+                assert (vrStateInChild == VRState_InGP || vrStateInChild == VRState_NonWideInXmm
+                        || vrStateInChild == VRState_WideInXmm);
+
+                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We need to load v%d into register", vR));
+
+                //If parent has it in memory but child has it in register then we need
+                //to load it.
+                actions.virtualRegistersToLoad.insert (vR);
+            }
+            else if (vrStateInChild == VRState_InMemory)
+            {
+                //The high bits of this VR will be taken care of along with the low bits since
+                //we know we have a wide VR.
+                if (vrStateInParent == VRState_HighOfWideVR)
+                {
+                    continue;
+                }
+
+                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We need to store v%d back on stack", vR));
+
+                //Add it to set to store back
+                actions.virtualRegistersToStore.insert (vR);
+            }
+            else if (vrStateInParent == VRState_Constant && vrStateInChild == VRState_InGP)
+            {
+                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We need to move immediate into GP for v%d", vR));
+
+                //Add it to set to do imm to reg move
+                actions.virtualRegistersImmToReg.insert (vR);
+            }
+            else
+            {
+                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We must store v%d in memory and then reload in "
+                        "proper place due to mismatch", vR));
+
+                //On state mismatch, the easiest solution is to store the VR into memory and then load
+                //it back into proper state
+                actions.virtualRegistersToStore.insert (vR);
+
+                //If child believes that this VR is the high part of the wide VR,
+                //then the load of the low part into xmm will take care of this case
+                if (vrStateInChild != VRState_HighOfWideVR)
+                {
+                    actions.virtualRegistersToLoad.insert (vR);
+                }
+            }
+        }
+        else
+        {
+            if (vrStateInParent == VRState_InGP || vrStateInParent == VRState_NonWideInXmm
+                    || vrStateInParent == VRState_WideInXmm)
+            {
+                DEBUG_ASSOCIATION_MERGE (ALOGD(">> We need to do a reg to reg move for v%d", vR));
+
+                // Insert it into set that needs to be handled via reg to reg moves
+                actions.virtualRegistersRegToReg.insert (vR);
+            }
+            else if (vrStateInParent == VRState_Constant)
+            {
+                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We need to check constants to ensure "
+                        "consistency for v%d", vR));
+
+                //We will need to do a constant check to make sure we have same constant
+                actions.virtualRegistersCheckConstants.insert (vR);
+            }
+            else
+            {
+                //We have nothing to do
+                DEBUG_ASSOCIATION_MERGE (ALOGD (">> We have nothing to do because state matches for v%d", vR));
+            }
+        }
+    }
+
+    //If we make it here everything went okay
+    return true;
+}
+
+/**
+ * @brief Compares the constant in each virtual register in order to figure out that they match.
+ * @param parentAssociations The association table of parent.
+ * @param childAssociations The association table of child.
+ * @param virtualRegistersCheckConstants
+ * @return Returns false if it finds a case when the constant value for same VR differs between parent and child.
+ */
+static bool checkConstants (const AssociationTable &parentAssociations,
+                            const AssociationTable &childAssociations,
+                            const std::set<int> &virtualRegistersCheckConstants)
+{
+    //Iterate through all VRs that are constants in both parent and child to check that
+    //the constant value matches
+    for (std::set<int>::const_iterator iter = virtualRegistersCheckConstants.begin ();
+            iter != virtualRegistersCheckConstants.end (); iter++)
+    {
+        int vR = *iter;
+
+        //Get value parent believes for this VR
+        int parentValue = parentAssociations.getVRConstValue (vR);
+
+        //Get value child believes for this VR
+        int childValue = childAssociations.getVRConstValue (vR);
+
+        if (parentValue != childValue)
+        {
+            //If there is a mismatch, there's nothing we can do about it
+            ALOGI ("JIT_INFO: Both child and parent believe VR is constant but each believes "
+                        "it is a different value");
+            SET_JIT_ERROR(kJitErrorBERegisterization);
+            return false;
+        }
+    }
+
+    //If we make it here, all constants match in value
+    return true;
+}
+
+/**
+ * @brief Decides whether merging state of parent to match its child can be done safely.
+ * @param childAssociations The association table of child.
+ * @param actions Updated by function to contain the actions to take in order to merge the two states.
+ * @return Returns true if all state merging can be handled. Return false if mismatch of state is detected which
+ * cannot be handled safely.
+ */
+static bool canHandleMismatch (const AssociationTable &childAssociations,
+                               VirtualRegisterStateActions &actions)
+{
+    //We want to make it easy to compare state of child and state of parent and thus
+    //we load the compile table into an association table. The parent associations
+    //will no longer be valid once we start actioning on mismatch
+    AssociationTable parentAssociations;
+    if (AssociationTable::syncAssociationsWithCompileTable (parentAssociations) == false)
+    {
+        //When loading from compile table problems were found. It's best to bail early.
+        return false;
+    }
+
+    //Now figure out which virtual registers are used in each state so we can start
+    //figuring out any state mismatch
+    std::set<int> virtualRegisters;
+    filterVirtualRegisters (parentAssociations, childAssociations, virtualRegisters);
+
+    //For each virtual register we want to figure out the state in both parent and child
+    std::map<int, VirtualRegisterState> childState, parentState;
+    if (determineVirtualRegisterState(childAssociations, virtualRegisters, childState) == false)
+    {
+        return false;
+    }
+    if (determineVirtualRegisterState(parentAssociations, virtualRegisters, parentState) == false)
+    {
+        return false;
+    }
+
+    //Now we need to make a decision when we have a mismatch
+
+    bool result = decideOnMismatchAction (parentState, childState, virtualRegisters, actions);
+
+    if (result == false)
+    {
+        //While searching for action on mismatch, we found a state we couldn't deal with
+        //so we now return false.
+        return false;
+    }
+
+    // Now that we figured out mismatch and also actions for each, let's look at constants
+    // for both parent and child. We want to make sure that if child believes a VR is constant,
+    // the parent believes it is the same constant.
+    result = checkConstants (parentAssociations, childAssociations, actions.virtualRegistersCheckConstants);
+
+    if (result == false)
+    {
+        //If we found non matching constants, we must bail out because there's nothing we can do
+        return false;
+    }
+
+    //If we make it here, we can handle the mismatch
+    return true;
+}
+
+/**
+ * @brief Sets up mapping between virtual registers and their physical registers.
+ * @param associationsToUse the association to compare ourselves with
+ * @param otherVRToPhysicalReg what is the associationsToUse having as associations (updated by the function)
+ * @param currentVRToPhysicalReg what is the current association state between VRs to Physical (updated by the function)
+ */
+static void initAssociationHelperTables (const AssociationTable &associationsToUse,
+                                         std::map<int, PhysicalReg> &otherVRToPhysicalReg,
+                                         std::map<int, PhysicalReg> &currentVRToPhysicalReg)
+{
+    // First we need to go through each of the child's association entries
+    // to figure out each VR's association with physical register
+    for (AssociationTable::const_iterator assocIter = associationsToUse.begin();
+            assocIter != associationsToUse.end(); assocIter++)
+    {
+        if (assocIter->second.physicalReg != PhysicalReg_Null)
+        {
+            // Save the mapping to physical register
+            otherVRToPhysicalReg[assocIter->first] =
+                    static_cast<PhysicalReg>(assocIter->second.physicalReg);
+        }
+    }
+
+    // Now go through current compile table to figure out what VRs are in
+    // physical registers
+    for (int entry = 0; entry < compileTable.size (); entry++)
+    {
+        if (isVirtualReg(compileTable[entry].physicalType)
+                && compileTable[entry].physicalReg != PhysicalReg_Null)
+        {
+            // Save the mapping to physical register
+            currentVRToPhysicalReg[compileTable[entry].regNum] =
+                    static_cast<PhysicalReg>(compileTable[entry].physicalReg);
+        }
+    }
+}
+
+/**
+ * @brief Writes back virtual registers to stack.
+ * @param virtualRegistersToStore Set of virtual registers to write back.
+ * @param trySkipWriteBack This enables an optimization where the write backs are only handled
+ * if VR is in the BB's write back requests. This can be useful when spilling to memory for loop
+ * entry but loop never reads the written value.
+ * @param writeBackRequests Used when trySkipWriteBack is true. This is a vector of VR writeback
+ * requests from the basic block.
+ * @param registersToFree Used when trySkipWriteBack is true. This is a set of physical registers
+ * to ensure that the VR associated with it gets written back.
+ * @return Returns true if successfully writes back all VRs to memory.
+ */
+static bool writeBackVirtualRegistersToMemory (const std::set<int> &virtualRegistersToStore,
+                                               bool trySkipWriteBack = false,
+                                               const BitVector *writeBackRequests = 0,
+                                               const std::set<PhysicalReg> *registersToFree = 0)
+{
+    //Write back anything that is in the set of VRs to store
+    for (std::set<int>::const_iterator setOpIter = virtualRegistersToStore.begin ();
+            setOpIter != virtualRegistersToStore.end ();
+            setOpIter++)
+    {
+        int vR = *setOpIter;
+
+        //Now look through compile table to find the matching entry
+        for (int entry = 0; entry < compileTable.size (); entry++)
+        {
+            CompileTableEntry &compileEntry = compileTable[entry];
+
+            //Do we have a match in compile table with this VR we want to write back?
+            if (compileEntry.isVirtualReg () == true && compileEntry.getRegisterNumber () == vR)
+            {
+                //We want to skip the write back if the optimization is enabled.
+                bool skipWriteBack = (trySkipWriteBack == true);
+
+                //However, we do NOT want to skip writeback if it is in set of registers to free
+                //because someone wants this VR out of that physical register.
+                if (skipWriteBack == true && registersToFree != 0)
+                {
+                    skipWriteBack = registersToFree->find (compileTable[entry].getPhysicalReg ())
+                            == registersToFree->end ();
+                }
+
+                //Finally we do NOT want to skip writeback if this VR is in vector of writeback requests
+                if (skipWriteBack == true && writeBackRequests != 0)
+                {
+                    skipWriteBack = dvmIsBitSet (writeBackRequests, vR) == false;
+                }
+
+                //If we are not skipping the write back, then we actually need to do it
+                if (skipWriteBack == false)
+                {
+                    DEBUG_ASSOCIATION_MERGE (ALOGD ("Writing v%d back to memory", vR));
+
+                    //Handle the write back when in physical register
+                    if (compileEntry.inPhysicalRegister () == true)
+                    {
+                        //Try to write back the virtual register
+                        if (spillLogicalReg (entry, true) < 0)
+                        {
+                            return false;
+                        }
+                    }
+                    else
+                    {
+                        //We make it here if the VR is not in physical register. Try figuring out
+                        //if this is a constant. If it isn't a constant, we are okay because there's
+                        //nothing we need to write back.
+                        bool res = writeBackVRIfConstant (vR, LowOpndRegType_gp);
+
+                        //If this VR was constant, then since we wrote it back we mark it as non-constant.
+                        if (res == true)
+                        {
+                            setVRToNonConst (vR, OpndSize_32);
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+    //Since we have spilled VRs, lets make sure we properly keep track
+    //of which physical registers are currently being used
+    syncAllRegs();
+
+    //If we make it here, everything is okay.
+    return true;
+}
+
+/**
+ * @brief Find scratch registers and fill the scratchReg set
+ * @param childUsedReg registers used by the child at entrance
+ * @param scratchRegs any scratch register at the end of parent's code generation (updated by the function)
+ */
+static void findScratchRegisters (const std::set<PhysicalReg> &childUsedReg,
+                                  std::set<PhysicalReg> &scratchRegs)
+{
+    // All free registers are candidates for use as scratch
+    std::set<PhysicalReg> parentFreeReg;
+    findFreeRegisters (parentFreeReg);
+
+    // Subtract child used registers from parent free registers
+    // so we can figure out what we can use as scratch
+    std::set_difference(parentFreeReg.begin(), parentFreeReg.end(),
+            childUsedReg.begin(), childUsedReg.end(),
+            std::inserter(scratchRegs, scratchRegs.end()));
+
+#ifdef DEBUG_REGISTERIZATION
+    //Debugging purposes
+    std::set<PhysicalReg>::const_iterator scratchRegIter;
+    for (scratchRegIter = scratchRegs.begin();
+            scratchRegIter != scratchRegs.end(); scratchRegIter++) {
+        DEBUG_ASSOCIATION_MERGE(ALOGD("%s is free for use as scratch\n",
+                physicalRegToString(*scratchRegIter)));
+    }
+#endif
+}
+
+/**
+ * @brief Find the registers to be moved and fill the regToRegMoves map
+ * @param virtualRegistersRegToReg Set of virtual registers that must be moved to new registers
+ * @param childVRToPhysicalReg child association between VRs and physical registers at child code generation entrance
+ * @param currentVRToPhysicalReg child association between VRs and physical registers at current code generation exit
+ * @param regToRegMoves register to register moves to be done (updated by function)
+ * @return Returns true if we successfully can determine the moves to be done.
+ */
+static bool findRegistersToMove (const std::set<int> &virtualRegistersRegToReg,
+                                 std::map<int, PhysicalReg> &childVRToPhysicalReg,
+                                 std::map<int, PhysicalReg> &currentVRToPhysicalReg,
+                                 std::map<PhysicalReg, PhysicalReg> &regToRegMoves)
+{
+    std::set<int>::const_iterator setOpIter;
+
+    //Now we need to filter the reg to reg moves so walk through all of them
+    for (setOpIter = virtualRegistersRegToReg.begin ();
+            setOpIter != virtualRegistersRegToReg.end ();
+            setOpIter++)
+    {
+        int VR = *setOpIter;
+        PhysicalReg childReg = childVRToPhysicalReg[VR];
+        PhysicalReg currentReg = currentVRToPhysicalReg[VR];
+
+        // Check whether they are in the same physical register
+        if(childReg != currentReg) {
+            DEBUG_ASSOCIATION_MERGE(ALOGD("We are moving %s to %s",
+                    physicalRegToString(currentReg),
+                    physicalRegToString(childReg)));
+
+            if (regToRegMoves.find(currentReg) != regToRegMoves.end()) {
+                ALOGI ("JIT_INFO: We are overwriting the reg to reg move from %s",
+                        physicalRegToString(currentReg));
+                SET_JIT_ERROR(kJitErrorBERegisterization);
+                return false;
+            }
+
+            // We want to generate a move that takes value from current physical
+            // register and puts it in the physical register child expects it
+            regToRegMoves[currentReg] = childReg;
+        }
+    }
+
+    //Report success
+    return true;
+}
+
+/**
+ * @brief Move registers following the regToReg map
+ * @param regToRegMoves the register to register move order
+ * @param scratchRegs the registers that are scratch and can be safely used for moving registers
+ * @param currentVRToPhysicalReg current VR to physical registers at end of current BasicBlock generation
+ * @return whether or not the move registers succeeds
+ */
+static bool moveRegisters (std::map<PhysicalReg, PhysicalReg> &regToRegMoves,
+                           const std::set<PhysicalReg> &scratchRegs,
+                           const std::map<int, PhysicalReg> &currentVRToPhysicalReg)
+{
+    std::map<PhysicalReg, PhysicalReg>::const_iterator regToRegIter;
+
+    //Go through each register to register request
+    for (regToRegIter = regToRegMoves.begin (); regToRegIter != regToRegMoves.end (); regToRegIter++)
+    {
+        std::vector<PhysicalReg> toBeMoved;
+        std::vector<PhysicalReg>::reverse_iterator moveIter;
+        PhysicalReg source = regToRegIter->first;
+        PhysicalReg dest = regToRegIter->second;
+
+        // Paranoid because we cannot move to null register
+        if (dest == PhysicalReg_Null)
+        {
+            continue;
+        }
+
+        if (regToRegMoves.count (source) > 1)
+        {
+            ALOGI ("JIT_INFO: We have the same physical register as source "
+                    "multiple times.");
+            SET_JIT_ERROR(kJitErrorBERegisterization);
+            return false;
+        }
+
+        DEBUG_ASSOCIATION_MERGE (ALOGD ("We want to move from %s to %s", physicalRegToString(source),
+                physicalRegToString(dest)));
+
+        toBeMoved.push_back (source);
+        toBeMoved.push_back (dest);
+
+        // We eagerly assume that we won't find a cycle but we want to do something special if we do
+        bool cycleFound = false;
+
+        // Now look through the rest of the moves to see if anyone
+        // is going to replace source register
+        std::map<PhysicalReg, PhysicalReg>::const_iterator regToRegFinder;
+        for (regToRegFinder = regToRegMoves.find (dest); regToRegFinder != regToRegMoves.end (); regToRegFinder =
+                regToRegMoves.find (regToRegFinder->second))
+        {
+
+            // If we already have this register in the toBeMoved list,
+            // it means we found a cycle. Instead of doing a find,
+            // keeping track of this information in a bit vector would be
+            // better. However, PhysicalReg enum contains invalid
+            // physical registers so it is hard to decide which ones we
+            // need to keep track of.
+            if (std::find (toBeMoved.begin (), toBeMoved.end (), regToRegFinder->second) != toBeMoved.end ())
+            {
+                cycleFound = true;
+                // Save this because we will need to move value into it
+                toBeMoved.push_back (regToRegFinder->second);
+                break;
+            }
+
+            // Save this because we will need to move value into it
+            toBeMoved.push_back (regToRegFinder->second);
+        }
+
+        // If we have a cycle, we might need to use memory for doing the swap
+        bool useMemoryForSwap = false;
+
+        if (cycleFound == true)
+        {
+            // If we find a cycle, the last value in the toBeMoved list
+            // is the register that caused cycle. Lets pop it off
+            // for now so we can use std::replace below but we will
+            // reinsert it
+            PhysicalReg cycleCause = toBeMoved.back ();
+            toBeMoved.pop_back ();
+
+            // Lets hope we have a scratch register to break the cycle
+            PhysicalReg scratch = getScratch (scratchRegs, getTypeOfRegister (source));
+
+            if (scratch != PhysicalReg_Null)
+            {
+                // When we get here we found a scratch register
+
+                // Thus if we had C->A B->C A->B
+                // Now we want to have A->T C->A B->C T->B
+
+                // Which means that toBeMoved contains A B C when we get here and now we
+                // want it to have T B C A T
+
+                // Replace A with T so we have T B C instead of A B C
+                std::replace (toBeMoved.begin (), toBeMoved.end (), cycleCause, scratch);
+
+                // Now add A so we have T B C A
+                toBeMoved.push_back (cycleCause);
+
+                // Now add T so we have T B C A T
+                toBeMoved.push_back (scratch);
+            }
+            else
+            {
+                useMemoryForSwap = true;
+            }
+        }
+
+        if (useMemoryForSwap == true)
+        {
+            ALOGI ("JIT_INFO: We have no scratch registers so we must use memory for swap");
+            SET_JIT_ERROR(kJitErrorBERegisterization);
+            return false;
+        }
+
+        // Now handle the actual reg to reg moves
+        PhysicalReg tmpIter = PhysicalReg_Null;
+        for (moveIter = toBeMoved.rbegin (); moveIter != toBeMoved.rend (); moveIter++)
+        {
+            PhysicalReg dest = tmpIter;
+            PhysicalReg source = *(moveIter);
+
+            //Remember source
+            tmpIter = source;
+
+            //If destination is null, next iteration
+            if (dest == PhysicalReg_Null)
+            {
+                continue;
+            }
+
+            DEBUG_ASSOCIATION_MERGE(ALOGD("Moving %s to %s", physicalRegToString(source), physicalRegToString(dest)));
+
+            OpndSize regSize = OpndSize_32;
+
+            //If we have an xmm to xmm move, then we set the operand size to 64-bits. The reason
+            //for this is because move_reg_to_reg function expects this size so it can use a MOVQ.
+            //We may be able to get away with doing a MOVD if we have a 32-bit FP loaded with a MOVSS,
+            //but we don't have the API for it and we would need additional logic here.
+            if (source >= PhysicalReg_StartOfXmmMarker && source <= PhysicalReg_EndOfXmmMarker)
+            {
+                regSize = OpndSize_64;
+            }
+
+            // Do the actual reg to reg move
+            move_reg_to_reg_noalloc (regSize, source, true, dest, true);
+
+            // We have moved from reg to reg, but we must also update the
+            // entry in the compile table
+            std::map<int, PhysicalReg>::const_iterator currentVRToRegIter;
+            for (currentVRToRegIter = currentVRToPhysicalReg.begin ();
+                    currentVRToRegIter != currentVRToPhysicalReg.end (); currentVRToRegIter++)
+            {
+                int VR = currentVRToRegIter->first;
+                int oldReg = currentVRToRegIter->second;
+
+                if (oldReg == source)
+                {
+                    updatePhysicalRegForVR (VR, source, dest);
+                }
+            }
+
+            // Now remove entry from map
+            regToRegMoves[source] = PhysicalReg_Null;
+        }
+    }
+
+    // Since we update the physical registers for some of the VRs lets sync up register usage with compile table
+    syncAllRegs();
+
+    //Report success
+    return true;
+}
+
+/**
+ * @brief Load virtual registers to child's physical registers requests
+ * @param virtualRegistersToLoad Set of virtual registers we need to load into physical registers.
+ * @param associationsToUse the association table for the child at code generation entrance
+ * @param childVRToPhysicalReg the child map of Virtual Register to physical register at start of code generation
+ * @return whether or not the function succeeds
+ */
+static bool loadVirtualRegistersForChild (const std::set<int> &virtualRegistersToLoad,
+                                          const AssociationTable &associationsToUse,
+                                          const std::map<int, PhysicalReg> &childVRToPhysicalReg)
+{
+    std::set<int>::const_iterator setOpIter;
+
+    //Now walk through the VRs we need to load
+    for (setOpIter = virtualRegistersToLoad.begin();
+            setOpIter != virtualRegistersToLoad.end(); setOpIter++)
+    {
+        int VR = *setOpIter;
+
+        //Look to see if we have a physical register for this VR
+        std::map<int, PhysicalReg>::const_iterator findRegIter;
+        findRegIter = childVRToPhysicalReg.find (VR);
+
+        //This should never happen but could happen in rare cases. For example,
+        //if a VR is wide in child, then it might be possible we get a request
+        //to load the high part of VR into a non-existent register. However if
+        //the VR is wide, we should have already handled case of loading both
+        //low and high parts into an XMM.
+        if (findRegIter == childVRToPhysicalReg.end ())
+        {
+            //If we don't actually have a physical register then there's nowhere
+            //to load this VR. Thus we can safely skip it.
+            continue;
+        }
+
+        PhysicalReg targetReg = findRegIter->second;
+
+        //This should never happen but it will make buffer overflow checkers happy
+        if (targetReg >= PhysicalReg_Null)
+        {
+            continue;
+        }
+
+        const CompileTableEntry *childCompileEntry = 0;
+
+        // Look through child's association entries to find the type of the VR
+        // so we can load it properly into the physical register
+        for (AssociationTable::const_iterator assocIter = associationsToUse.begin ();
+                assocIter != associationsToUse.end (); assocIter++)
+        {
+            const CompileTableEntry &compileEntry = assocIter->second;
+
+            if (compileEntry.getPhysicalReg () == targetReg)
+            {
+                //We found the proper entry so let's get some information from it
+                childCompileEntry = &compileEntry;
+                break;
+            }
+        }
+
+        //Paranoid: this should never happen
+        if (childCompileEntry == 0)
+        {
+            ALOGD ("JIT_INFO: Trying to load virtual register for child but cannot find compile entry");
+            SET_JIT_ERROR (kJitErrorBERegisterization);
+            return false;
+        }
+
+        //Paranoid
+        assert (childCompileEntry->isVirtualReg () == true);
+
+        //Get the physical type
+        LowOpndRegType type = childCompileEntry->getPhysicalType ();
+
+        DEBUG_ASSOCIATION_MERGE (ALOGD ("Loading v%d to %s", VR, physicalRegToString(targetReg)));
+
+        // Load VR into the target physical register
+        if (type == LowOpndRegType_ss)
+        {
+            move_ss_mem_to_reg_noalloc (4 * VR, PhysicalReg_FP, true, MemoryAccess_VR, VR, targetReg, true);
+        }
+        else
+        {
+            OpndSize size = childCompileEntry->getSize ();
+            get_virtual_reg_noalloc (VR, size, targetReg, true);
+        }
+
+        //Look for the entry to update in compile table
+        CompileTable::iterator entryToUpdate = compileTable.findVirtualRegister (VR, type);
+
+        if (entryToUpdate != compileTable.end ())
+        {
+            //We found a matching entry so simply update its physical register
+            entryToUpdate->setPhysicalReg (targetReg);
+        }
+        else
+        {
+            //If we were not able to find an entry, then we can just copy it from child's association table
+
+            //Make a copy of new entry
+            CompileTableEntry newEntry = (*childCompileEntry);
+
+            //Since we copied it over, let's reset it
+            newEntry.reset ();
+
+            //Make sure that the physical register is set
+            newEntry.setPhysicalReg (targetReg);
+
+            //We now copy into the compile table
+            compileTable.insert (newEntry);
+
+            //Since we just loaded it from memory, we keep it marked as being in memory and add it to the
+            //memory table in order to keep track of it.
+            addToMemVRTable (newEntry.regNum, true);
+
+            //If it is a 64-bit wide operand, we also need to add its high part to the memory table.
+            if (newEntry.getSize() == OpndSize_64)
+            {
+                addToMemVRTable (newEntry.regNum + 1, true);
+            }
+        }
+    }
+
+    // We loaded some VRs into physical registers. Lets keep registers synced
+    syncAllRegs();
+
+    //Report success
+    return true;
+}
+
+/**
+ * @brief Moves constant virtual registers values into physical registers.
+ * @details The parent must believe VR is constant and child must want it in physical register.
+ * @param immToRegMoves The virtual registers whose constant value must be moved to physical register.
+ * @param childVRToPhysicalReg Mapping between child virtual register and the physical register they are in.
+ * @return Returns true if successfully moves all immediates into physical registers. Otherwise it sets
+ * error and returns false.
+ */
+static bool moveImmediates (const std::set<int> &immToRegMoves,
+                            const std::map<int, PhysicalReg> &childVRToPhysicalReg)
+{
+    std::set<int>::const_iterator setOpIter;
+
+    //Now walk through the constant VRs we need to move to physical registers
+    for (setOpIter = immToRegMoves.begin (); setOpIter != immToRegMoves.end (); setOpIter++)
+    {
+        int vR = *setOpIter;
+
+        //We can only handle immediate to GP moves so we can preset the type
+        LowOpndRegType type = LowOpndRegType_gp;
+        OpndSize size = getRegSize (static_cast<int> (LowOpndRegType_gp));
+
+        //Make space for constant value
+        int constantValue;
+
+        //We want to get the constant value so we check if virtual register is constant.
+        //Since we just care to do immediate to GP register move, we pass only enough space
+        //for non-wide VR.
+        if (isVirtualRegConstant (vR, type, &constantValue) == VR_IS_NOT_CONSTANT)
+        {
+            ALOGI ("JIT_INFO: We decided that we need to do an imm to reg move but now VR is no longer constant.");
+            SET_JIT_ERROR (kJitErrorBERegisterization);
+            return false;
+        }
+
+        //Look to see if we have a physical register for this VR
+        std::map<int, PhysicalReg>::const_iterator findRegIter;
+        findRegIter = childVRToPhysicalReg.find (vR);
+
+        //This should never happen
+        if (findRegIter == childVRToPhysicalReg.end ())
+        {
+            ALOGI ("JIT_INFO: We decided that we need to do an imm to reg move but we cannot find register.");
+            SET_JIT_ERROR (kJitErrorBERegisterization);
+            return false;
+        }
+
+        PhysicalReg targetReg = findRegIter->second;
+
+        //Paranoid because we only support GP moves
+        assert (targetReg >= PhysicalReg_StartOfGPMarker && targetReg <= PhysicalReg_EndOfGPMarker);
+
+        //Do the actual move now
+        move_imm_to_reg_noalloc (size, constantValue, targetReg, true);
+
+        //Since we have it in physical register, lets invalidate its constantness
+        setVRToNonConst (vR, size);
+
+        //Look for the entry to update in compile table
+        CompileTable::iterator entryToUpdate = compileTable.findVirtualRegister (vR, type);
+
+        if (entryToUpdate != compileTable.end ())
+        {
+            //We found a matching entry so simply update its physical register
+            entryToUpdate->setPhysicalReg (targetReg);
+        }
+        else
+        {
+            //Since we don't have an entry already we can make one right now
+            CompileTableEntry newEntry (vR, type, LowOpndRegType_virtual);
+
+            //We now copy into the compile table
+            compileTable.insert (newEntry);
+
+            //If the constant was already marked as being in memory, then our VR is still technically
+            //in memory and thus we don't need to update its in memory state right now
+        }
+    }
+
+    //Report success
+    return true;
+}
+
+bool AssociationTable::satisfyBBAssociations (BasicBlock_O1 * parent,
+        BasicBlock_O1 * child, bool isBackward)
+{
+    // To get here, it must be the case that this child's associations have
+    // already been finalized
+    assert (child->associationTable.hasBeenFinalized() == true);
+
+    /**
+     * This function merges associations, therefore it needs to know:
+     *   - The child's associations
+     *   - The parent's associations
+     *   - How both associations can be synchronized
+     */
+
+    AssociationTable &childAssociations = child->associationTable;
+    VirtualRegisterStateActions actions;
+
+    // 1) Gather information on current associations and the child's and decide
+    // on actions for dealing with state mismatch between VRs
+    if (canHandleMismatch (childAssociations, actions) == false)
+    {
+        return false;
+    }
+
+    //Look at child to see what physical registers it is using
+    std::set<PhysicalReg> childUsedReg;
+    childAssociations.findUsedRegisters (childUsedReg);
+
+    // 2) We write back anything child wants in memory because this will allow us to have scratch
+    // registers in case we need to do reg to reg moves.
+    if (writeBackVirtualRegistersToMemory (actions.virtualRegistersToStore, isBackward == true,
+            parent->requestWriteBack, &childUsedReg) == false)
+    {
+        return false;
+    }
+
+    // 3) Prepare for doing reg to reg moves by finding scratch registers, finding mapping between
+    // VRs and their physical register, and for deciding which registers to move.
+
+    //Figure out the scratch registers we have available.
+    std::set<PhysicalReg> scratchRegs;
+    findScratchRegisters (childUsedReg, scratchRegs);
+
+    //Initialize helper maps in regards to parent and child associations
+    std::map<int, PhysicalReg> childVRToPhysicalReg, currentVRToPhysicalReg;
+    initAssociationHelperTables (childAssociations, childVRToPhysicalReg, currentVRToPhysicalReg);
+
+    //Find the registers that should be moved
+    std::map<PhysicalReg, PhysicalReg> regToRegMoves;
+
+    if (findRegistersToMove (actions.virtualRegistersRegToReg, childVRToPhysicalReg,
+            currentVRToPhysicalReg, regToRegMoves) == false)
+    {
+        //If findRegistersToMove fails, we bail too
+        return false;
+    }
+
+    // 4) Do the actual moving of registers to the correct physical register
+    if (moveRegisters (regToRegMoves, scratchRegs, currentVRToPhysicalReg) == false)
+    {
+        //If moveRegisters fails, we bail too
+        return false;
+    }
+
+    // 5) Load any VRs we believe is in memory because child wants it in physical register
+    if (loadVirtualRegistersForChild (actions.virtualRegistersToLoad, childAssociations, childVRToPhysicalReg) == false)
+    {
+        //If moveToChildPhysical fails, we bail too
+        return false;
+    }
+
+    // 6) Now handle any immediate to GP register moves
+    if (moveImmediates (actions.virtualRegistersImmToReg, childVRToPhysicalReg) == false)
+    {
+        //If move immediates fails it will set error message. We simply propagate it now.
+        return false;
+    }
+
+    //If we make it here, everything went okay so we report success
+    return true;
+}
+
+bool AssociationTable::handleSpillRequestsFromME (BasicBlock_O1 *bb)
+{
+    //Initialize empty set of VRs to write back
+    std::set<int> virtualRegisterToWriteBack;
+
+    //We need to iterate through the writeback requests to add them to our set of VRs
+    BitVectorIterator bvIterator;
+    dvmBitVectorIteratorInit(bb->requestWriteBack, &bvIterator);
+
+    //Go through each VR so we can add it to our set
+    for (int vR = dvmBitVectorIteratorNext (&bvIterator); vR != -1; vR = dvmBitVectorIteratorNext (&bvIterator))
+    {
+        virtualRegisterToWriteBack.insert (vR);
+    }
+
+    //Do the actual write back
+    bool result = writeBackVirtualRegistersToMemory (virtualRegisterToWriteBack);
+
+    return result;
+}
+
+/**
+ * @details First we handle any spill requests for the current basic block
+ * so we do not pass useless associations to child. Then if child already
+ * has an existing association table, we generate instructions to match
+ * our state to that. If the child does not, then we tell it what our
+ * current associations are. If the child is a chaining cell or exit block,
+ * we spill everything because those BBs are handled specially and are exit
+ * points.
+ */
+bool AssociationTable::createOrSyncTable (BasicBlock_O1 * bb, bool forFallthrough)
+{
+    // Before we pass association tables, lets handle spill requests from ME
+    // so we don't pass anything useless for associations
+    if (handleSpillRequestsFromME (bb) == false)
+    {
+        return false;
+    }
+
+    //Get child depending on the forFallthrough boolean
+    BasicBlock_O1 * child = reinterpret_cast<BasicBlock_O1 *> (forFallthrough ? bb->fallThrough : bb->taken);
+
+    //If there is a child
+    if (child != NULL) {
+
+        //If it is not a dalvik code and it's not prebackward block,
+        //then write back and free all registers because we might
+        //be exiting to interpreter.
+        if (child->blockType != kDalvikByteCode && child->blockType != kPreBackwardBlock)
+        {
+            freeReg (true);
+        }
+        else
+        {
+            if (child->associationTable.hasBeenFinalized () == false)
+            {
+                // If the child's association table has not been finalized then we can
+                // update it now. However, if we don't have any MIRs in this BB,
+                // it means the compile table has not been updated and thus we can
+                // just copy associations
+                if (syncAssociationsWithCompileTable (child->associationTable) == false)
+                {
+                    return false;
+                }
+            }
+            else
+            {
+                //Otherwise, let's satisfy the associations for the child
+                if (satisfyBBAssociations (bb, child) == false)
+                {
+                    return false;
+                }
+            }
+        }
+    }
+
+    //Report success
+    return true;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/RegisterizationBE.h b/vm/compiler/codegen/x86/lightcg/RegisterizationBE.h
new file mode 100644
index 0000000..8bb11d7
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/RegisterizationBE.h
@@ -0,0 +1,271 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef REGISTERIZATIONBE_H_
+#define REGISTERIZATIONBE_H_
+
+#include <map>
+
+// Forward declarations
+struct BasicBlock_O1;
+class CompileTableEntry;
+struct ConstVRInfo;
+struct MemoryVRInfo;
+
+/**
+ * @brief Used to keep track of virtual registers and their various associations.
+ * @details Keeps track of compile table information associated with VR including
+ * the physical register, the in memory state of a VR, and the constantness of VR.
+ */
+class AssociationTable {
+public:
+    /**
+     * @brief Looks through all associations and finds used physical registers
+     * @param outUsedRegisters is a set that is updated with the used physical
+     * registers
+     */
+    void findUsedRegisters(std::set<PhysicalReg> & outUsedRegisters) const;
+
+    /**
+     * @brief Once association table is been finalized, this can be called to
+     * find out if the virtual register was in memory.
+     * @param vR virtual register
+     * @return Returns whether or not VR was in memory
+     */
+    bool wasVRInMemory(int vR) const;
+
+    /**
+     * @brief Once association table is been finalized, this can be called to
+     * find out if the virtual register was a constant.
+     * @details For wide VRs, this should be called twice to find out if both
+     * low order bits and high order bits were constant.
+     * @param vR virtual register
+     * @return Returns whether or not virtual register was a constant.
+     */
+    bool wasVRConstant(int vR) const;
+
+    /**
+     * @brief Returns the 32 bit constant value associated with VR
+     * @pre wasVRConstant should return true
+     * @param vR virtual register
+     * @return Returns the constant value of virtual register.
+     */
+    int getVRConstValue(int vR) const;
+
+    /**
+     * @brief Updates association table given a compile entry from the compile table.
+     * @param compileEntry compilation entry
+     * @return Returns whether adding compile entry to associations was successful
+     */
+    bool associate(const CompileTableEntry &compileEntry);
+
+    /**
+     * @brief Updates association table given a memory VR information
+     * @param memVRInfo the memory virtual register information entry
+     * @return Returns whether adding compile entry to associations was successful
+     */
+    bool associate(const MemoryVRInfo &memVRInfo);
+
+    /**
+     * @brief Updates association table given a constant VR information
+     * @param constVRInfo the constant virtual register entry
+     * @return Returns whether adding compile entry to associations was successful
+     */
+    bool associate(const ConstVRInfo &constVRInfo);
+
+    /**
+     * @brief Used to determine whether the association table can be
+     * updated anymore.
+     * @return Returns whether the associations are final and cannot be updated.
+     */
+    bool hasBeenFinalized(void) const {
+        return isFinal;
+    }
+
+    /**
+     * @brief Used to tell association table that it cannot accept anymore
+     * updates.
+     */
+    void finalize();
+
+    /**
+     * @brief Clears the association table.
+     */
+    void clear(void);
+
+    /**
+     * @brief Used to copy another association table into current one
+     * @param source association table to read from
+     * @return whether the copy was successful
+     */
+    bool copy(AssociationTable & source);
+
+    /**
+     * Returns number of entries in association table
+     * @return size of association table
+     */
+    size_t size(void) const {
+        return this->associations.size();
+    }
+
+    /**
+     * @brief Prints the association table to a file separating entries with
+     * vertical bar.
+     * @param file File to print to.
+     */
+    void printToDot(FILE * file);
+
+    /**
+     * @brief Random access const iterator. This does not modify structure it is iterating.
+     */
+    typedef std::map<int, CompileTableEntry>::const_iterator const_iterator;
+
+    /**
+     * @brief Random access iterator. This may modify structure it is iterating.
+     */
+    typedef std::map<int, CompileTableEntry>::iterator iterator;
+
+    /**
+     * @brief Returns an iterator pointing to the first association.
+     * @return iterator to beginning
+     */
+    iterator begin() {
+        return associations.begin();
+    }
+
+    /**
+     * @brief Returns a const iterator pointing to the first association.
+     * @return iterator to beginning
+     */
+    const_iterator begin() const {
+        return associations.begin();
+    }
+
+    /**
+     * @brief Returns an iterator referring to the past-the-end association.
+     * @return iterator past end
+     */
+    iterator end() {
+        return associations.end();
+    }
+
+    /**
+     * @brief Returns a const iterator referring to the past-the-end
+     * association.
+     * @return iterator past end
+     */
+    const_iterator end() const {
+        return associations.end();
+    }
+
+    /**
+     * @brief Returns a const interator to the compile table entry matching the desired VR.
+     * @param vR The virtual register to look for.
+     * @return Returns iterator matching the result found or iterator that equals end() when
+     * no match is found.
+     */
+    const_iterator find (const int &vR) const {
+        return associations.find(vR);
+    }
+
+    AssociationTable(void); /**< @brief Constructor */
+
+    ~AssociationTable(void); /**< @brief Destructor */
+
+    //Static functions of the RegisterizationBE class
+
+    /**
+     * @brief Updates a given association table using the current state of the
+     * compile table.
+     * @param associationsToUpdate Association table to update.
+     * @return Returns whether the association table was updated successfully.
+     */
+    static bool syncAssociationsWithCompileTable(AssociationTable & associationsToUpdate);
+
+    /**
+     * @brief Updates the current state of the compile table to all VR entries
+     * in the association table
+     * @param associationsToUse Association table to use for compile table updated.
+     * @return Returns whether the compile table update was successful.
+     */
+    static bool syncCompileTableWithAssociations(AssociationTable & associationsToUse);
+
+    /**
+     * @brief Creates association table for child or generates instructions to
+     * match it.
+     * @param bb Parent basic block
+     * @param forFallthrough Flag on whether should update fallthrough child. Else
+     * we update taken child.
+     * @return Whether the sync was successful.
+     */
+    static bool createOrSyncTable(BasicBlock_O1 * bb, bool forFallthrough = true);
+
+    /**
+     * @brief Generates instructions to match current state of parent basic block
+     * to the association table state of child.
+     * @param parent Parent basic block.
+     * @param child Child basic block.
+     * @param isBackward Used to denote whether we are satisfying associations
+     * of loop entry. If yes, then we only write back phi nodes.
+     * @return Returns whether the state of parent successfully matches state of
+     * child.
+     */
+    static bool satisfyBBAssociations (BasicBlock_O1 * parent,
+            BasicBlock_O1 * child, bool isBackward = false);
+
+    /**
+     * @brief Spills virtual registers marked for spilling by the middle end.
+     * @param bb Basic block whose spill requests we need to handle.
+     * @return Returns whether we successfully handled spill requests.
+     */
+    static bool handleSpillRequestsFromME(BasicBlock_O1 * bb);
+
+private:
+    /**
+     * @brief Map for every VR to its corresponding compile table entry
+     * when association occurred.
+     */
+    std::map<int, CompileTableEntry> associations;
+
+    /**
+     * @brief Map for every VR to its state in memory when the association
+     * occurred.
+     */
+    std::map<int, MemoryVRInfo> inMemoryTracker;
+
+    /**
+     * @typedef Iterator for use with inMemoryTracker.
+     */
+    typedef std::map<int, MemoryVRInfo>::const_iterator inMemoryTrackerConstIterator;
+
+    /**
+     * @brief Map for every VR to its constant value (if it had any) when the
+     * association occurred.
+     */
+    std::map<int, ConstVRInfo> constTracker;
+
+    /**
+     * @typedef Iterator for use with constTracker
+     */
+    typedef std::map<int, ConstVRInfo>::const_iterator constantTrackerConstIterator;
+
+    /**
+     * @brief Keeps track whether association table has been finalized.
+     */
+    bool isFinal;
+};
+
+#endif /* REGISTERIZATIONBE_H_ */
diff --git a/vm/compiler/codegen/x86/lightcg/Scheduler.cpp b/vm/compiler/codegen/x86/lightcg/Scheduler.cpp
new file mode 100644
index 0000000..0e06436
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/Scheduler.cpp
@@ -0,0 +1,2073 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*! \file Scheduler.cpp
+    \brief This file implements the Atom Instruction Scheduler.
+    \details Scheduling algorithm implemented is basic block scheduling.
+*/
+
+#include "Lower.h"
+#include "interp/InterpDefs.h"
+#include "Scheduler.h"
+
+//! \def DISABLE_ATOM_SCHEDULING_STATISTICS
+//! \brief Disables printing of scheduling statistics.
+//! \details Defining this macro disables printing of scheduling statistics pre
+//! and post scheduling. Undefine macro when statistics are needed.
+#define DISABLE_ATOM_SCHEDULING_STATISTICS
+
+//! \def DISABLE_DEBUG_ATOM_SCHEDULER
+//! \brief Disables debug printing for atom scheduler.
+//! \details Defining macro DISABLE_DEBUG_ATOM_SCHEDULER disables debug printing.
+//! Undefine macro when debugging scheduler implementation.
+#define DISABLE_DEBUG_ATOM_SCHEDULER
+
+//! \def DISABLE_DEPENDENGY_GRAPH_DEBUG
+//! \brief Disables printing of dependency graph
+//! \details Undefine this macro when wanting to debug dependency graph.
+//! The dot files for each basic block will be dumped to folder /data/local/tmp
+//! and the name for each file will be depengraph_<pid>_<stream_start_of_BB>.dot
+#define DISABLE_DEPENDENGY_GRAPH_DEBUG
+
+#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
+#include <fstream>
+#include <sstream>
+#include <string>
+#include <set>
+#endif
+
+//! \enum IssuePort
+//! \brief Defines possible combinations of port-binding information for use
+//! with information about each x86 mnemonic.
+enum IssuePort {
+    //! \brief invalid port, used for table only when some
+    //! operands are not supported for the mnemonic
+    INVALID_PORT = -1,
+    //! \brief the mnemonic can only be issued on port 0
+    PORT0 = 0,
+    //! \brief the mnemonic can only be issued on port 1
+    PORT1 = 1,
+    //! \brief the mnemonic can be issued on either port
+    EITHER_PORT,
+    //! \brief both ports are used for the mnemonic
+    BOTH_PORTS
+};
+
+#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
+//! \brief Transforms from IssuePort enum to a string representation.
+inline const char * getIssuePort(IssuePort port) {
+    switch (port) {
+    case INVALID_PORT:
+        return "invalid";
+    case PORT0:
+        return "0";
+    case PORT1:
+        return "1";
+    case EITHER_PORT:
+        return "either";
+    case BOTH_PORTS:
+        return "both";
+    }
+    return "Invalid";
+}
+#endif
+
+//! \class MachineModelEntry
+//! \brief Information needed to define the machine model for each x86 mnemonic.
+struct MachineModelEntry {
+    //! \brief which port the instruction can execute on
+    IssuePort issuePortType;
+    //! \brief execute to execute time for one instruction
+    int executeToExecuteLatency;
+};
+
+//! \def INVP
+//! \brief This is an abbreviation of INVALID_PORT and is used for readability
+//! reasons.
+#define INVP INVALID_PORT
+
+//! \def INVN
+//! \brief This is an abbreviation of invalid node latency and is used for
+//! readability reasons.
+#define INVN 0
+
+//! \def REG_NOT_USED
+//! \brief This is an abbreviation for register not used and is used for
+//! readability reasons whenever a Scheduler method needs register type
+//! to update some data structure but a register number does not make
+//! sense in the context.
+#define REG_NOT_USED -1
+
+//! \brief This table lists the parameters for each Mnemonic in the Atom Machine Model.
+//! \details This table includes port and latency information for each mnemonic for each possible
+//! configuration of operands. 6 entries of MachineModelEntry are reserved for each Mnemonic:
+//! - If a Mnemonic has zero operand, only the first entry is valid
+//! - If a Mnemonic has a single operand, the first 3 entries are valid, for operand type
+//! imm, reg and mem respectively
+//! - If a Mnemonic has two operands, the last 5 entries are valid, for operand types
+//! imm_to_reg, imm_to_mem, reg_to_reg, mem_to_reg and reg_to_mem
+//!
+//! This table matches content from Intel 64 and IA-32 Architectures Optimization
+//! Reference Manual (April 2012), Section 13.4
+//! \warning This table is not complete and if new mnemonics are used that do not have an
+//! entry, then the schedule selection will not be optimal.
+MachineModelEntry atomMachineModel[Mnemonic_Count*6] = {
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NULL, Null
+
+    {PORT1,1},{PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //JMP
+
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //MOV
+
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_O
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NO
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_B
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NB
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_Z
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NZ
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_BE
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NBE
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_S
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NS
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_P
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NP
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_L
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NL
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_LE
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NLE
+
+    {BOTH_PORTS,1},{BOTH_PORTS,1},{EITHER_PORT,2},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CALL
+
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //ADC
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //ADD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //ADDSD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //ADDSS
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //AND
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //BSF
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //BSR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMC
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{INVP,INVN},{INVP,INVN}, //CWD, CDQ
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_O
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NO
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_B,NAE,C
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NB,AE,NC
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_Z,E
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NZ,NE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_BE,NA
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NBE,A
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_S
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_P,PE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NP,PO
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_L,NGE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NL,GE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_LE,NG
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //CMOV_NLE,G
+
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //CMP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{PORT0,5}, //CMPXCHG Note: info missed in section 13.4
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{PORT0,5}, //CMPXCHG8B Note: info missed in section 13.4
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPSB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPSW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPSD
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,7},{BOTH_PORTS,8},{INVP,INVN}, //CVTSD2SS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,8},{BOTH_PORTS,9},{INVP,INVN}, //CVTSD2SI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,8},{BOTH_PORTS,9},{INVP,INVN}, //CVTTSD2SI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,7},{BOTH_PORTS,8},{INVP,INVN}, //CVTSS2SD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //CVTSS2SI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //CVTTSS2SI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,7},{BOTH_PORTS,8},{INVP,INVN}, //CVTSI2SD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,6},{BOTH_PORTS,7},{INVP,INVN}, //CVTSI2SS
+
+    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //COMISD
+    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //COMISS
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //DEC
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,62},{BOTH_PORTS,62},{INVP,INVN}, //DIVSD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,34},{BOTH_PORTS,34},{INVP,INVN}, //DIVSS
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //ENTER
+    {INVP,INVN},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN}, //FLDCW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{INVP,INVN}, //FADDP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDZ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{INVP,INVN}, //FADD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{INVP,INVN}, //FSUBP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{INVP,INVN}, //FSUB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FISUB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{INVP,INVN}, //FMUL
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{INVP,INVN}, //FMULP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,65},{INVP,INVN}, //FDIVP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,65},{INVP,INVN}, //FDIV
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,1},{INVP,INVN},{INVP,INVN}, //FUCOM
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FUCOMI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,1},{INVP,INVN},{INVP,INVN}, //FUCOMP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FUCOMIP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FUCOMPP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FRNDINT
+    {INVP,INVN},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,5}, //FNSTCW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSTSW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FNSTSW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN}, //FILD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{INVP,INVN}, //FLD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDLG2
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDLN2
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLD1
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FCLEX
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FCHS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FNCLEX
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,6}, //FIST
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,6}, //FISTP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FISTTP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPREM
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPREM1
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1}, //FST fp_mem
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1}, //FSTP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,65},{INVP,INVN}, //FSQRT
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN}, //FABS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSIN
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FCOS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPTAN
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FYL2X
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FYL2XP1
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //F2XM1
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPATAN
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FXCH
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSCALE
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //XCHG
+    // There is no way to differentiate operand sizes in this table, so just assume 32-bit
+    {INVP,INVN},{BOTH_PORTS,57},{BOTH_PORTS,57},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //DIV
+    {INVP,INVN},{BOTH_PORTS,57},{BOTH_PORTS,57},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //IDIV
+    {INVP,INVN},{BOTH_PORTS,6},{BOTH_PORTS,7},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MUL
+    // This table does not support IMUL with single reg or mem operand
+    {INVP,INVN},{PORT0,5},{PORT0,5},{PORT0,5},{PORT0,5},{INVP,INVN}, //IMUL
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //INC
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //INT3
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,1},{INVP,INVN}, //LEA
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LEAVE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LOOP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LOOPE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LOOPNE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LAHF
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1}, //MOVD
+    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{PORT0,1},{PORT0,1}, //MOVQ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS8
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS16
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS32
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS64
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{PORT0,1}, //MOVAPD
+    {INVP,INVN},{PORT0,1},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //MOVSD
+    {INVP,INVN},{PORT0,1},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //MOVSS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{PORT0,1},{INVP,INVN}, //MOVSX
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{PORT0,1},{INVP,INVN}, //MOVZX
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{PORT0,5},{INVP,INVN}, //MULSD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,4},{PORT0,4},{INVP,INVN}, //MULSS
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,10},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NEG
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NOP
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,10},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NOT
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //OR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //PREFETCH
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,2},{EITHER_PORT,3},{INVP,INVN}, //PADDQ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PAND
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //POR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,2},{EITHER_PORT,3},{INVP,INVN}, //PSUBQ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PANDN
+    {INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{EITHER_PORT,2},{EITHER_PORT,3},{INVP,INVN}, //PSLLQ
+    {INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{EITHER_PORT,2},{EITHER_PORT,3},{INVP,INVN}, //PSRLQ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PXOR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //POP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //POPFD
+    {INVP,INVN},{BOTH_PORTS,1},{BOTH_PORTS,2},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //PUSH
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //PUSHFD
+    {BOTH_PORTS,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //RET
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_O
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NO
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_B
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_Z
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NZ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_BE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NBE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_S
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_P
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_L
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NL
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_LE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NLE
+
+    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{INVP,INVN},{INVP,INVN}, //SAL,SHL
+    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{INVP,INVN},{INVP,INVN}, //SAR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //ROR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //RCR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //ROL
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //RCL
+    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{INVP,INVN},{INVP,INVN}, //SHR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,4},{BOTH_PORTS,4},{BOTH_PORTS,2}, //SHRD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,4},{BOTH_PORTS,4},{BOTH_PORTS,2}, //SHLD
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //SBB
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //SUB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //SUBSD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //SUBSS
+
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{INVP,INVN},{PORT0,1}, //TEST
+    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //UCOMISD
+    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //UCOMISS
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //XOR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //XORPD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //XORPS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTDQ2PD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTTPD2DQ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTDQ2PS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTTPS2DQ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //STD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CLD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SCAS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //STOS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //WAIT
+};
+
+//! \brief Get issue port for mnemonic with no operands
+inline IssuePort getAtomMnemonicPort(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6].issuePortType;
+}
+//! \brief Get issue port for mnemonic with one immediate operand
+inline IssuePort getAtomMnemonicPort_imm(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6].issuePortType;
+}
+//! \brief Get issue port for mnemonic with one register operand
+inline IssuePort getAtomMnemonicPort_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+1].issuePortType;
+}
+//! \brief Get issue port for mnemonic with one memory operand
+inline IssuePort getAtomMnemonicPort_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+2].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: immediate to register
+inline IssuePort getAtomMnemonicPort_imm_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+1].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: immediate to memory
+inline IssuePort getAtomMnemonicPort_imm_to_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+2].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: register to register
+inline IssuePort getAtomMnemonicPort_reg_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+3].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: memory to register
+inline IssuePort getAtomMnemonicPort_mem_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+4].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: register to memory
+inline IssuePort getAtomMnemonicPort_reg_to_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+5].issuePortType;
+}
+
+//! \brief Get execute to execute latency for mnemonic with no operands
+inline int getAtomMnemonicLatency(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with one immediate operand
+inline int getAtomMnemonicLatency_imm(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with one register operand
+inline int getAtomMnemonicLatency_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+1].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with one memory operand
+inline int getAtomMnemonicLatency_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+2].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: immediate to register
+inline int getAtomMnemonicLatency_imm_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+1].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: immediate to memory
+inline int getAtomMnemonicLatency_imm_to_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+2].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: register to register
+inline int getAtomMnemonicLatency_reg_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+3].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: memory to register
+inline int getAtomMnemonicLatency_mem_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+4].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: register to memory
+inline int getAtomMnemonicLatency_reg_to_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+5].executeToExecuteLatency;
+}
+
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+//! \brief Transforms from LowOpndDefUse enum to string representation of the usedef
+//! \see LowOpndDefUse
+inline const char * getUseDefType(LowOpndDefUse defuse) {
+    switch (defuse) {
+    case LowOpndDefUse_Def:
+        return "Def";
+    case LowOpndDefUse_Use:
+        return "Use";
+    case LowOpndDefUse_UseDef:
+        return "UseDef";
+    }
+    return "-";
+}
+#endif
+
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+//! \brief Transforms from UseDefEntryType enum to a string representation.
+//! \see UseDefEntryType
+inline const char * getUseDefEntryType(UseDefEntryType type) {
+    switch (type) {
+    case UseDefType_Ctrl:
+        return "Ctrl";
+    case UseDefType_Float:
+        return "Float";
+    case UseDefType_MemVR:
+        return "MemVR";
+    case UseDefType_MemSpill:
+        return "MemSpill";
+    case UseDefType_MemUnknown:
+        return "MemUnknown";
+    case UseDefType_Reg:
+        return "Reg";
+    }
+    return "-";
+}
+#endif
+
+//! \brief Returns true if mnemonic is a variant of MOV.
+inline bool isMoveMnemonic(Mnemonic m) {
+    return m == Mnemonic_MOV || m == Mnemonic_MOVQ || m == Mnemonic_MOVSD
+            || m == Mnemonic_MOVSS || m == Mnemonic_MOVZX || m == Mnemonic_MOVSX;
+}
+
+//! \brief Returns true if mnemonic is used for comparisons.
+//! \details Returns false for FPU comparison mnemonics.
+inline bool isCompareMnemonic(Mnemonic m) {
+    return m == Mnemonic_CMP || m == Mnemonic_COMISD || m == Mnemonic_COMISS
+            || m == Mnemonic_TEST;
+}
+
+//! \brief Returns true if mnemonic is SSE conversion routine
+inline bool isConvertMnemonic(Mnemonic m) {
+    return m == Mnemonic_CVTSD2SS || m == Mnemonic_CVTSD2SI
+            || m == Mnemonic_CVTTSD2SI || m == Mnemonic_CVTSS2SD
+            || m == Mnemonic_CVTSS2SI || m == Mnemonic_CVTTSS2SI
+            || m == Mnemonic_CVTSI2SD || m == Mnemonic_CVTSI2SS;
+}
+
+//! \brief Returns true if mnemonic uses and then defines the FLAGS register
+inline bool usesAndDefinesFlags(Mnemonic m) {
+    return m == Mnemonic_ADC || m == Mnemonic_SBB;
+}
+
+//! \brief Returns true if mnemonic is CMPXCHG, which use and define EAX register
+inline bool isCmpxchgMnemonic(Mnemonic m) {
+    return m == Mnemonic_CMPXCHG;
+}
+
+//! \brief Returns true if ALU mnemonic has a variant that has implicit
+//! register usage.
+//! \details Returns true for div, idiv, mul, imul, and cdq. However, note
+//! that implicit register usage is dependent on variant being used. For example,
+//! only idiv with single reg operand has implicit register usage.
+inline bool isAluOpWithImplicitRegisterUsage(Mnemonic m) {
+    return m == Mnemonic_DIV || m == Mnemonic_IDIV
+            || m == Mnemonic_IMUL || m == Mnemonic_MUL
+            || m == Mnemonic_CDQ;
+}
+
+//! \brief Detects whether the mnemonic is a native basic block delimiter.
+//! \details Unconditional jumps, conditional jumps, calls, and returns
+//! always end a native basic block.
+inline bool Scheduler::isBasicBlockDelimiter(Mnemonic m) {
+    return (m == Mnemonic_JMP || m == Mnemonic_CALL
+            || (m >= Mnemonic_Jcc && m <= Mnemonic_JG) || m == Mnemonic_RET);
+}
+
+//! \details Defines a mapping between the reason for edge latencies between
+//! instructions and the actual latency value.
+//! \see LatencyBetweenNativeInstructions
+static int mapLatencyReasonToValue[] = {
+    // Latency_None
+    0,
+    // Latency_Agen_stall
+    3,
+    //Latency_Load_blocked_by_store
+    0,
+    //Latency_Memory_Load
+    0,
+};
+
+//! \brief Atom scheduler destructor
+Scheduler::~Scheduler(void) {
+    // Clear all scheduler data structures
+    this->reset();
+}
+
+//! \brief Resets data structures used by Scheduler
+void Scheduler::reset(void) {
+    queuedLIREntries.clear();
+    scheduledLIREntries.clear();
+
+    for (std::vector<UseDefUserEntry>::iterator it = userEntries.begin();
+            it != userEntries.end(); ++it) {
+        it->useSlotsList.clear();
+    }
+    userEntries.clear();
+
+    for (std::map<LowOp*, Dependencies>::iterator it =
+            dependencyAssociation.begin(); it != dependencyAssociation.end();
+            it++) {
+        Dependencies &d = it->second;
+        d.predecessorDependencies.clear();
+        d.successorDependencies.clear();
+    }
+    dependencyAssociation.clear();
+
+    // Safe to clear
+    producerEntries.clear();
+    ctrlEntries.clear();
+}
+
+//! \brief Returns true if Scheduler has no LIRs in its
+//! queue for scheduling.
+//! \return true when Scheduler queue is empty
+bool Scheduler::isQueueEmpty() const {
+    return queuedLIREntries.empty();
+}
+
+//! \brief Given an access to a resource (Control, register, VR, unknown memory
+//! access), this updates dependency graph, usedef information, and control flags.
+//! \details Algorithm description for dependency update:
+//! - for Use or UseDef:
+//!   -# insert RAW dependency from producer for this op
+//! - for Def or UseDef:
+//!   -# insert WAR dependency from earlier user for this op
+//!   -# insert WAW dependency from earlier producer for this op
+//! - Internal data structure updates for record keeping:
+//!   -# for Def or UseDef: update producerEntries
+//!   -# for Def: clear corresponding use slots for entry in userEntries
+//!   -# for UseDef: clear corresponding use slots for entry in userEntries
+//!   -# for Use: update userEntries
+//!
+//! \param type resource that causes dependency
+//! \param regNum is a number corresponding to a physical register or a Dalvik
+//! virtual register. When physical, this is of enum type PhysicalReg.
+//! \param defuse definition, usage, or both
+//! \param causeOfLatency Weight to use on the edge.
+//! \param op LIR for which to update dependencies
+void Scheduler::updateDependencyGraph(UseDefEntryType type, int regNum,
+        LowOpndDefUse defuse, LatencyBetweenNativeInstructions causeOfLatency,
+        LowOp* op) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+    const char * string_defuse = getUseDefType(defuse);
+    const char * string_type = getUseDefEntryType(type);
+    ALOGD("---updateDependencyGraph for resource <%s %d> "
+            "at slot %d with %s---\n", string_type,
+            regNum, op->slotId, string_defuse);
+#endif
+
+    unsigned int k;
+    unsigned int index_for_user = userEntries.size();
+    unsigned int index_for_producer = producerEntries.size();
+
+    // Look for the producer of this resource. If none is found, then
+    // index_for_producer will remain length of producerEntries list.
+    if (type != UseDefType_Ctrl) {
+        for (k = 0; k < producerEntries.size(); ++k) {
+            if (producerEntries[k].type == type
+                    && producerEntries[k].regNum == regNum) {
+                index_for_producer = k;
+                break;
+            }
+        }
+    }
+
+    // Look for the users of this resource. If none are found, then
+    // index_for_user will remain length of userEntries list.
+    for (k = 0; k < userEntries.size(); ++k) {
+        if (userEntries[k].type == type && userEntries[k].regNum == regNum) {
+            index_for_user = k;
+            break;
+        }
+    }
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+    ALOGD("index_for_producer %d %d index_for_user %d %d\n",
+            index_for_producer, producerEntries.size(),
+            index_for_user, userEntries.size());
+#endif
+
+    if (defuse == LowOpndDefUse_Use || defuse == LowOpndDefUse_UseDef) {
+        // If use or usedef, then there is a RAW dependency from producer
+        // of the resource.
+        if (type != UseDefType_Ctrl
+                && index_for_producer != producerEntries.size()) {
+            assert(producerEntries[index_for_producer].producerSlot != op->slotId);
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            ALOGD("RAW dependency from %d to %d due to resource <%s %d>\n",
+                    producerEntries[index_for_producer].producerSlot, op->slotId, string_type, regNum);
+#endif
+            DependencyInformation ds;
+            ds.dataHazard = Dependency_RAW;
+            ds.lowopSlotId = producerEntries[index_for_producer].producerSlot;
+            ds.causeOfEdgeLatency = causeOfLatency;
+            ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
+            // If producer is a memory load, then also add memory latency
+            if (isMoveMnemonic(queuedLIREntries[ds.lowopSlotId]->opCode) &&
+                    queuedLIREntries[ds.lowopSlotId]->opndSrc.type == LowOpndType_Mem) {
+                // If memory load latency is greater than current latency,
+                // replace it with the memory load
+                if (mapLatencyReasonToValue[Latency_Memory_Load] > ds.edgeLatency) {
+                    ds.causeOfEdgeLatency = Latency_Memory_Load;
+                    ds.edgeLatency += mapLatencyReasonToValue[Latency_Memory_Load];
+                }
+            }
+            dependencyAssociation[op].predecessorDependencies.push_back(ds);
+        }
+        // For Ctrl dependencies, when there is a user of a resource
+        // it depends on the last producer. However, the last producer
+        // depends on all previous producers. This is done as an
+        // optimization because flag writers don't need to depend on
+        // each other unless there is a flag reader.
+        if (type == UseDefType_Ctrl && ctrlEntries.size() > 0) {
+            // insert RAW from the last producer
+            assert(ctrlEntries.back() != op->slotId);
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            ALOGD("insert RAW from %d to %d due to Ctrl\n",
+                    ctrlEntries.back(), op->slotId);
+#endif
+            DependencyInformation ds;
+            ds.dataHazard = Dependency_RAW;
+            ds.lowopSlotId = ctrlEntries.back();
+            ds.causeOfEdgeLatency = causeOfLatency;
+            ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
+            dependencyAssociation[op].predecessorDependencies.push_back(ds);
+            // insert WAW from earlier producers to the last producer
+            LowOp* opLast = (queuedLIREntries[ctrlEntries.back()]);
+            for (k = 0; k < ctrlEntries.size() - 1; k++) {
+                assert(ctrlEntries[k] != opLast->slotId);
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+                ALOGD("insert WAW from %d to %d due to Ctrl\n", ctrlEntries[k], ctrlEntries.back());
+#endif
+                DependencyInformation ds;
+                ds.dataHazard = Dependency_WAW;
+                ds.lowopSlotId = ctrlEntries[k];
+                ds.causeOfEdgeLatency = causeOfLatency;
+                ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
+                dependencyAssociation[opLast].predecessorDependencies.push_back(ds);
+            }
+        }
+
+        // If this is the first use of this resource, then an
+        // entry should be created in the userEntries list
+        if (index_for_user == userEntries.size()) {
+            UseDefUserEntry entry;
+            entry.type = type;
+            entry.regNum = regNum;
+            userEntries.push_back(entry);
+        } else if (type == UseDefType_Ctrl) {
+            userEntries[index_for_user].useSlotsList.clear();
+        }
+        // Add current op as user of resource
+        userEntries[index_for_user].useSlotsList.push_back(op->slotId);
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        ALOGD("op with slot %d uses resource <%s %d>\n", op->slotId, string_type, regNum);
+#endif
+
+        if (type == UseDefType_Ctrl)
+            ctrlEntries.clear();
+    }
+
+    if (defuse == LowOpndDefUse_Def || defuse == LowOpndDefUse_UseDef) {
+        // If def or usedef, then there is a WAR dependency from earlier users
+        // of this resource and a WAW dependency due to earlier producer
+        if (index_for_user != userEntries.size()) {
+            // Go through every user of resource and update current op with a WAR
+            // from each user.
+            for (k = 0; k < userEntries[index_for_user].useSlotsList.size();
+                    k++) {
+                if (userEntries[index_for_user].useSlotsList[k] == op->slotId)
+                    continue; // No need to create dependency on self
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+                ALOGD("WAR dependency from %d to %d due to resource <%s %d>\n",
+                        userEntries[index_for_user].useSlotsList[k], op->slotId, string_type, regNum);
+#endif
+                DependencyInformation ds;
+                ds.dataHazard = Dependency_WAR;
+                ds.lowopSlotId = userEntries[index_for_user].useSlotsList[k];
+                ds.causeOfEdgeLatency = causeOfLatency;
+                ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
+                dependencyAssociation[op].predecessorDependencies.push_back(ds);
+            }
+        }
+        if (type != UseDefType_Ctrl
+                && index_for_producer != producerEntries.size()) {
+            // There is WAW dependency from earlier producer to current producer
+            // For Ctrl resource, WAW is not relevant until there is a reader
+            assert(producerEntries[index_for_producer].producerSlot != op->slotId);
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            ALOGD("WAW dependency from %d to %d due to resource <%s %d>\n",
+                    producerEntries[index_for_producer].producerSlot, op->slotId, string_type, regNum);
+#endif
+            DependencyInformation ds;
+            ds.dataHazard = Dependency_WAW;
+            ds.lowopSlotId = producerEntries[index_for_producer].producerSlot;
+            ds.causeOfEdgeLatency = causeOfLatency;
+            ds.edgeLatency = mapLatencyReasonToValue[causeOfLatency];
+            dependencyAssociation[op].predecessorDependencies.push_back(ds);
+        }
+
+        if (type != UseDefType_Ctrl
+                && index_for_producer == producerEntries.size()) {
+            // If we get here it means that this the first known producer
+            // of this resource and therefore we should keep track of this
+            UseDefProducerEntry entry;
+            entry.type = type;
+            entry.regNum = regNum;
+            producerEntries.push_back(entry);
+        }
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        ALOGD("op with slot %d produces/defines resource <%s %d>\n",
+                op->slotId, string_type, regNum);
+#endif
+        if (type != UseDefType_Ctrl)
+            // Add current op as producer of resource
+            producerEntries[index_for_producer].producerSlot = op->slotId;
+        else {
+            // Save the current op as one of the producers of this resource
+            ctrlEntries.push_back(op->slotId);
+        }
+
+        // Since this a new producer of the resource, we can now forget
+        // all past users. This behavior is also correct if current op is
+        // user and then producer because when handling usedef, use is
+        // handled first.
+        if (type != UseDefType_Ctrl && index_for_user != userEntries.size()) {
+            userEntries[index_for_user].useSlotsList.clear();
+        }
+    }
+}
+
+//! \brief Given an access to a memory location this updates dependency graph,
+//! usedef information, and control flags.
+//! \details This method uses updateDependencyGraph internally to update
+//! dependency graph, but knows the type of memory resource that is being used.
+//! \param mOpnd reference to the structure for memory operand
+//! \param defuse definition, usage, or both
+//! \param op LIR for which to update dependencies
+void Scheduler::updateDependencyGraphForMem(LowOpndMem & mOpnd, LowOpndDefUse defuse,
+        LowOp* op) {
+    MemoryAccessType mType = mOpnd.mType;
+    int index = mOpnd.index;
+    bool is64 = false;
+
+    // We know that if we have an access to the memory holding constants
+    // then it is okay to reorder instructions accessing that since it
+    // is not updated at runtime. Thus, we don't add any dependencies for
+    // this operand.
+    if (mType == MemoryAccess_Constants)
+    {
+        // All accesses to the constants section should be done with
+        // null base register
+        assert(mOpnd.m_base.regNum == PhysicalReg_Null);
+
+        return;
+    }
+
+    // Update dependency on registers used
+    updateDependencyGraph(UseDefType_Reg, mOpnd.m_base.regNum,
+            LowOpndDefUse_Use, Latency_Agen_stall, op);
+    if (mOpnd.hasScale)
+        updateDependencyGraph(UseDefType_Reg, mOpnd.m_index.regNum,
+                LowOpndDefUse_Use, Latency_Agen_stall, op);
+
+    // In order to be safe, if one of the operands has size 64, assume it is size 64
+    if (op->numOperands >= 1 && op->opndDest.size == OpndSize_64)
+        is64 = true;
+    if (op->numOperands >= 2 && op->opndSrc.size == OpndSize_64)
+        is64 = true;
+
+    // At this point make a decision on whether or not to do memory disambiguation.
+    // If it is not VR or SPILL access, then it may not be safe to disambiguate.
+    if (mType == MemoryAccess_VR) {
+        // All VR accesses should be made via Java frame pointer
+        assert(mOpnd.m_base.regNum == PhysicalReg_FP);
+
+        updateDependencyGraph(UseDefType_MemVR, index, defuse, Latency_None, op);
+        if (is64)
+            updateDependencyGraph(UseDefType_MemVR, index + 1, defuse, Latency_None, op);
+    } else if (mType == MemoryAccess_SPILL) {
+        // All spill accesses should be made via offset from EBP
+        assert(mOpnd.m_base.regNum == PhysicalReg_EBP);
+
+        updateDependencyGraph(UseDefType_MemSpill, index, defuse, Latency_None, op);
+        if (is64) {
+            updateDependencyGraph(UseDefType_MemSpill, index + 4, defuse, Latency_None, op);
+        }
+    } else // No disambiguation
+        updateDependencyGraph(UseDefType_MemUnknown, REG_NOT_USED, defuse,
+                Latency_None, op);
+}
+
+//! \brief Updates dependency information for PUSH which uses then defines %esp
+//! and also updates native stack.
+void inline Scheduler::handlePushDependencyUpdate(LowOp* op) {
+    if (op->opCode == Mnemonic_PUSH) {
+        updateDependencyGraph(UseDefType_Reg, PhysicalReg_ESP,
+                LowOpndDefUse_UseDef, Latency_Agen_stall, op);
+        updateDependencyGraph(UseDefType_MemUnknown, REG_NOT_USED,
+                LowOpndDefUse_Def, Latency_None, op);
+    }
+}
+
+//! \brief Updates dependency information for operations on floating point stack.
+//! \details This should be called for all x87 instructions. This will ensure
+//! that they are never reordered.
+void inline Scheduler::handleFloatDependencyUpdate(LowOp* op) {
+    // UseDef dependency is used so that x87 instructions won't be reordered
+    // Whenever reordering support is added, this function should be replaced
+    // and new resources defined like FPU flags, control word, status word, etc.
+    updateDependencyGraph(UseDefType_Float, REG_NOT_USED, LowOpndDefUse_UseDef,
+            Latency_None, op);
+}
+
+//! \brief Sets up dependencies on resources that must be live out.
+//! \details Last write to a resource should be ensured to be live
+//! out.
+void Scheduler::setupLiveOutDependencies() {
+    // Handle live out control flags. Namely, make sure that last flag
+    // writer depends on all previous flag writers.
+    if (ctrlEntries.size() != 0) {
+        // If the ctrlEntries list is empty, it means we have no flag producers
+        // that we need to update. This is caused if there really were not flag
+        // producers or a flag reader has already cleared this list which means
+        // the flag read already will be the one live out.
+        LowOp* lastFlagWriter = (queuedLIREntries[ctrlEntries.back()]);
+
+        // Don't include last flag writer in the iteration
+        for (std::vector<unsigned int>::const_iterator iter = ctrlEntries.begin ();
+                                                       (iter + 1) != ctrlEntries.end ();
+                                                       iter++) {
+            // Add a WAW dependency to the last flag writer from all other
+            // flag writers
+            DependencyInformation ds;
+            ds.dataHazard = Dependency_WAW;
+            ds.lowopSlotId = *iter;
+            ds.causeOfEdgeLatency = Latency_None;
+            ds.edgeLatency = mapLatencyReasonToValue[Latency_None];
+            dependencyAssociation[lastFlagWriter].predecessorDependencies.push_back(
+                    ds);
+        }
+    }
+
+    //! @todo Take care of live out dependencies for all types of resources
+    //! including physical registers.
+}
+
+//! \brief Updates dependency graph with the implicit dependencies on eax
+//! and edx for imul, mul, div, idiv, and cdq
+//! \warning Assumes that operand size is 32 bits
+void inline Scheduler::handleImplicitDependenciesEaxEdx(LowOp* op) {
+    if (isAluOpWithImplicitRegisterUsage(op->opCode)) {
+        // mul and imul with a reg operand implicitly usedef eax and def edx
+        // div and idiv with a reg operand implicitly usedef eax and usedef edx
+        // cdq implicitly usedef eax and def edx
+        if (op->opCode == Mnemonic_MUL || op->opCode == Mnemonic_IMUL
+                || op->opCode == Mnemonic_CDQ) {
+            updateDependencyGraph(UseDefType_Reg, PhysicalReg_EAX,
+                    LowOpndDefUse_UseDef, Latency_None, op);
+            updateDependencyGraph(UseDefType_Reg, PhysicalReg_EDX,
+                    LowOpndDefUse_Def, Latency_None, op);
+        } else if (op->opCode == Mnemonic_IDIV || op->opCode == Mnemonic_DIV) {
+            updateDependencyGraph(UseDefType_Reg, PhysicalReg_EAX,
+                    LowOpndDefUse_UseDef, Latency_None, op);
+            updateDependencyGraph(UseDefType_Reg, PhysicalReg_EDX,
+                    LowOpndDefUse_UseDef, Latency_None, op);
+        }
+    }
+}
+
+//! \brief Updates dependency information for LowOps with zero operands.
+//! \param op has mnemonic RET
+void Scheduler::updateUseDefInformation(LowOp * op) {
+    assert(op->opCode == Mnemonic_RET);
+    op->instructionLatency = getAtomMnemonicLatency(op->opCode);
+    op->portType = getAtomMnemonicPort(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
+            Latency_None, op);
+    signalEndOfNativeBasicBlock(); // RET ends native basic block
+}
+
+//! \brief Updates dependency information for LowOps with a single immediate
+//! operand.
+//! \param op has mnemonic JMP, Jcc, or CALL
+void Scheduler::updateUseDefInformation_imm(LowOp * op) {
+    assert((op->opCode >= Mnemonic_Jcc && op->opCode <= Mnemonic_JG)
+            || op->opCode == Mnemonic_JMP || op->opCode == Mnemonic_CALL);
+    op->instructionLatency = getAtomMnemonicLatency_imm(op->opCode);
+    op->portType = getAtomMnemonicPort_imm(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    if (op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_JMP)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
+                Latency_None, op);
+    else
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                Latency_None, op);
+    if (isBasicBlockDelimiter(op->opCode))
+        signalEndOfNativeBasicBlock();
+}
+
+//! \brief Updates dependency information for LowOps with a single register operand.
+//! \param op has mnemonic JMP, CALL, PUSH or it is an ALU instruction
+void Scheduler::updateUseDefInformation_reg(LowOpReg * op) {
+    assert(op->opCode == Mnemonic_JMP || op->opCode == Mnemonic_CALL
+            || op->opCode == Mnemonic_PUSH || op->opCode2 == ATOM_NORMAL_ALU);
+    op->instructionLatency = getAtomMnemonicLatency_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_reg(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    if (op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_JMP
+            || op->opCode == Mnemonic_PUSH
+            || isAluOpWithImplicitRegisterUsage(op->opCode))
+        op->opndSrc.defuse = LowOpndDefUse_Use;
+    else // ALU ops with a single operand and no implicit operands use and then define
+        op->opndSrc.defuse = LowOpndDefUse_UseDef;
+    updateDependencyGraph(UseDefType_Reg, op->regOpnd.regNum,
+            op->opndSrc.defuse, Latency_None, op);
+
+    // PUSH will not update control flag
+    if (op->opCode != Mnemonic_PUSH)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
+                Latency_None, op);
+
+    handleImplicitDependenciesEaxEdx(op);
+    handlePushDependencyUpdate(op);
+
+    if (isBasicBlockDelimiter(op->opCode))
+        signalEndOfNativeBasicBlock();
+}
+
+//! \brief Updates dependency information for for LowOps with a single
+//! memory operand.
+//! \param op has mnemonic CALL, FLDCW, FNSTCW, PUSH or it is an ALU
+//! instruction
+void Scheduler::updateUseDefInformation_mem(LowOpMem * op) {
+    assert(op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_FLDCW
+            || op->opCode == Mnemonic_FNSTCW || op->opCode == Mnemonic_PUSH
+            || op->opCode2 == ATOM_NORMAL_ALU);
+    op->instructionLatency = getAtomMnemonicLatency_mem(op->opCode);
+    op->portType = getAtomMnemonicPort_mem(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    if (op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_FLDCW
+            || op->opCode == Mnemonic_PUSH
+            || isAluOpWithImplicitRegisterUsage(op->opCode))
+        op->opndSrc.defuse = LowOpndDefUse_Use;
+    else if (op->opCode == Mnemonic_FNSTCW)
+        op->opndSrc.defuse = LowOpndDefUse_Def;
+    else // ALU ops with a single operand and no implicit operands use and then define
+        op->opndSrc.defuse = LowOpndDefUse_UseDef;
+    updateDependencyGraphForMem(op->memOpnd, op->opndSrc.defuse, op);
+
+    // PUSH will not update control flag
+    if (op->opCode != Mnemonic_PUSH && op->opCode != Mnemonic_FLDCW
+            && op->opCode != Mnemonic_FNSTCW)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
+                Latency_None, op);
+
+    handleImplicitDependenciesEaxEdx(op);
+    handlePushDependencyUpdate(op);
+
+    if (op->opCode == Mnemonic_FLDCW || op->opCode == Mnemonic_FNSTCW)
+        handleFloatDependencyUpdate(op);
+    if (isBasicBlockDelimiter(op->opCode))
+        signalEndOfNativeBasicBlock();
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! immediate to register
+//! \param op must be a MOV variant, a comparison (CMP, TEST, COMISS, COMISD),
+//! or an ALU instruction
+void Scheduler::updateUseDefInformation_imm_to_reg(LowOpImmReg * op) {
+    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
+            || op->opCode2 == ATOM_NORMAL_ALU);
+    bool isMove = isMoveMnemonic(op->opCode);
+    op->instructionLatency = getAtomMnemonicLatency_imm_to_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_imm_to_reg(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    if (usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                Latency_None, op);
+    if (!isMove)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
+                Latency_None, op);
+
+    if (isMove)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (isCompareMnemonic(op->opCode))
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    else // ALU ops
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum,
+            op->opndDest.defuse, Latency_None, op);
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! immediate and memory.
+//! \param op must be a MOV variant, a comparison (CMP, TEST, COMISS, COMISD),
+//! or an ALU instruction
+void Scheduler::updateUseDefInformation_imm_to_mem(LowOpImmMem * op) {
+    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
+            || op->opCode2 == ATOM_NORMAL_ALU);
+    bool isMove = isMoveMnemonic(op->opCode);
+    op->instructionLatency = getAtomMnemonicLatency_imm_to_mem(op->opCode);
+    op->portType = getAtomMnemonicPort_imm_to_mem(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    if (usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                Latency_None, op);
+    if (!isMove)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
+                Latency_None, op);
+
+    if (isMove)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (isCompareMnemonic(op->opCode))
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    else // ALU ops
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! register to register.
+//! \param op must be a MOV variant, a comparison (CMP, TEST, COMISS, COMISD),
+//! an ALU instruction including SSE variants SD and SS, SSE conversion,
+//! or must have mnemonic FUCOM, FUCOMP, CMOVcc, or CDQ.
+void Scheduler::updateUseDefInformation_reg_to_reg(LowOpRegReg * op) {
+    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
+            || isConvertMnemonic(op->opCode) ||op->opCode2 == ATOM_NORMAL_ALU
+            || op->opCode == Mnemonic_FUCOM || op->opCode == Mnemonic_FUCOMP
+            || op->opCode == Mnemonic_CDQ ||
+            (op->opCode >= Mnemonic_CMOVcc && op->opCode < Mnemonic_CMP));
+    bool isMove = isMoveMnemonic(op->opCode);
+    bool isConvert = isConvertMnemonic(op->opCode);
+    op->instructionLatency = getAtomMnemonicLatency_reg_to_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_reg_to_reg(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    if ((op->opCode >= Mnemonic_CMOVcc && op->opCode < Mnemonic_CMP)
+            || usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                Latency_None, op);
+    else if (!isMove && !isConvert && op->opCode != Mnemonic_FUCOM
+            && op->opCode != Mnemonic_FUCOMP && op->opCode != Mnemonic_CDQ)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
+                Latency_None, op);
+
+    if (op->opCode == Mnemonic_CDQ) {
+        // CDQ has no explicit operands but for encoding reasons it is treated like
+        // it does and therefore comes to Scheduler via this interface function.
+        // We can handle it here.
+        assert(op->opndSrc.size == OpndSize_32
+                && op->opndDest.size == OpndSize_32);
+        handleImplicitDependenciesEaxEdx(op);
+        return;
+    }
+
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, op->regSrc.regNum, op->opndSrc.defuse,
+            Latency_None, op);
+
+    if (isMove || isConvert
+            || (op->opCode >= Mnemonic_CMOVcc && op->opCode < Mnemonic_CMP))
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (isCompareMnemonic(op->opCode))
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    else // ALU ops
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum,
+            op->opndDest.defuse, Latency_None, op);
+
+    if (op->opCode == Mnemonic_FUCOM || op->opCode == Mnemonic_FUCOMP)
+        handleFloatDependencyUpdate(op);
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! memory to register.
+//! \param op must be a MOV variant, a comparison (CMP, COMISS, COMISD),
+//! an ALU instruction including SSE variants SD and SS, SSE conversion,
+//! or must have mnemonic LEA
+void Scheduler::updateUseDefInformation_mem_to_reg(LowOpMemReg * op) {
+    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
+            || isConvertMnemonic(op->opCode) || op->opCode2 == ATOM_NORMAL_ALU
+            || op->opCode == Mnemonic_LEA);
+    bool isMove = isMoveMnemonic(op->opCode);
+    bool isConvert = isConvertMnemonic(op->opCode);
+    op->instructionLatency = getAtomMnemonicLatency_mem_to_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_mem_to_reg(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    if (usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                Latency_None, op);
+    if (!isMove && !isConvert && op->opCode != Mnemonic_LEA)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
+                Latency_None, op);
+
+    // Read from memory
+    // However, LEA does not load from memory, and instead it uses the register
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    if (op->opCode != Mnemonic_LEA)
+        updateDependencyGraphForMem(op->memSrc, op->opndSrc.defuse, op);
+    else {
+        updateDependencyGraph(UseDefType_Reg, op->memSrc.m_base.regNum,
+                op->opndSrc.defuse, Latency_Agen_stall, op);
+        if(op->memSrc.hasScale)
+            updateDependencyGraph(UseDefType_Reg, op->memSrc.m_index.regNum,
+                    op->opndSrc.defuse, Latency_Agen_stall, op);
+    }
+
+    if (isMove || isConvert || op->opCode == Mnemonic_LEA)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (isCompareMnemonic(op->opCode))
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    else // ALU ops
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum,
+            op->opndDest.defuse, Latency_None, op);
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! register to memory.
+//! \param op must be a MOV variant, a comparison (CMP), a cmpxchange (CMPXCHG), or an ALU instruction
+void Scheduler::updateUseDefInformation_reg_to_mem(LowOpRegMem * op) {
+    assert(isMoveMnemonic(op->opCode) || isCompareMnemonic(op->opCode)
+            || op->opCode2 == ATOM_NORMAL_ALU || isCmpxchgMnemonic(op->opCode));
+    bool isMove = isMoveMnemonic(op->opCode);
+    bool isCmpxchg = isCmpxchgMnemonic(op->opCode);
+    op->instructionLatency = getAtomMnemonicLatency_reg_to_mem(op->opCode);
+    op->portType = getAtomMnemonicPort_reg_to_mem(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    if (usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                Latency_None, op);
+    if (!isMove)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def,
+                Latency_None, op);
+
+    //CMPXCHG uses and defines EAX
+    if (isCmpxchg == true) {
+        updateDependencyGraph(UseDefType_Reg, PhysicalReg_EAX, LowOpndDefUse_UseDef,
+            Latency_None, op);
+    }
+
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, op->regSrc.regNum, op->opndSrc.defuse,
+            Latency_None, op);
+
+    if (isMove)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (isCompareMnemonic(op->opCode))
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    else // ALU ops
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! floating point stack to memory.
+//! \param op must have mnemonic FSTP, FST, FISTP, or FIST
+void Scheduler::updateUseDefInformation_fp_to_mem(LowOpRegMem * op) {
+    assert(op->opCode == Mnemonic_FSTP || op->opCode == Mnemonic_FST
+            || op->opCode == Mnemonic_FISTP || op->opCode == Mnemonic_FIST);
+    op->instructionLatency = getAtomMnemonicLatency_reg_to_mem(op->opCode);
+    op->portType = getAtomMnemonicPort_reg_to_mem(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    handleFloatDependencyUpdate(op);
+
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, PhysicalReg_ST0, op->opndSrc.defuse,
+            Latency_None, op);
+    op->opndDest.defuse = LowOpndDefUse_Def;
+    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! memory to floating point stack.
+//! \param op must have mnemonic FLD or FILD, or must be an x87 ALU op
+void Scheduler::updateUseDefInformation_mem_to_fp(LowOpMemReg * op) {
+    assert(op->opCode == Mnemonic_FLD || op->opCode == Mnemonic_FILD
+            || op->opCode2 == ATOM_NORMAL_ALU);
+    op->instructionLatency = getAtomMnemonicLatency_mem_to_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_mem_to_reg(op->opCode);
+    assert(op->instructionLatency != INVN);
+    assert(op->portType != INVP);
+
+    handleFloatDependencyUpdate(op);
+
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    updateDependencyGraphForMem(op->memSrc, op->opndSrc.defuse, op);
+    if (op->opCode == Mnemonic_FLD || op->opCode == Mnemonic_FILD)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else // x87 ALU ops
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    updateDependencyGraph(UseDefType_Reg, PhysicalReg_ST0, op->opndDest.defuse,
+            Latency_None, op);
+}
+
+//! \brief Generates IA native code for given LowOp
+//! \details This method takes a LowOp and generates x86 instructions into the
+//! code stream by making calls to the encoder.
+//! \param op to be encoded and placed into code stream.
+void Scheduler::generateAssembly(LowOp * op) {
+    if(IS_ANY_JIT_ERROR_SET())
+        return;
+    if (op->numOperands == 0) {
+        stream = encoder_return(stream);
+    } else if (op->numOperands == 1) {
+        if (op->opndSrc.type == LowOpndType_Label) {
+            bool unknown;
+            OpndSize size;
+            int imm;
+            if (op->opCode == Mnemonic_JMP)
+                imm = getRelativeOffset(((LowOpLabel*) op)->labelOpnd.label,
+                        ((LowOpLabel*) op)->labelOpnd.isLocal, JmpCall_uncond,
+                        &unknown, &size);
+            else if (op->opCode == Mnemonic_CALL)
+                imm = getRelativeOffset(((LowOpLabel*) op)->labelOpnd.label,
+                        ((LowOpLabel*) op)->labelOpnd.isLocal, JmpCall_call,
+                        &unknown, &size);
+            else
+                imm = getRelativeOffset(((LowOpLabel*) op)->labelOpnd.label,
+                        ((LowOpLabel*) op)->labelOpnd.isLocal, JmpCall_cond,
+                        &unknown, &size);
+            op->opndSrc.size = size;
+            stream = encoder_imm(op->opCode, op->opndSrc.size, imm, stream);
+        } else if (op->opndSrc.type == LowOpndType_BlockId) {
+            LowOpBlock * blockOp = reinterpret_cast<LowOpBlock *>(op);
+
+            // If the immediate needs aligned, then do a test dump to see
+            // if the immediate will cross the 16-byte boundary. If we plan
+            // on aligning immediate, we expect that its size will be 32-bit
+            if (blockOp->blockIdOpnd.immediateNeedsAligned) {
+                // Dump to stream but don't update stream pointer
+                char * newStream = encoder_imm(blockOp->opCode, OpndSize_32, 0,
+                        stream);
+
+                // Immediates are assumed to be at end of instruction, so just check
+                // that the updated stream pointer does not break up the 32-bit immediate
+                unsigned int bytesCrossing =
+                        reinterpret_cast<unsigned int>(newStream) % 16;
+                bool needNops =
+                        (bytesCrossing > OpndSize_Null
+                                && bytesCrossing < OpndSize_32) ? true : false;
+
+                if (needNops)
+                    stream = encoder_nops(OpndSize_32 - bytesCrossing, stream);
+            }
+
+            bool unknown;
+            OpndSize actualSize = OpndSize_Null;
+            int imm;
+            if (blockOp->opCode == Mnemonic_JMP)
+                imm = getRelativeNCG(blockOp->blockIdOpnd.value,
+                        JmpCall_uncond, &unknown, &actualSize);
+            else
+                imm = getRelativeNCG(blockOp->blockIdOpnd.value,
+                        JmpCall_cond, &unknown, &actualSize);
+
+            // When we need to align, we expect that the size of the immediate is
+            // 32-bit so we make sure of that now.
+            blockOp->opndSrc.size =
+                    blockOp->blockIdOpnd.immediateNeedsAligned ?
+                            OpndSize_32 : actualSize;
+
+            stream = encoder_imm(blockOp->opCode, blockOp->opndSrc.size, imm, stream);
+        } else if (op->opndSrc.type == LowOpndType_Imm) {
+            stream = encoder_imm(op->opCode, op->opndSrc.size,
+                    ((LowOpImm*) op)->immOpnd.value, stream);
+        } else if (op->opndSrc.type == LowOpndType_Reg) {
+            stream = encoder_reg(op->opCode, op->opndSrc.size,
+                    ((LowOpReg*) op)->regOpnd.regNum,
+                    ((LowOpReg*) op)->regOpnd.isPhysical,
+                    ((LowOpReg*) op)->regOpnd.regType, stream);
+        } else { // Corresponds to lower_mem
+            stream = encoder_mem(op->opCode, op->opndSrc.size,
+                    ((LowOpMem*) op)->memOpnd.m_disp.value,
+                    ((LowOpMem*) op)->memOpnd.m_base.regNum,
+                    ((LowOpMem*) op)->memOpnd.m_base.isPhysical, stream);
+        }
+    }
+    // Number of operands is 2
+    // Handles LowOps coming from  lower_imm_reg, lower_imm_mem,
+    // lower_reg_mem, lower_mem_reg, lower_mem_scale_reg,
+    // lower_reg_mem_scale, lower_reg_reg, lower_fp_mem, and lower_mem_fp
+    else if (op->opndDest.type == LowOpndType_Reg
+            && op->opndSrc.type == LowOpndType_Imm) {
+        stream = encoder_imm_reg_diff_sizes(op->opCode, op->opndSrc.size,
+                ((LowOpImmReg*) op)->immSrc.value,
+                op->opndDest.size,
+                ((LowOpImmReg*) op)->regDest.regNum,
+                ((LowOpImmReg*) op)->regDest.isPhysical,
+                ((LowOpImmReg*) op)->regDest.regType, stream);
+    } else if (op->opndDest.type == LowOpndType_Reg
+            && op->opndSrc.type == LowOpndType_Chain) {
+        // The immediates used for chaining must be aligned within a 16-byte
+        // region so we need to ensure that now.
+
+        // First, dump to code stream but do not update stream pointer
+        char * newStream = encoder_imm_reg(op->opCode, op->opndDest.size,
+                ((LowOpImmReg*) op)->immSrc.value,
+                ((LowOpImmReg*) op)->regDest.regNum,
+                ((LowOpImmReg*) op)->regDest.isPhysical,
+                ((LowOpImmReg*) op)->regDest.regType, stream);
+
+        // Immediates are assumed to be at end of instruction, so just check
+        // that the updated stream pointer does not break up the immediate
+        unsigned int bytesCrossing =
+                reinterpret_cast<unsigned int>(newStream) % 16;
+        bool needNops =
+                (bytesCrossing > OpndSize_Null
+                        && bytesCrossing < op->opndDest.size) ? true : false;
+
+        if (needNops)
+            stream = encoder_nops(op->opndDest.size - bytesCrossing, stream);
+
+        // Now we are ready to do the actual encoding
+        insertChainingWorklist(((LowOpImmReg*) op)->immSrc.value, stream);
+        stream = encoder_imm_reg(op->opCode, op->opndDest.size,
+                ((LowOpImmReg*) op)->immSrc.value,
+                ((LowOpImmReg*) op)->regDest.regNum,
+                ((LowOpImmReg*) op)->regDest.isPhysical,
+                ((LowOpImmReg*) op)->regDest.regType, stream);
+    } else if (op->opndDest.type == LowOpndType_Mem
+            && op->opndSrc.type == LowOpndType_Imm) {
+        stream = encoder_imm_mem(op->opCode, op->opndDest.size,
+                ((LowOpImmMem*) op)->immSrc.value,
+                ((LowOpImmMem*) op)->memDest.m_disp.value,
+                ((LowOpImmMem*) op)->memDest.m_base.regNum,
+                ((LowOpImmMem*) op)->memDest.m_base.isPhysical, stream);
+    } else if (op->opndDest.type == LowOpndType_Mem
+            && op->opndSrc.type == LowOpndType_Chain) {
+        // The immediates used for chaining must be aligned within a 16-byte
+        // region so we need to ensure that now.
+
+        // First, dump to code stream but do not update stream pointer
+        char * newStream = encoder_imm_mem(op->opCode, op->opndDest.size,
+                ((LowOpImmMem*) op)->immSrc.value,
+                ((LowOpImmMem*) op)->memDest.m_disp.value,
+                ((LowOpImmMem*) op)->memDest.m_base.regNum,
+                ((LowOpImmMem*) op)->memDest.m_base.isPhysical, stream);
+
+        // Immediates are assumed to be at end of instruction, so just check
+        // that the updated stream pointer does not break up the immediate
+        unsigned int bytesCrossing =
+                reinterpret_cast<unsigned int>(newStream) % 16;
+        bool needNops =
+                (bytesCrossing > OpndSize_Null
+                        && bytesCrossing < op->opndDest.size) ? true : false;
+
+        if (needNops)
+            stream = encoder_nops(op->opndDest.size - bytesCrossing, stream);
+
+        // Now we are ready to do the actual encoding
+        insertChainingWorklist(((LowOpImmMem*) op)->immSrc.value, stream);
+        stream = encoder_imm_mem(op->opCode, op->opndDest.size,
+                ((LowOpImmMem*) op)->immSrc.value,
+                ((LowOpImmMem*) op)->memDest.m_disp.value,
+                ((LowOpImmMem*) op)->memDest.m_base.regNum,
+                ((LowOpImmMem*) op)->memDest.m_base.isPhysical, stream);
+    } else if (op->opndDest.type == LowOpndType_Reg
+            && op->opndSrc.type == LowOpndType_Reg) {
+        if (op->opCode == Mnemonic_FUCOMP || op->opCode == Mnemonic_FUCOM) {
+            stream = encoder_compare_fp_stack(op->opCode == Mnemonic_FUCOMP,
+                    ((LowOpRegReg*) op)->regSrc.regNum
+                            - ((LowOpRegReg*) op)->regDest.regNum,
+                    op->opndDest.size == OpndSize_64, stream);
+        } else {
+            stream = encoder_reg_reg_diff_sizes(op->opCode, op->opndSrc.size,
+                    ((LowOpRegReg*) op)->regSrc.regNum,
+                    ((LowOpRegReg*) op)->regSrc.isPhysical,
+                    op->opndDest.size,
+                    ((LowOpRegReg*) op)->regDest.regNum,
+                    ((LowOpRegReg*) op)->regDest.isPhysical,
+                    ((LowOpRegReg*) op)->regDest.regType, stream);
+        }
+    } else if (op->opndDest.type == LowOpndType_Reg
+            && op->opndSrc.type == LowOpndType_Mem) {
+        // Corresponds to lower_mem_reg, lower_mem_fp, or lower_mem_scale_reg
+        LowOpMemReg * regmem_op = (LowOpMemReg*) op;
+
+        // Constant initialization for 64 bit data requires saving stream address location
+        struct ConstInfo* tmpPtr;
+        tmpPtr = regmem_op->constLink;
+        if (tmpPtr != NULL && tmpPtr->constAddr == NULL){
+            // save address of instruction post scheduling
+            tmpPtr->streamAddr = stream;
+        }
+
+        if (regmem_op->regDest.regType == LowOpndRegType_fs)
+            stream = encoder_mem_fp(regmem_op->opCode, regmem_op->opndSrc.size,
+                    regmem_op->memSrc.m_disp.value,
+                    regmem_op->memSrc.m_base.regNum,
+                    regmem_op->memSrc.m_base.isPhysical,
+                    regmem_op->regDest.regNum - PhysicalReg_ST0, stream);
+        else if (regmem_op->memSrc.hasScale)
+            stream = encoder_mem_disp_scale_to_reg_diff_sizes(regmem_op->opCode,
+                    regmem_op->opndSrc.size, regmem_op->memSrc.m_base.regNum,
+                    regmem_op->memSrc.m_base.isPhysical,
+                    regmem_op->memSrc.m_disp.value,
+                    regmem_op->memSrc.m_index.regNum,
+                    regmem_op->memSrc.m_index.isPhysical,
+                    regmem_op->memSrc.m_scale.value, regmem_op->opndDest.size,
+                    regmem_op->regDest.regNum, regmem_op->regDest.isPhysical,
+                    regmem_op->regDest.regType, stream);
+        else
+            stream = encoder_mem_to_reg_diff_sizes(regmem_op->opCode,
+                    regmem_op->opndSrc.size, regmem_op->memSrc.m_disp.value,
+                    regmem_op->memSrc.m_base.regNum,
+                    regmem_op->memSrc.m_base.isPhysical,
+                    regmem_op->opndDest.size, regmem_op->regDest.regNum,
+                    regmem_op->regDest.isPhysical, regmem_op->regDest.regType,
+                    stream);
+    } else if (op->opndDest.type == LowOpndType_Mem
+            && op->opndSrc.type == LowOpndType_Reg) {
+        // Corresponds to lower_reg_mem, lower_fp_mem, or lower_reg_mem_scale
+        LowOpRegMem * memreg_op = (LowOpRegMem*) op;
+        if (memreg_op->regSrc.regType == LowOpndRegType_fs)
+            stream = encoder_fp_mem(memreg_op->opCode, memreg_op->opndDest.size,
+                    memreg_op->regSrc.regNum - PhysicalReg_ST0,
+                    memreg_op->memDest.m_disp.value,
+                    memreg_op->memDest.m_base.regNum,
+                    memreg_op->memDest.m_base.isPhysical, stream);
+        else if (memreg_op->memDest.hasScale)
+            stream = encoder_reg_mem_disp_scale(memreg_op->opCode,
+                    memreg_op->opndDest.size, memreg_op->regSrc.regNum,
+                    memreg_op->regSrc.isPhysical,
+                    memreg_op->memDest.m_base.regNum,
+                    memreg_op->memDest.m_base.isPhysical,
+                    memreg_op->memDest.m_disp.value,
+                    memreg_op->memDest.m_index.regNum,
+                    memreg_op->memDest.m_index.isPhysical,
+                    memreg_op->memDest.m_scale.value,
+                    memreg_op->regSrc.regType, stream);
+        else
+            stream = encoder_reg_mem(op->opCode, op->opndDest.size,
+                    memreg_op->regSrc.regNum, memreg_op->regSrc.isPhysical,
+                    memreg_op->memDest.m_disp.value,
+                    memreg_op->memDest.m_base.regNum,
+                    memreg_op->memDest.m_base.isPhysical,
+                    memreg_op->regSrc.regType, stream);
+    }
+    if(gDvmJit.codeCacheByteUsed + (stream - streamStart) +
+       CODE_CACHE_PADDING > gDvmJit.codeCacheSize) {
+        ALOGI("JIT_INFO: Code cache full after Scheduler::generateAssembly (trace uses %uB)", (stream - streamStart));
+        SET_JIT_ERROR(kJitErrorCodeCacheFull);
+        gDvmJit.codeCacheFull = true;
+    }
+}
+
+//! \brief Figures out which LowOps are ready after an instruction at chosenIdx
+//! is scheduled.
+//! \details It also updates the readyTime of every LowOp waiting to be scheduled.
+//! \param chosenIdx is the index of the chosen instruction for scheduling
+//! \param scheduledOps is an input list of scheduled LowOps
+//! \param readyOps is an output list of LowOps that are ready
+void Scheduler::updateReadyOps(int chosenIdx, BitVector * scheduledOps,
+        BitVector * readyOps) {
+    // Go through each successor LIR that depends on selected LIR
+    for (unsigned int k = 0; k < dependencyAssociation[queuedLIREntries[chosenIdx]].successorDependencies.size(); ++k) {
+        int dst = dependencyAssociation[queuedLIREntries[chosenIdx]].successorDependencies[k].lowopSlotId;
+        bool isReady = true;
+        int readyTime = -1;
+        // If all predecessors are scheduled, insert into ready queue
+        for (unsigned int k2 = 0; k2 < dependencyAssociation[queuedLIREntries[dst]].predecessorDependencies.size(); ++k2) {
+            int src = dependencyAssociation[queuedLIREntries[dst]].predecessorDependencies[k2].lowopSlotId;
+            if (dvmIsBitSet(scheduledOps, src) == false) {
+                // If one of parents hasn't been scheduled, then current instruction is not ready
+                isReady = false;
+                break;
+            }
+
+            // If our candidate is a RAW, then we must wait until parent finishes
+            // executing. However, if we have a WAW, WAR, or RAR, then we can issue
+            // next cycle.
+            int readyDelay =
+                    (dependencyAssociation[queuedLIREntries[dst]].predecessorDependencies[k2].dataHazard
+                            == Dependency_RAW) ?
+                            queuedLIREntries[src]->instructionLatency : 1;
+
+            // Candidate ready time is the sum of the scheduled time of parent,
+            // the latency of parent, and the weight of edge between parent
+            // and self
+            int candidateReadyTime = queuedLIREntries[src]->scheduledTime + readyDelay
+                    + dependencyAssociation[queuedLIREntries[dst]].predecessorDependencies[k2].edgeLatency;
+
+            if (readyTime < candidateReadyTime) {
+                // This is ready after ALL predecessors have finished executing
+                readyTime = candidateReadyTime;
+            }
+        }
+        if (isReady) {
+            dvmSetBit(readyOps, dst);
+            queuedLIREntries[dst]->readyTime = readyTime;
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            ALOGD("update readyTime of slot %d: %d\n", dst, readyTime);
+#endif
+        }
+    }
+}
+
+//! This method constructs the inverse topological sort of the
+//! dependency graph of current basic block (queuedLIREntries)
+//! \param nodeId Index of LowOp in queuedLIREntries
+//! \param visitedList List with same indexing as queuedLIREntries
+//! that keeps track of LowOps that have already been visited
+//! \param inverseTopologicalOrder A list that will eventually
+//! hold the inverse topological order of the dependency graph.
+//! Inverse order means that parent nodes come later in the list
+//! compared to its children.
+void Scheduler::visitNodeTopologicalSort(unsigned int nodeId, int * visitedList,
+        NativeBasicBlock & inverseTopologicalOrder) {
+    // If it has been visited already, there's no need to do anything
+    if (visitedList[nodeId] == 0) {
+        assert(queuedLIREntries[nodeId]->slotId == nodeId);
+        // Mark as visited
+        visitedList[nodeId]++;
+        for (unsigned int child = 0;
+                child < dependencyAssociation[queuedLIREntries[nodeId]].successorDependencies.size();
+                ++child) {
+            // visit children
+            visitNodeTopologicalSort(
+                    dependencyAssociation[queuedLIREntries[nodeId]].successorDependencies[child].lowopSlotId,
+                    visitedList, inverseTopologicalOrder);
+        }
+        // Since all children have been visited, can now add node to list
+        inverseTopologicalOrder.push_back(queuedLIREntries[nodeId]);
+    }
+}
+
+//! \brief Finds longest path latency for every node in every tree in the
+//! dependency graph.
+//! \details This updates the longest path field of every LowOp in the current
+//! native basic block.
+//! \see queuedLIREntries
+void Scheduler::findLongestPath() {
+    NativeBasicBlock inverseTopologicalOrder;
+
+    // Initialize visited list to 0 (false) for all nodes
+    int visitedList[queuedLIREntries.size()];
+    memset(visitedList, 0, queuedLIREntries.size() * sizeof(int));
+
+    // Determine topological order.
+    for (unsigned int node = 0; node < queuedLIREntries.size(); ++node) {
+        visitNodeTopologicalSort(node, visitedList, inverseTopologicalOrder);
+    }
+
+    assert(queuedLIREntries.size() == inverseTopologicalOrder.size());
+
+    // for each node in inverse topological order
+    for(unsigned int vindex = 0; vindex < inverseTopologicalOrder.size(); ++vindex) {
+        int bestLongestPath = 0;
+        // Go through each child find the best longest path.
+        // Since we are doing this in inverse topological order,
+        // we know the longest path for all children has already
+        // been updated.
+        for(unsigned int windex = 0; windex < dependencyAssociation[inverseTopologicalOrder[vindex]].successorDependencies.size();
+                ++windex) {
+            int successorSlotId = dependencyAssociation[inverseTopologicalOrder[vindex]].successorDependencies[windex].lowopSlotId;
+            int edgeLatency = dependencyAssociation[inverseTopologicalOrder[vindex]].successorDependencies[windex].edgeLatency;
+            if (queuedLIREntries[successorSlotId]->longestPath > bestLongestPath) {
+                bestLongestPath = queuedLIREntries[successorSlotId]->longestPath + edgeLatency;
+            }
+        }
+        // Longest path to self is sum of best longest path to children plus
+        // instruction latency of self
+        inverseTopologicalOrder[vindex]->longestPath = inverseTopologicalOrder[vindex]->instructionLatency
+                + bestLongestPath;
+    }
+}
+
+//! \brief Reorders basic block to minimize block latency and make use of both
+//! Atom issue ports.
+//! \details The result of scheduling is stored in scheduledLIREntries. Additionally,
+//! each LowOp individually stores its scheduled time (logical based on index ordering).
+//!
+//! Algorithm details:
+//! - select one LIR from readyQueue with 2 criteria:
+//!   -# smallest readyTime
+//!   -# on critical path
+//! - A pair of LowOps can be issued at the same time slot if they use different issue ports.
+//! - A LowOp can be issued if
+//!   -# all pending ops can commit ahead of this LowOp (restriction reflected in readyTime)
+//!   -# it is ready
+//! - At end, currentTime is advanced to readyTime of the selected LowOps
+//! - If any LIR has jmp, jcc, call, or ret mnemonic, it must be scheduled last
+//!
+//! \see scheduledLIREntries
+//! \post Scheduler::scheduledLIREntries is same size as Scheduler::queuedLIREntries
+//! \post If last LIR in Scheduler::queuedLIREntries is a jump, call, or return, it must
+//! also be the last LIR in Scheduler::scheduledLIREntries
+void Scheduler::schedule() {
+    // Declare data structures for scheduling
+    unsigned int candidateArray[queuedLIREntries.size()]; // ready candidates for scheduling
+    unsigned int num_candidates = 0 /*index for candidateArray*/, numScheduled = 0, lirID;
+    int currentTime = 0;
+
+    // LIRs ready for scheduling
+    BitVector * readyOps = dvmCompilerAllocBitVector(queuedLIREntries.size(), false);
+    dvmClearAllBits(readyOps);
+    // LIRs that have been scheduled
+    BitVector * scheduledOps = dvmCompilerAllocBitVector(queuedLIREntries.size(), false);
+    dvmClearAllBits(scheduledOps);
+
+    // Set up the live out dependencies
+    setupLiveOutDependencies();
+
+    // Predecessor dependencies have already been initialized in the dependency graph building.
+    // Now, initialize successor dependencies to complete dependency graph.
+    for (lirID = 0; lirID < queuedLIREntries.size(); ++lirID) {
+        for (unsigned int k2 = 0; k2 < dependencyAssociation[queuedLIREntries[lirID]].predecessorDependencies.size(); ++k2) {
+            int src = dependencyAssociation[queuedLIREntries[lirID]].predecessorDependencies[k2].lowopSlotId;
+            DependencyInformation ds;
+            ds.lowopSlotId = lirID;
+
+            // Since edges are directional, no need to invert weight.
+            ds.edgeLatency = dependencyAssociation[queuedLIREntries[lirID]].predecessorDependencies[k2].edgeLatency;
+            dependencyAssociation[queuedLIREntries[src]].successorDependencies.push_back(ds);
+        }
+    }
+
+    // Find longest path from each LIR to the leaves of the dependency trees
+    findLongestPath();
+
+    // When a LowOp is ready, it means all its predecessors are scheduled
+    // and the readyTime of this LowOp has been set already.
+    for (lirID = 0; lirID < queuedLIREntries.size(); lirID++) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        ALOGD("-- slot %d: latency %d port type %d\n", lirID, queuedLIREntries[lirID]->instructionLatency,
+                queuedLIREntries[lirID]->portType);
+#endif
+        if (dependencyAssociation[queuedLIREntries[lirID]].predecessorDependencies.size() == 0) {
+            dvmSetBit(readyOps, lirID);
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            ALOGD("slot %d is ready\n", lirID);
+#endif
+            queuedLIREntries[lirID]->readyTime = 0;
+        }
+    }
+
+    // Schedule each of LIRs in the basic block
+    while (numScheduled < queuedLIREntries.size()) {
+        // Set chosen indices to BB size since no LIR will have
+        // this id.
+        unsigned int chosenIdx1 = queuedLIREntries.size();
+        unsigned int chosenIdx2 = queuedLIREntries.size();
+
+        // Reset number of picked candidates
+        num_candidates = 0;
+
+        // Select candidates that are ready (readyTime <= currentTime)
+        for (lirID = 0; lirID < queuedLIREntries.size(); lirID++) {
+            if (dvmIsBitSet(readyOps, lirID)
+                    && queuedLIREntries[lirID]->readyTime <= currentTime
+                    && (!isBasicBlockDelimiter(queuedLIREntries[lirID]->opCode)))
+                candidateArray[num_candidates++] = lirID;
+        }
+
+        // If no candidate is ready to be issued, just pick the one with
+        // the smallest readyTime
+        if (num_candidates == 0) {
+
+            // First, find the smallest ready time out of instructions that are
+            // ready.
+            for (lirID = 0; lirID < queuedLIREntries.size(); lirID++) {
+                if (dvmIsBitSet(readyOps, lirID)
+                        && (!isBasicBlockDelimiter(queuedLIREntries[lirID]->opCode))) {
+                    if (chosenIdx1 == queuedLIREntries.size()
+                            || queuedLIREntries[lirID]->readyTime
+                                    < queuedLIREntries[chosenIdx1]->readyTime) {
+                        chosenIdx1 = lirID;
+                        // Update current time with smallest ready time
+                        currentTime = queuedLIREntries[lirID]->readyTime;
+                    }
+                }
+            }
+
+            // Select any other candidates that also are ready at the same time.
+            for (lirID = 0; lirID < queuedLIREntries.size(); lirID++) {
+                if (dvmIsBitSet(readyOps, lirID)
+                        && (!isBasicBlockDelimiter(queuedLIREntries[lirID]->opCode))
+                        && queuedLIREntries[lirID]->readyTime <= currentTime) {
+                    candidateArray[num_candidates++] = lirID;
+                }
+            }
+        }
+
+        // This is the last gate for picking a candidate.
+        // By this point if still we don't have a candidate, it means that
+        // only the sync point instruction remains.
+        if (num_candidates == 0)
+            candidateArray[num_candidates++] = queuedLIREntries.size() - 1;
+
+        // Reinitialize chosenIdx1 since it was used earlier for
+        // finding smallest ready time
+        chosenIdx1 = queuedLIREntries.size();
+
+        // Pick candidate that is on the critical path
+        for (unsigned int i = 0; i < num_candidates; i++) {
+            lirID = candidateArray[i];
+            // Always try to pick a candidate. Once we've picked one,
+            // then we can start looking for another with a longer
+            // critical path
+            if (chosenIdx1 == queuedLIREntries.size()
+                    || queuedLIREntries[lirID]->longestPath
+                            > queuedLIREntries[chosenIdx1]->longestPath) {
+                chosenIdx1 = lirID;
+            }
+        }
+
+        // By the time we get to this point, we better have picked
+        // an instruction to schedule OR ELSE ...
+        assert (chosenIdx1 < queuedLIREntries.size());
+
+        // Pick 2 candidates if possible.
+        // If current candidate must issue on both ports, we cannot pick another
+        if (queuedLIREntries[chosenIdx1]->portType == BOTH_PORTS)
+            num_candidates = 0;
+
+        // The only way we will go through this logic is if chosen instruction
+        // doesn't issue on both ports
+        for (unsigned int i = 0; i < num_candidates; i++) {
+            lirID = candidateArray[i];
+            if (lirID == chosenIdx1)
+                continue; // Should skip the one already chosen
+
+            // Check for port conflict
+            if (queuedLIREntries[lirID]->portType == BOTH_PORTS)
+                continue; // Look for another one that doesn't issue on both ports
+            if (queuedLIREntries[chosenIdx1]->portType == EITHER_PORT
+                    || queuedLIREntries[lirID]->portType == EITHER_PORT
+                    || (queuedLIREntries[chosenIdx1]->portType == PORT0
+                            && queuedLIREntries[lirID]->portType == PORT1)
+                    || (queuedLIREntries[chosenIdx1]->portType == PORT1
+                            && queuedLIREntries[lirID]->portType == PORT0)) {
+                // Looks like we found one that doesn't conflict on ports
+                // However, still try to find one on critical path
+                if (chosenIdx2 == queuedLIREntries.size()
+                        || queuedLIREntries[lirID]->longestPath
+                                > queuedLIREntries[chosenIdx2]->longestPath) {
+                    chosenIdx2 = lirID;
+                }
+            }
+        }
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        ALOGD("pick ready instructions at slots %d %d\n", chosenIdx1, chosenIdx2);
+#endif
+
+        scheduledLIREntries.push_back(queuedLIREntries[chosenIdx1]);
+        dvmSetBit(scheduledOps, chosenIdx1);
+        dvmClearBit(readyOps, chosenIdx1);
+        queuedLIREntries[chosenIdx1]->scheduledTime = currentTime;
+        numScheduled++;
+
+        if (chosenIdx2 < queuedLIREntries.size()) {
+            scheduledLIREntries.push_back(queuedLIREntries[chosenIdx2]);
+            dvmSetBit(scheduledOps, chosenIdx2);
+            dvmClearBit(readyOps, chosenIdx2);
+            queuedLIREntries[chosenIdx2]->scheduledTime = currentTime;
+            numScheduled++;
+        }
+
+        // Since we have scheduled instructions in this cycle, we should
+        // update the ready queue now to find new instructions whose
+        // dependencies have been satisfied
+        updateReadyOps(chosenIdx1, scheduledOps, readyOps);
+        if (chosenIdx2 < queuedLIREntries.size())
+            updateReadyOps(chosenIdx2, scheduledOps, readyOps);
+
+        // Advance time to next cycle
+        currentTime++;
+    }
+
+    // Make sure that original and scheduled basic blocks are same size
+    if (scheduledLIREntries.size() != queuedLIREntries.size()) {
+        ALOGI("JIT_INFO: (Atom Scheduler) Original basic block is not same \
+                size as the scheduled basic block");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return;
+    }
+
+    // Make sure that basic block delimiter mnemonic is always last one in
+    // scheduled basic block
+    if (isBasicBlockDelimiter(queuedLIREntries.back()->opCode)
+            && !isBasicBlockDelimiter(scheduledLIREntries.back()->opCode)) {
+        ALOGI("JIT_INFO: (Atom Scheduler) Sync point should be the last \
+                scheduled instruction.");
+        SET_JIT_ERROR(kJitErrorInsScheduling);
+        return;
+    }
+}
+
+//! \brief Called to signal the scheduler that the native basic block it has
+//! been building is finished.
+//! \details This method should be called from other modules to signal that the
+//! native basic block the Scheduler has been building is finished. This has
+//! side effects because it starts the scheduling process using already created
+//! dependency graphs and then updates the code stream with the scheduled
+//! instructions.
+//! \warning Jumps to immediate must signal end of native basic block for target.
+//! If the target has a label, then this is not a problem. But if jumping to an
+//! address without label, this method must be called before building dependency
+//! graph for target basic block.
+void Scheduler::signalEndOfNativeBasicBlock() {
+    if(queuedLIREntries.empty())
+            return; // No need to do any work
+
+#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
+    std::ostringstream dependGraphFileName;
+    char * streamStartBasicBlock = stream;
+    dependGraphFileName << "depengraph_" << gDvm.threadList[0].systemTid << "_"
+            << std::hex << (int)streamStartBasicBlock;
+#endif
+
+    printStatistics(true /*prescheduling*/);
+    schedule();
+    printStatistics(false /*prescheduling*/);
+
+    for(unsigned int k = 0; k < scheduledLIREntries.size(); ++k) {
+        generateAssembly(scheduledLIREntries[k]);
+    }
+
+#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
+    printDependencyGraph("/data/local/tmp/", dependGraphFileName.str(),
+            streamStartBasicBlock, true, true, true, true, true);
+#endif
+
+    // Clear all scheduler data structures
+    this->reset();
+}
+
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+//! \brief Transforms LowOpndType enum to string
+//! \see LowOpndType
+inline const char * operandTypeToString(LowOpndType type) {
+    switch (type) {
+    case LowOpndType_Imm:
+        return "Imm";
+    case LowOpndType_Reg:
+        return "Reg";
+    case LowOpndType_Mem:
+        return "Mem";
+    case LowOpndType_Label:
+        return "Label";
+    case LowOpndType_BlockId:
+        return "BlockId";
+    case LowOpndType_Chain:
+        return "Chain";
+    }
+    return "-";
+}
+#endif
+
+//! \brief Returns a scaled distance between two basic blocks.
+//! \details Computes the Hamming distance between two basic blocks and then scales
+//! result by block size and turns it into percentage. block1 and block2 must
+//! have same size.
+//! \retval scaled Hamming distance
+inline double Scheduler::basicBlockEditDistance(const NativeBasicBlock & block1,
+        const NativeBasicBlock & block2) {
+#ifndef DISABLE_ATOM_SCHEDULING_STATISTICS
+    int distance = 0;
+    assert(block1.size() == block2.size());
+    for(unsigned int i = 0; i < block1.size(); ++i) {
+        if(block1[i] != block2[i]) {
+            distance += 1;
+        }
+    }
+    return (distance * 100.0) / block1.size();
+#else
+    return 0.0;
+#endif
+}
+
+//! \brief Prints Atom Instruction Scheduling statistics.
+//! Details prints block size and basic block difference.
+//! \todo Comparing basic block latencies pre and post scheduling is a useful
+//! statistic.
+//! \param prescheduling is used to indicate whether the statistics are requested
+//! before the scheduling
+void Scheduler::printStatistics(bool prescheduling) {
+#ifndef DISABLE_ATOM_SCHEDULING_STATISTICS
+    const char * message_tag =
+            prescheduling ?
+                    "Atom Sched Stats: Pre-schedule:" :
+                    "Atom Sched Stats: Post-schedule:";
+    NativeBasicBlock * lowOpList;
+    if (prescheduling)
+        lowOpList = &queuedLIREntries;
+    else
+        lowOpList = &scheduledLIREntries;
+
+    ALOGD("%s The block size is %d\n", message_tag, lowOpList->size());
+    if (!prescheduling) {
+        ALOGD("%s Difference in basic blocks after scheduling is %5.2f%%\n",
+                message_tag, basicBlockEditDistance(queuedLIREntries, scheduledLIREntries));
+    }
+#endif
+}
+
+//! \brief Prints dependency graph in dot format
+//! \details Creates dot files in /data/local/tmp with every basic block
+//! that has been scheduled.
+//! \param directoryPath the path to the directory
+//! \param dgfilename Name to use for dot file created
+//! \param startStream The pointer to the start of code cache stream where
+//! the basic block has been encoded
+//! \param printScheduledTime Allow printing of scheduled time of each LIR
+//! \param printIssuePort Allow printing of issue port of each LIR
+//! \param printInstructionLatency Allow printing of latency of each LIR
+//! \param printCriticalPath Allows printing of longest path latency for each
+//! LIR.
+//! \param printOriginalOrder Appends to front of instruction the original
+//! instruction order before scheduling.
+void Scheduler::printDependencyGraph(const char * directoryPath,
+        const std::string &dgfilename, const char * startStream,
+        bool printScheduledTime, bool printIssuePort,
+        bool printInstructionLatency, bool printCriticalPath,
+        bool printOriginalOrder) {
+#ifndef DISABLE_DEPENDENGY_GRAPH_DEBUG
+    std::ofstream depengraphfile;
+    const unsigned int maxInstSize = 30;
+    char decodedInst[maxInstSize];
+
+    // Create dot file
+    std::string completeFSPath = directoryPath;
+    completeFSPath.append(dgfilename);
+    completeFSPath.append(".dot");
+    ALOGD("Dumping dependency graph to %s", completeFSPath.c_str());
+    depengraphfile.open(completeFSPath.c_str(), std::ios::out);
+
+    // A little error handling
+    if (depengraphfile.fail()) {
+        ALOGD("Encountered error when trying to open the file %s",
+                completeFSPath.c_str());
+        depengraphfile.close();
+        return;
+    }
+
+    // Print header
+    depengraphfile << "digraph BB" << dgfilename.c_str() << " {" << std::endl;
+    depengraphfile << "forcelabels = true" << std::endl;
+
+    // Print nodes
+    for (unsigned int i = 0; i < scheduledLIREntries.size(); ++i) {
+        startStream = decoder_disassemble_instr(const_cast<char *>(startStream),
+                decodedInst, maxInstSize);
+        // Add node with the x86 instruction as label
+        depengraphfile << "LIR" << scheduledLIREntries[i]->slotId
+                << " [shape=record, label=\"{";
+        if (printOriginalOrder)
+            depengraphfile << scheduledLIREntries[i]->slotId << ": ";
+        depengraphfile << decodedInst;
+        if (printScheduledTime) // Conditional print of time instruction was scheduled
+            depengraphfile << " | ScheduledTime:"
+                    << scheduledLIREntries[i]->scheduledTime;
+        if (printIssuePort) // Conditional print of issue port of instruction
+            // I promise the cast is safe
+            depengraphfile << " | IssuePort:"
+                    << getIssuePort(
+                            static_cast<IssuePort>(scheduledLIREntries[i]->portType));
+        if (printInstructionLatency) // Conditional print of instruction latency
+            depengraphfile << " | Latency:"
+                    << scheduledLIREntries[i]->instructionLatency;
+        if (printCriticalPath) // Conditional print of critical path
+            depengraphfile << " | LongestPath:"
+                    << scheduledLIREntries[i]->longestPath;
+        // Close label
+        depengraphfile << "}\"";
+        // Close node attributes
+        depengraphfile << "]" << std::endl;
+    }
+
+    // Print edge between each node and its successors
+    for (unsigned int i = 0; i < scheduledLIREntries.size(); ++i) {
+        // It is possible that successorDependencies contains duplicates
+        // So we set up a set here to avoid creating multiple edges
+        std::set<int> successors;
+        for (unsigned int j = 0;
+                j < dependencyAssociation[scheduledLIREntries[i]].successorDependencies.size(); ++j) {
+            int successorSlotId = dependencyAssociation[scheduledLIREntries[i]].successorDependencies[j].lowopSlotId;
+            if(successors.find(successorSlotId) != successors.end())
+                continue; // If we already generated edge for this successor, don't generate another
+            successors.insert(successorSlotId);
+            depengraphfile << "LIR" << scheduledLIREntries[i]->slotId << "->LIR"
+                    << dependencyAssociation[scheduledLIREntries[i]].successorDependencies[j].lowopSlotId
+                    << std::endl;
+        }
+    }
+    depengraphfile << "}" << std::endl;
+    depengraphfile.close();
+#endif
+}
diff --git a/vm/compiler/codegen/x86/lightcg/Scheduler.h b/vm/compiler/codegen/x86/lightcg/Scheduler.h
new file mode 100644
index 0000000..e19b4f6
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/Scheduler.h
@@ -0,0 +1,171 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*! \file Scheduler.h
+    \brief This file implements the interface for Atom Scheduler
+*/
+
+#ifndef ATOM_SCHEDULER_H_
+#define ATOM_SCHEDULER_H_
+
+#include "Lower.h"
+#include <map>
+#include <vector>
+#include "BitVector.h"
+
+/**
+ * @class Dependencies
+ * @brief Provides vectors for the dependencies
+ */
+struct Dependencies
+{
+    //! \brief Holds information about LowOps on which current LowOp
+    //! depends on (predecessors).
+    //! \details For example, if a LowOp with slotId of 3 depends on
+    //! LowOp with slotId of 2 because of a RAW, then the LowOp with
+    //! slotId of 3 will have an entry in the predecessorDependencies
+    //! with a Dependency_RAW and slotId of 2. This field is used
+    //! only for scheduling.
+    std::vector<DependencyInformation> predecessorDependencies;
+    //! \brief Holds information about LowOps that depend on current
+    //! LowOp (successors).
+    //! \details For example, if a LowOp with slotId of 3 depends on
+    //! LowOp with slotId of 2 because of a RAW, then the LowOp with
+    //! slotId of 2 will have an entry in the successorDependencies
+    //! with a Dependency_RAW and slotId of 3. This field is used
+    //! only for scheduling.
+    std::vector<DependencyInformation> successorDependencies;
+};
+
+//! \brief Interface for Atom Instruction Scheduler
+class Scheduler {
+private:
+    //! \brief Defines implementation of a native basic block for Atom LIRs.
+    typedef std::vector<LowOp*> NativeBasicBlock;
+
+    //! \brief A map providing a link between LowOp and scheduling dependencies
+    std::map<LowOp *, Dependencies> dependencyAssociation;
+
+    //! \brief Holds a list of all LIRs allocated via allocateNewEmptyLIR
+    //! which are not yet in code stream.
+    //! \details The field LowOp::slotId corresponds to index into this list
+    //! when the LIR is allocated by scheduler via allocateNewEmptyLIR.
+    //! \see allocateNewEmptyLIR
+    NativeBasicBlock queuedLIREntries;
+
+    //! \brief Holds a list of scheduled LIRs in their scheduled order.
+    //! \details It contains the same LIRs are queuedLIREntries, just
+    //! in a possibly different order.
+    //! \see queuedLIREntries
+    NativeBasicBlock scheduledLIREntries;
+
+    //! \brief Used to keep track of writes to a resource.
+    //! \details This is used only during dependency building but corresponding
+    //! LIRs are also updated to keep track of their own dependencies which is
+    //! used during scheduling.
+    //! \see LowOp::predecessorDependencies
+    //! \see UseDefUserEntry
+    std::vector<UseDefProducerEntry> producerEntries;
+
+    //! \brief Used to keep track of reads from a resource.
+    //! \details This is used only during dependency building but corresponding
+    //! LIRs are also updated to keep track of their own dependencies which is
+    //! used during scheduling.
+    //! \see LowOp::predecessorDependencies
+    //! \see UseDefUserEntry
+    std::vector<UseDefUserEntry> userEntries;
+
+    //! \brief Used to keep track of dependencies on control flags. It keeps a
+    //! a list of all flag writers until a flag reader is seen.
+    //! \details This is used only during dependency building but corresponding
+    //! LIRs are also updated to keep track of their own dependencies which is
+    //! used during scheduling. This list holds values from LowOp::slotId.
+    //! \see LowOp::predecessorDependencies
+    //! \see LowOp::slotId
+    std::vector<unsigned int> ctrlEntries;
+
+    // See documentation in Schedule.cpp or in doxygen output for undocumented prototypes.
+    void updateDependencyGraph(UseDefEntryType type, int regNum,
+            LowOpndDefUse defuse,
+            LatencyBetweenNativeInstructions causeOfLatency, LowOp* op);
+    void updateDependencyGraphForMem(LowOpndMem & mOpnd, LowOpndDefUse defuse,
+            LowOp* op);
+    void handlePushDependencyUpdate(LowOp* op);
+    void handleFloatDependencyUpdate(LowOp* op);
+    void handleImplicitDependenciesEaxEdx(LowOp* op);
+    void setupLiveOutDependencies();
+
+    bool isBasicBlockDelimiter(Mnemonic m);
+    void generateAssembly(LowOp * op);
+
+    void visitNodeTopologicalSort(unsigned int nodeId, int * visitedList,
+            NativeBasicBlock & inverseTopologicalOrder);
+    void findLongestPath();
+    void updateReadyOps(int chosenIdx, BitVector * scheduledOps,
+            BitVector * readyOps);
+    void schedule();
+
+    double basicBlockEditDistance(const NativeBasicBlock & block1,
+            const NativeBasicBlock & block2);
+    void printStatistics(bool prescheduling);
+    void printDependencyGraph(const char * directoryPath,
+            const std::string &filename, const char * startStream,
+            bool printScheduledTime, bool printIssuePort,
+            bool printInstructionLatency, bool printCriticalPath,
+            bool printOriginalOrder);
+
+    //! \brief Reset the internal structures
+    void reset(void);
+public:
+    ~Scheduler(void);
+
+    //! \brief Called by users of scheduler to allocate an empty LIR (no mnemonic
+    //! or operands).
+    //! \details The caller of this method takes the LIR, updates the mnemonic
+    //! and operand information, and then calls one of the updateUseDefInformation
+    //! methods in the scheduler with this LIR as parameter. This method should not
+    //! be called when scheduling is not enabled because the LIR will never be freed.
+    //! Internally, the scheduler will add this LIR to the native basic block it
+    //! is building and also assign it an id.
+    //! Because of specialization this method definition must stay in the header in
+    //! order to prevent linker errors.
+    //! \tparam is a LowOp or any of its specialized children.
+    //! \see LowOp
+    template<typename LowOpType> LowOpType * allocateNewEmptyLIR() {
+        LowOpType * op = static_cast<LowOpType *>(dvmCompilerNew(
+                sizeof(LowOpType), true /*zero*/));
+        op->slotId = queuedLIREntries.size();
+        queuedLIREntries.push_back(op);
+        return op;
+    }
+
+    // See documentation in Schedule.cpp or in doxygen output for undocumented prototypes.
+    void updateUseDefInformation(LowOp * op);
+    void updateUseDefInformation_imm(LowOp * op);
+    void updateUseDefInformation_reg(LowOpReg * op);
+    void updateUseDefInformation_mem(LowOpMem * op);
+    void updateUseDefInformation_imm_to_reg(LowOpImmReg * op);
+    void updateUseDefInformation_imm_to_mem(LowOpImmMem * op);
+    void updateUseDefInformation_reg_to_reg(LowOpRegReg * op);
+    void updateUseDefInformation_mem_to_reg(LowOpMemReg * op);
+    void updateUseDefInformation_reg_to_mem(LowOpRegMem * op);
+    void updateUseDefInformation_fp_to_mem(LowOpRegMem * op);
+    void updateUseDefInformation_mem_to_fp(LowOpMemReg * op);
+    void signalEndOfNativeBasicBlock();
+    bool isQueueEmpty() const;
+};
+
+#endif /* ATOM_SCHEDULER_H_ */
diff --git a/vm/compiler/codegen/x86/lightcg/Singleton.h b/vm/compiler/codegen/x86/lightcg/Singleton.h
new file mode 100644
index 0000000..59ce8db
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/Singleton.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * @file Singleton.h
+ * @brief Implements the interface for singleton pattern.
+ */
+
+/**
+ * @details Calling this function allows any non-singleton class to be treated
+ * as a singleton. However, please note that making a copy of the instance
+ * is still allowed and your new copy is no longer the singleton instance.
+ * @return singleton instance
+ */
+template <class T>
+T& singleton() {
+    // Better not to use dynamic allocation so that we can make sure
+    // that it will be destroyed.
+    static T instance;
+    return instance;
+}
+
+/**
+ * @details Calling this function allows any non-singleton class to be treated
+ * as a singleton.
+ * @return pointer to singleton instance
+ * @warning This is not thread safe. It also does not call class destructor
+ * and leaks on program exit. Calls to this should not be mixed with calls to
+ * singleton<T>() because they will not retrieve the same instance.
+ */
+template <class T>
+T* singletonPtr() {
+    static T * instance = 0;
+
+    // Create a new instance if one doesn't already exist
+    if (instance == 0)
+        instance = new T();
+
+    return instance;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/StackExtensionX86.cpp b/vm/compiler/codegen/x86/lightcg/StackExtensionX86.cpp
new file mode 100644
index 0000000..94f6dc8
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/StackExtensionX86.cpp
@@ -0,0 +1,58 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "Dalvik.h"
+#include "StackExtensionX86.h"
+
+/**
+ * @brief Gives number of available scratch registers for x86.
+ * @return Total number of scratch registers
+ */
+unsigned int dvmArchSpecGetNumberOfScratch (void)
+{
+    return StackTemporaries::getTotalScratchVRs ();
+}
+
+/**
+ * @brief Given a stratch register index, it gives the VR register number.
+ * @param method Method that contains the MIR for which we want to
+ * use scratch register.
+ * @param idx Index of scratch register. Must be in range [0 .. N-1] where
+ * N is the maximum number of scratch registers available.
+ * @return Return virtual register number when it finds one for the index.
+ * Otherwise, it returns -1.
+ */
+int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx)
+{
+    unsigned int maxScratch = dvmArchSpecGetNumberOfScratch ();
+
+    //Sanity check to make sure that requested index is in
+    //range [0 .. maxScratch-1]
+    if (idx > (maxScratch - 1))
+    {
+        return -1;
+    }
+
+    //We know the index is okay. Index of 0 corresponds to virtual register
+    //whose number is: 0 + locals + ins
+    int numLocals = method->registersSize - method->insSize;
+    int numIns = method->insSize;
+
+    //Calculate the regnum
+    int regnum = idx + numLocals + numIns;
+
+    return regnum;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/StackExtensionX86.h b/vm/compiler/codegen/x86/lightcg/StackExtensionX86.h
new file mode 100644
index 0000000..2df5657
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/StackExtensionX86.h
@@ -0,0 +1,68 @@
+/*
+ * Copyright (C) 2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef STACKEXTENSIONX86_H_
+#define STACKEXTENSIONX86_H_
+
+/**
+ * @brief Space in frame to use for scratch registers.
+ */
+class StackTemporaries
+{
+public:
+    /**
+     * @brief Gives the total number of scratch VRs available for every frame.
+     * @return Maximum number of scratch VRs.
+     */
+    static unsigned int getTotalScratchVRs (void)
+    {
+        return numScratch;
+    }
+
+private:
+    /**
+     * @brief Hardcoded number of scratch registers per frame.
+     */
+#ifdef EXTRA_SCRATCH_VR
+    static const unsigned int numScratch = 4;
+#else
+    static const unsigned int numScratch = 0;
+#endif
+
+    /**
+     * @brief Allocated space for the scratch registers.
+     */
+    u4 scratchVirtualRegisters[numScratch];
+};
+
+/**
+ * @brief Stack frame extension for x86.
+ */
+struct ArchSpecificStackExtension
+{
+
+#ifdef EXTRA_SCRATCH_VR
+    /**
+     * @brief Allocated space for temporaries.
+     * @warning If this structure gets moved, dvmArchSpecGetScratchRegister
+     * must be updated to provide a new mapping.
+     */
+    StackTemporaries temps;
+#endif
+
+};
+
+#endif /* STACKEXTENSIONX86_H_ */
diff --git a/vm/compiler/codegen/x86/lightcg/Translator.h b/vm/compiler/codegen/x86/lightcg/Translator.h
new file mode 100644
index 0000000..5873ddf
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/Translator.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright (C) 2010-2013 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+
+#ifndef _DALVIK_TRANSLATOR
+#define _DALVIK_TRANSLATOR
+
+#include "Dalvik.h"
+#include "enc_wrapper.h"
+
+/* initialization for trace-based JIT */
+void initJIT(const char* curFileName, DvmDex *pDvmDex);
+
+#endif
diff --git a/vm/compiler/codegen/x86/lightcg/doxygen-config-x86-jit b/vm/compiler/codegen/x86/lightcg/doxygen-config-x86-jit
new file mode 100644
index 0000000..ff1d620
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/doxygen-config-x86-jit
@@ -0,0 +1,9 @@
+PROJECT_NAME = "X86 JIT Compiler for Dalvik"
+INPUT = AnalysisO1.cpp GlueOpt.cpp LowerHelper.cpp  NcgAot.cpp AnalysisO1.h LowerAlu.cpp LowerInvoke.cpp NcgAot.h Schedule.cpp BytecodeVisitor.cpp LowerConst.cpp LowerJump.cpp NcgCodegenO1.cpp Scheduler.h CodegenInterface.cpp Lower.cpp LowerMove.cpp NcgHelper.cpp Translator.h DataFlow.cpp LowerGetPut.cpp LowerObject.cpp NcgHelper.h Lower.h LowerReturn.cpp NullCheckElim.cpp
+OUTPUT_DIRECTORY = x86-jit-docs
+GENERATE_HTML = YES
+HTML_OUTPUT = html/
+HTML_FILE_EXTENSION = .html
+EXTRACT_PRIVATE = YES
+EXTRACT_STATIC = YES
+EXTRACT_LOCAL_CLASSES = YES
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/Android.mk b/vm/compiler/codegen/x86/lightcg/libenc/Android.mk
new file mode 100644
index 0000000..6fe9cdb
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/Android.mk
@@ -0,0 +1,65 @@
+#
+# Copyright (C) 2012 The Android Open Source Project
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#      http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Only include the x86 encoder/decoder for x86 architecture
+ifeq ($(TARGET_ARCH),x86)
+
+LOCAL_PATH:= $(call my-dir)
+
+ifneq ($(LIBENC_INCLUDED),true)
+
+LIBENC_INCLUDED := true
+
+enc_src_files := \
+        enc_base.cpp \
+        dec_base.cpp \
+        enc_wrapper.cpp \
+        enc_tabl.cpp
+
+enc_include_files :=
+
+##
+##
+## Build the device version of libenc
+##
+##
+ifneq ($(SDK_ONLY),true)  # SDK_only doesn't need device version
+
+include $(CLEAR_VARS)
+LOCAL_SRC_FILES := $(enc_src_files)
+LOCAL_C_INCLUDES += $(enc_include_files)
+LOCAL_MODULE_TAGS := optional
+LOCAL_MODULE := libenc
+include $(BUILD_STATIC_LIBRARY)
+
+endif # !SDK_ONLY
+
+
+##
+##
+## Build the host version of libenc
+##
+##
+include $(CLEAR_VARS)
+LOCAL_SRC_FILES := $(enc_src_files)
+LOCAL_C_INCLUDES += $(enc_include_files)
+LOCAL_MODULE_TAGS := optional
+LOCAL_MODULE := libenc
+include $(BUILD_HOST_STATIC_LIBRARY)
+
+endif   # ifneq ($(LIBENC_INCLUDED),true)
+
+endif   # ifeq ($(TARGET_ARCH),x86)
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/README.txt b/vm/compiler/codegen/x86/lightcg/libenc/README.txt
new file mode 100644
index 0000000..30df760
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/README.txt
@@ -0,0 +1,26 @@
+Original source from Apache Harmony 5.0M15 (r991518 from 2010-09-01) at
+http://harmony.apache.org/.
+
+The following files are from drlvm/vm/port/src/encoder/ia32_em64t.
+
+    dec_base.cpp
+    dec_base.h
+    enc_base.cpp
+    enc_base.h
+    enc_defs.h
+    enc_prvt.h
+    enc_tabl.cpp
+    encoder.cpp
+    encoder.h
+    encoder.inl
+
+The following files are derived partially from the original Apache
+Harmony files.
+
+    enc_defs_ext.h -- derived from enc_defs.h
+    enc_wrapper.h  -- derived from encoder.h
+
+
+
+
+
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/dec_base.cpp b/vm/compiler/codegen/x86/lightcg/libenc/dec_base.cpp
new file mode 100644
index 0000000..17471c7
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/dec_base.cpp
@@ -0,0 +1,542 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+
+/**
+ * @file
+ * @brief Main decoding (disassembling) routines implementation.
+ */
+
+#include "dec_base.h"
+#include "enc_prvt.h"
+#include <stdio.h>
+//#include "open/common.h"
+
+bool DecoderBase::is_prefix(const unsigned char * bytes)
+{
+    unsigned char b0 = *bytes;
+    unsigned char b1 = *(bytes+1);
+    if (b0 == 0xF0) { // LOCK
+        return true;
+    }
+    if (b0==0xF2 || b0==0xF3) { // REPNZ/REPZ prefixes
+        if (b1 == 0x0F) {   // .... but may be a part of SIMD opcode
+            return false;
+        }
+        return true;
+    }
+    if (b0 == 0x2E || b0 == 0x36 || b0==0x3E || b0==0x26 || b0==0x64 || b0==0x3E) {
+        // branch hints, segment prefixes
+        return true;
+    }
+    if (b0==0x66) { // operand-size prefix
+        if (b1 == 0x0F) {   // .... but may be a part of SIMD opcode
+            return false;
+        }
+        return false; //XXX - currently considered as part of opcode//true;
+    }
+    if (b0==0x67) { // address size prefix
+        return true;
+    }
+    return false;
+}
+
+// Returns prefix count from 0 to 4, or ((unsigned int)-1) on error
+unsigned int DecoderBase::fill_prefs(const unsigned char * bytes, Inst * pinst)
+{
+    const unsigned char * my_bytes = bytes;
+
+    while( 1 )
+    {
+        unsigned char by1 = *my_bytes;
+        unsigned char by2 = *(my_bytes + 1);
+        Inst::PrefGroups where;
+
+        switch( by1 )
+        {
+        case InstPrefix_REPNE:
+        case InstPrefix_REP:
+        {
+            if( 0x0F == by2)
+            {
+                return pinst->prefc;
+            }
+        }
+        case InstPrefix_LOCK:
+        {
+            where = Inst::Group1;
+            break;
+        }
+        case InstPrefix_CS:
+        case InstPrefix_SS:
+        case InstPrefix_DS:
+        case InstPrefix_ES:
+        case InstPrefix_FS:
+        case InstPrefix_GS:
+//      case InstPrefix_HintTaken: the same as CS override
+//      case InstPrefix_HintNotTaken: the same as DS override
+        {
+            where = Inst::Group2;
+            break;
+        }
+        case InstPrefix_OpndSize:
+        {
+//NOTE:   prefix does not work for JMP Sz16, the opcode is 0x66 0xe9
+//        here 0x66 will be treated as prefix, try_mn will try to match the code starting at 0xe9
+//        it will match JMP Sz32 ...
+//HACK:   assume it is the last prefix, return any way
+            if( 0x0F == by2)
+            {
+                return pinst->prefc;
+            }
+            return pinst->prefc;
+            where = Inst::Group3;
+            break;
+        }
+        case InstPrefix_AddrSize:
+        {
+            where = Inst::Group4;
+            break;
+        }
+        default:
+        {
+            return pinst->prefc;
+        }
+        }
+        // Assertions are not allowed here.
+        // Error situations should result in returning error status
+        if (InstPrefix_Null != pinst->pref[where]) //only one prefix in each group
+            return (unsigned int)-1;
+
+        pinst->pref[where] = (InstPrefix)by1;
+
+        if (pinst->prefc >= 4) //no more than 4 prefixes
+            return (unsigned int)-1;
+
+        pinst->prefc++;
+        ++my_bytes;
+    }
+}
+
+
+
+unsigned DecoderBase::decode(const void * addr, Inst * pinst)
+{
+    Inst tmp;
+
+    //assert( *(unsigned char*)addr != 0x66);
+
+    const unsigned char * bytes = (unsigned char*)addr;
+
+    // Load up to 4 prefixes
+    // for each Mnemonic
+    unsigned int pref_count = fill_prefs(bytes, &tmp);
+
+    if (pref_count == (unsigned int)-1) // Wrong prefix sequence, or >4 prefixes
+        return 0; // Error
+
+    bytes += pref_count;
+
+    //  for each opcodedesc
+    //      if (raw_len == 0) memcmp(, raw_len)
+    //  else check the mixed state which is one of the following:
+    //      /digit /i /rw /rd /rb
+
+    bool found = false;
+    const unsigned char * saveBytes = bytes;
+    for (unsigned mn=1; mn<Mnemonic_Count; mn++) {
+        bytes = saveBytes;
+        found=try_mn((Mnemonic)mn, &bytes, &tmp);
+        if (found) {
+            tmp.mn = (Mnemonic)mn;
+            break;
+        }
+    }
+    if (!found) {
+        // Unknown opcode
+        return 0;
+    }
+    tmp.size = (unsigned)(bytes-(const unsigned char*)addr);
+    if (pinst) {
+        *pinst = tmp;
+    }
+    return tmp.size;
+}
+
+#ifdef _EM64T_
+#define EXTEND_REG(reg, flag)                        \
+    ((NULL == rex || 0 == rex->flag) ? reg : (reg + 8))
+#else
+#define EXTEND_REG(reg, flag) (reg)
+#endif
+
+//don't know the use of rex, seems not used when _EM64T_ is not enabled
+bool DecoderBase::decode_aux(const EncoderBase::OpcodeDesc& odesc, unsigned aux,
+    const unsigned char ** pbuf, Inst * pinst
+#ifdef _EM64T_
+    , const Rex UNREF *rex
+#endif
+    )
+{
+    OpcodeByteKind kind = (OpcodeByteKind)(aux & OpcodeByteKind_KindMask);
+    unsigned byte = (aux & OpcodeByteKind_OpcodeMask);
+    unsigned data_byte = **pbuf;
+    EncoderBase::Operand& opnd = pinst->operands[pinst->argc];
+    const EncoderBase::OpndDesc& opndDesc = odesc.opnds[pinst->argc];
+
+    switch (kind) {
+    case OpcodeByteKind_SlashR:
+        {
+            RegName reg;
+            OpndKind okind;
+            const ModRM& modrm = *(ModRM*)*pbuf;
+            if (opndDesc.kind & OpndKind_Mem) { // 1st operand is memory
+#ifdef _EM64T_
+                decodeModRM(odesc, pbuf, pinst, rex);
+#else
+                decodeModRM(odesc, pbuf, pinst);
+#endif
+                ++pinst->argc;
+                const EncoderBase::OpndDesc& opndDesc2 = odesc.opnds[pinst->argc];
+                okind = ((opndDesc2.kind & OpndKind_XMMReg) || opndDesc2.size==OpndSize_64) ? OpndKind_XMMReg : OpndKind_GPReg;
+                EncoderBase::Operand& regOpnd = pinst->operands[pinst->argc];
+                reg = getRegName(okind, opndDesc2.size, EXTEND_REG(modrm.reg, r));
+                regOpnd = EncoderBase::Operand(reg);
+            } else {                            // 2nd operand is memory
+                okind = ((opndDesc.kind & OpndKind_XMMReg) || opndDesc.size==OpndSize_64) ? OpndKind_XMMReg : OpndKind_GPReg;
+                EncoderBase::Operand& regOpnd = pinst->operands[pinst->argc];
+                reg = getRegName(okind, opndDesc.size, EXTEND_REG(modrm.reg, r));
+                regOpnd = EncoderBase::Operand(reg);
+                ++pinst->argc;
+#ifdef _EM64T_
+                decodeModRM(odesc, pbuf, pinst, rex);
+#else
+                decodeModRM(odesc, pbuf, pinst);
+#endif
+            }
+            ++pinst->argc;
+        }
+        return true;
+    case OpcodeByteKind_rb:
+    case OpcodeByteKind_rw:
+    case OpcodeByteKind_rd:
+        {
+            // Gregory -
+            // Here we don't parse register because for current needs
+            // disassembler doesn't require to parse all operands
+            unsigned regid = data_byte - byte;
+            if (regid>7) {
+                return false;
+            }
+            OpndSize opnd_size;
+            switch(kind)
+            {
+            case OpcodeByteKind_rb:
+            {
+                opnd_size = OpndSize_8;
+                break;
+            }
+            case OpcodeByteKind_rw:
+            {
+                opnd_size = OpndSize_16;
+                break;
+            }
+            case OpcodeByteKind_rd:
+            {
+                opnd_size = OpndSize_32;
+                break;
+            }
+            default:
+                opnd_size = OpndSize_32;  // so there is no compiler warning
+                assert( false );
+            }
+            opnd = EncoderBase::Operand( getRegName(OpndKind_GPReg, opnd_size, regid) );
+
+            ++pinst->argc;
+            ++*pbuf;
+            return true;
+        }
+    case OpcodeByteKind_cb:
+        {
+        char offset = *(char*)*pbuf;
+        *pbuf += 1;
+        opnd = EncoderBase::Operand(offset);
+        ++pinst->argc;
+        //pinst->direct_addr = (void*)(pinst->offset + *pbuf);
+        }
+        return true;
+    case OpcodeByteKind_cw:
+        // not an error, but not expected in current env
+        // Android x86
+        {
+        short offset = *(short*)*pbuf;
+        *pbuf += 2;
+        opnd = EncoderBase::Operand(offset);
+        ++pinst->argc;
+        }
+        return true;
+        //return false;
+    case OpcodeByteKind_cd:
+        {
+        int offset = *(int*)*pbuf;
+        *pbuf += 4;
+        opnd = EncoderBase::Operand(offset);
+        ++pinst->argc;
+        }
+        return true;
+    case OpcodeByteKind_SlashNum:
+        {
+        const ModRM& modrm = *(ModRM*)*pbuf;
+        if (modrm.reg != byte) {
+            return false;
+        }
+        decodeModRM(odesc, pbuf, pinst
+#ifdef _EM64T_
+                        , rex
+#endif
+                        );
+        ++pinst->argc;
+        }
+        return true;
+    case OpcodeByteKind_ib:
+        {
+        char ival = *(char*)*pbuf;
+        opnd = EncoderBase::Operand(ival);
+        ++pinst->argc;
+        *pbuf += 1;
+        }
+        return true;
+    case OpcodeByteKind_iw:
+        {
+        short ival = *(short*)*pbuf;
+        opnd = EncoderBase::Operand(ival);
+        ++pinst->argc;
+        *pbuf += 2;
+        }
+        return true;
+    case OpcodeByteKind_id:
+        {
+        int ival = *(int*)*pbuf;
+        opnd = EncoderBase::Operand(ival);
+        ++pinst->argc;
+        *pbuf += 4;
+        }
+        return true;
+#ifdef _EM64T_
+    case OpcodeByteKind_io:
+        {
+        long long int ival = *(long long int*)*pbuf;
+        opnd = EncoderBase::Operand(OpndSize_64, ival);
+        ++pinst->argc;
+        *pbuf += 8;
+        }
+        return true;
+#endif
+    case OpcodeByteKind_plus_i:
+        {
+            unsigned regid = data_byte - byte;
+            if (regid>7) {
+                return false;
+            }
+            ++*pbuf;
+            return true;
+        }
+    case OpcodeByteKind_ZeroOpcodeByte: // cant be here
+        return false;
+    default:
+        // unknown kind ? how comes ?
+        break;
+    }
+    return false;
+}
+
+bool DecoderBase::try_mn(Mnemonic mn, const unsigned char ** pbuf, Inst * pinst) {
+    const unsigned char * save_pbuf = *pbuf;
+    EncoderBase::OpcodeDesc * opcodes = EncoderBase::opcodes[mn];
+
+    for (unsigned i=0; !opcodes[i].last; i++) {
+        const EncoderBase::OpcodeDesc& odesc = opcodes[i];
+        char *opcode_ptr = const_cast<char *>(odesc.opcode);
+        int opcode_len = odesc.opcode_len;
+#ifdef _EM64T_
+        Rex *prex = NULL;
+        Rex rex;
+#endif
+
+        *pbuf = save_pbuf;
+#ifdef _EM64T_
+        // Match REX prefixes
+        unsigned char rex_byte = (*pbuf)[0];
+        if ((rex_byte & 0xf0) == 0x40)
+        {
+            if ((rex_byte & 0x08) != 0)
+            {
+                // Have REX.W
+                if (opcode_len > 0 && opcode_ptr[0] == 0x48)
+                {
+                    // Have REX.W in opcode. All mnemonics that allow
+                    // REX.W have to have specified it in opcode,
+                    // otherwise it is not allowed
+                    rex = *(Rex *)*pbuf;
+                    prex = &rex;
+                    (*pbuf)++;
+                    opcode_ptr++;
+                    opcode_len--;
+                }
+            }
+            else
+            {
+                // No REX.W, so it doesn't have to be in opcode. We
+                // have REX.B, REX.X, REX.R or their combination, but
+                // not in opcode, they may extend any part of the
+                // instruction
+                rex = *(Rex *)*pbuf;
+                prex = &rex;
+                (*pbuf)++;
+            }
+        }
+#endif
+        if (opcode_len != 0) {
+            if (memcmp(*pbuf, opcode_ptr, opcode_len)) {
+                continue;
+            }
+            *pbuf += opcode_len;
+        }
+        if (odesc.aux0 != 0) {
+
+            if (!decode_aux(odesc, odesc.aux0, pbuf, pinst
+#ifdef _EM64T_
+                            , prex
+#endif
+                            )) {
+                continue;
+            }
+            if (odesc.aux1 != 0) {
+                if (!decode_aux(odesc, odesc.aux1, pbuf, pinst
+#ifdef _EM64T_
+                            , prex
+#endif
+                            )) {
+                    continue;
+                }
+            }
+            pinst->odesc = &opcodes[i];
+            return true;
+        }
+        else {
+            // Can't have empty opcode
+            assert(opcode_len != 0);
+            pinst->odesc = &opcodes[i];
+            return true;
+        }
+    }
+    return false;
+}
+
+bool DecoderBase::decodeModRM(const EncoderBase::OpcodeDesc& odesc,
+    const unsigned char ** pbuf, Inst * pinst
+#ifdef _EM64T_
+    , const Rex *rex
+#endif
+    )
+{
+    EncoderBase::Operand& opnd = pinst->operands[pinst->argc];
+    const EncoderBase::OpndDesc& opndDesc = odesc.opnds[pinst->argc];
+
+    //XXX debug ///assert(0x66 != *(*pbuf-2));
+    const ModRM& modrm = *(ModRM*)*pbuf;
+    *pbuf += 1;
+
+    RegName base = RegName_Null;
+    RegName index = RegName_Null;
+    int disp = 0;
+    unsigned scale = 0;
+
+    // On x86_64 all mnemonics that allow REX.W have REX.W in opcode.
+    // Therefore REX.W is simply ignored, and opndDesc.size is used
+
+    if (modrm.mod == 3) {
+        // we have only modrm. no sib, no disp.
+        // Android x86: Use XMMReg for 64b operand.
+        OpndKind okind = ((opndDesc.kind & OpndKind_XMMReg) || opndDesc.size == OpndSize_64) ? OpndKind_XMMReg : OpndKind_GPReg;
+        RegName reg = getRegName(okind, opndDesc.size, EXTEND_REG(modrm.rm, b));
+        opnd = EncoderBase::Operand(reg);
+        return true;
+    }
+    //Android x86: m16, m32, m64: mean a byte[word|doubleword] operand in memory
+    //base and index should be 32 bits!!!
+    const SIB& sib = *(SIB*)*pbuf;
+    // check whether we have a sib
+    if (modrm.rm == 4) {
+        // yes, we have SIB
+        *pbuf += 1;
+        if (sib.index != 4) {
+            index = getRegName(OpndKind_GPReg, OpndSize_32, EXTEND_REG(sib.index, x)); //Android x86: OpndDesc.size
+        } else {
+            // (sib.index == 4) => no index
+            //%esp can't be sib.index
+        }
+
+        // scale = sib.scale == 0 ? 0 : (1<<sib.scale);
+        // scale = (1<<sib.scale);
+        scale = (index == RegName_Null) ? 0 : (1<<sib.scale);
+
+        if (sib.base != 5 || modrm.mod != 0) {
+            base = getRegName(OpndKind_GPReg, OpndSize_32, EXTEND_REG(sib.base, b)); //Android x86: OpndDesc.size
+        } else {
+            // (sib.base == 5 && modrm.mod == 0) => no base
+        }
+    }
+    else {
+        if (modrm.mod != 0 || modrm.rm != 5) {
+            base = getRegName(OpndKind_GPReg, OpndSize_32, EXTEND_REG(modrm.rm, b)); //Android x86: OpndDesc.size
+        }
+        else {
+            // mod=0 && rm == 5 => only disp32
+        }
+    }
+
+    //update disp and pbuf
+    if (modrm.mod == 2) {
+        // have disp32
+        disp = *(int*)*pbuf;
+        *pbuf += 4;
+    }
+    else if (modrm.mod == 1) {
+        // have disp8
+        disp = *(char*)*pbuf;
+        *pbuf += 1;
+    }
+    else {
+        assert(modrm.mod == 0);
+        if (modrm.rm == 5) {
+            // have disp32 w/o sib
+            disp = *(int*)*pbuf;
+            *pbuf += 4;
+        }
+        else if (modrm.rm == 4 && sib.base == 5) {
+            // have disp32 with SI in sib
+            disp = *(int*)*pbuf;
+            *pbuf += 4;
+        }
+    }
+    opnd = EncoderBase::Operand(opndDesc.size, base, index, scale, disp);
+    return true;
+}
+
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/dec_base.h b/vm/compiler/codegen/x86/lightcg/libenc/dec_base.h
new file mode 100644
index 0000000..909c743
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/dec_base.h
@@ -0,0 +1,136 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+
+/**
+ * @file
+ * @brief Main decoding (disassembling) routines and structures.
+ *
+ * @note Quick and rough implementation, subject for a change.
+ */
+
+#ifndef __DEC_BASE_H_INCLUDED__
+#define __DEC_BASE_H_INCLUDED__
+
+
+#include "enc_base.h"
+#include "enc_prvt.h"
+
+#ifdef ENCODER_ISOLATE
+using namespace enc_ia32;
+#endif
+
+#define IF_CONDITIONAL  (0x00000000)
+#define IF_SYMMETRIC    (0x00000000)
+#define IF_BRANCH       (0x00000000)
+
+struct Inst {
+    Inst() {
+        mn = Mnemonic_Null;
+        prefc = 0;
+        size = 0;
+        flags = 0;
+        //offset = 0;
+        //direct_addr = NULL;
+        argc = 0;
+        for(int i = 0; i < 4; ++i)
+        {
+            pref[i] = InstPrefix_Null;
+        }
+    }
+    /**
+     * Mnemonic of the instruction.s
+     */
+    Mnemonic mn;
+    /**
+     * Enumerating of indexes in the pref array.
+     */
+    enum PrefGroups
+    {
+        Group1 = 0,
+        Group2,
+        Group3,
+        Group4
+    };
+    /**
+     * Number of prefixes (1 byte each).
+     */
+    unsigned int prefc;
+    /**
+     * Instruction prefixes. Prefix should be placed here according to its group.
+     */
+    InstPrefix pref[4];
+    /**
+     * Size, in bytes, of the instruction.
+     */
+    unsigned size;
+    /**
+     * Flags of the instruction.
+     * @see MF_
+     */
+    unsigned flags;
+    /**
+     * An offset of target address, in case of 'CALL offset',
+     * 'JMP/Jcc offset'.
+     */
+    //int      offset;
+    /**
+     * Direct address of the target (on Intel64/IA-32 is 'instruction IP' +
+     * 'instruction length' + offset).
+     */
+    //void *   direct_addr;
+    /**
+     * Number of arguments of the instruction.
+     */
+    unsigned argc;
+    //
+    EncoderBase::Operand operands[3];
+    //
+    const EncoderBase::OpcodeDesc * odesc;
+};
+
+inline bool is_jcc(Mnemonic mn)
+{
+    return Mnemonic_JO <= mn && mn<=Mnemonic_JG;
+}
+
+class DecoderBase {
+public:
+    static unsigned decode(const void * addr, Inst * pinst);
+private:
+    static bool decodeModRM(const EncoderBase::OpcodeDesc& odesc,
+        const unsigned char ** pbuf, Inst * pinst
+#ifdef _EM64T_
+        , const Rex *rex
+#endif
+        );
+    static bool decode_aux(const EncoderBase::OpcodeDesc& odesc,
+        unsigned aux, const unsigned char ** pbuf,
+        Inst * pinst
+#ifdef _EM64T_
+        , const Rex *rex
+#endif
+        );
+    static bool try_mn(Mnemonic mn, const unsigned char ** pbuf, Inst * pinst);
+    static unsigned int fill_prefs( const unsigned char * bytes, Inst * pinst);
+    static bool is_prefix(const unsigned char * bytes);
+};
+
+#endif  // ~ __DEC_BASE_H_INCLUDED__
+
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_base.cpp b/vm/compiler/codegen/x86/lightcg/libenc/enc_base.cpp
new file mode 100644
index 0000000..0562ce8
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_base.cpp
@@ -0,0 +1,1137 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+#include "enc_base.h"
+//#include <climits>
+#include <string.h>
+#define USE_ENCODER_DEFINES
+#include "enc_prvt.h"
+#include <stdio.h>
+
+//#define JET_PROTO
+
+#ifdef JET_PROTO
+#include "dec_base.h"
+#include "jvmti_dasm.h"
+#endif
+
+ENCODER_NAMESPACE_START
+
+/**
+ * @file
+ * @brief Main encoding routines and structures.
+ */
+
+#ifndef _WIN32
+    #define strcmpi strcasecmp
+#endif
+
+int EncoderBase::dummy = EncoderBase::buildTable();
+
+const unsigned char EncoderBase::size_hash[OpndSize_64+1] = {
+    //
+    0xFF,   // OpndSize_Null        = 0,
+    3,              // OpndSize_8           = 0x1,
+    2,              // OpndSize_16          = 0x2,
+    0xFF,   // 0x3
+    1,              // OpndSize_32          = 0x4,
+    0xFF,   // 0x5
+    0xFF,   // 0x6
+    0xFF,   // 0x7
+    0,              // OpndSize_64          = 0x8,
+    //
+};
+
+const unsigned char EncoderBase::kind_hash[OpndKind_Mem+1] = {
+    //
+    //gp reg                -> 000 = 0
+    //memory                -> 001 = 1
+    //immediate             -> 010 = 2
+    //xmm reg               -> 011 = 3
+    //segment regs  -> 100 = 4
+    //fp reg                -> 101 = 5
+    //mmx reg               -> 110 = 6
+    //
+    0xFF,                          // 0    OpndKind_Null=0,
+    0<<2,                          // 1    OpndKind_GPReg =
+                                   //           OpndKind_MinRegKind=0x1,
+    4<<2,                          // 2    OpndKind_SReg=0x2,
+
+#ifdef _HAVE_MMX_
+    6<<2,                          // 3
+#else
+    0xFF,                          // 3
+#endif
+
+    5<<2,                          // 4    OpndKind_FPReg=0x4,
+    0xFF, 0xFF, 0xFF,              // 5, 6, 7
+    3<<2,                                   //      OpndKind_XMMReg=0x8,
+    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 9, 0xA, 0xB, 0xC, 0xD,
+                                              // 0xE, 0xF
+    0xFF,                          // OpndKind_MaxRegKind =
+                                   // OpndKind_StatusReg =
+                                   // OpndKind_OtherReg=0x10,
+    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 0x11-0x18
+    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,               // 0x19-0x1F
+    2<<2,                                   // OpndKind_Immediate=0x20,
+    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 0x21-0x28
+    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 0x29-0x30
+    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, // 0x31-0x38
+    0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,               // 0x39-0x3F
+    1<<2,                                   // OpndKind_Memory=0x40
+};
+
+char * EncoderBase::curRelOpnd[3];
+
+char* EncoderBase::encode_aux(char* stream, unsigned aux,
+                              const Operands& opnds, const OpcodeDesc * odesc,
+                              unsigned * pargsCount, Rex * prex)
+{
+    const unsigned byte = aux;
+    OpcodeByteKind kind = (OpcodeByteKind)(byte & OpcodeByteKind_KindMask);
+    // The '>>' here is to force the switch to be table-based) instead of
+    // set of CMP+Jcc.
+    if (*pargsCount >= COUNTOF(opnds)) {
+        assert(false);
+        return stream;
+    }
+    switch(kind>>8) {
+    case OpcodeByteKind_SlashR>>8:
+        // /r - Indicates that the ModR/M byte of the instruction contains
+        // both a register operand and an r/m operand.
+        {
+        assert(opnds.count() > 1);
+    // not true anymore for MOVQ xmm<->r
+        //assert((odesc->opnds[0].kind & OpndKind_Mem) ||
+        //       (odesc->opnds[1].kind & OpndKind_Mem));
+        unsigned memidx = odesc->opnds[0].kind & OpndKind_Mem ? 0 : 1;
+        unsigned regidx = memidx == 0 ? 1 : 0;
+        memidx += *pargsCount;
+        regidx += *pargsCount;
+        ModRM& modrm = *(ModRM*)stream;
+        if (memidx >= COUNTOF(opnds) || regidx >= COUNTOF(opnds)) {
+            assert(false);
+            break;
+        }
+        if (opnds[memidx].is_mem()) {
+            stream = encodeModRM(stream, opnds, memidx, odesc, prex);
+        }
+        else {
+            modrm.mod = 3; // 11
+            modrm.rm = getHWRegIndex(opnds[memidx].reg());
+#ifdef _EM64T_
+            if (opnds[memidx].need_rex() && needs_rex_r(opnds[memidx].reg())) {
+                prex->b = 1;
+            }
+#endif
+            ++stream;
+        }
+        modrm.reg = getHWRegIndex(opnds[regidx].reg());
+#ifdef _EM64T_
+        if (opnds[regidx].need_rex() && needs_rex_r(opnds[regidx].reg())) {
+            prex->r = 1;
+        }
+#endif
+        *pargsCount += 2;
+        }
+        break;
+    case OpcodeByteKind_SlashNum>>8:
+        //  /digit - A digit between 0 and 7 indicates that the
+        //  ModR/M byte of the instruction uses only the r/m
+        //  (register or memory) operand. The reg field contains
+        //  the digit that provides an extension to the instruction's
+        //  opcode.
+        {
+        const unsigned lowByte = (byte & OpcodeByteKind_OpcodeMask);
+        assert(lowByte <= 7);
+        ModRM& modrm = *(ModRM*)stream;
+        unsigned idx = *pargsCount;
+        assert(opnds[idx].is_mem() || opnds[idx].is_reg());
+        if (opnds[idx].is_mem()) {
+            stream = encodeModRM(stream, opnds, idx, odesc, prex);
+        }
+        else {
+            modrm.mod = 3; // 11
+            modrm.rm = getHWRegIndex(opnds[idx].reg());
+#ifdef _EM64T_
+            if (opnds[idx].need_rex() && needs_rex_r(opnds[idx].reg())) {
+                prex->b = 1;
+            }
+#endif
+            ++stream;
+        }
+        modrm.reg = (char)lowByte;
+        *pargsCount += 1;
+        }
+        break;
+    case OpcodeByteKind_plus_i>>8:
+        //  +i - A number used in floating-point instructions when one
+        //  of the operands is ST(i) from the FPU register stack. The
+        //  number i (which can range from 0 to 7) is added to the
+        //  hexadecimal byte given at the left of the plus sign to form
+        //  a single opcode byte.
+        {
+            unsigned idx = *pargsCount;
+            const unsigned lowByte = (byte & OpcodeByteKind_OpcodeMask);
+            *stream = (char)lowByte + getHWRegIndex(opnds[idx].reg());
+            ++stream;
+            *pargsCount += 1;
+        }
+        break;
+    case OpcodeByteKind_ib>>8:
+    case OpcodeByteKind_iw>>8:
+    case OpcodeByteKind_id>>8:
+#ifdef _EM64T_
+    case OpcodeByteKind_io>>8:
+#endif //_EM64T_
+        //  ib, iw, id - A 1-byte (ib), 2-byte (iw), or 4-byte (id)
+        //  immediate operand to the instruction that follows the
+        //  opcode, ModR/M bytes or scale-indexing bytes. The opcode
+        //  determines if the operand is a signed value. All words
+        //  and double words are given with the low-order byte first.
+        {
+            unsigned idx = *pargsCount;
+            *pargsCount += 1;
+            assert(opnds[idx].is_imm());
+            if (kind == OpcodeByteKind_ib) {
+                *(unsigned char*)stream = (unsigned char)opnds[idx].imm();
+                curRelOpnd[idx] = stream;
+                stream += 1;
+            }
+            else if (kind == OpcodeByteKind_iw) {
+                *(unsigned short*)stream = (unsigned short)opnds[idx].imm();
+                curRelOpnd[idx] = stream;
+                stream += 2;
+            }
+            else if (kind == OpcodeByteKind_id) {
+                *(unsigned*)stream = (unsigned)opnds[idx].imm();
+                curRelOpnd[idx] = stream;
+                stream += 4;
+            }
+#ifdef _EM64T_
+            else {
+                assert(kind == OpcodeByteKind_io);
+                *(long long*)stream = (long long)opnds[idx].imm();
+                curRelOpnd[idx] = stream;
+                stream += 8;
+            }
+#else
+            else {
+                assert(false);
+            }
+#endif
+        }
+        break;
+    case OpcodeByteKind_cb>>8:
+        assert(opnds[*pargsCount].is_imm());
+        *(unsigned char*)stream = (unsigned char)opnds[*pargsCount].imm();
+        curRelOpnd[*pargsCount]= stream;
+        stream += 1;
+        *pargsCount += 1;
+        break;
+    case OpcodeByteKind_cw>>8:
+        assert(opnds[*pargsCount].is_imm());
+        *(unsigned short*)stream = (unsigned short)opnds[*pargsCount].imm();
+        curRelOpnd[*pargsCount]= stream;
+        stream += 2;
+        *pargsCount += 1;
+        break;
+    case OpcodeByteKind_cd>>8:
+        assert(opnds[*pargsCount].is_imm());
+        *(unsigned*)stream = (unsigned)opnds[*pargsCount].imm();
+        curRelOpnd[*pargsCount]= stream;
+        stream += 4;
+        *pargsCount += 1;
+        break;
+    //OpcodeByteKind_cp                             = 0x0B00,
+    //OpcodeByteKind_co                             = 0x0C00,
+    //OpcodeByteKind_ct                             = 0x0D00,
+    case OpcodeByteKind_rb>>8:
+    case OpcodeByteKind_rw>>8:
+    case OpcodeByteKind_rd>>8:
+        //  +rb, +rw, +rd - A register code, from 0 through 7,
+        //  added to the hexadecimal byte given at the left of
+        //  the plus sign to form a single opcode byte.
+        assert(opnds.count() > 0);
+        assert(opnds[*pargsCount].is_reg());
+        {
+        const unsigned lowByte = (byte & OpcodeByteKind_OpcodeMask);
+        *(unsigned char*)stream = (unsigned char)lowByte +
+                                   getHWRegIndex(opnds[*pargsCount].reg());
+#ifdef _EM64T_
+        if (opnds[*pargsCount].need_rex() && needs_rex_r(opnds[*pargsCount].reg())) {
+        prex->b = 1;
+        }
+#endif
+        ++stream;
+        *pargsCount += 1;
+        }
+        break;
+    default:
+        assert(false);
+        break;
+    }
+    return stream;
+}
+
+char * EncoderBase::encode(char * stream, Mnemonic mn, const Operands& opnds)
+{
+#ifdef _DEBUG
+    if (opnds.count() > 0) {
+        if (opnds[0].is_mem()) {
+            assert(getRegKind(opnds[0].base()) != OpndKind_SReg);
+        }
+        else if (opnds.count() >1 && opnds[1].is_mem()) {
+            assert(getRegKind(opnds[1].base()) != OpndKind_SReg);
+        }
+    }
+#endif
+
+#ifdef JET_PROTO
+    char* saveStream = stream;
+#endif
+
+    const OpcodeDesc * odesc = lookup(mn, opnds);
+#if !defined(_EM64T_)
+    bool copy_opcode = true;
+    Rex *prex = NULL;
+#else
+    // We need rex if
+    //  either of registers used as operand or address form is new extended register
+    //  it's explicitly specified by opcode
+    // So, if we don't have REX in opcode but need_rex, then set rex here
+    // otherwise, wait until opcode is set, and then update REX
+
+    bool copy_opcode = true;
+    unsigned char _1st = odesc->opcode[0];
+
+    Rex *prex = (Rex*)stream;
+    if (opnds.need_rex() &&
+        ((_1st == 0x66) || (_1st == 0xF2 || _1st == 0xF3) && odesc->opcode[1] == 0x0F)) {
+        // Special processing
+        //
+        copy_opcode = false;
+        //
+        *(unsigned char*)stream = _1st;
+        ++stream;
+        //
+        prex = (Rex*)stream;
+        prex->dummy = 4;
+        prex->w = 0;
+        prex->b = 0;
+        prex->x = 0;
+        prex->r = 0;
+        ++stream;
+        //
+        memcpy(stream, &odesc->opcode[1], odesc->opcode_len-1);
+        stream += odesc->opcode_len-1;
+    }
+    else if (_1st != 0x48 && opnds.need_rex()) {
+        prex = (Rex*)stream;
+        prex->dummy = 4;
+        prex->w = 0;
+        prex->b = 0;
+        prex->x = 0;
+        prex->r = 0;
+        ++stream;
+    }
+#endif  // ifndef EM64T
+
+    if (copy_opcode) {
+        if (odesc->opcode_len==1) {
+            unsigned char *dest = (unsigned char *) (stream);
+            unsigned char *src = (unsigned char *) (& (odesc->opcode));
+            *dest = *src;
+        }
+        else if (odesc->opcode_len==2) {
+            short *dest = (short *) (stream);
+            void *ptr = (void *) (& (odesc->opcode));
+            short *src = (short *) (ptr);
+            *dest = *src;
+        }
+        else if (odesc->opcode_len==3) {
+            unsigned short *dest = (unsigned short *) (stream);
+            void *ptr = (void *) (& (odesc->opcode));
+            unsigned short *src = (unsigned short *) (ptr);
+            *dest = *src;
+
+            //Now handle the last part
+            unsigned char *dest2 = (unsigned char *) (stream + 2);
+            *dest2 = odesc->opcode[2];
+        }
+        else if (odesc->opcode_len==4) {
+            unsigned int *dest = (unsigned int *) (stream);
+            void *ptr = (void *) (& (odesc->opcode));
+            unsigned int *src = (unsigned int *) (ptr);
+            *dest = *src;
+        }
+        stream += odesc->opcode_len;
+    }
+
+    unsigned argsCount = odesc->first_opnd;
+
+    if (odesc->aux0) {
+        stream = encode_aux(stream, odesc->aux0, opnds, odesc, &argsCount, prex);
+        if (odesc->aux1) {
+            stream = encode_aux(stream, odesc->aux1, opnds, odesc, &argsCount, prex);
+        }
+    }
+#ifdef JET_PROTO
+    //saveStream
+    Inst inst;
+    unsigned len = DecoderBase::decode(saveStream, &inst);
+    assert(inst.mn == mn);
+    assert(len == (unsigned)(stream-saveStream));
+    if (mn == Mnemonic_CALL || mn == Mnemonic_JMP ||
+        Mnemonic_RET == mn ||
+        (Mnemonic_JO<=mn && mn<=Mnemonic_JG)) {
+        assert(inst.argc == opnds.count());
+
+        InstructionDisassembler idi(saveStream);
+
+        for (unsigned i=0; i<inst.argc; i++) {
+            const EncoderBase::Operand& original = opnds[i];
+            const EncoderBase::Operand& decoded = inst.operands[i];
+            assert(original.kind() == decoded.kind());
+            assert(original.size() == decoded.size());
+            if (original.is_imm()) {
+                assert(original.imm() == decoded.imm());
+                assert(idi.get_opnd(0).kind == InstructionDisassembler::Kind_Imm);
+                if (mn == Mnemonic_CALL) {
+                    assert(idi.get_type() == InstructionDisassembler::RELATIVE_CALL);
+                }
+                else if (mn == Mnemonic_JMP) {
+                    assert(idi.get_type() == InstructionDisassembler::RELATIVE_JUMP);
+                }
+                else if (mn == Mnemonic_RET) {
+                    assert(idi.get_type() == InstructionDisassembler::RET);
+                }
+                else {
+                    assert(idi.get_type() == InstructionDisassembler::RELATIVE_COND_JUMP);
+                }
+            }
+            else if (original.is_mem()) {
+                assert(original.base() == decoded.base());
+                assert(original.index() == decoded.index());
+                assert(original.scale() == decoded.scale());
+                assert(original.disp() == decoded.disp());
+                assert(idi.get_opnd(0).kind == InstructionDisassembler::Kind_Mem);
+                if (mn == Mnemonic_CALL) {
+                    assert(idi.get_type() == InstructionDisassembler::INDIRECT_CALL);
+                }
+                else if (mn == Mnemonic_JMP) {
+                    assert(idi.get_type() == InstructionDisassembler::INDIRECT_JUMP);
+                }
+                else {
+                    assert(false);
+                }
+            }
+            else {
+                assert(original.is_reg());
+                assert(original.reg() == decoded.reg());
+                assert(idi.get_opnd(0).kind == InstructionDisassembler::Kind_Reg);
+                if (mn == Mnemonic_CALL) {
+                    assert(idi.get_type() == InstructionDisassembler::INDIRECT_CALL);
+                }
+                else if (mn == Mnemonic_JMP) {
+                    assert(idi.get_type() == InstructionDisassembler::INDIRECT_JUMP);
+                }
+                else {
+                    assert(false);
+                }
+            }
+        }
+
+        Inst inst2;
+        len = DecoderBase::decode(saveStream, &inst2);
+    }
+
+ //   if(idi.get_length_with_prefix() != (int)len) {
+	//__asm { int 3 };
+ //   }
+#endif
+
+    return stream;
+}
+
+char* EncoderBase::encodeModRM(char* stream, const Operands& opnds,
+                               unsigned idx, const OpcodeDesc * odesc,
+                               Rex * prex)
+{
+    const Operand& op = opnds[idx];
+    assert(op.is_mem());
+    assert(idx < COUNTOF(curRelOpnd));
+    ModRM& modrm = *(ModRM*)stream;
+    ++stream;
+    SIB& sib = *(SIB*)stream;
+
+    // we need SIB if
+    //      we have index & scale (nb: having index w/o base and w/o scale
+    //      treated as error)
+    //      the base is EBP w/o disp, BUT let's use a fake disp8
+    //      the base is ESP (nb: cant have ESP as index)
+
+    RegName base = op.base();
+    // only disp ?..
+    if (base == RegName_Null && op.index() == RegName_Null) {
+        assert(op.scale() == 0); // 'scale!=0' has no meaning without index
+        // ... yes - only have disp
+        // On EM64T, the simply [disp] addressing means 'RIP-based' one -
+        // must have to use SIB to encode 'DS: based'
+#ifdef _EM64T_
+        modrm.mod = 0;  // 00 - ..
+        modrm.rm = 4;   // 100 - have SIB
+
+        sib.base = 5;   // 101 - none
+        sib.index = 4;  // 100 - none
+        sib.scale = 0;  //
+        ++stream; // bypass SIB
+#else
+        // ignore disp_fits8, always use disp32.
+        modrm.mod = 0;
+        modrm.rm = 5;
+#endif
+        *(unsigned*)stream = (unsigned)op.disp();
+        curRelOpnd[idx]= stream;
+        stream += 4;
+        return stream;
+    }
+
+    //climits: error when targeting compal
+#define CHAR_MIN -127
+#define CHAR_MAX 127
+    const bool disp_fits8 = CHAR_MIN <= op.disp() && op.disp() <= CHAR_MAX;
+    /*&& op.base() != RegName_Null - just checked above*/
+    if (op.index() == RegName_Null && getHWRegIndex(op.base()) != getHWRegIndex(REG_STACK)) {
+        assert(op.scale() == 0); // 'scale!=0' has no meaning without index
+        // ... luckily no SIB, only base and may be a disp
+
+        // EBP base is a special case. Need to use [EBP] + disp8 form
+        if (op.disp() == 0  && getHWRegIndex(op.base()) != getHWRegIndex(RegName_EBP)) {
+            modrm.mod = 0; // mod=00, no disp et all
+        }
+        else if (disp_fits8) {
+            modrm.mod = 1; // mod=01, use disp8
+            *(unsigned char*)stream = (unsigned char)op.disp();
+            curRelOpnd[idx]= stream;
+            ++stream;
+        }
+        else {
+            modrm.mod = 2; // mod=10, use disp32
+            *(unsigned*)stream = (unsigned)op.disp();
+            curRelOpnd[idx]= stream;
+            stream += 4;
+        }
+        modrm.rm = getHWRegIndex(op.base());
+    if (is_em64t_extra_reg(op.base())) {
+        prex->b = 1;
+    }
+        return stream;
+    }
+
+    // cool, we do have SIB.
+    ++stream; // bypass SIB in stream
+
+    // {E|R}SP cannot be scaled index, however, R12 which has the same index in modrm - can
+    assert(op.index() == RegName_Null || !equals(op.index(), REG_STACK));
+
+    // Only GPRegs can be encoded in the SIB
+    assert(op.base() == RegName_Null ||
+            getRegKind(op.base()) == OpndKind_GPReg);
+    assert(op.index() == RegName_Null ||
+            getRegKind(op.index()) == OpndKind_GPReg);
+
+    modrm.rm = 4;   // r/m = 100, means 'we have SIB here'
+    if (op.base() == RegName_Null) {
+        // no base.
+        // already checked above if
+        // the first if() //assert(op.index() != RegName_Null);
+
+        modrm.mod = 0;  // mod=00 - here it means 'no base, but disp32'
+        sib.base = 5;   // 101 with mod=00  ^^^
+
+        // encode at least fake disp32 to avoid having [base=ebp]
+        *(unsigned*)stream = op.disp();
+        curRelOpnd[idx]= stream;
+        stream += 4;
+
+        unsigned sc = op.scale();
+        if (sc == 1 || sc==0)   { sib.scale = 0; }    // SS=00
+        else if (sc == 2)       { sib.scale = 1; }    // SS=01
+        else if (sc == 4)       { sib.scale = 2; }    // SS=10
+        else if (sc == 8)       { sib.scale = 3; }    // SS=11
+        sib.index = getHWRegIndex(op.index());
+    if (is_em64t_extra_reg(op.index())) {
+        prex->x = 1;
+    }
+
+        return stream;
+    }
+
+    if (op.disp() == 0 && getHWRegIndex(op.base()) != getHWRegIndex(RegName_EBP)) {
+        modrm.mod = 0;  // mod=00, no disp
+    }
+    else if (disp_fits8) {
+        modrm.mod = 1;  // mod=01, use disp8
+        *(unsigned char*)stream = (unsigned char)op.disp();
+        curRelOpnd[idx]= stream;
+        stream += 1;
+    }
+    else {
+        modrm.mod = 2;  // mod=10, use disp32
+        *(unsigned*)stream = (unsigned)op.disp();
+        curRelOpnd[idx]= stream;
+        stream += 4;
+    }
+
+    if (op.index() == RegName_Null) {
+        assert(op.scale() == 0); // 'scale!=0' has no meaning without index
+        // the only reason we're here without index, is that we have {E|R}SP
+        // or R12 as a base. Another possible reason - EBP without a disp -
+        // is handled above by adding a fake disp8
+#ifdef _EM64T_
+        assert(op.base() != RegName_Null && (equals(op.base(), REG_STACK) ||
+                                             equals(op.base(), RegName_R12)));
+#else  // _EM64T_
+        assert(op.base() != RegName_Null && equals(op.base(), REG_STACK));
+#endif //_EM64T_
+        sib.scale = 0;  // SS = 00
+        sib.index = 4;  // SS + index=100 means 'no index'
+    }
+    else {
+        unsigned sc = op.scale();
+        if (sc == 1 || sc==0)   { sib.scale = 0; }    // SS=00
+        else if (sc == 2)       { sib.scale = 1; }    // SS=01
+        else if (sc == 4)       { sib.scale = 2; }    // SS=10
+        else if (sc == 8)       { sib.scale = 3; }    // SS=11
+        sib.index = getHWRegIndex(op.index());
+    if (is_em64t_extra_reg(op.index())) {
+        prex->x = 1;
+    }
+        // not an error by itself, but the usage of [index*1] instead
+        // of [base] is discouraged
+        assert(op.base() != RegName_Null || op.scale() != 1);
+    }
+    sib.base = getHWRegIndex(op.base());
+    if (is_em64t_extra_reg(op.base())) {
+    prex->b = 1;
+    }
+    return stream;
+}
+
+char * EncoderBase::nops(char * stream, unsigned howMany)
+{
+    // Recommended multi-byte NOPs from the Intel architecture manual
+    static const unsigned char nops[10][9] = {
+        { 0, },                                                     // 0, this line is dummy and not used in the loop below
+        { 0x90, },                                                  // 1-byte NOP
+        { 0x66, 0x90, },                                            // 2
+        { 0x0F, 0x1F, 0x00, },                                      // 3
+        { 0x0F, 0x1F, 0x40, 0x00, },                                // 4
+        { 0x0F, 0x1F, 0x44, 0x00, 0x00, },                          // 5
+        { 0x66, 0x0F, 0x1F, 0x44, 0x00, 0x00, },                    // 6
+        { 0x0F, 0x1F, 0x80, 0x00, 0x00, 0x00, 0x00, },              // 7
+        { 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, },        // 8
+        { 0x66, 0x0F, 0x1F, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00 },   // 9-byte NOP
+    };
+
+    // Start from delivering the longest possible NOPs, then proceed with shorter ones
+    for (unsigned nopSize=9; nopSize!=0; nopSize--) {
+        while(howMany>=nopSize) {
+            const unsigned char* nopBytes = nops[nopSize];
+            for (unsigned i=0; i<nopSize; i++) {
+                stream[i] = nopBytes[i];
+            }
+            stream += nopSize;
+            howMany -= nopSize;
+        }
+    }
+    char* end = stream + howMany;
+    return end;
+}
+
+char * EncoderBase::prefix(char* stream, InstPrefix pref)
+{
+    if (pref== InstPrefix_Null) {
+        // nothing to do
+        return stream;
+    }
+    *stream = (char)pref;
+    return stream + 1;
+}
+
+
+/**
+ *
+ */
+bool EncoderBase::extAllowed(OpndExt opndExt, OpndExt instExt) {
+    if (instExt == opndExt || instExt == OpndExt_Any || opndExt == OpndExt_Any) {
+            return true;
+    }
+//asm("int3");
+assert(0);
+    return false;
+}
+
+static bool try_match(const EncoderBase::OpcodeDesc& odesc,
+                      const EncoderBase::Operands& opnds, bool strict) {
+
+    assert(odesc.roles.count == opnds.count());
+
+    for(unsigned j=0; j<odesc.roles.count; j++) {
+        // - the location must match exactly
+        if ((odesc.opnds[j].kind & opnds[j].kind()) != opnds[j].kind()) {
+            return false;
+        }
+        if (strict) {
+            // the size must match exactly
+            if (odesc.opnds[j].size != opnds[j].size()) {
+                return false;
+            }
+        }
+        else {
+            // must match only for def operands, and dont care about use ones
+            // situations like 'mov r8, imm32/mov r32, imm8' so the
+            // destination operand defines the overall size
+            if (EncoderBase::getOpndRoles(odesc.roles, j) & OpndRole_Def) {
+                if (odesc.opnds[j].size != opnds[j].size()) {
+                    return false;
+                }
+            }
+        }
+    }
+    return true;
+}
+
+//
+//Subhash implementaion - may be useful in case of many misses during fast
+//opcode lookup.
+//
+
+#ifdef ENCODER_USE_SUBHASH
+static unsigned subHash[32];
+
+static unsigned find(Mnemonic mn, unsigned hash)
+{
+    unsigned key = hash % COUNTOF(subHash);
+    unsigned pack = subHash[key];
+    unsigned _hash = pack & 0xFFFF;
+    if (_hash != hash) {
+        stat.miss(mn);
+        return EncoderBase::NOHASH;
+    }
+    unsigned _mn = (pack >> 24)&0xFF;
+    if (_mn != _mn) {
+        stat.miss(mn);
+        return EncoderBase::NOHASH;
+    }
+    unsigned idx = (pack >> 16) & 0xFF;
+    stat.hit(mn);
+    return idx;
+}
+
+static void put(Mnemonic mn, unsigned hash, unsigned idx)
+{
+    unsigned pack = hash | (idx<<16) | (mn << 24);
+    unsigned key = hash % COUNTOF(subHash);
+    subHash[key] = pack;
+}
+#endif
+
+const EncoderBase::OpcodeDesc *
+EncoderBase::lookup(Mnemonic mn, const Operands& opnds)
+{
+    const unsigned hash = opnds.hash();
+    unsigned opcodeIndex = opcodesHashMap[mn][hash];
+#ifdef ENCODER_USE_SUBHASH
+    if (opcodeIndex == NOHASH) {
+        opcodeIndex = find(mn, hash);
+    }
+#endif
+
+    if (opcodeIndex == NOHASH) {
+        // fast-path did no work. try to lookup sequentially
+        const OpcodeDesc * odesc = opcodes[mn];
+        int idx = -1;
+        bool found = false;
+        for (idx=0; !odesc[idx].last; idx++) {
+            const OpcodeDesc& opcode = odesc[idx];
+            if (opcode.platf == OpcodeInfo::decoder) {
+                continue;
+            }
+            if (opcode.roles.count != opnds.count()) {
+                continue;
+            }
+            if (try_match(opcode, opnds, true)) {
+                found = true;
+                break;
+            }
+        }
+        if (!found) {
+            for (idx=0; !odesc[idx].last; idx++) {
+                const OpcodeDesc& opcode = odesc[idx];
+                if (opcode.platf == OpcodeInfo::decoder) {
+                    continue;
+                }
+                if (opcode.roles.count != opnds.count()) {
+                    continue;
+                }
+                if (try_match(opcode, opnds, false)) {
+                    found = true;
+                    break;
+                }
+            }
+        }
+        assert(found);
+        opcodeIndex = idx;
+#ifdef ENCODER_USE_SUBHASH
+        put(mn, hash, opcodeIndex);
+#endif
+    }
+    assert(opcodeIndex != NOHASH);
+    const OpcodeDesc * odesc = &opcodes[mn][opcodeIndex];
+    assert(!odesc->last);
+    assert(odesc->roles.count == opnds.count());
+    assert(odesc->platf != OpcodeInfo::decoder);
+#if !defined(_EM64T_)
+    // tuning was done for IA32 only, so no size restriction on EM64T
+    //assert(sizeof(OpcodeDesc)==128);
+#endif
+    return odesc;
+}
+
+char* EncoderBase::getOpndLocation(int index) {
+     assert(index < 3);
+     return curRelOpnd[index];
+}
+
+
+Mnemonic EncoderBase::str2mnemonic(const char * mn_name)
+{
+    for (unsigned m = 1; m<Mnemonic_Count; m++) {
+        if (!strcmpi(mnemonics[m].name, mn_name)) {
+            return (Mnemonic)m;
+        }
+    }
+    return Mnemonic_Null;
+}
+
+static const char * conditionStrings[ConditionMnemonic_Count] = {
+    "O",
+    "NO",
+    "B",
+    "AE",
+    "Z",
+    "NZ",
+    "BE",
+    "A",
+
+    "S",
+    "NS",
+    "P",
+    "NP",
+    "L",
+    "GE",
+    "LE",
+    "G",
+};
+
+const char * getConditionString(ConditionMnemonic cm) {
+    return conditionStrings[cm];
+}
+
+static const struct {
+        char            sizeString[12];
+        OpndSize        size;
+}
+sizes[] = {
+    { "Sz8", OpndSize_8 },
+    { "Sz16", OpndSize_16 },
+    { "Sz32", OpndSize_32 },
+    { "Sz64", OpndSize_64 },
+#if !defined(TESTING_ENCODER)
+    { "Sz80", OpndSize_80 },
+    { "Sz128", OpndSize_128 },
+#endif
+    { "SzAny", OpndSize_Any },
+};
+
+
+OpndSize getOpndSize(const char * sizeString)
+{
+    assert(sizeString);
+    for (unsigned i = 0; i<COUNTOF(sizes); i++) {
+        if (!strcmpi(sizeString, sizes[i].sizeString)) {
+            return sizes[i].size;
+        }
+    }
+    return OpndSize_Null;
+}
+
+const char * getOpndSizeString(OpndSize size) {
+    for( unsigned i = 0; i<COUNTOF(sizes); i++ ) {
+        if( sizes[i].size==size ) {
+            return sizes[i].sizeString;
+        }
+    }
+    return NULL;
+}
+
+static const struct {
+    char            kindString[16];
+    OpndKind        kind;
+}
+kinds[] = {
+    { "Null", OpndKind_Null },
+    { "GPReg", OpndKind_GPReg },
+    { "SReg", OpndKind_SReg },
+    { "FPReg", OpndKind_FPReg },
+    { "XMMReg", OpndKind_XMMReg },
+#ifdef _HAVE_MMX_
+    { "MMXReg", OpndKind_MMXReg },
+#endif
+    { "StatusReg", OpndKind_StatusReg },
+    { "Reg", OpndKind_Reg },
+    { "Imm", OpndKind_Imm },
+    { "Mem", OpndKind_Mem },
+    { "Any", OpndKind_Any },
+};
+
+const char * getOpndKindString(OpndKind kind)
+{
+    for (unsigned i = 0; i<COUNTOF(kinds); i++) {
+        if (kinds[i].kind==kind) {
+            return kinds[i].kindString;
+        }
+    }
+    return NULL;
+}
+
+OpndKind getOpndKind(const char * kindString)
+{
+    assert(kindString);
+    for (unsigned i = 0; i<COUNTOF(kinds); i++) {
+        if (!strcmpi(kindString, kinds[i].kindString)) {
+            return kinds[i].kind;
+        }
+    }
+    return OpndKind_Null;
+}
+
+/**
+ * A mapping between register string representation and its RegName constant.
+ */
+static const struct {
+        char    regstring[7];
+        RegName regname;
+}
+
+registers[] = {
+#ifdef _EM64T_
+    {"RAX",         RegName_RAX},
+    {"RBX",         RegName_RBX},
+    {"RCX",         RegName_RCX},
+    {"RDX",         RegName_RDX},
+    {"RBP",         RegName_RBP},
+    {"RSI",         RegName_RSI},
+    {"RDI",         RegName_RDI},
+    {"RSP",         RegName_RSP},
+    {"R8",          RegName_R8},
+    {"R9",          RegName_R9},
+    {"R10",         RegName_R10},
+    {"R11",         RegName_R11},
+    {"R12",         RegName_R12},
+    {"R13",         RegName_R13},
+    {"R14",         RegName_R14},
+    {"R15",         RegName_R15},
+#endif
+
+    {"EAX",         RegName_EAX},
+    {"ECX",         RegName_ECX},
+    {"EDX",         RegName_EDX},
+    {"EBX",         RegName_EBX},
+    {"ESP",         RegName_ESP},
+    {"EBP",         RegName_EBP},
+    {"ESI",         RegName_ESI},
+    {"EDI",         RegName_EDI},
+#ifdef _EM64T_
+    {"R8D",         RegName_R8D},
+    {"R9D",         RegName_R9D},
+    {"R10D",        RegName_R10D},
+    {"R11D",        RegName_R11D},
+    {"R12D",        RegName_R12D},
+    {"R13D",        RegName_R13D},
+    {"R14D",        RegName_R14D},
+    {"R15D",        RegName_R15D},
+#endif
+
+    {"AX",          RegName_AX},
+    {"CX",          RegName_CX},
+    {"DX",          RegName_DX},
+    {"BX",          RegName_BX},
+    {"SP",          RegName_SP},
+    {"BP",          RegName_BP},
+    {"SI",          RegName_SI},
+    {"DI",          RegName_DI},
+
+    {"AL",          RegName_AL},
+    {"CL",          RegName_CL},
+    {"DL",          RegName_DL},
+    {"BL",          RegName_BL},
+#if !defined(_EM64T_)
+    {"AH",          RegName_AH},
+    {"CH",          RegName_CH},
+    {"DH",          RegName_DH},
+    {"BH",          RegName_BH},
+#else
+    {"SPL",         RegName_SPL},
+    {"BPL",         RegName_BPL},
+    {"SIL",         RegName_SIL},
+    {"DIL",         RegName_DIL},
+    {"R8L",         RegName_R8L},
+    {"R9L",         RegName_R9L},
+    {"R10L",        RegName_R10L},
+    {"R11L",        RegName_R11L},
+    {"R12L",        RegName_R12L},
+    {"R13L",        RegName_R13L},
+    {"R14L",        RegName_R14L},
+    {"R15L",        RegName_R15L},
+#endif
+    {"ES",          RegName_ES},
+    {"CS",          RegName_CS},
+    {"SS",          RegName_SS},
+    {"DS",          RegName_DS},
+    {"FS",          RegName_FS},
+    {"GS",          RegName_GS},
+
+    {"FP0",         RegName_FP0},
+/*
+    {"FP1",         RegName_FP1},
+    {"FP2",         RegName_FP2},
+    {"FP3",         RegName_FP3},
+    {"FP4",         RegName_FP4},
+    {"FP5",         RegName_FP5},
+    {"FP6",         RegName_FP6},
+    {"FP7",         RegName_FP7},
+*/
+    {"FP0S",        RegName_FP0S},
+    {"FP1S",        RegName_FP1S},
+    {"FP2S",        RegName_FP2S},
+    {"FP3S",        RegName_FP3S},
+    {"FP4S",        RegName_FP4S},
+    {"FP5S",        RegName_FP5S},
+    {"FP6S",        RegName_FP6S},
+    {"FP7S",        RegName_FP7S},
+
+    {"FP0D",        RegName_FP0D},
+    {"FP1D",        RegName_FP1D},
+    {"FP2D",        RegName_FP2D},
+    {"FP3D",        RegName_FP3D},
+    {"FP4D",        RegName_FP4D},
+    {"FP5D",        RegName_FP5D},
+    {"FP6D",        RegName_FP6D},
+    {"FP7D",        RegName_FP7D},
+
+    {"XMM0",        RegName_XMM0},
+    {"XMM1",        RegName_XMM1},
+    {"XMM2",        RegName_XMM2},
+    {"XMM3",        RegName_XMM3},
+    {"XMM4",        RegName_XMM4},
+    {"XMM5",        RegName_XMM5},
+    {"XMM6",        RegName_XMM6},
+    {"XMM7",        RegName_XMM7},
+#ifdef _EM64T_
+    {"XMM8",       RegName_XMM8},
+    {"XMM9",       RegName_XMM9},
+    {"XMM10",      RegName_XMM10},
+    {"XMM11",      RegName_XMM11},
+    {"XMM12",      RegName_XMM12},
+    {"XMM13",      RegName_XMM13},
+    {"XMM14",      RegName_XMM14},
+    {"XMM15",      RegName_XMM15},
+#endif
+
+
+    {"XMM0S",       RegName_XMM0S},
+    {"XMM1S",       RegName_XMM1S},
+    {"XMM2S",       RegName_XMM2S},
+    {"XMM3S",       RegName_XMM3S},
+    {"XMM4S",       RegName_XMM4S},
+    {"XMM5S",       RegName_XMM5S},
+    {"XMM6S",       RegName_XMM6S},
+    {"XMM7S",       RegName_XMM7S},
+#ifdef _EM64T_
+    {"XMM8S",       RegName_XMM8S},
+    {"XMM9S",       RegName_XMM9S},
+    {"XMM10S",      RegName_XMM10S},
+    {"XMM11S",      RegName_XMM11S},
+    {"XMM12S",      RegName_XMM12S},
+    {"XMM13S",      RegName_XMM13S},
+    {"XMM14S",      RegName_XMM14S},
+    {"XMM15S",      RegName_XMM15S},
+#endif
+
+    {"XMM0D",       RegName_XMM0D},
+    {"XMM1D",       RegName_XMM1D},
+    {"XMM2D",       RegName_XMM2D},
+    {"XMM3D",       RegName_XMM3D},
+    {"XMM4D",       RegName_XMM4D},
+    {"XMM5D",       RegName_XMM5D},
+    {"XMM6D",       RegName_XMM6D},
+    {"XMM7D",       RegName_XMM7D},
+#ifdef _EM64T_
+    {"XMM8D",       RegName_XMM8D},
+    {"XMM9D",       RegName_XMM9D},
+    {"XMM10D",      RegName_XMM10D},
+    {"XMM11D",      RegName_XMM11D},
+    {"XMM12D",      RegName_XMM12D},
+    {"XMM13D",      RegName_XMM13D},
+    {"XMM14D",      RegName_XMM14D},
+    {"XMM15D",      RegName_XMM15D},
+#endif
+
+    {"EFLGS",       RegName_EFLAGS},
+};
+
+
+const char * getRegNameString(RegName reg)
+{
+    for (unsigned i = 0; i<COUNTOF(registers); i++) {
+        if (registers[i].regname == reg) {
+            return registers[i].regstring;
+        }
+    }
+    return "(null)";
+}
+
+RegName getRegName(const char * regname)
+{
+    if (NULL == regname) {
+        return RegName_Null;
+    }
+
+    for (unsigned i = 0; i<COUNTOF(registers); i++) {
+        if (!strcmpi(regname,registers[i].regstring)) {
+            return registers[i].regname;
+        }
+    }
+    return RegName_Null;
+}
+
+ENCODER_NAMESPACE_END
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_base.h b/vm/compiler/codegen/x86/lightcg/libenc/enc_base.h
new file mode 100644
index 0000000..e88443f
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_base.h
@@ -0,0 +1,745 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+
+/**
+ * @file
+ * @brief Main encoding routines and structures.
+ */
+
+#ifndef __ENC_BASE_H_INCLUDED__
+#define __ENC_BASE_H_INCLUDED__
+
+#include "enc_defs.h"
+
+
+#include <stdlib.h>
+#include <assert.h>
+#include <memory.h>
+
+ENCODER_NAMESPACE_START
+struct MnemonicInfo;
+struct OpcodeInfo;
+struct Rex;
+
+/**
+ * @brief Basic facilities for generation of processor's instructions.
+ *
+ * The class EncoderBase represents the basic facilities for the encoding of
+ * processor's instructions on IA32 and EM64T platforms.
+ *
+ * The class provides general interface to generate the instructions as well
+ * as to retrieve some static data about instructions (number of arguments,
+ * their roles, etc).
+ *
+ * Currently, the EncoderBase class is used for both LIL and Jitrino code
+ * generators. Each of these code generators has its own wrapper to adapt
+ * this general interface for specific needs - see encoder.h for LIL wrappers
+ * and Ia32Encoder.h for Jitrino's adapter.
+ *
+ * Interface is provided through static methods, no instances of EncoderBase
+ * to be created.
+ *
+ * @todo RIP-based addressing on EM64T - it's not yet supported currently.
+ */
+class EncoderBase {
+public:
+    class Operands;
+    struct MnemonicDesc;
+    /**
+     * @brief Generates processor's instruction.
+     *
+     * @param stream - a buffer to generate into
+     * @param mn - \link Mnemonic mnemonic \endlink of the instruction
+     * @param opnds - operands for the instruction
+     * @returns (stream + length of the just generated instruction)
+     */
+    static char * encode(char * stream, Mnemonic mn, const Operands& opnds);
+    static char * getOpndLocation(int index);
+
+    /**
+     * @brief Generates the smallest possible number of NOP-s.
+     *
+     * Effectively generates the smallest possible number of instructions,
+     * which are NOP-s for CPU. Normally used to make a code alignment.
+     *
+     * The method inserts exactly number of bytes specified. It's a caller's
+     * responsibility to make sure the buffer is big enough.
+     *
+     * @param stream - buffer where to generate code into, can not be NULL
+     * @param howMany - how many bytes to fill with NOP-s
+     * @return \c (stream+howMany)
+     */
+    static char * nops(char * stream, unsigned howMany);
+
+    /**
+     * @brief Inserts a prefix into the code buffer.
+     *
+     * The method writes no more than one byte into the buffer. This is a
+     * caller's responsibility to make sure the buffer is big enough.
+     *
+     * @param stream - buffer where to insert the prefix
+     * @param pref - prefix to be inserted. If it's InstPrefix_Null, then
+     *        no action performed and return value is \c stream.
+     * @return \c (stream+1) if pref is not InstPrefix_Null, or \c stream
+     *         otherwise
+     */
+     static char * prefix(char* stream, InstPrefix pref);
+
+    /**
+     * @brief Determines if operand with opndExt suites the position with instExt.
+     */
+    static bool extAllowed(OpndExt opndExt, OpndExt instExt);
+
+    /**
+     * @brief Returns MnemonicDesc by the given Mnemonic.
+     */
+    static const MnemonicDesc * getMnemonicDesc(Mnemonic mn)
+    {
+        assert(mn < Mnemonic_Count);
+        return mnemonics + mn;
+    }
+
+    /**
+     * @brief Returns a Mnemonic for the given name.
+     *
+     * The lookup is case insensitive, if no mnemonic found for the given
+     * string, then Mnemonic_Null returned.
+     */
+    static Mnemonic str2mnemonic(const char * mn_name);
+
+    /**
+     * @brief Returns a string representation of the given Mnemonic.
+     *
+     * If invalid mnemonic passed, then the behavior is unpredictable.
+     */
+    static const char * getMnemonicString(Mnemonic mn)
+    {
+        return getMnemonicDesc(mn)->name;
+    }
+
+    static const char * toStr(Mnemonic mn)
+    {
+        return getMnemonicDesc(mn)->name;
+    }
+
+
+    /**
+     * @brief Description of operand.
+     *
+     * Description of an operand in opcode - its kind, size or RegName if
+     * operand must be a particular register.
+     */
+    struct OpndDesc {
+        /**
+         * @brief Location of the operand.
+         *
+         * May be a mask, i.e. OpndKind_Imm|OpndKind_Mem.
+         */
+        OpndKind        kind;
+        /**
+         * @brief Size of the operand.
+         */
+        OpndSize        size;
+        /**
+         * @brief Extention of the operand.
+         */
+        OpndExt         ext;
+        /**
+         * @brief Appropriate RegName if operand must reside on a particular
+         *        register (i.e. CWD/CDQ instructions), RegName_Null
+         *        otherwise.
+         */
+        RegName         reg;
+    };
+
+    /**
+     * @brief Description of operands' roles in instruction.
+     */
+    struct OpndRolesDesc {
+        /**
+         * @brief Total number of operands in the operation.
+         */
+        unsigned                count;
+        /**
+         * @brief Number of defs in the operation.
+         */
+        unsigned                defCount;
+        /**
+         * @brief Number of uses in the operation.
+         */
+        unsigned                useCount;
+        /**
+         * @brief Operand roles, bit-packed.
+         *
+         * A bit-packed info about operands' roles. Each operand's role is
+         * described by two bits, counted from right-to-left - the less
+         * significant bits (0,1) represent operand#0.
+         *
+         * The mask is build by ORing #OpndRole_Def and #OpndRole_Use
+         * appropriately and shifting left, i.e. operand#0's role would be
+         * - '(OpndRole_Def|OpndRole_Use)'
+         * - opnd#1's role would be 'OpndRole_Use<<2'
+         * - and operand#2's role would be, say, 'OpndRole_Def<<4'.
+         */
+        unsigned                roles;
+    };
+
+    /**
+     * @brief Extracts appropriate OpndRole for a given operand.
+     *
+     * The order of operands is left-to-right, i.e. for MOV, it
+     * would be 'MOV op0, op1'
+     */
+    static OpndRole getOpndRoles(OpndRolesDesc ord, unsigned idx)
+    {
+        assert(idx < ord.count);
+        return (OpndRole)(ord.roles>>((ord.count-1-idx)*2) & 0x3);
+    }
+
+    /**
+     * @brief Defines the maximum number of operands for an opcode.
+     *
+     * The 3 mostly comes from IDIV/IMUL which both may have up to
+     * 3 operands.
+     */
+    static const unsigned int MAX_NUM_OPCODE_OPERANDS = 3;
+
+    /**
+     * @brief Info about single opcode - its opcode bytes, operands,
+     *        operands' roles.
+     */
+   union OpcodeDesc {
+       char dummy[128]; // To make total size a power of 2
+
+       struct {
+           /**
+           * @brief Raw opcode bytes.
+           *
+           * 'Raw' opcode bytes which do not require any analysis and are
+           * independent from arguments/sizes/etc (may include opcode size
+           * prefix).
+           */
+           char        opcode[5];
+           unsigned    opcode_len;
+           unsigned    aux0;
+           unsigned    aux1;
+           /**
+           * @brief Info about opcode's operands.
+           */
+           OpndDesc        opnds[MAX_NUM_OPCODE_OPERANDS];
+           unsigned        first_opnd;
+           /**
+           * @brief Info about operands - total number, number of uses/defs,
+           *        operands' roles.
+           */
+           OpndRolesDesc   roles;
+           /**
+           * @brief If not zero, then this is final OpcodeDesc structure in
+           *        the list of opcodes for a given mnemonic.
+           */
+           char            last;
+           char            platf;
+       };
+   };
+public:
+    /**
+     * @brief General info about mnemonic.
+     */
+    struct MnemonicDesc {
+        /**
+        * @brief The mnemonic itself.
+        */
+        Mnemonic        mn;
+        /**
+        * Various characteristics of mnemonic.
+        * @see MF_
+         */
+        unsigned    flags;
+        /**
+         * @brief Operation's operand's count and roles.
+         *
+         * For the operations whose opcodes may use different number of
+         * operands (i.e. IMUL/SHL) either most common value used, or empty
+         * value left.
+         */
+        OpndRolesDesc   roles;
+        /**
+         * @brief Print name of the mnemonic.
+         */
+        const char *    name;
+    };
+
+
+    /**
+     * @brief Magic number, shows a maximum value a hash code can take.
+     *
+     * For meaning and arithmetics see enc_tabl.cpp.
+     *
+     * The value was increased from '5155' to '8192' to make it aligned
+     * for faster access in EncoderBase::lookup().
+     */
+    static const unsigned int               HASH_MAX = 8192; //5155;
+    /**
+     * @brief Empty value, used in hash-to-opcode map to show an empty slot.
+     */
+    static const unsigned char              NOHASH = 0xFF;
+    /**
+     * @brief The name says it all.
+     */
+    static const unsigned char              HASH_BITS_PER_OPERAND = 5;
+
+    /**
+     * @brief Contains info about a single instructions's operand - its
+     *        location, size and a value for immediate or RegName for
+     *        register operands.
+     */
+    class Operand {
+    public:
+        /**
+         * @brief Initializes the instance with empty size and kind.
+         */
+        Operand() : m_kind(OpndKind_Null), m_size(OpndSize_Null), m_ext(OpndExt_None), m_need_rex(false) {}
+        /**
+         * @brief Creates register operand from given RegName.
+         */
+        Operand(RegName reg, OpndExt ext = OpndExt_None) : m_kind(getRegKind(reg)),
+                               m_size(getRegSize(reg)),
+                               m_ext(ext), m_reg(reg)
+        {
+            hash_it();
+        }
+        /**
+         * @brief Creates register operand from given RegName and with the
+         *        specified size and kind.
+         *
+         * Used to speedup Operand creation as there is no need to extract
+         * size and kind from the RegName.
+         * The provided size and kind must match the RegName's ones though.
+         */
+        Operand(OpndSize sz, OpndKind kind, RegName reg, OpndExt ext = OpndExt_None) :
+            m_kind(kind), m_size(sz), m_ext(ext), m_reg(reg)
+        {
+            assert(m_size == getRegSize(reg));
+            assert(m_kind == getRegKind(reg));
+            hash_it();
+        }
+        /**
+         * @brief Creates immediate operand with the given size and value.
+         */
+        Operand(OpndSize size, long long ival, OpndExt ext = OpndExt_None) :
+            m_kind(OpndKind_Imm), m_size(size), m_ext(ext), m_imm64(ival)
+        {
+            hash_it();
+        }
+        /**
+         * @brief Creates immediate operand of OpndSize_32.
+         */
+        Operand(int ival, OpndExt ext = OpndExt_None) :
+            m_kind(OpndKind_Imm), m_size(OpndSize_32), m_ext(ext), m_imm64(ival)
+        {
+            hash_it();
+        }
+        /**
+         * @brief Creates immediate operand of OpndSize_16.
+         */
+        Operand(short ival, OpndExt ext = OpndExt_None) :
+            m_kind(OpndKind_Imm), m_size(OpndSize_16), m_ext(ext), m_imm64(ival)
+        {
+            hash_it();
+        }
+
+        /**
+         * @brief Creates immediate operand of OpndSize_8.
+         */
+        Operand(char ival, OpndExt ext = OpndExt_None) :
+            m_kind(OpndKind_Imm), m_size(OpndSize_8), m_ext(ext), m_imm64(ival)
+        {
+            hash_it();
+        }
+
+        /**
+         * @brief Creates memory operand.
+         */
+        Operand(OpndSize size, RegName base, RegName index, unsigned scale,
+                int disp, OpndExt ext = OpndExt_None) : m_kind(OpndKind_Mem), m_size(size), m_ext(ext)
+        {
+            m_base = base;
+            m_index = index;
+            m_scale = scale;
+            m_disp = disp;
+            hash_it();
+        }
+
+        /**
+         * @brief Creates memory operand with only base and displacement.
+         */
+        Operand(OpndSize size, RegName base, int disp, OpndExt ext = OpndExt_None) :
+            m_kind(OpndKind_Mem), m_size(size), m_ext(ext)
+        {
+            m_base = base;
+            m_index = RegName_Null;
+            m_scale = 0;
+            m_disp = disp;
+            hash_it();
+        }
+        //
+        // general info
+        //
+        /**
+         * @brief Returns kind of the operand.
+         */
+        OpndKind kind(void) const { return m_kind; }
+        /**
+         * @brief Returns size of the operand.
+         */
+        OpndSize size(void) const { return m_size; }
+        /**
+         * @brief Returns extention of the operand.
+         */
+        OpndExt ext(void) const { return m_ext; }
+        /**
+         * @brief Returns hash of the operand.
+         */
+        unsigned hash(void) const { return m_hash; }
+        //
+#ifdef _EM64T_
+        bool need_rex(void) const { return m_need_rex; }
+#else
+        bool need_rex(void) const { return false; }
+#endif
+        /**
+         * @brief Tests whether operand is memory operand.
+         */
+        bool is_mem(void) const { return is_placed_in(OpndKind_Mem); }
+        /**
+         * @brief Tests whether operand is immediate operand.
+         */
+        bool is_imm(void) const { return is_placed_in(OpndKind_Imm); }
+        /**
+         * @brief Tests whether operand is register operand.
+         */
+        bool is_reg(void) const { return is_placed_in(OpndKind_Reg); }
+        /**
+         * @brief Tests whether operand is general-purpose register operand.
+         */
+        bool is_gpreg(void) const { return is_placed_in(OpndKind_GPReg); }
+        /**
+         * @brief Tests whether operand is float-point pseudo-register operand.
+         */
+        bool is_fpreg(void) const { return is_placed_in(OpndKind_FPReg); }
+        /**
+         * @brief Tests whether operand is XMM register operand.
+         */
+        bool is_xmmreg(void) const { return is_placed_in(OpndKind_XMMReg); }
+#ifdef _HAVE_MMX_
+        /**
+         * @brief Tests whether operand is MMX register operand.
+         */
+        bool is_mmxreg(void) const { return is_placed_in(OpndKind_MMXReg); }
+#endif
+        /**
+         * @brief Tests whether operand is signed immediate operand.
+         */
+        //bool is_signed(void) const { assert(is_imm()); return m_is_signed; }
+
+        /**
+         * @brief Returns base of memory operand (RegName_Null if not memory).
+         */
+        RegName base(void) const { return is_mem() ? m_base : RegName_Null; }
+        /**
+         * @brief Returns index of memory operand (RegName_Null if not memory).
+         */
+        RegName index(void) const { return is_mem() ? m_index : RegName_Null; }
+        /**
+         * @brief Returns scale of memory operand (0 if not memory).
+         */
+        unsigned scale(void) const { return is_mem() ? m_scale : 0; }
+        /**
+         * @brief Returns displacement of memory operand (0 if not memory).
+         */
+        int disp(void) const { return is_mem() ? m_disp : 0; }
+        /**
+         * @brief Returns RegName of register operand (RegName_Null if not
+         *        register).
+         */
+        RegName reg(void) const { return is_reg() ? m_reg : RegName_Null; }
+        /**
+         * @brief Returns value of immediate operand (0 if not immediate).
+         */
+        long long imm(void) const { return is_imm() ? m_imm64 : 0; }
+    private:
+        bool is_placed_in(OpndKind kd) const
+        {
+                return kd == OpndKind_Reg ?
+                        m_kind == OpndKind_GPReg ||
+#ifdef _HAVE_MMX_
+                        m_kind == OpndKind_MMXReg ||
+#endif
+                        m_kind == OpndKind_FPReg ||
+                        m_kind == OpndKind_XMMReg
+                        : kd == m_kind;
+        }
+        void hash_it(void)
+        {
+            m_hash = get_size_hash(m_size) | get_kind_hash(m_kind);
+#ifdef _EM64T_
+            m_need_rex = false;
+            if (is_reg() && is_em64t_extra_reg(m_reg)) {
+                m_need_rex = true;
+            }
+            else if (is_mem() && (is_em64t_extra_reg(m_base) ||
+                                  is_em64t_extra_reg(m_index))) {
+                m_need_rex = true;
+            }
+#endif
+        }
+        // general info
+        OpndKind    m_kind;
+        OpndSize    m_size;
+        OpndExt     m_ext;
+        // complex address form support
+        RegName     m_base;
+        RegName     m_index;
+        unsigned    m_scale;
+        union {
+            int         m_disp;
+            RegName     m_reg;
+            long long   m_imm64;
+        };
+        unsigned    m_hash;
+        bool        m_need_rex;
+        friend class EncoderBase::Operands;
+    };
+    /**
+     * @brief Simple container for up to 3 Operand-s.
+     */
+    class Operands {
+    public:
+        Operands(void)
+        {
+            clear();
+        }
+        Operands(const Operand& op0)
+        {
+            clear();
+            add(op0);
+        }
+
+        Operands(const Operand& op0, const Operand& op1)
+        {
+            clear();
+            add(op0); add(op1);
+        }
+
+        Operands(const Operand& op0, const Operand& op1, const Operand& op2)
+        {
+            clear();
+            add(op0); add(op1); add(op2);
+        }
+
+        unsigned count(void) const { return m_count; }
+        unsigned hash(void) const { return m_hash; }
+        const Operand& operator[](unsigned idx) const
+        {
+            assert(idx<m_count);
+            return m_operands[idx];
+        }
+
+        void add(const Operand& op)
+        {
+            assert(m_count < COUNTOF(m_operands));
+            m_hash = (m_hash<<HASH_BITS_PER_OPERAND) | op.hash();
+            m_operands[m_count++] = op;
+            m_need_rex = m_need_rex || op.m_need_rex;
+        }
+#ifdef _EM64T_
+        bool need_rex(void) const { return m_need_rex; }
+#else
+        bool need_rex(void) const { return false; }
+#endif
+        void clear(void)
+        {
+            m_count = 0; m_hash = 0; m_need_rex = false;
+        }
+    private:
+        unsigned    m_count;
+        Operand     m_operands[COUNTOF( ((OpcodeDesc*)NULL)->opnds )];
+        unsigned    m_hash;
+        bool        m_need_rex;
+    };
+public:
+#ifdef _DEBUG
+    /**
+     * Verifies some presumptions about encoding data table.
+     * Called automaticaly during statics initialization.
+     */
+    static int verify(void);
+#endif
+
+private:
+    /**
+     * @brief Returns found OpcodeDesc by the given Mnemonic and operands.
+     */
+    static const OpcodeDesc * lookup(Mnemonic mn, const Operands& opnds);
+    /**
+     * @brief Encodes mod/rm byte.
+     */
+    static char* encodeModRM(char* stream, const Operands& opnds,
+                             unsigned idx, const OpcodeDesc * odesc, Rex * prex);
+    /**
+     * @brief Encodes special things of opcode description - '/r', 'ib', etc.
+     */
+    static char* encode_aux(char* stream, unsigned aux,
+                            const Operands& opnds, const OpcodeDesc * odesc,
+                            unsigned * pargsCount, Rex* prex);
+#ifdef _EM64T_
+    /**
+     * @brief Returns true if the 'reg' argument represents one of the new
+     *        EM64T registers - R8(D)-R15(D).
+     *
+     * The 64 bits versions of 'old-fashion' registers, i.e. RAX are not
+     * considered as 'extra'.
+     */
+    static bool is_em64t_extra_reg(const RegName reg)
+    {
+        if (needs_rex_r(reg)) {
+            return true;
+        }
+        if (RegName_SPL <= reg && reg <= RegName_R15L) {
+            return true;
+        }
+        return false;
+    }
+    static bool needs_rex_r(const RegName reg)
+    {
+        if (RegName_R8 <= reg && reg <= RegName_R15) {
+            return true;
+        }
+        if (RegName_R8D <= reg && reg <= RegName_R15D) {
+            return true;
+        }
+        if (RegName_R8S <= reg && reg <= RegName_R15S) {
+            return true;
+        }
+        if (RegName_R8L <= reg && reg <= RegName_R15L) {
+            return true;
+        }
+        if (RegName_XMM8 <= reg && reg <= RegName_XMM15) {
+            return true;
+        }
+        if (RegName_XMM8D <= reg && reg <= RegName_XMM15D) {
+            return true;
+        }
+        if (RegName_XMM8S <= reg && reg <= RegName_XMM15S) {
+            return true;
+        }
+        return false;
+    }
+    /**
+     * @brief Returns an 'processor's index' of the register - the index
+     *        used to encode the register in ModRM/SIB bytes.
+     *
+     * For the new EM64T registers the 'HW index' differs from the index
+     * encoded in RegName. For old-fashion registers it's effectively the
+     * same as ::getRegIndex(RegName).
+     */
+    static unsigned char getHWRegIndex(const RegName reg)
+    {
+        if (getRegKind(reg) != OpndKind_GPReg) {
+            return getRegIndex(reg);
+        }
+        if (RegName_SPL <= reg && reg<=RegName_DIL) {
+            return getRegIndex(reg);
+        }
+        if (RegName_R8L<= reg && reg<=RegName_R15L) {
+            return getRegIndex(reg) - getRegIndex(RegName_R8L);
+        }
+        return is_em64t_extra_reg(reg) ?
+                getRegIndex(reg)-getRegIndex(RegName_R8D) : getRegIndex(reg);
+    }
+#else
+    static unsigned char getHWRegIndex(const RegName reg)
+    {
+        return getRegIndex(reg);
+    }
+    static bool is_em64t_extra_reg(const RegName reg)
+    {
+        return false;
+    }
+#endif
+public:
+    static unsigned char get_size_hash(OpndSize size) {
+        return (size <= OpndSize_64) ? size_hash[size] : 0xFF;
+    }
+    static unsigned char get_kind_hash(OpndKind kind) {
+        return (kind <= OpndKind_Mem) ? kind_hash[kind] : 0xFF;
+    }
+
+    /**
+     * @brief A table used for the fast computation of hash value.
+     *
+     * A change must be strictly balanced with hash-related functions and data
+     * in enc_base.h/.cpp.
+     */
+    static const unsigned char size_hash[OpndSize_64+1];
+    /**
+     * @brief A table used for the fast computation of hash value.
+     *
+     * A change must be strictly balanced with hash-related functions and data
+     * in enc_base.h/.cpp.
+     */
+    static const unsigned char kind_hash[OpndKind_Mem+1];
+    /**
+     * @brief Maximum number of opcodes used for a single mnemonic.
+     *
+     * No arithmetics behind the number, simply estimated.
+     */
+    static const unsigned int   MAX_OPCODES = 32; //20;
+    /**
+     * @brief Mapping between operands hash code and operands.
+     */
+    static unsigned char    opcodesHashMap[Mnemonic_Count][HASH_MAX];
+    /**
+     * @brief Array of mnemonics.
+     */
+    static MnemonicDesc         mnemonics[Mnemonic_Count];
+    /**
+     * @brief Array of available opcodes.
+     */
+    static OpcodeDesc opcodes[Mnemonic_Count][MAX_OPCODES];
+
+    static int buildTable(void);
+    static void buildMnemonicDesc(const MnemonicInfo * minfo);
+    /**
+     * @brief Computes hash value for the given operands.
+     */
+    static unsigned short getHash(const OpcodeInfo* odesc);
+    /**
+     * @brief Dummy variable, for automatic invocation of buildTable() at
+     *        startup.
+     */
+    static int dummy;
+
+    static char * curRelOpnd[3];
+};
+
+ENCODER_NAMESPACE_END
+
+#endif // ifndef __ENC_BASE_H_INCLUDED__
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_defs.h b/vm/compiler/codegen/x86/lightcg/libenc/enc_defs.h
new file mode 100644
index 0000000..10409d2
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_defs.h
@@ -0,0 +1,786 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+#ifndef _ENCODER_DEFS_H_
+#define _ENCODER_DEFS_H_
+
+
+// Used to isolate experimental or being tuned encoder into a separate
+// namespace so it can coexist with a stable one in the same bundle.
+#ifdef ENCODER_ISOLATE
+    #define ENCODER_NAMESPACE_START namespace enc_ia32 {
+    #define ENCODER_NAMESPACE_END };
+#else
+    #define ENCODER_NAMESPACE_START
+    #define ENCODER_NAMESPACE_END
+#endif
+
+#include <assert.h>
+#include "enc_defs_ext.h"
+
+#ifndef COUNTOF
+    /**
+     * Number of items in an array.
+     */
+    #define COUNTOF(a)      (sizeof(a)/sizeof(a[0]))
+#endif
+
+#ifdef _EM64T_
+    /**
+     * A stack pointer of default platform's size.
+     */
+    #define REG_STACK       RegName_RSP
+    /**
+     * A max GP register (with a highest index number)
+     */
+    #define REG_MAX         RegName_R15
+    /**
+     * Total number of GP registers including stack pointer.
+     */
+    #define MAX_REGS        15
+#else
+    #define REG_STACK       RegName_ESP
+    #define REG_MAX         RegName_EDI
+    #define MAX_REGS        8
+#endif
+
+ENCODER_NAMESPACE_START
+
+/**
+ * A number of bytes 'eaten' by an ordinary PUSH/POP.
+ */
+#define STACK_SLOT_SIZE (sizeof(void*))
+
+
+/**
+ * A recommended by Intel Arch Manual aligment for instructions that
+ * are targets for jmps.
+ */
+#define JMP_TARGET_ALIGMENT     (16)
+/**
+ * A maximum possible size of native instruction.
+ */
+#define MAX_NATIVE_INST_SIZE (15)
+/**
+ * The enum OpndKind describes an operand's location - memory, immediate or a register.
+ * It can be used as a bit mask.
+ */
+typedef enum OpndKind {
+    /**
+     * A change must be balanced with at least the following places:
+     *              Ia32::Constraint-s use the OpndKind as a mask
+     *              encoder.cpp & encoder_master_info.cpp uses OpndKind as an index for hashing
+     *              - perhaps there are much more places
+     *
+     * NOTE: an MMXReg kind is incompatible with the current constraints framework,
+     *              as it's not encoded as a mask.
+     */
+    OpndKind_Null=0,
+    OpndKind_GPReg          = 0x01, OpndKind_MinRegKind = OpndKind_GPReg,
+    OpndKind_SReg           = 0x02,
+#ifdef _HAVE_MMX_
+    OpndKind_MMXReg         = 0x03,
+#endif
+    OpndKind_FPReg          = 0x04,
+    OpndKind_XMMReg         = 0x08,
+    OpndKind_OtherReg       = 0x10,
+    OpndKind_StatusReg      = OpndKind_OtherReg,
+    OpndKind_MaxRegKind     = OpndKind_StatusReg,   // a max existing kind of register
+    OpndKind_MaxReg,                                // -'- + 1 to be used in array defs
+    //
+    OpndKind_Immediate      = 0x20, OpndKind_Imm=OpndKind_Immediate,
+    OpndKind_Memory         = 0x40, OpndKind_Mem=OpndKind_Memory,
+    //
+    OpndKind_Reg            = 0x1F,
+    OpndKind_Any            = 0x7F,
+    // syntetic constants. Normally not used anywhere, but are used for
+    // human-readable showing under the debugger
+    OpndKind_GPReg_Mem      = OpndKind_GPReg|OpndKind_Mem,
+#ifdef _HAVE_MMX_
+    OpndKind_MMXReg_Mem     = OpndKind_MMXReg|OpndKind_Mem,
+#endif
+    OpndKind_XMMReg_Mem     = OpndKind_XMMReg|OpndKind_Mem,
+} OpndKind;
+
+/**
+ * Defines type of extention allowed for particular operand.
+ * For example imul r32,r_m32,imm8 sign extend imm8 before performing multiplication.
+ * To satisfy instruction constraints immediate operand should be either OpndExt_Signed
+ * or OpndExt_Any.
+ */
+typedef enum OpndExt {
+    OpndExt_None    = 0x0,
+    OpndExt_Signed  = 0x1,
+    OpndExt_Zero    = 0x2,
+    OpndExt_Any     = 0x3,
+}OpndExt;
+
+/**
+ * enum OpndRole defines the role of an operand in an instruction
+ * Can be used as mask to combine def and use. The complete def+use
+ * info can be combined in 2 bits which is used, say in Encoder::OpndRole.
+ */
+//TODO: this duplicates an Role used in the Ia32::Inst. That duplicate enum should be removed.
+typedef enum OpndRole {
+    OpndRole_Null=0,
+    OpndRole_Use=0x1,
+    OpndRole_Def=0x2,
+    OpndRole_UseDef=OpndRole_Use|OpndRole_Def,
+    OpndRole_All=0xffff,
+} OpndRole;
+
+
+#define REGNAME(k,s,i) ( ((k & OpndKind_Any)<<24) | ((s & OpndSize_Any)<<16) | (i&0xFF) )
+
+// Gregory -
+// It is critical that all register indexes (3rd number) inside of the
+// following table go in ascending order. That is R8 goes after
+// RDI. It is necessary for decoder when extending registers from RAX-RDI
+// to R8-R15 by simply adding 8 to the index on EM64T architecture
+typedef enum RegName {
+
+    RegName_Null = 0,
+
+#ifdef _EM64T_
+    /*
+    An index part of the RegName-s for RAX-RDI, EAX-ESI, AX-SI and AL-BH is
+    the same as the index used during instructions encoding. The same rule
+    applies for XMM regsters for IA32.
+    For new EM64T registers (both GP and XMM) the index need to be corrected to
+    obtain the index used in processor's instructions.
+    */
+    RegName_RAX = REGNAME(OpndKind_GPReg,OpndSize_64,0),
+    RegName_RCX = REGNAME(OpndKind_GPReg,OpndSize_64,1),
+    RegName_RDX = REGNAME(OpndKind_GPReg,OpndSize_64,2),
+    RegName_RBX = REGNAME(OpndKind_GPReg,OpndSize_64,3),
+    RegName_RSP = REGNAME(OpndKind_GPReg,OpndSize_64,4),
+    RegName_RBP = REGNAME(OpndKind_GPReg,OpndSize_64,5),
+    RegName_RSI = REGNAME(OpndKind_GPReg,OpndSize_64,6),
+    RegName_RDI = REGNAME(OpndKind_GPReg,OpndSize_64,7),
+
+    RegName_R8  = REGNAME(OpndKind_GPReg,OpndSize_64,8),
+    RegName_R9  = REGNAME(OpndKind_GPReg,OpndSize_64,9),
+    RegName_R10 = REGNAME(OpndKind_GPReg,OpndSize_64,10),
+    RegName_R11 = REGNAME(OpndKind_GPReg,OpndSize_64,11),
+    RegName_R12 = REGNAME(OpndKind_GPReg,OpndSize_64,12),
+    RegName_R13 = REGNAME(OpndKind_GPReg,OpndSize_64,13),
+    RegName_R14 = REGNAME(OpndKind_GPReg,OpndSize_64,14),
+    RegName_R15 = REGNAME(OpndKind_GPReg,OpndSize_64,15),
+#endif //~_EM64T_
+
+    RegName_EAX=REGNAME(OpndKind_GPReg,OpndSize_32,0),
+    RegName_ECX=REGNAME(OpndKind_GPReg,OpndSize_32,1),
+    RegName_EDX=REGNAME(OpndKind_GPReg,OpndSize_32,2),
+    RegName_EBX=REGNAME(OpndKind_GPReg,OpndSize_32,3),
+    RegName_ESP=REGNAME(OpndKind_GPReg,OpndSize_32,4),
+    RegName_EBP=REGNAME(OpndKind_GPReg,OpndSize_32,5),
+    RegName_ESI=REGNAME(OpndKind_GPReg,OpndSize_32,6),
+    RegName_EDI=REGNAME(OpndKind_GPReg,OpndSize_32,7),
+
+#ifdef _EM64T_
+    RegName_R8D  = REGNAME(OpndKind_GPReg,OpndSize_32,8),
+    RegName_R9D  = REGNAME(OpndKind_GPReg,OpndSize_32,9),
+    RegName_R10D = REGNAME(OpndKind_GPReg,OpndSize_32,10),
+    RegName_R11D = REGNAME(OpndKind_GPReg,OpndSize_32,11),
+    RegName_R12D = REGNAME(OpndKind_GPReg,OpndSize_32,12),
+    RegName_R13D = REGNAME(OpndKind_GPReg,OpndSize_32,13),
+    RegName_R14D = REGNAME(OpndKind_GPReg,OpndSize_32,14),
+    RegName_R15D = REGNAME(OpndKind_GPReg,OpndSize_32,15),
+#endif //~_EM64T_
+
+    RegName_AX=REGNAME(OpndKind_GPReg,OpndSize_16,0),
+    RegName_CX=REGNAME(OpndKind_GPReg,OpndSize_16,1),
+    RegName_DX=REGNAME(OpndKind_GPReg,OpndSize_16,2),
+    RegName_BX=REGNAME(OpndKind_GPReg,OpndSize_16,3),
+    RegName_SP=REGNAME(OpndKind_GPReg,OpndSize_16,4),
+    RegName_BP=REGNAME(OpndKind_GPReg,OpndSize_16,5),
+    RegName_SI=REGNAME(OpndKind_GPReg,OpndSize_16,6),
+    RegName_DI=REGNAME(OpndKind_GPReg,OpndSize_16,7),
+
+#ifdef _EM64T_
+    RegName_R8S  = REGNAME(OpndKind_GPReg,OpndSize_16,8),
+    RegName_R9S  = REGNAME(OpndKind_GPReg,OpndSize_16,9),
+    RegName_R10S = REGNAME(OpndKind_GPReg,OpndSize_16,10),
+    RegName_R11S = REGNAME(OpndKind_GPReg,OpndSize_16,11),
+    RegName_R12S = REGNAME(OpndKind_GPReg,OpndSize_16,12),
+    RegName_R13S = REGNAME(OpndKind_GPReg,OpndSize_16,13),
+    RegName_R14S = REGNAME(OpndKind_GPReg,OpndSize_16,14),
+    RegName_R15S = REGNAME(OpndKind_GPReg,OpndSize_16,15),
+#endif //~_EM64T_
+
+    RegName_AL=REGNAME(OpndKind_GPReg,OpndSize_8,0),
+    RegName_CL=REGNAME(OpndKind_GPReg,OpndSize_8,1),
+    RegName_DL=REGNAME(OpndKind_GPReg,OpndSize_8,2),
+    RegName_BL=REGNAME(OpndKind_GPReg,OpndSize_8,3),
+    // FIXME: Used in enc_tabl.cpp
+    // AH is not accessible on EM64T, instead encoded register is SPL, so decoded
+    // register will return incorrect enum
+    RegName_AH=REGNAME(OpndKind_GPReg,OpndSize_8,4),
+#if !defined(_EM64T_)
+    RegName_CH=REGNAME(OpndKind_GPReg,OpndSize_8,5),
+    RegName_DH=REGNAME(OpndKind_GPReg,OpndSize_8,6),
+    RegName_BH=REGNAME(OpndKind_GPReg,OpndSize_8,7),
+#else
+    RegName_SPL=REGNAME(OpndKind_GPReg,OpndSize_8,4),
+    RegName_BPL=REGNAME(OpndKind_GPReg,OpndSize_8,5),
+    RegName_SIL=REGNAME(OpndKind_GPReg,OpndSize_8,6),
+    RegName_DIL=REGNAME(OpndKind_GPReg,OpndSize_8,7),
+    RegName_R8L=REGNAME(OpndKind_GPReg,OpndSize_8,8),
+    RegName_R9L=REGNAME(OpndKind_GPReg,OpndSize_8,9),
+    RegName_R10L=REGNAME(OpndKind_GPReg,OpndSize_8,10),
+    RegName_R11L=REGNAME(OpndKind_GPReg,OpndSize_8,11),
+    RegName_R12L=REGNAME(OpndKind_GPReg,OpndSize_8,12),
+    RegName_R13L=REGNAME(OpndKind_GPReg,OpndSize_8,13),
+    RegName_R14L=REGNAME(OpndKind_GPReg,OpndSize_8,14),
+    RegName_R15L=REGNAME(OpndKind_GPReg,OpndSize_8,15),
+#endif
+
+    RegName_ES=REGNAME(OpndKind_SReg,OpndSize_16,0),
+    RegName_CS=REGNAME(OpndKind_SReg,OpndSize_16,1),
+    RegName_SS=REGNAME(OpndKind_SReg,OpndSize_16,2),
+    RegName_DS=REGNAME(OpndKind_SReg,OpndSize_16,3),
+    RegName_FS=REGNAME(OpndKind_SReg,OpndSize_16,4),
+    RegName_GS=REGNAME(OpndKind_SReg,OpndSize_16,5),
+
+    RegName_EFLAGS=REGNAME(OpndKind_StatusReg,OpndSize_32,0),
+
+#if !defined(TESTING_ENCODER)
+    RegName_FP0=REGNAME(OpndKind_FPReg,OpndSize_80,0),
+    RegName_FP1=REGNAME(OpndKind_FPReg,OpndSize_80,1),
+    RegName_FP2=REGNAME(OpndKind_FPReg,OpndSize_80,2),
+    RegName_FP3=REGNAME(OpndKind_FPReg,OpndSize_80,3),
+    RegName_FP4=REGNAME(OpndKind_FPReg,OpndSize_80,4),
+    RegName_FP5=REGNAME(OpndKind_FPReg,OpndSize_80,5),
+    RegName_FP6=REGNAME(OpndKind_FPReg,OpndSize_80,6),
+    RegName_FP7=REGNAME(OpndKind_FPReg,OpndSize_80,7),
+#endif
+    RegName_FP0S=REGNAME(OpndKind_FPReg,OpndSize_32,0),
+    RegName_FP1S=REGNAME(OpndKind_FPReg,OpndSize_32,1),
+    RegName_FP2S=REGNAME(OpndKind_FPReg,OpndSize_32,2),
+    RegName_FP3S=REGNAME(OpndKind_FPReg,OpndSize_32,3),
+    RegName_FP4S=REGNAME(OpndKind_FPReg,OpndSize_32,4),
+    RegName_FP5S=REGNAME(OpndKind_FPReg,OpndSize_32,5),
+    RegName_FP6S=REGNAME(OpndKind_FPReg,OpndSize_32,6),
+    RegName_FP7S=REGNAME(OpndKind_FPReg,OpndSize_32,7),
+
+    RegName_FP0D=REGNAME(OpndKind_FPReg,OpndSize_64,0),
+    RegName_FP1D=REGNAME(OpndKind_FPReg,OpndSize_64,1),
+    RegName_FP2D=REGNAME(OpndKind_FPReg,OpndSize_64,2),
+    RegName_FP3D=REGNAME(OpndKind_FPReg,OpndSize_64,3),
+    RegName_FP4D=REGNAME(OpndKind_FPReg,OpndSize_64,4),
+    RegName_FP5D=REGNAME(OpndKind_FPReg,OpndSize_64,5),
+    RegName_FP6D=REGNAME(OpndKind_FPReg,OpndSize_64,6),
+    RegName_FP7D=REGNAME(OpndKind_FPReg,OpndSize_64,7),
+
+#if !defined(TESTING_ENCODER)
+    RegName_XMM0=REGNAME(OpndKind_XMMReg,OpndSize_128,0),
+    RegName_XMM1=REGNAME(OpndKind_XMMReg,OpndSize_128,1),
+    RegName_XMM2=REGNAME(OpndKind_XMMReg,OpndSize_128,2),
+    RegName_XMM3=REGNAME(OpndKind_XMMReg,OpndSize_128,3),
+    RegName_XMM4=REGNAME(OpndKind_XMMReg,OpndSize_128,4),
+    RegName_XMM5=REGNAME(OpndKind_XMMReg,OpndSize_128,5),
+    RegName_XMM6=REGNAME(OpndKind_XMMReg,OpndSize_128,6),
+    RegName_XMM7=REGNAME(OpndKind_XMMReg,OpndSize_128,7),
+
+#ifdef _EM64T_
+    RegName_XMM8  = REGNAME(OpndKind_XMMReg,OpndSize_128,0),
+    RegName_XMM9  = REGNAME(OpndKind_XMMReg,OpndSize_128,1),
+    RegName_XMM10 = REGNAME(OpndKind_XMMReg,OpndSize_128,2),
+    RegName_XMM11 = REGNAME(OpndKind_XMMReg,OpndSize_128,3),
+    RegName_XMM12 = REGNAME(OpndKind_XMMReg,OpndSize_128,4),
+    RegName_XMM13 = REGNAME(OpndKind_XMMReg,OpndSize_128,5),
+    RegName_XMM14 = REGNAME(OpndKind_XMMReg,OpndSize_128,6),
+    RegName_XMM15 = REGNAME(OpndKind_XMMReg,OpndSize_128,7),
+#endif //~_EM64T_
+
+#endif  // ~TESTING_ENCODER
+
+    RegName_XMM0S=REGNAME(OpndKind_XMMReg,OpndSize_32,0),
+    RegName_XMM1S=REGNAME(OpndKind_XMMReg,OpndSize_32,1),
+    RegName_XMM2S=REGNAME(OpndKind_XMMReg,OpndSize_32,2),
+    RegName_XMM3S=REGNAME(OpndKind_XMMReg,OpndSize_32,3),
+    RegName_XMM4S=REGNAME(OpndKind_XMMReg,OpndSize_32,4),
+    RegName_XMM5S=REGNAME(OpndKind_XMMReg,OpndSize_32,5),
+    RegName_XMM6S=REGNAME(OpndKind_XMMReg,OpndSize_32,6),
+    RegName_XMM7S=REGNAME(OpndKind_XMMReg,OpndSize_32,7),
+#ifdef _EM64T_
+    RegName_XMM8S=REGNAME(OpndKind_XMMReg,OpndSize_32,8),
+    RegName_XMM9S=REGNAME(OpndKind_XMMReg,OpndSize_32,9),
+    RegName_XMM10S=REGNAME(OpndKind_XMMReg,OpndSize_32,10),
+    RegName_XMM11S=REGNAME(OpndKind_XMMReg,OpndSize_32,11),
+    RegName_XMM12S=REGNAME(OpndKind_XMMReg,OpndSize_32,12),
+    RegName_XMM13S=REGNAME(OpndKind_XMMReg,OpndSize_32,13),
+    RegName_XMM14S=REGNAME(OpndKind_XMMReg,OpndSize_32,14),
+    RegName_XMM15S=REGNAME(OpndKind_XMMReg,OpndSize_32,15),
+#endif // ifdef _EM64T_
+    RegName_XMM0D=REGNAME(OpndKind_XMMReg,OpndSize_64,0),
+    RegName_XMM1D=REGNAME(OpndKind_XMMReg,OpndSize_64,1),
+    RegName_XMM2D=REGNAME(OpndKind_XMMReg,OpndSize_64,2),
+    RegName_XMM3D=REGNAME(OpndKind_XMMReg,OpndSize_64,3),
+    RegName_XMM4D=REGNAME(OpndKind_XMMReg,OpndSize_64,4),
+    RegName_XMM5D=REGNAME(OpndKind_XMMReg,OpndSize_64,5),
+    RegName_XMM6D=REGNAME(OpndKind_XMMReg,OpndSize_64,6),
+    RegName_XMM7D=REGNAME(OpndKind_XMMReg,OpndSize_64,7),
+#ifdef _EM64T_
+    RegName_XMM8D=REGNAME(OpndKind_XMMReg,OpndSize_64,8),
+    RegName_XMM9D=REGNAME(OpndKind_XMMReg,OpndSize_64,9),
+    RegName_XMM10D=REGNAME(OpndKind_XMMReg,OpndSize_64,10),
+    RegName_XMM11D=REGNAME(OpndKind_XMMReg,OpndSize_64,11),
+    RegName_XMM12D=REGNAME(OpndKind_XMMReg,OpndSize_64,12),
+    RegName_XMM13D=REGNAME(OpndKind_XMMReg,OpndSize_64,13),
+    RegName_XMM14D=REGNAME(OpndKind_XMMReg,OpndSize_64,14),
+    RegName_XMM15D=REGNAME(OpndKind_XMMReg,OpndSize_64,15),
+#endif // ifdef _EM64T_
+#ifdef _HAVE_MMX_
+    RegName_MMX0=REGNAME(OpndKind_MMXReg,OpndSize_64,0),
+    RegName_MMX1=REGNAME(OpndKind_MMXReg,OpndSize_64,1),
+    RegName_MMX2=REGNAME(OpndKind_MMXReg,OpndSize_64,2),
+    RegName_MMX3=REGNAME(OpndKind_MMXReg,OpndSize_64,3),
+    RegName_MMX4=REGNAME(OpndKind_MMXReg,OpndSize_64,4),
+    RegName_MMX5=REGNAME(OpndKind_MMXReg,OpndSize_64,5),
+    RegName_MMX6=REGNAME(OpndKind_MMXReg,OpndSize_64,6),
+    RegName_MMX7=REGNAME(OpndKind_MMXReg,OpndSize_64,7),
+#endif  // _HAVE_MMX_
+} RegName;
+
+#if 0   // Android x86: use mnemonics defined in enc_defs_ext.h
+/**
+ * Conditional mnemonics.
+ * The values match the 'real' (==processor's) values of the appropriate
+ * condition values used in the opcodes.
+ */
+enum ConditionMnemonic {
+
+    ConditionMnemonic_O=0,
+    ConditionMnemonic_NO=1,
+    ConditionMnemonic_B=2, ConditionMnemonic_NAE=ConditionMnemonic_B, ConditionMnemonic_C=ConditionMnemonic_B,
+    ConditionMnemonic_NB=3, ConditionMnemonic_AE=ConditionMnemonic_NB, ConditionMnemonic_NC=ConditionMnemonic_NB,
+    ConditionMnemonic_Z=4, ConditionMnemonic_E=ConditionMnemonic_Z,
+    ConditionMnemonic_NZ=5, ConditionMnemonic_NE=ConditionMnemonic_NZ,
+    ConditionMnemonic_BE=6, ConditionMnemonic_NA=ConditionMnemonic_BE,
+    ConditionMnemonic_NBE=7, ConditionMnemonic_A=ConditionMnemonic_NBE,
+
+    ConditionMnemonic_S=8,
+    ConditionMnemonic_NS=9,
+    ConditionMnemonic_P=10, ConditionMnemonic_PE=ConditionMnemonic_P,
+    ConditionMnemonic_NP=11, ConditionMnemonic_PO=ConditionMnemonic_NP,
+    ConditionMnemonic_L=12, ConditionMnemonic_NGE=ConditionMnemonic_L,
+    ConditionMnemonic_NL=13, ConditionMnemonic_GE=ConditionMnemonic_NL,
+    ConditionMnemonic_LE=14, ConditionMnemonic_NG=ConditionMnemonic_LE,
+    ConditionMnemonic_NLE=15, ConditionMnemonic_G=ConditionMnemonic_NLE,
+    ConditionMnemonic_Count=16
+};
+
+
+#define CCM(prefix,cond) Mnemonic_##prefix##cond=Mnemonic_##prefix##cc+ConditionMnemonic_##cond
+
+//=========================================================================================================
+enum Mnemonic {
+
+Mnemonic_NULL=0, Mnemonic_Null=Mnemonic_NULL,
+Mnemonic_ADC,                           // Add with Carry
+Mnemonic_ADD,                           // Add
+Mnemonic_ADDSD,                         // Add Scalar Double-Precision Floating-Point Values
+Mnemonic_ADDSS,                         // Add Scalar Single-Precision Floating-Point Values
+Mnemonic_AND,                           // Logical AND
+
+Mnemonic_BSF,                           // Bit scan forward
+Mnemonic_BSR,                           // Bit scan reverse
+
+Mnemonic_CALL,                          // Call Procedure
+Mnemonic_CMC,                           // Complement Carry Flag
+Mnemonic_CWD, Mnemonic_CDQ=Mnemonic_CWD,// Convert Word to Doubleword/Convert Doubleword to Qua T dword
+Mnemonic_CMOVcc,                        // Conditional Move
+    CCM(CMOV,O),
+    CCM(CMOV,NO),
+    CCM(CMOV,B), CCM(CMOV,NAE), CCM(CMOV,C),
+    CCM(CMOV,NB), CCM(CMOV,AE), CCM(CMOV,NC),
+    CCM(CMOV,Z), CCM(CMOV,E),
+    CCM(CMOV,NZ), CCM(CMOV,NE),
+    CCM(CMOV,BE), CCM(CMOV,NA),
+    CCM(CMOV,NBE), CCM(CMOV,A),
+
+    CCM(CMOV,S),
+    CCM(CMOV,NS),
+    CCM(CMOV,P), CCM(CMOV,PE),
+    CCM(CMOV,NP), CCM(CMOV,PO),
+    CCM(CMOV,L), CCM(CMOV,NGE),
+    CCM(CMOV,NL), CCM(CMOV,GE),
+    CCM(CMOV,LE), CCM(CMOV,NG),
+    CCM(CMOV,NLE), CCM(CMOV,G),
+
+Mnemonic_CMP,                           // Compare Two Operands
+Mnemonic_CMPXCHG,                       // Compare and exchange
+Mnemonic_CMPXCHG8B,                     // Compare and Exchange 8 Bytes
+Mnemonic_CMPSB,                         // Compare Two Bytes at DS:ESI and ES:EDI
+Mnemonic_CMPSW,                         // Compare Two Words at DS:ESI and ES:EDI
+Mnemonic_CMPSD,                        // Compare Two Doublewords at DS:ESI and ES:EDI
+//
+// double -> float
+Mnemonic_CVTSD2SS,                      // Convert Scalar Double-Precision Floating-Point Value to Scalar Single-Precision Floating-Point Value
+// double -> I_32
+Mnemonic_CVTSD2SI,                      // Convert Scalar Double-Precision Floating-Point Value to Doubleword Integer
+// double [truncated] -> I_32
+Mnemonic_CVTTSD2SI,                     // Convert with Truncation Scalar Double-Precision Floating-Point Value to Signed Doubleword Integer
+//
+// float -> double
+Mnemonic_CVTSS2SD,                      // Convert Scalar Single-Precision Floating-Point Value to Scalar Double-Precision Floating-Point Value
+// float -> I_32
+Mnemonic_CVTSS2SI,                      // Convert Scalar Single-Precision Floating-Point Value to Doubleword Integer
+// float [truncated] -> I_32
+Mnemonic_CVTTSS2SI,                     // Convert with Truncation Scalar Single-Precision Floating-Point Value to Doubleword Integer
+//
+// I_32 -> double
+Mnemonic_CVTSI2SD,                      // Convert Doubleword Integer to Scalar Double-Precision Floating-Point Value
+// I_32 -> float
+Mnemonic_CVTSI2SS,                      // Convert Doubleword Integer to Scalar Single-Precision Floating-Point Value
+
+Mnemonic_COMISD,                        // Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS
+Mnemonic_COMISS,                        // Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS
+Mnemonic_DEC,                           // Decrement by 1
+//Mnemonic_DIV,                         // Unsigned Divide
+Mnemonic_DIVSD,                         // Divide Scalar Double-Precision Floating-Point Values
+Mnemonic_DIVSS,                         // Divide Scalar Single-Precision Floating-Point Values
+
+#ifdef _HAVE_MMX_
+Mnemonic_EMMS,                          // Empty MMX Technology State
+#endif
+
+Mnemonic_ENTER,                         // ENTER-Make Stack Frame for Procedure Parameters
+Mnemonic_FLDCW,                         // Load FPU control word
+Mnemonic_FADDP,
+Mnemonic_FLDZ,
+Mnemonic_FADD,
+Mnemonic_FSUBP,
+Mnemonic_FSUB,
+Mnemonic_FISUB,
+Mnemonic_FMUL,
+Mnemonic_FMULP,
+Mnemonic_FDIVP,
+Mnemonic_FDIV,
+Mnemonic_FUCOMPP,
+Mnemonic_FRNDINT,
+Mnemonic_FNSTCW,                        // Store FPU control word
+Mnemonic_FSTSW,                         // Store FPU status word
+Mnemonic_FNSTSW,                         // Store FPU status word
+//Mnemonic_FDECSTP,                     // Decrement Stack-Top Pointer
+Mnemonic_FILD,                          // Load Integer
+Mnemonic_FLD,                           // Load Floating Point Value
+Mnemonic_FLDLG2,
+Mnemonic_FLDLN2,
+Mnemonic_FLD1,
+
+Mnemonic_FCLEX,                         // Clear Exceptions
+Mnemonic_FCHS,                          // Change sign of ST0
+Mnemonic_FNCLEX,                        // Clear Exceptions
+
+//Mnemonic_FINCSTP,                     // Increment Stack-Top Pointer
+Mnemonic_FIST,                          // Store Integer
+Mnemonic_FISTP,                         // Store Integer, pop FPU stack
+Mnemonic_FISTTP,                        // Store Integer with Truncation
+Mnemonic_FPREM,                         // Partial Remainder
+Mnemonic_FPREM1,                        // Partial Remainder
+Mnemonic_FST,                           // Store Floating Point Value
+Mnemonic_FSTP,                          // Store Floating Point Value and pop the FP stack
+Mnemonic_FSQRT,                         //Computes the square root of the source value in the stack and pop the FP stack
+Mnemonic_FABS,                          //Computes the absolute value of the source value in the stack and pop the FP stack
+Mnemonic_FSIN,                          //Computes the sine of the source value in the stack and pop the FP stack
+Mnemonic_FCOS,                          //Computes the cosine of the source value in the stack and pop the FP stack
+Mnemonic_FPTAN,                         //Computes the tangent of the source value in the stack and pop the FP stack
+Mnemonic_FYL2X,
+Mnemonic_FYL2XP1,
+Mnemonic_F2XM1,
+Mnemonic_FPATAN,
+Mnemonic_FXCH,
+Mnemonic_FSCALE,
+
+Mnemonic_XCHG,
+Mnemonic_DIV,                           // Unsigned Divide
+Mnemonic_IDIV,                          // Signed Divide
+Mnemonic_MUL,                           // Unsigned Multiply
+Mnemonic_IMUL,                          // Signed Multiply
+Mnemonic_INC,                           // Increment by 1
+Mnemonic_INT3,                          // Call break point
+Mnemonic_Jcc,                           // Jump if Condition Is Met
+    CCM(J,O),
+    CCM(J,NO),
+    CCM(J,B), CCM(J,NAE), CCM(J,C),
+    CCM(J,NB), CCM(J,AE), CCM(J,NC),
+    CCM(J,Z), CCM(J,E),
+    CCM(J,NZ), CCM(J,NE),
+    CCM(J,BE), CCM(J,NA),
+    CCM(J,NBE), CCM(J,A),
+    CCM(J,S),
+    CCM(J,NS),
+    CCM(J,P), CCM(J,PE),
+    CCM(J,NP), CCM(J,PO),
+    CCM(J,L), CCM(J,NGE),
+    CCM(J,NL), CCM(J,GE),
+    CCM(J,LE), CCM(J,NG),
+    CCM(J,NLE), CCM(J,G),
+Mnemonic_JMP,                           // Jump
+Mnemonic_LEA,                           // Load Effective Address
+Mnemonic_LEAVE,                         // High Level Procedure Exit
+Mnemonic_LOOP,                          // Loop according to ECX counter
+Mnemonic_LOOPE,                          // Loop according to ECX counter
+Mnemonic_LOOPNE, Mnemonic_LOOPNZ = Mnemonic_LOOPNE, // Loop according to ECX
+Mnemonic_LAHF,                          // Load Flags into AH
+Mnemonic_MOV,                           // Move
+Mnemonic_MOVD,                          // Move Double word
+Mnemonic_MOVQ,                          // Move Quadword
+/*Mnemonic_MOVS,                        // Move Data from String to String*/
+// MOVS is a special case: see encoding table for more details,
+Mnemonic_MOVS8, Mnemonic_MOVS16, Mnemonic_MOVS32, Mnemonic_MOVS64,
+//
+Mnemonic_MOVAPD,                         // Move Scalar Double-Precision Floating-Point Value
+Mnemonic_MOVSD,                         // Move Scalar Double-Precision Floating-Point Value
+Mnemonic_MOVSS,                         // Move Scalar Single-Precision Floating-Point Values
+Mnemonic_MOVSX,                         // Move with Sign-Extension
+Mnemonic_MOVZX,                         // Move with Zero-Extend
+//Mnemonic_MUL,                         // Unsigned Multiply
+Mnemonic_MULSD,                         // Multiply Scalar Double-Precision Floating-Point Values
+Mnemonic_MULSS,                         // Multiply Scalar Single-Precision Floating-Point Values
+Mnemonic_NEG,                           // Two's Complement Negation
+Mnemonic_NOP,                           // No Operation
+Mnemonic_NOT,                           // One's Complement Negation
+Mnemonic_OR,                            // Logical Inclusive OR
+Mnemonic_PREFETCH,                      // prefetch
+
+#ifdef _HAVE_MMX_
+    Mnemonic_PADDQ,                     // Add Packed Quadword Integers
+    Mnemonic_PAND,                      // Logical AND
+    Mnemonic_POR,                       // Bitwise Logical OR
+    Mnemonic_PSUBQ,                     // Subtract Packed Quadword Integers
+#endif
+
+Mnemonic_PXOR,                          // Logical Exclusive OR
+Mnemonic_POP,                           // Pop a Value from the Stack
+Mnemonic_POPFD,                         // Pop a Value of EFLAGS register from the Stack
+Mnemonic_PUSH,                          // Push Word or Doubleword Onto the Stack
+Mnemonic_PUSHFD,                        // Push EFLAGS Doubleword Onto the Stack
+Mnemonic_RET,                           // Return from Procedure
+
+Mnemonic_SETcc,                         // Set Byte on Condition
+    CCM(SET,O),
+    CCM(SET,NO),
+    CCM(SET,B), CCM(SET,NAE), CCM(SET,C),
+    CCM(SET,NB), CCM(SET,AE), CCM(SET,NC),
+    CCM(SET,Z), CCM(SET,E),
+    CCM(SET,NZ), CCM(SET,NE),
+    CCM(SET,BE), CCM(SET,NA),
+    CCM(SET,NBE), CCM(SET,A),
+    CCM(SET,S),
+    CCM(SET,NS),
+    CCM(SET,P), CCM(SET,PE),
+    CCM(SET,NP), CCM(SET,PO),
+    CCM(SET,L), CCM(SET,NGE),
+    CCM(SET,NL), CCM(SET,GE),
+    CCM(SET,LE), CCM(SET,NG),
+    CCM(SET,NLE), CCM(SET,G),
+
+Mnemonic_SAL, Mnemonic_SHL=Mnemonic_SAL,// Shift left
+Mnemonic_SAR,                           // Shift right
+Mnemonic_ROR,                           // Rotate right
+Mnemonic_RCR,                           // Rotate right through CARRY flag
+Mnemonic_ROL,                           // Rotate left
+Mnemonic_RCL,                           // Rotate left through CARRY flag
+Mnemonic_SHR,                           // Unsigned shift right
+Mnemonic_SHRD,                          // Double Precision Shift Right
+Mnemonic_SHLD,                          // Double Precision Shift Left
+
+Mnemonic_SBB,                           // Integer Subtraction with Borrow
+Mnemonic_SUB,                           // Subtract
+Mnemonic_SUBSD,                         // Subtract Scalar Double-Precision Floating-Point Values
+Mnemonic_SUBSS,                         // Subtract Scalar Single-Precision Floating-Point Values
+
+Mnemonic_TEST,                          // Logical Compare
+
+Mnemonic_UCOMISD,                       // Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS
+Mnemonic_UCOMISS,                       // Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS
+
+Mnemonic_XOR,                           // Logical Exclusive OR
+//
+// packed things,
+//
+Mnemonic_XORPD,                         // Bitwise Logical XOR for Double-Precision Floating-Point Values
+Mnemonic_XORPS,                         // Bitwise Logical XOR for Single-Precision Floating-Point Values
+
+Mnemonic_CVTDQ2PD,                      // Convert Packed Doubleword Integers to Packed Double-Precision Floating-Point Values
+Mnemonic_CVTTPD2DQ,                     // Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Doubleword Integers
+
+Mnemonic_CVTDQ2PS,                      // Convert Packed Doubleword Integers to Packed Single-Precision Floating-Point Values
+Mnemonic_CVTTPS2DQ,                     // Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Doubleword Integers
+//
+// String operations
+//
+Mnemonic_STD,                           // Set direction flag
+Mnemonic_CLD,                           // Clear direction flag
+Mnemonic_SCAS,                          // Scan string
+Mnemonic_STOS,                          // Store string
+
+//
+Mnemonic_WAIT,                          // Check pending pending unmasked floating-point exception
+//
+Mnemonic_Count
+};
+
+#undef CCM
+#endif
+
+/**
+ * @brief Instruction prefixes, according to arch manual.
+ */
+typedef enum InstPrefix {
+    InstPrefix_Null = 0,
+    // Group 1
+    InstPrefix_LOCK = 0xF0,
+    InstPrefix_REPNE = 0xF2,
+    InstPrefix_REPNZ = InstPrefix_REPNE,
+    InstPrefix_REP = 0xF3, InstPrefix_REPZ = InstPrefix_REP,
+    // Group 2
+    InstPrefix_CS = 0x2E,
+    InstPrefix_SS = 0x36,
+    InstPrefix_DS = 0x3E,
+    InstPrefix_ES = 0x26,
+    InstPrefix_FS = 0x64,
+    InstPrefix_GS = 0x65,
+    //
+    InstPrefix_HintTaken = 0x3E,
+    InstPrefix_HintNotTaken = 0x2E,
+    // Group 3
+    InstPrefix_OpndSize = 0x66,
+    // Group 4
+    InstPrefix_AddrSize = 0x67
+} InstPrefix;
+
+inline unsigned getSizeBytes(OpndSize sz)
+{
+    if (sz==OpndSize_64) { return 8; }
+    if (sz==OpndSize_32) { return 4; }
+    if (sz==OpndSize_16) { return 2; }
+    if (sz==OpndSize_8)  { return 1; }
+    assert(false);
+    return 0;
+}
+
+inline bool isRegKind(OpndKind kind)
+{
+    return OpndKind_GPReg<= kind && kind<=OpndKind_MaxRegKind;
+}
+
+/**
+ * @brief Returns RegName for a given name.
+ *
+ * Name is case-insensitive.
+ * @param regname - string name of a register
+ * @return RegName for the given name, or RegName_Null if name is invalid
+ */
+RegName         getRegName(const char * regname);
+/**
+ * Constructs RegName from the given OpndKind, size and index.
+ */
+inline RegName  getRegName(OpndKind k, OpndSize s, int idx)
+{
+    return (RegName)REGNAME(k,s,idx);
+}
+/**
+ * Extracts a bit mask with a bit set at the position of the register's index.
+ */
+inline unsigned getRegMask(RegName reg)
+{
+    return 1<<(reg&0xff);
+}
+/**
+ * @brief Extracts OpndKind from the RegName.
+ */
+inline OpndKind getRegKind(RegName reg)
+{
+    return (OpndKind)(reg>>24);
+}
+/**
+ * @brief Extracts OpndSize from RegName.
+ */
+inline OpndSize getRegSize(RegName reg)
+{
+    return (OpndSize)((reg>>16)&0xFF);
+}
+/**
+ * Extracts an index from the given RegName.
+ */
+inline unsigned char getRegIndex(RegName reg)
+{
+    return (unsigned char)(reg&0xFF);
+}
+/**
+ * Returns a string name of the given RegName. The name returned is in upper-case.
+ * Returns NULL if invalid RegName specified.
+ */
+const char *    getRegNameString(RegName reg);
+/**
+ * Returns string name of a given OpndSize.
+ * Returns NULL if invalid OpndSize passed.
+ */
+const char *    getOpndSizeString(OpndSize size);
+/**
+ * Returns OpndSize passed by its string representation (case insensitive).
+ * Returns OpndSize_Null if invalid string specified.
+ * The 'sizeString' can not be NULL.
+ */
+OpndSize        getOpndSize(const char * sizeString);
+/**
+ * Returns string name of a given OpndKind.
+ * Returns NULL if the passed kind is invalid.
+ */
+const char *    getOpndKindString(OpndKind kind);
+/**
+ * Returns OpndKind found by its string representation (case insensitive).
+ * Returns OpndKind_Null if the name is invalid.
+ * The 'kindString' can not be NULL.
+ */
+OpndKind        getOpndKind(const char * kindString);
+/**
+ *
+ */
+const char *    getConditionString(ConditionMnemonic cm);
+
+/**
+ * Constructs an RegName with the same index and kind, but with a different size from
+ * the given RegName (i.e. getRegAlias(EAX, OpndSize_16) => AX; getRegAlias(BL, OpndSize_32) => EBX).
+ * The constructed RegName is not checked in any way and thus may be invalid.
+ * Note, that the aliasing does not work for at least AH,BH,CH,DH, ESI, EDI, ESP and EBP regs.
+ */
+inline RegName getAliasReg(RegName reg, OpndSize sz)
+{
+    return (RegName)REGNAME(getRegKind(reg), sz, getRegIndex(reg));
+}
+
+/**
+ * brief Tests two RegName-s of the same kind for equality.
+ *
+ * @note Does work for 8 bit general purpose registers (AH, AL, BH, BL, etc).
+ */
+inline bool equals(RegName r0, RegName r1)
+{
+    return getRegKind(r0) == getRegKind(r1) &&
+           getRegIndex(r0) == getRegIndex(r1);
+}
+
+ENCODER_NAMESPACE_END
+
+#endif  // ifndef _ENCODER_DEFS_H_
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_defs_ext.h b/vm/compiler/codegen/x86/lightcg/libenc/enc_defs_ext.h
new file mode 100644
index 0000000..acc61d9
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_defs_ext.h
@@ -0,0 +1,346 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef _ENCODER_DEFS_EXT_H_
+#define _ENCODER_DEFS_EXT_H_
+
+
+// Used to isolate experimental or being tuned encoder into a separate
+// namespace so it can coexist with a stable one in the same bundle.
+#ifdef ENCODER_ISOLATE
+    #define ENCODER_NAMESPACE_START namespace enc_ia32 {
+    #define ENCODER_NAMESPACE_END };
+#else
+    #define ENCODER_NAMESPACE_START
+    #define ENCODER_NAMESPACE_END
+#endif
+
+ENCODER_NAMESPACE_START
+typedef enum OpndSize {
+    /**
+     * A change must be balanced with at least the following places:
+     *              Ia32IRConstants.h :: getByteSize() uses some presumptions about OpndSize_ values
+     *              Ia32::Constraint-s use the OpndSize as a mask
+     *              encoder.cpp & encoder_master_info.cpp uses OpndSize as an index for hashing
+     *              - perhaps there are much more places
+     */
+    OpndSize_Null           = 0,
+    OpndSize_8             = 0x01,
+    OpndSize_16            = 0x02,
+    OpndSize_32            = 0x04,
+    OpndSize_64            = 0x08,
+#if !defined(TESTING_ENCODER)
+    OpndSize_80            = 0x10,
+    OpndSize_128           = 0x20,
+#endif
+    OpndSize_Max,
+    OpndSize_Any            = 0x3F,
+    OpndSize_Default        = OpndSize_Any
+} OpndSize;
+
+/**
+ * Conditional mnemonics.
+ * The values match the 'real' (==processor's) values of the appropriate
+ * condition values used in the opcodes.
+ */
+typedef enum ConditionMnemonic {
+
+    ConditionMnemonic_O=0,
+    ConditionMnemonic_NO=1,
+    ConditionMnemonic_B=2, ConditionMnemonic_NAE=ConditionMnemonic_B, ConditionMnemonic_C=ConditionMnemonic_B,
+    ConditionMnemonic_NB=3, ConditionMnemonic_AE=ConditionMnemonic_NB, ConditionMnemonic_NC=ConditionMnemonic_NB,
+    ConditionMnemonic_Z=4, ConditionMnemonic_E=ConditionMnemonic_Z,
+    ConditionMnemonic_NZ=5, ConditionMnemonic_NE=ConditionMnemonic_NZ,
+    ConditionMnemonic_BE=6, ConditionMnemonic_NA=ConditionMnemonic_BE,
+    ConditionMnemonic_NBE=7, ConditionMnemonic_A=ConditionMnemonic_NBE,
+
+    ConditionMnemonic_S=8,
+    ConditionMnemonic_NS=9,
+    ConditionMnemonic_P=10, ConditionMnemonic_PE=ConditionMnemonic_P,
+    ConditionMnemonic_NP=11, ConditionMnemonic_PO=ConditionMnemonic_NP,
+    ConditionMnemonic_L=12, ConditionMnemonic_NGE=ConditionMnemonic_L,
+    ConditionMnemonic_NL=13, ConditionMnemonic_GE=ConditionMnemonic_NL,
+    ConditionMnemonic_LE=14, ConditionMnemonic_NG=ConditionMnemonic_LE,
+    ConditionMnemonic_NLE=15, ConditionMnemonic_G=ConditionMnemonic_NLE,
+    ConditionMnemonic_Count=16
+} ConditionMnemonic;
+
+
+#define CCM(prefix,cond) Mnemonic_##prefix##cond=Mnemonic_##prefix##cc+ConditionMnemonic_##cond
+
+//=========================================================================================================
+typedef enum Mnemonic {
+
+Mnemonic_NULL=0, Mnemonic_Null=Mnemonic_NULL,
+Mnemonic_JMP,                           // Jump
+Mnemonic_MOV,                           // Move
+Mnemonic_Jcc,                           // Jump if Condition Is Met
+    CCM(J,O),
+    CCM(J,NO),
+    CCM(J,B), CCM(J,NAE), CCM(J,C),
+    CCM(J,NB), CCM(J,AE), CCM(J,NC),
+    CCM(J,Z), CCM(J,E),
+    CCM(J,NZ), CCM(J,NE),
+    CCM(J,BE), CCM(J,NA),
+    CCM(J,NBE), CCM(J,A),
+    CCM(J,S),
+    CCM(J,NS),
+    CCM(J,P), CCM(J,PE),
+    CCM(J,NP), CCM(J,PO),
+    CCM(J,L), CCM(J,NGE),
+    CCM(J,NL), CCM(J,GE),
+    CCM(J,LE), CCM(J,NG),
+    CCM(J,NLE), CCM(J,G),
+Mnemonic_CALL,                          // Call Procedure
+
+Mnemonic_ADC,                           // Add with Carry
+Mnemonic_ADD,                           // Add
+Mnemonic_ADDSD,                         // Add Scalar Double-Precision Floating-Point Values
+Mnemonic_ADDSS,                         // Add Scalar Single-Precision Floating-Point Values
+Mnemonic_AND,                           // Logical AND
+
+Mnemonic_BSF,                           // Bit scan forward
+Mnemonic_BSR,                           // Bit scan reverse
+
+Mnemonic_CMC,                           // Complement Carry Flag
+Mnemonic_CWD, Mnemonic_CDQ=Mnemonic_CWD,// Convert Word to Doubleword/Convert Doubleword to Qua T dword
+Mnemonic_CMOVcc,                        // Conditional Move
+    CCM(CMOV,O),
+    CCM(CMOV,NO),
+    CCM(CMOV,B), CCM(CMOV,NAE), CCM(CMOV,C),
+    CCM(CMOV,NB), CCM(CMOV,AE), CCM(CMOV,NC),
+    CCM(CMOV,Z), CCM(CMOV,E),
+    CCM(CMOV,NZ), CCM(CMOV,NE),
+    CCM(CMOV,BE), CCM(CMOV,NA),
+    CCM(CMOV,NBE), CCM(CMOV,A),
+
+    CCM(CMOV,S),
+    CCM(CMOV,NS),
+    CCM(CMOV,P), CCM(CMOV,PE),
+    CCM(CMOV,NP), CCM(CMOV,PO),
+    CCM(CMOV,L), CCM(CMOV,NGE),
+    CCM(CMOV,NL), CCM(CMOV,GE),
+    CCM(CMOV,LE), CCM(CMOV,NG),
+    CCM(CMOV,NLE), CCM(CMOV,G),
+
+Mnemonic_CMP,                           // Compare Two Operands
+Mnemonic_CMPXCHG,                       // Compare and exchange
+Mnemonic_CMPXCHG8B,                     // Compare and Exchange 8 Bytes
+Mnemonic_CMPSB,                         // Compare Two Bytes at DS:ESI and ES:EDI
+Mnemonic_CMPSW,                         // Compare Two Words at DS:ESI and ES:EDI
+Mnemonic_CMPSD,                        // Compare Two Doublewords at DS:ESI and ES:EDI
+//
+// double -> float
+Mnemonic_CVTSD2SS,                      // Convert Scalar Double-Precision Floating-Point Value to Scalar Single-Precision Floating-Point Value
+// double -> I_32
+Mnemonic_CVTSD2SI,                      // Convert Scalar Double-Precision Floating-Point Value to Doubleword Integer
+// double [truncated] -> I_32
+Mnemonic_CVTTSD2SI,                     // Convert with Truncation Scalar Double-Precision Floating-Point Value to Signed Doubleword Integer
+//
+// float -> double
+Mnemonic_CVTSS2SD,                      // Convert Scalar Single-Precision Floating-Point Value to Scalar Double-Precision Floating-Point Value
+// float -> I_32
+Mnemonic_CVTSS2SI,                      // Convert Scalar Single-Precision Floating-Point Value to Doubleword Integer
+// float [truncated] -> I_32
+Mnemonic_CVTTSS2SI,                     // Convert with Truncation Scalar Single-Precision Floating-Point Value to Doubleword Integer
+//
+// I_32 -> double
+Mnemonic_CVTSI2SD,                      // Convert Doubleword Integer to Scalar Double-Precision Floating-Point Value
+// I_32 -> float
+Mnemonic_CVTSI2SS,                      // Convert Doubleword Integer to Scalar Single-Precision Floating-Point Value
+
+Mnemonic_COMISD,                        // Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS
+Mnemonic_COMISS,                        // Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS
+Mnemonic_DEC,                           // Decrement by 1
+//Mnemonic_DIV,                         // Unsigned Divide
+Mnemonic_DIVSD,                         // Divide Scalar Double-Precision Floating-Point Values
+Mnemonic_DIVSS,                         // Divide Scalar Single-Precision Floating-Point Values
+
+#ifdef _HAVE_MMX_
+Mnemonic_EMMS,                          // Empty MMX Technology State
+#endif
+
+Mnemonic_ENTER,                         // ENTER-Make Stack Frame for Procedure Parameters
+Mnemonic_FLDCW,                         // Load FPU control word
+Mnemonic_FADDP,
+Mnemonic_FLDZ,
+Mnemonic_FADD,
+Mnemonic_FSUBP,
+Mnemonic_FSUB,
+Mnemonic_FISUB,
+Mnemonic_FMUL,
+Mnemonic_FMULP,
+Mnemonic_FDIVP,
+Mnemonic_FDIV,
+Mnemonic_FUCOM,
+Mnemonic_FUCOMI,
+Mnemonic_FUCOMP,
+Mnemonic_FUCOMIP,
+Mnemonic_FUCOMPP,
+Mnemonic_FRNDINT,
+Mnemonic_FNSTCW,                        // Store FPU control word
+Mnemonic_FSTSW,                         // Store FPU status word
+Mnemonic_FNSTSW,                         // Store FPU status word
+//Mnemonic_FDECSTP,                     // Decrement Stack-Top Pointer
+Mnemonic_FILD,                          // Load Integer
+Mnemonic_FLD,                           // Load Floating Point Value
+Mnemonic_FLDLG2,
+Mnemonic_FLDLN2,
+Mnemonic_FLD1,
+
+Mnemonic_FCLEX,                         // Clear Exceptions
+Mnemonic_FCHS,                          // Change sign of ST0
+Mnemonic_FNCLEX,                        // Clear Exceptions
+
+//Mnemonic_FINCSTP,                     // Increment Stack-Top Pointer
+Mnemonic_FIST,                          // Store Integer
+Mnemonic_FISTP,                         // Store Integer, pop FPU stack
+Mnemonic_FISTTP,                        // Store Integer with Truncation
+Mnemonic_FPREM,                         // Partial Remainder
+Mnemonic_FPREM1,                        // Partial Remainder
+Mnemonic_FST,                           // Store Floating Point Value
+Mnemonic_FSTP,                          // Store Floating Point Value and pop the FP stack
+Mnemonic_FSQRT,                         //Computes the square root of the source value in the stack and pop the FP stack
+Mnemonic_FABS,                          //Computes the absolute value of the source value in the stack and pop the FP stack
+Mnemonic_FSIN,                          //Computes the sine of the source value in the stack and pop the FP stack
+Mnemonic_FCOS,                          //Computes the cosine of the source value in the stack and pop the FP stack
+Mnemonic_FPTAN,                         //Computes the tangent of the source value in the stack and pop the FP stack
+Mnemonic_FYL2X,
+Mnemonic_FYL2XP1,
+Mnemonic_F2XM1,
+Mnemonic_FPATAN,
+Mnemonic_FXCH,
+Mnemonic_FSCALE,
+
+Mnemonic_XCHG,
+Mnemonic_DIV,                           // Unsigned Divide
+Mnemonic_IDIV,                          // Signed Divide
+Mnemonic_MUL,                           // Unsigned Multiply
+Mnemonic_IMUL,                          // Signed Multiply
+Mnemonic_INC,                           // Increment by 1
+Mnemonic_INT3,                          // Call break point
+
+Mnemonic_LEA,                           // Load Effective Address
+Mnemonic_LEAVE,                         // High Level Procedure Exit
+Mnemonic_LOOP,                          // Loop according to ECX counter
+Mnemonic_LOOPE,                          // Loop according to ECX counter
+Mnemonic_LOOPNE, Mnemonic_LOOPNZ = Mnemonic_LOOPNE, // Loop according to ECX
+Mnemonic_LAHF,                          // Load Flags into AH
+Mnemonic_MOVD,                          // Move Double word
+Mnemonic_MOVQ,                          // Move Quadword
+/*Mnemonic_MOVS,                        // Move Data from String to String*/
+// MOVS is a special case: see encoding table for more details,
+Mnemonic_MOVS8, Mnemonic_MOVS16, Mnemonic_MOVS32, Mnemonic_MOVS64,
+//
+Mnemonic_MOVAPD,                         // Move Scalar Double-Precision Floating-Point Value
+Mnemonic_MOVSD,                         // Move Scalar Double-Precision Floating-Point Value
+Mnemonic_MOVSS,                         // Move Scalar Single-Precision Floating-Point Values
+Mnemonic_MOVSX,                         // Move with Sign-Extension
+Mnemonic_MOVZX,                         // Move with Zero-Extend
+//Mnemonic_MUL,                         // Unsigned Multiply
+Mnemonic_MULSD,                         // Multiply Scalar Double-Precision Floating-Point Values
+Mnemonic_MULSS,                         // Multiply Scalar Single-Precision Floating-Point Values
+Mnemonic_NEG,                           // Two's Complement Negation
+Mnemonic_NOP,                           // No Operation
+Mnemonic_NOT,                           // One's Complement Negation
+Mnemonic_OR,                            // Logical Inclusive OR
+Mnemonic_PREFETCH,                      // prefetch
+
+#if 1 //def _HAVE_MMX_
+    Mnemonic_PADDQ,                     // Add Packed Quadword Integers
+    Mnemonic_PAND,                      // Logical AND
+    Mnemonic_POR,                       // Bitwise Logical OR
+    Mnemonic_PSUBQ,                     // Subtract Packed Quadword Integers
+#endif
+Mnemonic_PANDN,
+Mnemonic_PSLLQ,
+Mnemonic_PSRLQ,
+Mnemonic_PXOR,                          // Logical Exclusive OR
+Mnemonic_POP,                           // Pop a Value from the Stack
+Mnemonic_POPFD,                         // Pop a Value of EFLAGS register from the Stack
+Mnemonic_PUSH,                          // Push Word or Doubleword Onto the Stack
+Mnemonic_PUSHFD,                        // Push EFLAGS Doubleword Onto the Stack
+Mnemonic_RET,                           // Return from Procedure
+
+Mnemonic_SETcc,                         // Set Byte on Condition
+    CCM(SET,O),
+    CCM(SET,NO),
+    CCM(SET,B), CCM(SET,NAE), CCM(SET,C),
+    CCM(SET,NB), CCM(SET,AE), CCM(SET,NC),
+    CCM(SET,Z), CCM(SET,E),
+    CCM(SET,NZ), CCM(SET,NE),
+    CCM(SET,BE), CCM(SET,NA),
+    CCM(SET,NBE), CCM(SET,A),
+    CCM(SET,S),
+    CCM(SET,NS),
+    CCM(SET,P), CCM(SET,PE),
+    CCM(SET,NP), CCM(SET,PO),
+    CCM(SET,L), CCM(SET,NGE),
+    CCM(SET,NL), CCM(SET,GE),
+    CCM(SET,LE), CCM(SET,NG),
+    CCM(SET,NLE), CCM(SET,G),
+
+Mnemonic_SAL, Mnemonic_SHL=Mnemonic_SAL,// Shift left
+Mnemonic_SAR,                           // Unsigned shift right
+Mnemonic_ROR,                           // Rotate right
+Mnemonic_RCR,                           // Rotate right through CARRY flag
+Mnemonic_ROL,                           // Rotate left
+Mnemonic_RCL,                           // Rotate left through CARRY flag
+Mnemonic_SHR,                           // Signed shift right
+Mnemonic_SHRD,                          // Double Precision Shift Right
+Mnemonic_SHLD,                          // Double Precision Shift Left
+
+Mnemonic_SBB,                           // Integer Subtraction with Borrow
+Mnemonic_SUB,                           // Subtract
+Mnemonic_SUBSD,                         // Subtract Scalar Double-Precision Floating-Point Values
+Mnemonic_SUBSS,                         // Subtract Scalar Single-Precision Floating-Point Values
+
+Mnemonic_TEST,                          // Logical Compare
+
+Mnemonic_UCOMISD,                       // Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS
+Mnemonic_UCOMISS,                       // Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS
+
+Mnemonic_XOR,                           // Logical Exclusive OR
+//
+// packed things,
+//
+Mnemonic_XORPD,                         // Bitwise Logical XOR for Double-Precision Floating-Point Values
+Mnemonic_XORPS,                         // Bitwise Logical XOR for Single-Precision Floating-Point Values
+
+Mnemonic_CVTDQ2PD,                      // Convert Packed Doubleword Integers to Packed Double-Precision Floating-Point Values
+Mnemonic_CVTTPD2DQ,                     // Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Doubleword Integers
+
+Mnemonic_CVTDQ2PS,                      // Convert Packed Doubleword Integers to Packed Single-Precision Floating-Point Values
+Mnemonic_CVTTPS2DQ,                     // Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Doubleword Integers
+//
+// String operations
+//
+Mnemonic_STD,                           // Set direction flag
+Mnemonic_CLD,                           // Clear direction flag
+Mnemonic_SCAS,                          // Scan string
+Mnemonic_STOS,                          // Store string
+
+//
+Mnemonic_WAIT,                          // Check pending pending unmasked floating-point exception
+//
+Mnemonic_Count
+} Mnemonic;
+
+#undef CCM
+
+ENCODER_NAMESPACE_END
+
+#endif  // ifndef _ENCODER_DEFS_EXT_H_
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_prvt.h b/vm/compiler/codegen/x86/lightcg/libenc/enc_prvt.h
new file mode 100644
index 0000000..343b161
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_prvt.h
@@ -0,0 +1,382 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+#ifndef __ENC_PRVT_H_INCLUDED__
+#define __ENC_PRVT_H_INCLUDED__
+
+#include "enc_base.h"
+
+ENCODER_NAMESPACE_START
+/*
+ * @file
+ * @brief Contains some definitions/constants and other stuff used by the
+ *        Encoder internally.
+ */
+
+enum OpcodeByteKind {
+    //OpcodeByteKind_Opcode = 0x0000,
+    OpcodeByteKind_ZeroOpcodeByte           = 0x0100,
+    //
+    // The names _SlashR,  _SlahsNum, _ib, _iw, etc
+    // represent the appropriate abbreviations used
+    // in the mnemonic descriptions in the Intel's arch manual.
+    //
+    OpcodeByteKind_SlashR                   = 0x0200,
+    OpcodeByteKind_SlashNum                 = 0x0300,
+    OpcodeByteKind_ib                       = 0x0400,
+    OpcodeByteKind_iw                       = 0x0500,
+    OpcodeByteKind_id                       = 0x0600,
+#ifdef _EM64T_
+    OpcodeByteKind_io                       = 0x0700,
+#endif
+    OpcodeByteKind_cb                       = 0x0800,
+    OpcodeByteKind_cw                       = 0x0900,
+    OpcodeByteKind_cd                       = 0x0A00,
+    //OpcodeByteKind_cp                     = 0x0B00,
+    //OpcodeByteKind_co                     = 0x0C00,
+    //OpcodeByteKind_ct                     = 0x0D00,
+
+    OpcodeByteKind_rb                       = 0x0E00,
+    OpcodeByteKind_rw                       = 0x0F00,
+    OpcodeByteKind_rd                       = 0x1000,
+#ifdef _EM64T_
+    OpcodeByteKind_ro                       = 0x1100,
+    //OpcodeByteKind_REX                    = 0x1200,
+    OpcodeByteKind_REX_W                    = 0x1300,
+#endif
+    OpcodeByteKind_plus_i                   = 0x1400,
+    /**
+        * a special marker, means 'no opcode on the given position'
+        * used in opcodes array, to specify the empty slot, say
+        * to fill an em64t-specific opcode on ia32.
+        * last 'e' made lowercase to avoid a mess with 'F' in
+        * OpcodeByteKind_LAST .
+        */
+    OpcodeByteKind_EMPTY                    = 0xFFFE,
+    /**
+        * a special marker, means 'no more opcodes in the array'
+        * used in in opcodes array to show that there are no more
+        * opcodes in the array for a given mnemonic.
+        */
+    OpcodeByteKind_LAST                     = 0xFFFF,
+    /**
+        * a mask to extract the OpcodeByteKind
+        */
+    OpcodeByteKind_KindMask                 = 0xFF00,
+    /**
+        * a mask to extract the opcode byte when presented
+        */
+    OpcodeByteKind_OpcodeMask               = 0x00FF
+};
+
+#ifdef USE_ENCODER_DEFINES
+
+#define N           {0, 0, 0, 0 }
+#define U           {1, 0, 1, OpndRole_Use }
+#define D           {1, 1, 0, OpndRole_Def }
+#define DU          {1, 1, 1, OpndRole_Def|OpndRole_Use }
+
+#define U_U         {2, 0, 2, OpndRole_Use<<2 | OpndRole_Use }
+#define D_U         {2, 1, 1, OpndRole_Def<<2 | OpndRole_Use }
+#define D_DU        {2, 2, 1, OpndRole_Def<<2 | (OpndRole_Def|OpndRole_Use) }
+#define DU_U        {2, 1, 2, ((OpndRole_Def|OpndRole_Use)<<2 | OpndRole_Use) }
+#define DU_DU       {2, 2, 2, ((OpndRole_Def|OpndRole_Use)<<2 | (OpndRole_Def|OpndRole_Use)) }
+
+#define DU_DU_DU    {3, 3, 3, ((OpndRole_Def|OpndRole_Use)<<4) | ((OpndRole_Def|OpndRole_Use)<<2) | (OpndRole_Def|OpndRole_Use) }
+#define DU_DU_U     {3, 2, 3, (((OpndRole_Def|OpndRole_Use)<<4) | ((OpndRole_Def|OpndRole_Use)<<2) | OpndRole_Use) }
+#define D_DU_U      {3, 2, 2, (((OpndRole_Def)<<4) | ((OpndRole_Def|OpndRole_Use)<<2) | OpndRole_Use) }
+#define D_U_U       {3, 1, 2, (((OpndRole_Def)<<4) | ((OpndRole_Use)<<2) | OpndRole_Use) }
+
+// Special encoding of 0x00 opcode byte. Note: it's all O-s, not zeros.
+#define OxOO        OpcodeByteKind_ZeroOpcodeByte
+
+#define Size16      InstPrefix_OpndSize
+
+#define _r          OpcodeByteKind_SlashR
+
+#define _0          OpcodeByteKind_SlashNum|0
+#define _1          OpcodeByteKind_SlashNum|1
+#define _2          OpcodeByteKind_SlashNum|2
+#define _3          OpcodeByteKind_SlashNum|3
+#define _4          OpcodeByteKind_SlashNum|4
+#define _5          OpcodeByteKind_SlashNum|5
+#define _6          OpcodeByteKind_SlashNum|6
+#define _7          OpcodeByteKind_SlashNum|7
+
+// '+i' for floating-point instructions
+#define _i          OpcodeByteKind_plus_i
+
+
+#define ib          OpcodeByteKind_ib
+#define iw          OpcodeByteKind_iw
+#define id          OpcodeByteKind_id
+
+#define cb          OpcodeByteKind_cb
+#define cw          OpcodeByteKind_cw
+#define cd          OpcodeByteKind_cd
+
+#define rb          OpcodeByteKind_rb
+#define rw          OpcodeByteKind_rw
+#define rd          OpcodeByteKind_rd
+
+#define AL          {OpndKind_GPReg, OpndSize_8, OpndExt_Any, RegName_AL}
+#define AH          {OpndKind_GPReg, OpndSize_8, OpndExt_Any, RegName_AH}
+#define AX          {OpndKind_GPReg, OpndSize_16, OpndExt_Any, RegName_AX}
+#define EAX         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_EAX}
+#ifdef _EM64T_
+    #define RAX     {OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RAX }
+#endif
+
+#define CL          {OpndKind_GPReg, OpndSize_8, OpndExt_Any, RegName_CL}
+#define ECX         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_ECX}
+#ifdef _EM64T_
+    #define RCX         {OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RCX}
+#endif
+
+#define DX          {OpndKind_GPReg, OpndSize_16, OpndExt_Any, RegName_DX}
+#define EDX         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_EDX}
+#ifdef _EM64T_
+    #define RDX     { OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RDX }
+#endif
+
+#define ESI         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_ESI}
+#ifdef _EM64T_
+    #define RSI     { OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RSI }
+#endif
+
+#define EDI         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_EDI}
+#ifdef _EM64T_
+    #define RDI     { OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_RDI }
+#endif
+
+#define r8          {OpndKind_GPReg, OpndSize_8, OpndExt_Any, RegName_Null}
+#define r16         {OpndKind_GPReg, OpndSize_16, OpndExt_Any, RegName_Null}
+#define r32         {OpndKind_GPReg, OpndSize_32, OpndExt_Any, RegName_Null}
+#ifdef _EM64T_
+    #define r64     { OpndKind_GPReg, OpndSize_64, OpndExt_Any, RegName_Null }
+#endif
+
+#define r_m8        {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_8, OpndExt_Any, RegName_Null}
+#define r_m16       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_16, OpndExt_Any, RegName_Null}
+#define r_m32       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_32, OpndExt_Any, RegName_Null}
+
+#define r_m8s        {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_8, OpndExt_Signed, RegName_Null}
+#define r_m16s       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_16, OpndExt_Signed, RegName_Null}
+#define r_m32s       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_32, OpndExt_Signed, RegName_Null}
+
+#define r_m8u        {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_8, OpndExt_Zero, RegName_Null}
+#define r_m16u       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_16, OpndExt_Zero, RegName_Null}
+#define r_m32u       {(OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_32, OpndExt_Zero, RegName_Null}
+
+//'m' was only used in LEA mnemonic, but is replaced with
+// set of exact sizes. See more comments for LEA instruction in TheTable.
+//#define m           {OpndKind_Mem, OpndSize_Null, RegName_Null}
+#define m8          {OpndKind_Mem, OpndSize_8, OpndExt_Any, RegName_Null}
+#define m16         {OpndKind_Mem, OpndSize_16, OpndExt_Any, RegName_Null}
+#define m32         {OpndKind_Mem, OpndSize_32, OpndExt_Any, RegName_Null}
+#define m64         {OpndKind_Mem, OpndSize_64, OpndExt_Any, RegName_Null}
+#ifdef _EM64T_
+    #define r_m64   { (OpndKind)(OpndKind_GPReg|OpndKind_Mem), OpndSize_64, OpndExt_Any, RegName_Null }
+#endif
+
+#define imm8        {OpndKind_Imm, OpndSize_8, OpndExt_Any, RegName_Null}
+#define imm16       {OpndKind_Imm, OpndSize_16, OpndExt_Any, RegName_Null}
+#define imm32       {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
+
+#define imm8s        {OpndKind_Imm, OpndSize_8, OpndExt_Signed, RegName_Null}
+#define imm16s       {OpndKind_Imm, OpndSize_16, OpndExt_Signed, RegName_Null}
+#define imm32s       {OpndKind_Imm, OpndSize_32, OpndExt_Signed, RegName_Null}
+
+#define imm8u        {OpndKind_Imm, OpndSize_8, OpndExt_Zero, RegName_Null}
+#define imm16u       {OpndKind_Imm, OpndSize_16, OpndExt_Zero, RegName_Null}
+#define imm32u       {OpndKind_Imm, OpndSize_32, OpndExt_Zero, RegName_Null}
+
+#ifdef _EM64T_
+    #define imm64   {OpndKind_Imm, OpndSize_64, OpndExt_Any, RegName_Null }
+#endif
+
+//FIXME: moff-s are in fact memory refs, but presented as immediate.
+// Need to specify this in OpndDesc.
+#define moff8        {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
+#define moff16       {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
+#define moff32       {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
+#ifdef _EM64T_
+    #define moff64       {OpndKind_Imm, OpndSize_64, OpndExt_Any, RegName_Null}
+#endif
+
+
+#define rel8        {OpndKind_Imm, OpndSize_8, OpndExt_Any, RegName_Null}
+#define rel16       {OpndKind_Imm, OpndSize_16, OpndExt_Any, RegName_Null}
+#define rel32       {OpndKind_Imm, OpndSize_32, OpndExt_Any, RegName_Null}
+
+#define mm64        {OpndKind_MMXReg, OpndSize_64, OpndExt_Any, RegName_Null}
+#define mm_m64      {(OpndKind)(OpndKind_MMXReg|OpndKind_Mem), OpndSize_64, OpndExt_Any, RegName_Null}
+
+#define xmm64       {OpndKind_XMMReg, OpndSize_64, OpndExt_Any, RegName_Null}
+#define xmm_m64     {(OpndKind)(OpndKind_XMMReg|OpndKind_Mem), OpndSize_64, OpndExt_Any, RegName_Null}
+
+#define xmm32       {OpndKind_XMMReg, OpndSize_32, OpndExt_Any, RegName_Null}
+#define xmm_m32     {(OpndKind)(OpndKind_XMMReg|OpndKind_Mem), OpndSize_32, OpndExt_Any, RegName_Null}
+
+#define FP0S        {OpndKind_FPReg, OpndSize_32, OpndExt_Any, RegName_FP0S}
+#define FP0D        {OpndKind_FPReg, OpndSize_64, OpndExt_Any, RegName_FP0D}
+#define FP1S        {OpndKind_FPReg, OpndSize_32, OpndExt_Any, RegName_FP1S}
+#define FP1D        {OpndKind_FPReg, OpndSize_64, OpndExt_Any, RegName_FP1D}
+#define fp32        {OpndKind_FPReg, OpndSize_32, OpndExt_Any, RegName_Null}
+#define fp64        {OpndKind_FPReg, OpndSize_64, OpndExt_Any, RegName_Null}
+
+#ifdef _EM64T_
+    #define io      OpcodeByteKind_io
+    #define REX_W   OpcodeByteKind_REX_W
+
+#endif
+
+#endif // USE_ENCODER_DEFINES
+
+/**
+ * @brief Represents the REX part of instruction.
+ */
+struct  Rex {
+    unsigned char b : 1;
+    unsigned char x : 1;
+    unsigned char r : 1;
+    unsigned char w : 1;
+    unsigned char dummy : 4;        // must be '0100'b
+    unsigned int  :24;
+};
+
+/**
+ * @brief Describes SIB (scale,index,base) byte.
+ */
+struct SIB {
+    unsigned char base:3;
+    unsigned char index:3;
+    unsigned char scale:2;
+    unsigned int  padding:24;
+};
+/**
+ * @brief Describes ModRM byte.
+ */
+struct ModRM
+{
+    unsigned char rm:3;
+    unsigned char reg:3;
+    unsigned char mod:2;
+    unsigned int  padding:24;
+};
+
+
+
+/**
+* exactly the same as EncoderBase::OpcodeDesc, but also holds info about
+* platform on which the opcode is applicable.
+*/
+struct OpcodeInfo {
+    enum platform {
+        /// an opcode is valid on all platforms
+        all,
+        // opcode is valid on IA-32 only
+        em64t,
+        // opcode is valid on Intel64 only
+        ia32,
+        // opcode is added for the sake of disassembling, should not be used in encoding
+        decoder,
+        // only appears in master table, replaced with 'decoder' in hashed version
+        decoder32,
+        // only appears in master table, replaced with 'decoder' in hashed version
+        decoder64,
+    };
+    platform                        platf;
+    unsigned                        opcode[4+1+1];
+    EncoderBase::OpndDesc           opnds[EncoderBase::MAX_NUM_OPCODE_OPERANDS];
+    EncoderBase::OpndRolesDesc      roles;
+};
+
+/**
+ * @defgroup MF_ Mnemonic flags
+*/
+
+    /**
+ * Operation has no special properties.
+    */
+#define MF_NONE             (0x00000000)
+    /**
+ * Operation affects flags
+    */
+#define MF_AFFECTS_FLAGS    (0x00000001)
+    /**
+ * Operation uses flags - conditional operations, ADC/SBB/ETC
+    */
+#define MF_USES_FLAGS       (0x00000002)
+    /**
+ * Operation is conditional - MOVcc/SETcc/Jcc/ETC
+    */
+#define MF_CONDITIONAL      (0x00000004)
+/**
+ * Operation is symmetric - its args can be swapped (ADD/MUL/etc).
+ */
+#define MF_SYMMETRIC        (0x00000008)
+/**
+ * Operation is XOR-like - XOR, SUB - operations of 'arg,arg' is pure def,
+ * without use.
+ */
+#define MF_SAME_ARG_NO_USE  (0x00000010)
+
+///@} // ~MNF
+
+/**
+ * @see same structure as EncoderBase::MnemonicDesc, but carries
+ * MnemonicInfo::OpcodeInfo[] instead of OpcodeDesc[].
+ * Only used during prebuilding the encoding tables, thus it's hidden under
+ * the appropriate define.
+ */
+struct MnemonicInfo {
+    /**
+    * The mnemonic itself
+    */
+    Mnemonic    mn;
+    /**
+     * Various characteristics of mnemonic.
+     * @see MF_
+     */
+    unsigned    flags;
+    /**
+     * Number of args/des/uses/roles for the operation. For the operations
+     * which may use different number of operands (i.e. IMUL/SHL) use the
+     * most common value, or leave '0' if you are sure this info is not
+     * required.
+     */
+    EncoderBase::OpndRolesDesc              roles;
+    /**
+     * Print name of the mnemonic
+     */
+    const char *                            name;
+    /**
+     * Array of opcodes.
+     * The terminating opcode description always have OpcodeByteKind_LAST
+     * at the opcodes[i].opcode[0].
+     * The size of '25' has nothing behind it, just counted the max
+     * number of opcodes currently used (MOV instruction).
+     */
+    OpcodeInfo                              opcodes[25];
+};
+
+ENCODER_NAMESPACE_END
+
+#endif  // ~__ENC_PRVT_H_INCLUDED__
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_tabl.cpp b/vm/compiler/codegen/x86/lightcg/libenc/enc_tabl.cpp
new file mode 100644
index 0000000..af20bd8
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_tabl.cpp
@@ -0,0 +1,1969 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+
+
+#include <assert.h>
+#include <stdio.h>
+#include <stdlib.h> //qsort
+#include <string.h>
+#include <memory.h>
+#include <errno.h>
+#include <stdlib.h>
+
+
+// need to use EM64T-specifics - new registers, defines from enc_prvt, etc...
+#if !defined(_EM64T_)
+    #define UNDEF_EM64T
+    #define _EM64T_
+#endif
+
+#define USE_ENCODER_DEFINES
+#include "enc_prvt.h"
+#include "enc_defs.h"
+
+#ifdef UNDEF_EM64T
+    #undef _EM64T_
+#endif
+
+//Android x86
+#if 0 //!defined(_HAVE_MMX_)
+    #define Mnemonic_PADDQ  Mnemonic_Null
+    #define Mnemonic_PAND   Mnemonic_Null
+    #define Mnemonic_POR    Mnemonic_Null
+    #define Mnemonic_PSUBQ  Mnemonic_Null
+#endif
+
+ENCODER_NAMESPACE_START
+
+
+EncoderBase::MnemonicDesc EncoderBase::mnemonics[Mnemonic_Count];
+EncoderBase::OpcodeDesc EncoderBase::opcodes[Mnemonic_Count][MAX_OPCODES];
+unsigned char EncoderBase::opcodesHashMap[Mnemonic_Count][HASH_MAX];
+
+
+/**
+ * @file
+ * @brief 'Master' copy of encoding data.
+ */
+
+/*
+This file contains a 'master copy' of encoding table - this is the info used
+by both generator of native instructions (EncoderBase class) and by
+disassembling routines. The first one uses an info how to encode the
+instruction, and the second does an opposite - several separate tables are
+built at runtime from this main table.
+
+=============================================================================
+
+The table was designed for easy support and maintenance. Thus, it was made as
+much close as possible to the Intel's IA32 Architecture Manual descriptions.
+The info is based on the latest (at the moment of writing) revision which is
+June 2005, order number 253666-016.
+
+Normally, almost all of opcodes in the 'master' table represented exactly as
+they are shown in the Intel's Architecture manual (well, with slashes
+replaced with underscore). There are several exclusions especially marked.
+
+Normally, to add an opcode/instruction, one only need to copy the whole
+string from the manual, and simply replace '/' with '_'.
+
+I.e., TheManual reads for DEC:
+    (1)     FE /1 DEC r/m8 Valid Valid Decrement r/m8 by 1.
+    (2)     REX + FE /1 DEC r/m8* Valid N.E. Decrement r/m8 by 1.
+    (3)     REX.W + FF /1 DEC r/m64 Valid N.E. Decrement r/m64 by 1.
+
+1. Note, that there is no need to explicitly specify REX-based opcodes for
+    instruction to handle additional registers on EM64T:
+
+    (1)     FE /1 DEC r/m8 Valid Valid Decrement r/m8 by 1.
+    (3)     REX.W + FF /1 DEC r/m64 Valid N.E. Decrement r/m64 by 1.
+
+2. Copy the string, strip off the text comments, replace '/'=>'_'. Note, that
+    the second line is for EM64T only
+
+    (1)     FE /1 DEC r/m8
+    (3)     REX.W + FF /1 DEC r/m64
+
+3. Fill out the mnemonic, opcode parameters parts
+
+    BEGIN_MNEMONIC(DEC, MF_AFFECTS_FLAGS, DU)
+    BEGIN_OPCODES()
+        {OpcodeInfo::all,   {0xFE, _1},         {r_m8},         DU },
+        {OpcodeInfo::em64t, {REX_W, 0xFF, _1},  {r_m64},        DU },
+
+    DU here - one argument, it's used and defined
+
+4. That's it, that simple !
+
+The operand roles (DU here) are used by Jitrino's optimizing engine to
+perform data flow analysis. It also used to store/obtain number of operands.
+
+Special cases are (see the table for details):
+LEA
+Some FPU operations (i.e. FSTP)
+packed things (XORPD, XORPS, CVTDQ2PD, CVTTPD2DQ)
+
+Also, the Jitrino's needs require to specify all operands - including
+implicit ones (see IMUL).
+
+The master table iself does not need to be ordered - it's get sorted before
+processing. It's recommended (though it's not a law) to group similar
+instructions together - i.e. FPU instructions, MMX, etc.
+
+=============================================================================
+
+The encoding engine builds several tables basing on the 'master' one (here
+'mnemonic' is a kind of synonim for 'instruction'):
+
+- list of mnemonics which holds general info about instructions
+    (EncoderBase::mnemonics)
+- an array of opcodes descriptions (EncodeBase::opcodes)
+- a mapping between a hash value and an opcode description record for a given
+    mnemonic (EncoderBase::opcodesHashMap)
+
+The EncoderBase::mnemonics holds general info about instructions.
+The EncoderBase::opcodesHashMap is used for fast opcode selection basing on
+a hash value.
+The EncodeBase::opcodes is used for the encoding itself.
+
+=============================================================================
+The hash value is calculated and used as follows:
+
+JIT-ted code uses the following operand sizes: 8-, 16-, 32- and 64-bits and
+size for an operand can be encoded in just 2 bits.
+
+The following operand locations are available: one of registers - GP, FP,
+MMX, XMM (not taking segment registers), a memory and an immediate, which
+gives us 6 variants and can be enumerated in 3 bits.
+
+As a grand total, the the whole operand's info needed for opcode selection
+can be packed in 5 bits. Taking into account the IMUL mnemonic with its 3
+operands (including implicit ones), we're getting 15 bits per instruction and
+the complete table is about 32768 items per single instruction.
+
+Seems too many, but luckily, the 15 bit limit will never be reached: the
+worst case is IMUL with its 3 operands:
+(IMUL r64, r/m64, imm32)/(IMUL r32, r/m32, imm32).
+So, assigning lowest value to GP register, the max value of hash can be
+reduced.
+
+The hash values to use are:
+sizes:
+        8               -> 11
+        16              -> 10
+        32              -> 01
+        64              -> 00
+locations:
+        gp reg          -> 000
+        memory          -> 001
+        fp reg          -> 010
+        mmx reg         -> 011
+        xmm reg         -> 100
+        immediate       -> 101
+and the grand total for the worst case would be
+[ GP 32] [GP  32] [Imm 32]
+[000-01] [000-01] [101 01] = 1077
+
+However, the implicit operands adds additional value, and the worstest case
+is 'SHLD r_m32, r32, CL=r8'. This gives us the maximum number of:
+
+[mem 32] [GP  32] [GP  8b]
+[001-01] [000-01] [000-11] = 5155.
+
+The max number is pretty big and the hash functions is quite rare, thus it
+is not resonable to use a direct addressing i.e.
+OpcodeDesc[mnemonic][hash_code] - there would be a huge waste of space.
+
+Instead, we use a kind of mapping: the opcodes info is stored in packed
+(here: non rare) array. The max number of opcodes will not exceed 255 for
+each instruction. And we have an index array in which we store a mapping
+between a hash code value and opcode position for each given instruction.
+
+Sounds a bit sophisticated, but in real is simple, the opcode gets selected
+in 2 simple steps:
+
+1. Select [hash,mnemonic] => 'n'.
+
+The array is pretty rare - many cells contain 0xFF which
+means 'invalid hash - no opcode with given characteristics'
+
+char EnbcoderBase::opcodesHashMap[Mnemonic_Count][HASH_MAX] =
+
++----+----+----+----+----+----+
+| 00 | 05 | FF | FF | 03 | 12 | ...
+|---------+-------------------+
+| 12 | FF | FF |  n | 04 | 25 | ...   <- Mnemonic
+|-----------------------------+
+| FF | 11 | FF | 10 | 13 | .. | ...
++-----------------------------+
+     ...         ^
+                 |
+                hash
+
+2. Select [n,mnemonic] => 'opcode_desc11'
+
+OpcodeDesc      EncoderBase::opcodes[Mnemonic_Count][MAX_OPCODES] =
+
++---------------+---------------+---------------+---------------+
+| opcode_desc00 | opcode_desc01 | opcode_desc02 | last_opcode   | ...
++---------------+---------------+---------------+---------------+
+| opcode_desc10 | opcode_desc11 | last_opcode   | xxx           | <- Mnemonic
++---------------+---------------+---------------+---------------+
+| opcode_desc20 | opcode_desc21 | opcode_desc22 | opcode_desc23 | ...
++---------------+---------------+---------------+---------------+
+     ...
+                      ^
+                      |
+                      n
+
+Now, use 'opcode_desc11'.
+
+=============================================================================
+The array of opcodes descriptions (EncodeBase::opcodes) is specially prepared
+to maximize performance - the EncoderBase::encode() is quite hot on client
+applications for the Jitrino/Jitrino.JET.
+The preparation is that opcode descriptions from the 'master' encoding table
+are preprocessed and a special set of OpcodeDesc prepared:
+First, the 'raw' opcode bytes are extracted. Here, 'raw' means the bytes that
+do not depened on any operands values, do not require any analysis and can be
+simply copied into the output buffer during encoding. Also, number of these
+'raw' bytes is counted. The fields are OpcodeDesc::opcode and
+OpcodeDesc::opcode_len.
+
+Then the fisrt non-implicit operand found and its index is stored in
+OpcodeDesc::first_opnd.
+
+The bytes that require processing and analysis ('/r', '+i', etc) are
+extracted and stored in OpcodeDesc::aux0 and OpcodeDesc::aux1 fields.
+
+Here, a special trick is performed:
+    Some opcodes have register/memory operand, but this is not reflected in
+    opcode column - for example, (MOVQ xmm64, xmm_m64). In this case, a fake
+    '_r' added to OpcodeDesc::aux field.
+    Some other opcodes have immediate operands, but this is again not
+    reflected in opcode column - for example, CALL cd or PUSH imm32.
+    In this case, a fake '/cd' or fake '/id' added to appropriate
+    OpcodeDesc::aux field.
+
+The OpcodeDesc::last is non-zero for the final OpcodeDesc record (which does
+not have valid data itself).
+*/
+
+// TODO: To extend flexibility, replace bool fields in MnemonicDesc &
+// MnemonicInfo with a set of flags packed into integer field.
+
+unsigned short EncoderBase::getHash(const OpcodeInfo* odesc)
+{
+    /*
+    NOTE: any changes in the hash computation must be stricty balanced with
+    EncoderBase::Operand::hash_it and EncoderBase::Operands()
+    */
+    unsigned short hash = 0;
+    // The hash computation, uses fast way - table selection instead of if-s.
+    if (odesc->roles.count > 0) {
+        OpndKind kind = odesc->opnds[0].kind;
+        OpndSize size = odesc->opnds[0].size;
+        assert(kind<COUNTOF(kind_hash));
+        assert(size<COUNTOF(size_hash));
+        hash = get_kind_hash(kind) | get_size_hash(size);
+    }
+
+    if (odesc->roles.count > 1) {
+        OpndKind kind = odesc->opnds[1].kind;
+        OpndSize size = odesc->opnds[1].size;
+        assert(kind<COUNTOF(kind_hash));
+        assert(size<COUNTOF(size_hash));
+        hash = (hash<<HASH_BITS_PER_OPERAND) |
+               (get_kind_hash(kind) | get_size_hash(size));
+    }
+
+    if (odesc->roles.count > 2) {
+        OpndKind kind = odesc->opnds[2].kind;
+        OpndSize size = odesc->opnds[2].size;
+        assert(kind<COUNTOF(kind_hash));
+        assert(size<COUNTOF(size_hash));
+        hash = (hash<<HASH_BITS_PER_OPERAND) |
+            (get_kind_hash(kind) | get_size_hash(size));
+    }
+    assert(hash <= HASH_MAX);
+    return hash;
+}
+
+
+#define BEGIN_MNEMONIC(mn, flags, roles)     \
+        { Mnemonic_##mn, flags, roles, #mn,
+#define END_MNEMONIC() },
+#define BEGIN_OPCODES() {
+#define END_OPCODES()   { OpcodeInfo::all, {OpcodeByteKind_LAST}, {}, {0, 0, 0, 0}}}
+
+
+static MnemonicInfo masterEncodingTable[] = {
+//
+// Null
+//
+BEGIN_MNEMONIC(Null, MF_NONE, N)
+BEGIN_OPCODES()
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(LAHF, MF_USES_FLAGS, D)
+BEGIN_OPCODES()
+// TheManual says it's not always supported in em64t mode, thus excluding it
+    {OpcodeInfo::ia32,    {0x9F},         {EAX}, D },
+END_OPCODES()
+END_MNEMONIC()
+//
+// ALU mnemonics - add, adc, or, xor, and, cmp, sub, sbb
+// as they differ only in the opcode extention (/digit) number and
+// in which number the opcode start from, the opcode definitions
+// for those instructions are packed together
+//
+// The 'opcode_starts_from' and 'opcode_ext' in DEFINE_ALU_OPCODES()
+// are enough to define OpcodeInfo::all opcodes and the 'first_opcode'
+// parameter is only due to ADD instruction, which requires an zero opcode
+// byte which, in turn, is coded especially in the current coding scheme.
+//
+
+#define DEFINE_ALU_OPCODES( opc_ext, opcode_starts_from, first_opcode, def_use ) \
+\
+    {OpcodeInfo::decoder,   {opcode_starts_from + 4, ib},           {AL,    imm8},  DU_U },\
+    {OpcodeInfo::decoder,   {Size16, opcode_starts_from + 5, iw},   {AX,    imm16}, DU_U },\
+    {OpcodeInfo::decoder,   {opcode_starts_from + 5, id},           {EAX,   imm32}, DU_U },\
+    {OpcodeInfo::decoder64, {REX_W, opcode_starts_from+5, id},      {RAX,   imm32s},DU_U },\
+\
+    {OpcodeInfo::all,       {0x80, opc_ext, ib},          {r_m8,  imm8},    def_use },\
+    {OpcodeInfo::all,       {Size16, 0x81, opc_ext, iw},  {r_m16, imm16},   def_use },\
+    {OpcodeInfo::all,       {0x81, opc_ext, id},          {r_m32, imm32},   def_use },\
+    {OpcodeInfo::em64t,     {REX_W, 0x81, opc_ext, id},   {r_m64, imm32s},  def_use },\
+\
+    {OpcodeInfo::all,       {Size16, 0x83, opc_ext, ib},  {r_m16, imm8s},   def_use },\
+    {OpcodeInfo::all,       {0x83, opc_ext, ib},          {r_m32, imm8s},   def_use },\
+    {OpcodeInfo::em64t,     {REX_W, 0x83, opc_ext, ib},   {r_m64, imm8s},   def_use },\
+\
+    {OpcodeInfo::all,       {first_opcode,  _r},          {r_m8,  r8},      def_use },\
+\
+    {OpcodeInfo::all,       {Size16, opcode_starts_from+1,  _r},  {r_m16, r16},   def_use },\
+    {OpcodeInfo::all,       {opcode_starts_from+1,  _r},  {r_m32, r32},   def_use },\
+    {OpcodeInfo::em64t,     {REX_W, opcode_starts_from+1, _r},    {r_m64, r64},   def_use },\
+\
+    {OpcodeInfo::all,       {opcode_starts_from+2,  _r},  {r8,    r_m8},  def_use },\
+\
+    {OpcodeInfo::all,       {Size16, opcode_starts_from+3,  _r},  {r16,   r_m16}, def_use },\
+    {OpcodeInfo::all,       {opcode_starts_from+3,  _r},  {r32,   r_m32}, def_use },\
+    {OpcodeInfo::em64t,     {REX_W, opcode_starts_from+3, _r},    {r64,   r_m64}, def_use },
+
+BEGIN_MNEMONIC(ADD, MF_AFFECTS_FLAGS|MF_SYMMETRIC, DU_U)
+BEGIN_OPCODES()
+    DEFINE_ALU_OPCODES(_0, 0x00, OxOO, DU_U )
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(OR, MF_AFFECTS_FLAGS|MF_SYMMETRIC, DU_U)
+BEGIN_OPCODES()
+    DEFINE_ALU_OPCODES(_1, 0x08, 0x08, DU_U )
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(ADC, MF_AFFECTS_FLAGS|MF_USES_FLAGS|MF_SYMMETRIC, DU_U)
+BEGIN_OPCODES()
+    DEFINE_ALU_OPCODES(_2, 0x10, 0x10, DU_U )
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(SBB, MF_AFFECTS_FLAGS|MF_USES_FLAGS, DU_U)
+BEGIN_OPCODES()
+    DEFINE_ALU_OPCODES(_3, 0x18, 0x18, DU_U )
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(AND, MF_AFFECTS_FLAGS|MF_SYMMETRIC, DU_U)
+BEGIN_OPCODES()
+    DEFINE_ALU_OPCODES(_4, 0x20, 0x20, DU_U )
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(SUB, MF_AFFECTS_FLAGS|MF_SAME_ARG_NO_USE, DU_U)
+BEGIN_OPCODES()
+    DEFINE_ALU_OPCODES(_5, 0x28, 0x28, DU_U )
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(XOR, MF_AFFECTS_FLAGS|MF_SYMMETRIC|MF_SAME_ARG_NO_USE, DU_U)
+BEGIN_OPCODES()
+    DEFINE_ALU_OPCODES( _6, 0x30, 0x30, DU_U )
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CMP, MF_AFFECTS_FLAGS, U_U)
+BEGIN_OPCODES()
+    DEFINE_ALU_OPCODES( _7, 0x38, 0x38, U_U )
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CMPXCHG, MF_AFFECTS_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0xB0, _r},           {r_m8, r8, AL},     DU_DU_DU },
+    {OpcodeInfo::all,   {Size16, 0x0F, 0xB1, _r},   {r_m16, r16, AX},   DU_DU_DU },
+    {OpcodeInfo::all,   {0x0F, 0xB1, _r},           {r_m32, r32, EAX},  DU_DU_DU},
+    {OpcodeInfo::em64t, {REX_W, 0x0F, 0xB1, _r},    {r_m64, r64, RAX},  DU_DU_DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CMPXCHG8B, MF_AFFECTS_FLAGS, D)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0xC7, _1},         {m64},     DU },
+END_OPCODES()
+END_MNEMONIC()
+
+#undef DEFINE_ALU_OPCODES
+//
+//
+//
+BEGIN_MNEMONIC(ADDSD, MF_NONE, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF2, 0x0F, 0x58, _r},   {xmm64, xmm_m64},   DU_U},
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(ADDSS, MF_NONE, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0x58, _r},   {xmm32, xmm_m32},   DU_U},
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(BSF, MF_AFFECTS_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0xBC},   {r32, r_m32},   D_U},
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(BSR, MF_AFFECTS_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0xBD},   {r32, r_m32},   D_U},
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(CALL, MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xE8, cd},        {rel32},     U },
+    {OpcodeInfo::ia32,  {Size16, 0xE8, cw}, {rel16},    U },
+    {OpcodeInfo::ia32,  {0xFF, _2},        {r_m32},     U },
+    {OpcodeInfo::em64t, {0xFF, _2},        {r_m64},     U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CMC, MF_USES_FLAGS|MF_AFFECTS_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::decoder,   {0xF5},         {},     N },
+END_OPCODES()
+END_MNEMONIC()
+
+//TODO: Workaround. Actually, it's D_DU, but Jitrino's CG thinks it's D_U
+BEGIN_MNEMONIC(CDQ, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,       {0x99},         {DX, AX},       D_U },
+    {OpcodeInfo::all,       {0x99},         {EDX, EAX},     D_U },
+    {OpcodeInfo::em64t,     {REX_W, 0x99},  {RDX, RAX},     D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+#define DEFINE_CMOVcc_MNEMONIC( cc ) \
+        BEGIN_MNEMONIC(CMOV##cc, MF_USES_FLAGS|MF_CONDITIONAL, DU_U ) \
+BEGIN_OPCODES() \
+    {OpcodeInfo::all,   {Size16, 0x0F, 0x40 + ConditionMnemonic_##cc, _r},  {r16, r_m16},   DU_U }, \
+    {OpcodeInfo::all,   {0x0F, 0x40 + ConditionMnemonic_##cc, _r},          {r32, r_m32},   DU_U }, \
+    {OpcodeInfo::em64t, {REX_W, 0x0F, 0x40 + ConditionMnemonic_##cc, _r},   {r64, r_m64},   DU_U }, \
+END_OPCODES() \
+END_MNEMONIC()
+
+DEFINE_CMOVcc_MNEMONIC(O)
+DEFINE_CMOVcc_MNEMONIC(NO)
+DEFINE_CMOVcc_MNEMONIC(B)
+DEFINE_CMOVcc_MNEMONIC(NB)
+DEFINE_CMOVcc_MNEMONIC(Z)
+DEFINE_CMOVcc_MNEMONIC(NZ)
+DEFINE_CMOVcc_MNEMONIC(BE)
+DEFINE_CMOVcc_MNEMONIC(NBE)
+DEFINE_CMOVcc_MNEMONIC(S)
+DEFINE_CMOVcc_MNEMONIC(NS)
+DEFINE_CMOVcc_MNEMONIC(P)
+DEFINE_CMOVcc_MNEMONIC(NP)
+DEFINE_CMOVcc_MNEMONIC(L)
+DEFINE_CMOVcc_MNEMONIC(NL)
+DEFINE_CMOVcc_MNEMONIC(LE)
+DEFINE_CMOVcc_MNEMONIC(NLE)
+
+#undef DEFINE_CMOVcc_MNEMONIC
+
+/*****************************************************************************
+                                ***** SSE conversion routines *****
+*****************************************************************************/
+//
+// double -> float
+BEGIN_MNEMONIC(CVTSD2SS, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF2, 0x0F, 0x5A, _r},   {xmm32, xmm_m64}, D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+// double -> I_32
+BEGIN_MNEMONIC(CVTSD2SI, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF2, 0x0F, 0x2D, _r},         {r32, xmm_m64}, D_U },
+    {OpcodeInfo::em64t, {REX_W, 0xF2, 0x0F, 0x2D, _r},  {r64, xmm_m64}, D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+// double [truncated] -> I_32
+BEGIN_MNEMONIC(CVTTSD2SI, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF2, 0x0F, 0x2C, _r},         {r32, xmm_m64}, D_U },
+    {OpcodeInfo::em64t, {REX_W, 0xF2, 0x0F, 0x2C, _r},  {r64, xmm_m64}, D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+// float -> double
+BEGIN_MNEMONIC(CVTSS2SD, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0x5A, _r},         {xmm64, xmm_m32}, D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+// float -> I_32
+BEGIN_MNEMONIC(CVTSS2SI, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0x2D, _r},         {r32, xmm_m32}, D_U},
+    {OpcodeInfo::em64t, {REX_W, 0xF3, 0x0F, 0x2D, _r},  {r64, xmm_m32}, D_U},
+END_OPCODES()
+END_MNEMONIC()
+
+// float [truncated] -> I_32
+BEGIN_MNEMONIC(CVTTSS2SI, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0x2C, _r},         {r32, xmm_m32}, D_U},
+    {OpcodeInfo::em64t, {REX_W, 0xF3, 0x0F, 0x2C, _r},  {r64, xmm_m32}, D_U},
+END_OPCODES()
+END_MNEMONIC()
+
+// I_32 -> double
+BEGIN_MNEMONIC(CVTSI2SD, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF2, 0x0F, 0x2A, _r},         {xmm64, r_m32}, D_U},
+    {OpcodeInfo::em64t, {REX_W, 0xF2, 0x0F, 0x2A, _r},  {xmm64, r_m64}, D_U},
+END_OPCODES()
+END_MNEMONIC()
+
+// I_32 -> float
+BEGIN_MNEMONIC(CVTSI2SS, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0x2A, _r},         {xmm32, r_m32}, D_U},
+    {OpcodeInfo::em64t, {REX_W, 0xF3, 0x0F, 0x2A, _r},  {xmm32, r_m64}, D_U},
+END_OPCODES()
+END_MNEMONIC()
+
+//
+// ~ SSE conversions
+//
+
+BEGIN_MNEMONIC(DEC, MF_AFFECTS_FLAGS, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xFE, _1},         {r_m8},     DU },
+
+    {OpcodeInfo::all,   {Size16, 0xFF, _1}, {r_m16},    DU },
+    {OpcodeInfo::all,   {0xFF, _1},         {r_m32},    DU },
+    {OpcodeInfo::em64t, {REX_W, 0xFF, _1},  {r_m64},    DU },
+
+    {OpcodeInfo::ia32,  {Size16, 0x48|rw},  {r16},      DU },
+    {OpcodeInfo::ia32,  {0x48|rd},          {r32},      DU },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(DIVSD, MF_NONE, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all, {0xF2, 0x0F, 0x5E, _r},   {xmm64, xmm_m64},   DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(DIVSS, MF_NONE, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all, {0xF3, 0x0F, 0x5E, _r},   {xmm32, xmm_m32},   DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+/****************************************************************************
+                 ***** FPU operations *****
+****************************************************************************/
+
+BEGIN_MNEMONIC(FADDP, MF_NONE, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDE, 0xC1},       {FP0D}, DU },
+    {OpcodeInfo::all,   {0xDE, 0xC1},       {FP0S}, DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FLDZ,  MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, 0xEE},   {FP0D}, D },
+    {OpcodeInfo::all,   {0xD9, 0xEE},   {FP0S}, D },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FADD,  MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDC, _0},     {FP0D, m64}, DU_U },
+    {OpcodeInfo::all,   {0xD8, _0},     {FP0S, m32}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FSUBP, MF_NONE, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDE, 0xE9},   {FP0D}, DU },
+    {OpcodeInfo::all,   {0xDE, 0xE9},   {FP0S}, DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FSUB,   MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDC, _4},     {FP0D, m64}, DU_U },
+    {OpcodeInfo::all,   {0xD8, _4},     {FP0S, m32}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FISUB,   MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDA, _4},       {FP0S, m32}, DU_U },
+//    {OpcodeInfo::all,   {0xDE, _4},       {FP0S, m16}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+
+BEGIN_MNEMONIC(FMUL,   MF_NONE, DU_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD8, _1},     {FP0S, m32}, DU_U },
+    {OpcodeInfo::all,   {0xDC, _1},     {FP0D, m64}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FMULP, MF_NONE, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDE, 0xC9},   {FP0D}, DU },
+    {OpcodeInfo::all,   {0xDE, 0xC9},   {FP0S}, DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FDIVP, MF_NONE, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDE, 0xF9},   {FP0D}, DU },
+    {OpcodeInfo::all,   {0xDE, 0xF9},   {FP0S}, DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FDIV,   MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDC, _6},     {FP0D, m64}, DU_U },
+    {OpcodeInfo::all,   {0xD8, _6},     {FP0S, m32}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(FUCOM, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDD, 0xE1},         {FP0D, FP1D},    DU_U },
+    {OpcodeInfo::all,   {0xDD, 0xE1},         {FP0S, FP1S},    DU_U },
+    // A little trick: actually, these 2 opcodes take only index of the
+    // needed register. To make the things similar to other instructions
+    // we encode here as if they took FPREG.
+    {OpcodeInfo::all,   {0xDD, 0xE0|_i},    {fp32},         DU },
+    {OpcodeInfo::all,   {0xDD, 0xE0|_i},    {fp64},         DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FUCOMI, MF_NONE, D_U )
+BEGIN_OPCODES()
+    // A little trick: actually, these 2 opcodes take only index of the
+    // needed register. To make the things similar to other instructions
+    // we encode here as if they took FPREG.
+    {OpcodeInfo::all,   {0xDB, 0xE8|_i},    {fp32},         DU },
+    {OpcodeInfo::all,   {0xDB, 0xE8|_i},    {fp64},         DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FUCOMP, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDD, 0xE9},             {FP0D, FP1D},    DU_U },
+    {OpcodeInfo::all,   {0xDD, 0xE9},             {FP0S, FP1S},    DU_U },
+    // A little trick: actually, these 2 opcodes take only index of the
+    // needed register. To make the things similar to other instructions
+    // we encode here as if they took FPREG.
+    {OpcodeInfo::all,   {0xDD, 0xE8|_i},        {fp32},         DU },
+    {OpcodeInfo::all,   {0xDD, 0xE8|_i},        {fp64},         DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FUCOMIP, MF_NONE, D_U )
+BEGIN_OPCODES()
+    // A little trick: actually, these 2 opcodes take only index of the
+    // needed register. To make the things similar to other instructions
+    // we encode here as if they took FPREG.
+    {OpcodeInfo::all,   {0xDF, 0xE8|_i},        {fp32},         DU },
+    {OpcodeInfo::all,   {0xDF, 0xE8|_i},        {fp64},         DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FUCOMPP, MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDA, 0xE9},   {FP0D, FP1D}, DU_U },
+    {OpcodeInfo::all,   {0xDA, 0xE9},   {FP0S, FP1S}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FLDCW, MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, _5},     {m16},  U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FNSTCW, MF_NONE, D)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, _7},     {m16},  D },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FSTSW, MF_NONE, D)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x9B, 0xDF, 0xE0}, {EAX},  D },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FNSTSW, MF_NONE, D)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDF, 0xE0},   {EAX},  D },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FCHS, MF_NONE, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, 0xE0},   {FP0D}, DU },
+    {OpcodeInfo::all,   {0xD9, 0xE0},   {FP0S}, DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FCLEX, MF_NONE, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x9B, 0xDB, 0xE2}, {}, N },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FNCLEX, MF_NONE, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDB, 0xE2},       {}, N },
+END_OPCODES()
+END_MNEMONIC()
+
+//BEGIN_MNEMONIC(FDECSTP, MF_NONE, N)
+//  BEGIN_OPCODES()
+//          {OpcodeInfo::all, {0xD9, 0xF6},       {},     N },
+//  END_OPCODES()
+//END_MNEMONIC()
+
+BEGIN_MNEMONIC(FILD, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDB, _0}, {FP0S, m32},    D_U },
+    {OpcodeInfo::all,   {0xDF, _5}, {FP0D, m64},    D_U },
+    {OpcodeInfo::all,   {0xDB, _0}, {FP0S, m32},    D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+//BEGIN_MNEMONIC(FINCSTP, MF_NONE, N)
+//  BEGIN_OPCODES()
+//          {OpcodeInfo::all, {0xD9, 0xF7},       {},     N },
+//  END_OPCODES()
+//END_MNEMONIC()
+
+BEGIN_MNEMONIC(FIST, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDB, _2}, {m32, FP0S},    D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FISTP, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDB, _3}, {m32, FP0S},    D_U },
+    {OpcodeInfo::all,   {0xDF, _7}, {m64, FP0D},    D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FISTTP, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xDD, _1}, {m64, FP0D},    D_U },
+    {OpcodeInfo::all,   {0xDB, _1}, {m32, FP0S},    D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FRNDINT, MF_NONE, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, 0xFC}, {FP0S},    DU },
+    {OpcodeInfo::all,   {0xD9, 0xFC}, {FP0D},    DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FLD, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, _0}, {FP0S, m32},    D_U },
+    {OpcodeInfo::all,   {0xDD, _0}, {FP0D, m64},    D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FLDLG2, MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, 0xEC}, {FP0S},    D },
+    {OpcodeInfo::all,   {0xD9, 0xEC}, {FP0D},    D },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FLDLN2, MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, 0xED}, {FP0S},    D },
+    {OpcodeInfo::all,   {0xD9, 0xED}, {FP0D},    D },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FLD1, MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, 0xE8}, {FP0S},    D },
+    {OpcodeInfo::all,   {0xD9, 0xE8}, {FP0D},    D },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(FPREM, MF_NONE, N)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xF8},       {},     N },
+  END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FPREM1, MF_NONE, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, 0xF5},       {},     N },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FST, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, _2},         {m32, FP0S},    D_U },
+    {OpcodeInfo::all,   {0xDD, _2},         {m64, FP0D},    D_U },
+    // A little trick: actually, these 2 opcodes take only index of the
+    // needed register. To make the things similar to other instructions
+    // we encode here as if they took FPREG.
+    {OpcodeInfo::all,   {0xDD, 0xD0|_i},    {fp32},         D },
+    {OpcodeInfo::all,   {0xDD, 0xD0|_i},    {fp64},         D },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FSTP, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xD9, _3},             {m32, FP0S},    D_U },
+    {OpcodeInfo::all,   {0xDD, _3},             {m64, FP0D},    D_U },
+    // A little trick: actually, these 2 opcodes take only index of the
+    // needed register. To make the things similar to other instructions
+    // we encode here as if they took FPREG.
+    {OpcodeInfo::all,   {0xDD, 0xD8|_i},        {fp32},         D },
+    {OpcodeInfo::all,   {0xDD, 0xD8|_i},        {fp64},         D },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FSQRT, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xFA},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xFA},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(FYL2X, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xF1},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xF1},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(FYL2XP1, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xF9},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xF9},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(F2XM1, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xF0},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xF0},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FPATAN, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xF3},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xF3},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FXCH, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xC9},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xC9},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FSCALE, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xFD},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xFD},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FABS, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xE1},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xE1},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FSIN, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xFE},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xFE},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FCOS, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xFF},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xFF},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(FPTAN, MF_NONE, DU)
+  BEGIN_OPCODES()
+          {OpcodeInfo::all, {0xD9, 0xF2},       {FP0S},     DU   },
+          {OpcodeInfo::all, {0xD9, 0xF2},       {FP0D},     DU   },
+  END_OPCODES()
+END_MNEMONIC()
+
+//
+// ~ FPU
+//
+
+BEGIN_MNEMONIC(DIV, MF_AFFECTS_FLAGS, DU_DU_U)
+BEGIN_OPCODES()
+#if !defined(_EM64T_)
+    {OpcodeInfo::all,   {0xF6, _6},         {AH, AL, r_m8},     DU_DU_U },
+    {OpcodeInfo::all,   {Size16, 0xF7, _6}, {DX, AX, r_m16},    DU_DU_U },
+#endif
+    {OpcodeInfo::all,   {0xF7, _6},         {EDX, EAX, r_m32},  DU_DU_U },
+    {OpcodeInfo::em64t, {REX_W, 0xF7, _6},  {RDX, RAX, r_m64},  DU_DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(IDIV, MF_AFFECTS_FLAGS, DU_DU_U)
+BEGIN_OPCODES()
+#if !defined(_EM64T_)
+    {OpcodeInfo::all,   {0xF6, _7},         {AH, AL, r_m8},     DU_DU_U },
+    {OpcodeInfo::all,   {Size16, 0xF7, _7}, {DX, AX, r_m16},    DU_DU_U },
+#endif
+    {OpcodeInfo::all,   {0xF7, _7},         {EDX, EAX, r_m32},  DU_DU_U },
+    {OpcodeInfo::em64t, {REX_W, 0xF7, _7},  {RDX, RAX, r_m64},  DU_DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(IMUL, MF_AFFECTS_FLAGS, D_DU_U)
+BEGIN_OPCODES()
+    /*{OpcodeInfo::all,   {0xF6, _5},               {AH, AL,        r_m8},  D_DU_U },
+    {OpcodeInfo::all,     {Size16, 0xF7, _5},       {DX, AX,        r_m16}, D_DU_U },
+    */
+    //
+    {OpcodeInfo::all,     {0xF7, _5},               {EDX, EAX, r_m32},  D_DU_U },
+    //todo: this opcode's hash conflicts with IMUL r64,r_m64 - they're both 0.
+    // this particular is not currently used, so we may safely drop it, but need to
+    // revisit the hash implementation
+    // {OpcodeInfo::em64t,   {REX_W, 0xF7, _5},        {RDX, RAX, r_m64},  D_DU_U },
+    //
+    {OpcodeInfo::all,   {Size16, 0x0F, 0xAF, _r}, {r16,r_m16},        DU_U },
+    {OpcodeInfo::all,   {0x0F, 0xAF, _r},         {r32,r_m32},        DU_U },
+    {OpcodeInfo::em64t, {REX_W, 0x0F, 0xAF, _r},  {r64,r_m64},        DU_U },
+    {OpcodeInfo::all,   {Size16, 0x6B, _r, ib},   {r16,r_m16,imm8s},  D_DU_U },
+    {OpcodeInfo::all,   {0x6B, _r, ib},           {r32,r_m32,imm8s},  D_DU_U },
+    {OpcodeInfo::em64t, {REX_W, 0x6B, _r, ib},    {r64,r_m64,imm8s},  D_DU_U },
+    {OpcodeInfo::all,   {Size16, 0x6B, _r, ib},   {r16,imm8s},        DU_U },
+    {OpcodeInfo::all,   {0x6B, _r, ib},           {r32,imm8s},        DU_U },
+    {OpcodeInfo::em64t, {REX_W, 0x6B, _r, ib},    {r64,imm8s},        DU_U },
+    {OpcodeInfo::all,   {Size16, 0x69, _r, iw},   {r16,r_m16,imm16},  D_U_U },
+    {OpcodeInfo::all,   {0x69, _r, id},           {r32,r_m32,imm32},  D_U_U },
+    {OpcodeInfo::em64t, {REX_W, 0x69, _r, id},    {r64,r_m64,imm32s}, D_U_U },
+    {OpcodeInfo::all,   {Size16, 0x69, _r, iw},   {r16,imm16},        DU_U },
+    {OpcodeInfo::all,   {0x69, _r, id},           {r32,imm32},        DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MUL, MF_AFFECTS_FLAGS, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF6, _4},           {AX, AL, r_m8},     D_DU_U },
+    {OpcodeInfo::all,   {Size16, 0xF7, _4},   {DX, AX, r_m16},    D_DU_U },
+    {OpcodeInfo::all,   {0xF7, _4},           {EDX, EAX, r_m32},  D_DU_U },
+    {OpcodeInfo::em64t, {REX_W, 0xF7, _4},    {RDX, RAX, r_m64},  D_DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(INC, MF_AFFECTS_FLAGS, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xFE, _0},           {r_m8},         DU },
+    {OpcodeInfo::all,   {Size16, 0xFF, _0},   {r_m16},        DU },
+    {OpcodeInfo::all,   {0xFF, _0},           {r_m32},        DU },
+    {OpcodeInfo::em64t, {REX_W, 0xFF, _0},    {r_m64},        DU },
+    {OpcodeInfo::ia32,  {Size16, 0x40|rw},    {r16},          DU },
+    {OpcodeInfo::ia32,  {0x40|rd},            {r32},          DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(INT3, MF_NONE, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xCC},     {},     N },
+END_OPCODES()
+END_MNEMONIC()
+
+#define DEFINE_Jcc_MNEMONIC( cc ) \
+        BEGIN_MNEMONIC(J##cc, MF_USES_FLAGS|MF_CONDITIONAL, U ) \
+BEGIN_OPCODES() \
+    {OpcodeInfo::all,   {0x70 + ConditionMnemonic_##cc, cb },           { rel8 },       U }, \
+    {OpcodeInfo::ia32,  {Size16, 0x0F, 0x80 + ConditionMnemonic_##cc, cw},      { rel16 },      U }, \
+    {OpcodeInfo::all,   {0x0F, 0x80 + ConditionMnemonic_##cc, cd},      { rel32 },      U }, \
+END_OPCODES() \
+END_MNEMONIC()
+
+
+DEFINE_Jcc_MNEMONIC(O)
+DEFINE_Jcc_MNEMONIC(NO)
+DEFINE_Jcc_MNEMONIC(B)
+DEFINE_Jcc_MNEMONIC(NB)
+DEFINE_Jcc_MNEMONIC(Z)
+DEFINE_Jcc_MNEMONIC(NZ)
+DEFINE_Jcc_MNEMONIC(BE)
+DEFINE_Jcc_MNEMONIC(NBE)
+
+DEFINE_Jcc_MNEMONIC(S)
+DEFINE_Jcc_MNEMONIC(NS)
+DEFINE_Jcc_MNEMONIC(P)
+DEFINE_Jcc_MNEMONIC(NP)
+DEFINE_Jcc_MNEMONIC(L)
+DEFINE_Jcc_MNEMONIC(NL)
+DEFINE_Jcc_MNEMONIC(LE)
+DEFINE_Jcc_MNEMONIC(NLE)
+
+#undef DEFINE_Jcc_MNEMONIC
+
+BEGIN_MNEMONIC(JMP, MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xEB, cb},         {rel8},     U },
+    {OpcodeInfo::ia32,  {Size16, 0xE9, cw}, {rel16},    U },
+    {OpcodeInfo::all,   {0xE9, cd},         {rel32},    U },
+    {OpcodeInfo::ia32,  {Size16, 0xFF, _4}, {r_m16},    U },
+    {OpcodeInfo::ia32,  {0xFF, _4},         {r_m32},    U },
+    {OpcodeInfo::em64t, {0xFF, _4},         {r_m64},    U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(LEA, MF_NONE, D_U )
+BEGIN_OPCODES()
+    /*
+    A special case: the LEA instruction itself does not care about size of
+    second operand. This is obviuos why it is, and thus in The Manual, a
+    simple 'm' without size is used.
+    However, in the Jitrino's instrucitons we'll have an operand with a size.
+    Also, the hashing scheme is not supposed to handle OpndSize_Null, and
+    making it to do so will lead to unnecessary complication of hashing
+    scheme. Thus, instead of handling it as a special case, we simply make
+    copies of the opcodes with sizes set.
+        {OpcodeInfo::all,     {0x8D, _r},             {r32, m},       D_U },
+        {OpcodeInfo::em64t, {0x8D, _r},               {r64, m},       D_U },
+    */
+    //Android x86: keep r32, m32 only, otherwise, will have decoding error
+    //{OpcodeInfo::all,   {0x8D, _r},     {r32, m8},      D_U },
+    {OpcodeInfo::em64t, {REX_W, 0x8D, _r},     {r64, m8},      D_U },
+    //{OpcodeInfo::all,   {0x8D, _r},     {r32, m16},     D_U },
+    {OpcodeInfo::em64t, {REX_W, 0x8D, _r},     {r64, m16},     D_U },
+    {OpcodeInfo::all,   {0x8D, _r},     {r32, m32},     D_U },
+    {OpcodeInfo::em64t, {REX_W, 0x8D, _r},     {r64, m32},     D_U },
+    {OpcodeInfo::all,   {0x8D, _r},     {r32, m64},     D_U },
+    {OpcodeInfo::em64t, {REX_W, 0x8D, _r},     {r64, m64},     D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(LOOP, MF_AFFECTS_FLAGS|MF_USES_FLAGS, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xE2, cb},     {ECX, rel8},    DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(LOOPE, MF_AFFECTS_FLAGS|MF_USES_FLAGS, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xE1, cb},     {ECX, rel8},    DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(LOOPNE, MF_AFFECTS_FLAGS|MF_USES_FLAGS, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xE0, cb},     {ECX, rel8},    DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MOV, MF_NONE, D_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x88, _r},         {r_m8,r8},      D_U },
+
+    {OpcodeInfo::all,   {Size16, 0x89, _r}, {r_m16,r16},    D_U },
+    {OpcodeInfo::all,   {0x89, _r},         {r_m32,r32},    D_U },
+    {OpcodeInfo::em64t, {REX_W, 0x89, _r},  {r_m64,r64},    D_U },
+    {OpcodeInfo::all,   {0x8A, _r},         {r8,r_m8},      D_U },
+
+    {OpcodeInfo::all,   {Size16, 0x8B, _r}, {r16,r_m16},    D_U },
+    {OpcodeInfo::all,   {0x8B, _r},         {r32,r_m32},    D_U },
+    {OpcodeInfo::em64t, {REX_W, 0x8B, _r},  {r64,r_m64},    D_U },
+
+    {OpcodeInfo::all,   {0xB0|rb},          {r8,imm8},      D_U },
+
+    {OpcodeInfo::all,   {Size16, 0xB8|rw},  {r16,imm16},    D_U },
+    {OpcodeInfo::all,   {0xB8|rd},          {r32,imm32},    D_U },
+    {OpcodeInfo::em64t, {REX_W, 0xB8|rd},   {r64,imm64},    D_U },
+    {OpcodeInfo::all,   {0xC6, _0},         {r_m8,imm8},    D_U },
+
+    {OpcodeInfo::all,   {Size16, 0xC7, _0}, {r_m16,imm16},  D_U },
+    {OpcodeInfo::all,   {0xC7, _0},         {r_m32,imm32},  D_U },
+    {OpcodeInfo::em64t, {REX_W, 0xC7, _0},  {r_m64,imm32s}, D_U },
+
+    {OpcodeInfo::decoder,   {0xA0},         {AL,  moff8},  D_U },
+    {OpcodeInfo::decoder,   {Size16, 0xA1}, {AX,  moff16},  D_U },
+    {OpcodeInfo::decoder,   {0xA1},         {EAX, moff32},  D_U },
+    //{OpcodeInfo::decoder64,   {REX_W, 0xA1},  {RAX, moff64},  D_U },
+
+    {OpcodeInfo::decoder,   {0xA2},         {moff8, AL},  D_U },
+    {OpcodeInfo::decoder,   {Size16, 0xA3}, {moff16, AX},  D_U },
+    {OpcodeInfo::decoder,   {0xA3},         {moff32, EAX},  D_U },
+    //{OpcodeInfo::decoder64,   {REX_W, 0xA3},  {moff64, RAX},  D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+
+BEGIN_MNEMONIC(XCHG, MF_NONE, DU_DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x87, _r},   {r_m32,r32},    DU_DU },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(MOVQ, MF_NONE, D_U )
+BEGIN_OPCODES()
+#ifdef _HAVE_MMX_
+    {OpcodeInfo::all,   {0x0F, 0x6F, _r},   {mm64, mm_m64}, D_U },
+    {OpcodeInfo::all,   {0x0F, 0x7F, _r},   {mm_m64, mm64}, D_U },
+#endif
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0x7E },  {xmm64, xmm_m64},       D_U },
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xD6 },  {xmm_m64, xmm64},       D_U },
+//    {OpcodeInfo::em64t, {REX_W, 0x66, 0x0F, 0x6E, _r},  {xmm64, r_m64}, D_U },
+//    {OpcodeInfo::em64t, {REX_W, 0x66, 0x0F, 0x7E, _r},  {r_m64, xmm64}, D_U },
+    {OpcodeInfo::em64t, {REX_W, 0x66, 0x0F, 0x6E, _r},  {xmm64, r64}, D_U },
+    {OpcodeInfo::em64t, {REX_W, 0x66, 0x0F, 0x7E, _r},  {r64, xmm64}, D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(MOVD, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x66, 0x0F, 0x6E, _r}, {xmm32, r_m32}, D_U },
+    {OpcodeInfo::all,   {0x66, 0x0F, 0x7E, _r}, {r_m32, xmm32}, D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+//
+// A bunch of MMX instructions
+//
+#ifdef _HAVE_MMX_
+
+BEGIN_MNEMONIC(EMMS, MF_NONE, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0x77},       {},             N },
+END_OPCODES()
+END_MNEMONIC()
+
+#endif
+
+BEGIN_MNEMONIC(PADDQ, MF_NONE, DU_U)
+BEGIN_OPCODES()
+#ifdef _HAVE_MMX_
+    {OpcodeInfo::all,   {0x0F, 0xD4, _r},   {mm64, mm_m64}, DU_U },
+#endif
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xD4, _r},   {xmm64, xmm_m64}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(PAND, MF_NONE, DU_U)
+BEGIN_OPCODES()
+#ifdef _HAVE_MMX_
+    {OpcodeInfo::all,   {0x0F, 0xDB, _r},   {mm64, mm_m64}, DU_U },
+#endif
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xDB, _r},   {xmm64, xmm_m64}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(POR, MF_NONE, DU_U)
+BEGIN_OPCODES()
+#ifdef _HAVE_MMX_
+    {OpcodeInfo::all,   {0x0F, 0xEB, _r},   {mm64, mm_m64}, DU_U },
+#endif
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xEB, _r},   {xmm64, xmm_m64}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(PSUBQ, MF_NONE, DU_U)
+BEGIN_OPCODES()
+#ifdef _HAVE_MMX_
+    {OpcodeInfo::all,   {0x0F, 0xFB, _r},   {mm64, mm_m64}, DU_U },
+#endif
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xFB, _r},   {xmm64, xmm_m64}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(PANDN, MF_NONE, DU_U)
+BEGIN_OPCODES()
+#ifdef _HAVE_MMX_
+    {OpcodeInfo::all,   {0x0F, 0xDF, _r},   {mm64, mm_m64}, DU_U },
+#endif
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xDF, _r}, {xmm64, xmm_m64},   DU_U },
+END_OPCODES()
+END_MNEMONIC()
+BEGIN_MNEMONIC(PSLLQ, MF_NONE, DU_U)
+BEGIN_OPCODES()
+#ifdef _HAVE_MMX_
+    {OpcodeInfo::all,   {0x0F, 0xF3, _r},   {mm64, mm_m64}, DU_U },
+#endif
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xF3, _r}, {xmm64, xmm_m64},   DU_U },
+    {OpcodeInfo::all,   {0x66, 0x0F, 0x73, _6, ib}, {xmm64, imm8},   DU_U },
+END_OPCODES()
+END_MNEMONIC()
+BEGIN_MNEMONIC(PSRLQ, MF_NONE, DU_U)
+BEGIN_OPCODES()
+#ifdef _HAVE_MMX_
+    {OpcodeInfo::all,   {0x0F, 0xD3, _r},   {mm64, mm_m64}, DU_U },
+#endif
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xD3, _r}, {xmm64, xmm_m64},   DU_U },
+    {OpcodeInfo::all,   {0x66, 0x0F, 0x73, _2, ib}, {xmm64, imm8},   DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(PXOR, MF_NONE, DU_U)
+BEGIN_OPCODES()
+#ifdef _HAVE_MMX_
+    {OpcodeInfo::all,   {0x0F, 0xEF, _r},   {mm64, mm_m64}, DU_U },
+#endif
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xEF, _r}, {xmm64, xmm_m64},   DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(MOVAPD, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x66, 0x0F, 0x28, _r},   {xmm64, xmm_m64},   D_U },
+    {OpcodeInfo::all,   {0x66, 0x0F, 0x29, _r},   {xmm_m64, xmm64},   D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(MOVSD, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all, {0xF2, 0x0F, 0x10, _r},   {xmm64, xmm_m64},   D_U },
+    {OpcodeInfo::all, {0xF2, 0x0F, 0x11, _r},   {xmm_m64, xmm64},   D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MOVSS, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all, {0xF3, 0x0F, 0x10, _r},   {xmm32, xmm_m32}, D_U },
+    {OpcodeInfo::all, {0xF3, 0x0F, 0x11, _r},   {xmm_m32, xmm32}, D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MOVSX, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,       {Size16, 0x0F, 0xBE, _r}, {r16, r_m8s},     D_U },
+    {OpcodeInfo::all,       {0x0F, 0xBE, _r},         {r32, r_m8s},     D_U },
+    {OpcodeInfo::em64t,     {REX_W, 0x0F, 0xBE, _r},  {r64, r_m8s},     D_U },
+
+    {OpcodeInfo::all,       {0x0F, 0xBF, _r},         {r32, r_m16s},    D_U },
+    {OpcodeInfo::em64t,     {REX_W, 0x0F, 0xBF, _r},  {r64, r_m16s},    D_U },
+
+    {OpcodeInfo::em64t,     {REX_W, 0x63, _r},        {r64, r_m32s},    D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MOVZX, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,       {Size16, 0x0F, 0xB6, _r}, {r16, r_m8u},     D_U },
+    {OpcodeInfo::all,       {0x0F, 0xB6, _r},         {r32, r_m8u},     D_U },
+    {OpcodeInfo::em64t,     {REX_W, 0x0F, 0xB6, _r},  {r64, r_m8u},     D_U },
+
+    {OpcodeInfo::all,       {0x0F, 0xB7, _r},         {r32, r_m16u},    D_U },
+    {OpcodeInfo::em64t,     {REX_W, 0x0F, 0xB7, _r},  {r64, r_m16u},    D_U },
+    //workaround to get r/rm32->r64 ZX mov functionality:
+    //simple 32bit reg copying zeros high bits in 64bit reg
+    {OpcodeInfo::em64t,     {0x8B, _r},               {r64, r_m32u},    D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MULSD, MF_NONE, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF2, 0x0F, 0x59, _r}, {xmm64, xmm_m64},   DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MULSS, MF_NONE, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0x59, _r}, {xmm32, xmm_m32}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(NEG, MF_AFFECTS_FLAGS, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF6, _3},         {r_m8},         DU },
+
+    {OpcodeInfo::all,   {Size16, 0xF7, _3}, {r_m16},        DU },
+    {OpcodeInfo::all,   {0xF7, _3},         {r_m32},        DU },
+    {OpcodeInfo::em64t, {REX_W, 0xF7, _3},  {r_m64},        DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(NOP, MF_NONE, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x90}, {},     N },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(NOT, MF_AFFECTS_FLAGS, DU )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF6, _2},           {r_m8},         DU },
+    {OpcodeInfo::all,   {Size16, 0xF7, _2},   {r_m16},        DU },
+    {OpcodeInfo::all,   {0xF7, _2},           {r_m32},        DU },
+    {OpcodeInfo::em64t, {REX_W, 0xF7, _2},    {r_m64},        DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(POP, MF_NONE, D)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {Size16, 0x8F, _0}, {r_m16},    D },
+    {OpcodeInfo::ia32,  {0x8F, _0},         {r_m32},    D },
+    {OpcodeInfo::em64t, {0x8F, _0},         {r_m64},    D },
+
+    {OpcodeInfo::all,   {Size16, 0x58|rw }, {r16},      D },
+    {OpcodeInfo::ia32,  {0x58|rd },         {r32},      D },
+    {OpcodeInfo::em64t, {0x58|rd },         {r64},      D },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(POPFD, MF_AFFECTS_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x9D},     {},         N },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(PREFETCH, MF_NONE, U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0x18, _0},   {m8},         U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(PUSH, MF_NONE, U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {Size16, 0xFF, _6}, {r_m16},    U },
+    {OpcodeInfo::ia32,  {0xFF, _6},         {r_m32},    U },
+    {OpcodeInfo::em64t, {0xFF, _6},         {r_m64},    U },
+
+    {OpcodeInfo::all,   {Size16, 0x50|rw }, {r16},      U },
+    {OpcodeInfo::ia32,  {0x50|rd },         {r32},      U },
+    {OpcodeInfo::em64t, {0x50|rd },         {r64},      U },
+
+    {OpcodeInfo::all,   {0x6A},         {imm8},     U },
+    {OpcodeInfo::all,   {Size16, 0x68}, {imm16},    U },
+    {OpcodeInfo::ia32,  {0x68},         {imm32},    U },
+//          {OpcodeInfo::em64t,   {0x68},   {imm64},    U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(PUSHFD, MF_USES_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x9C},             {},        N },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(RET, MF_NONE, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xC3},       {},         N },
+    {OpcodeInfo::all,   {0xC2, iw},   {imm16},    U },
+END_OPCODES()
+END_MNEMONIC()
+
+#define DEFINE_SETcc_MNEMONIC( cc ) \
+        BEGIN_MNEMONIC(SET##cc, MF_USES_FLAGS|MF_CONDITIONAL, DU) \
+BEGIN_OPCODES() \
+    {OpcodeInfo::all,   {0x0F,     0x90 + ConditionMnemonic_##cc}, {r_m8},  DU }, \
+END_OPCODES() \
+END_MNEMONIC()
+
+DEFINE_SETcc_MNEMONIC(O)
+DEFINE_SETcc_MNEMONIC(NO)
+DEFINE_SETcc_MNEMONIC(B)
+DEFINE_SETcc_MNEMONIC(NB)
+DEFINE_SETcc_MNEMONIC(Z)
+DEFINE_SETcc_MNEMONIC(NZ)
+DEFINE_SETcc_MNEMONIC(BE)
+DEFINE_SETcc_MNEMONIC(NBE)
+
+DEFINE_SETcc_MNEMONIC(S)
+DEFINE_SETcc_MNEMONIC(NS)
+DEFINE_SETcc_MNEMONIC(P)
+DEFINE_SETcc_MNEMONIC(NP)
+DEFINE_SETcc_MNEMONIC(L)
+DEFINE_SETcc_MNEMONIC(NL)
+DEFINE_SETcc_MNEMONIC(LE)
+DEFINE_SETcc_MNEMONIC(NLE)
+
+#undef DEFINE_SETcc_MNEMONIC
+
+#define DEFINE_SHIFT_MNEMONIC(nam, slash_num, flags) \
+BEGIN_MNEMONIC(nam, flags, DU_U) \
+BEGIN_OPCODES()\
+    /* D0 & D1 opcodes are added w/o 2nd operand (1) because */\
+    /* they are used for decoding only so only instruction length is needed */\
+    {OpcodeInfo::decoder,   {0xD0, slash_num},            {r_m8/*,const_1*/},   DU },\
+    {OpcodeInfo::all,       {0xD2, slash_num},              {r_m8,  CL},        DU_U },\
+    {OpcodeInfo::all,       {0xC0, slash_num, ib},          {r_m8,  imm8},      DU_U },\
+\
+    {OpcodeInfo::decoder,   {Size16, 0xD1, slash_num},    {r_m16/*,const_1*/},  DU },\
+    {OpcodeInfo::all,       {Size16, 0xD3, slash_num},      {r_m16, CL},        DU_U },\
+    {OpcodeInfo::all,       {Size16, 0xC1, slash_num, ib},  {r_m16, imm8 },     DU_U },\
+\
+    {OpcodeInfo::decoder,   {0xD1, slash_num},              {r_m32/*,const_1*/}, DU },\
+    {OpcodeInfo::decoder64, {REX_W, 0xD1, slash_num},       {r_m64/*,const_1*/}, DU },\
+\
+    {OpcodeInfo::all,       {0xD3, slash_num},              {r_m32, CL},        DU_U },\
+    {OpcodeInfo::em64t,     {REX_W, 0xD3, slash_num},       {r_m64, CL},        DU_U },\
+\
+    {OpcodeInfo::all,       {0xC1, slash_num, ib},          {r_m32, imm8},      DU_U },\
+    {OpcodeInfo::em64t,     {REX_W, 0xC1, slash_num, ib},   {r_m64, imm8},      DU_U },\
+END_OPCODES()\
+END_MNEMONIC()
+
+
+DEFINE_SHIFT_MNEMONIC(ROL, _0, MF_AFFECTS_FLAGS)
+DEFINE_SHIFT_MNEMONIC(ROR, _1, MF_AFFECTS_FLAGS)
+DEFINE_SHIFT_MNEMONIC(RCL, _2, MF_AFFECTS_FLAGS|MF_USES_FLAGS)
+DEFINE_SHIFT_MNEMONIC(RCR, _3, MF_AFFECTS_FLAGS|MF_USES_FLAGS)
+
+DEFINE_SHIFT_MNEMONIC(SAL, _4, MF_AFFECTS_FLAGS)
+DEFINE_SHIFT_MNEMONIC(SHR, _5, MF_AFFECTS_FLAGS)
+DEFINE_SHIFT_MNEMONIC(SAR, _7, MF_AFFECTS_FLAGS)
+
+#undef DEFINE_SHIFT_MNEMONIC
+
+BEGIN_MNEMONIC(SHLD, MF_AFFECTS_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0xA5},   {r_m32, r32, CL}, DU_DU_U },
+    {OpcodeInfo::all,   {0x0F, 0xA4},   {r_m32, r32, imm8}, DU_DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(SHRD, MF_AFFECTS_FLAGS, N)
+// TODO: the def/use info is wrong
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0xAD},   {r_m32, r32, CL}, DU_DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(SUBSD, MF_NONE, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF2, 0x0F, 0x5C, _r}, {xmm64, xmm_m64}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(SUBSS, MF_NONE, DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0x5C, _r}, {xmm32, xmm_m32}, DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(TEST, MF_AFFECTS_FLAGS, U_U)
+BEGIN_OPCODES()
+
+    {OpcodeInfo::decoder,   {0xA8, ib},             { AL, imm8},    U_U },
+    {OpcodeInfo::decoder,   {0xA9, iw},             { AX, imm16},   U_U },
+    {OpcodeInfo::decoder,   {0xA9, id},             { EAX, imm32},  U_U },
+    {OpcodeInfo::decoder64, {REX_W, 0xA9, id},      { RAX, imm32s}, U_U },
+
+    {OpcodeInfo::all,       {0xF6, _0, ib},         {r_m8,imm8},    U_U },
+
+    {OpcodeInfo::all,       {Size16, 0xF7, _0, iw}, {r_m16,imm16},  U_U },
+    {OpcodeInfo::all,       {0xF7, _0, id},         {r_m32,imm32},  U_U },
+    {OpcodeInfo::em64t,     {REX_W, 0xF7, _0, id},  {r_m64,imm32s}, U_U },
+
+    {OpcodeInfo::all,       {0x84, _r},             {r_m8,r8},      U_U },
+
+    {OpcodeInfo::all,       {Size16, 0x85, _r},     {r_m16,r16},    U_U },
+    {OpcodeInfo::all,       {0x85, _r},             {r_m32,r32},    U_U },
+    {OpcodeInfo::em64t,     {REX_W, 0x85, _r},      {r_m64,r64},    U_U },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(UCOMISD, MF_AFFECTS_FLAGS, U_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x66, 0x0F, 0x2E, _r}, {xmm64, xmm_m64}, U_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(UCOMISS, MF_AFFECTS_FLAGS, U_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0x2E, _r},       {xmm32, xmm_m32}, U_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(COMISD, MF_AFFECTS_FLAGS, U_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x66, 0x0F, 0x2F, _r}, {xmm64, xmm_m64}, U_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(COMISS, MF_AFFECTS_FLAGS, U_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0x2F, _r},       {xmm32, xmm_m32}, U_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(XORPD, MF_SAME_ARG_NO_USE|MF_SYMMETRIC, DU_U)
+BEGIN_OPCODES()
+    //Note: they're actually 128 bits
+    {OpcodeInfo::all,   {0x66, 0x0F, 0x57, _r},   {xmm64, xmm_m64},   DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(XORPS, MF_SAME_ARG_NO_USE|MF_SYMMETRIC, DU_U)
+BEGIN_OPCODES()
+    //Note: they're actually 128 bits
+    {OpcodeInfo::all,   {0x0F, 0x57, _r},   {xmm32, xmm_m32},       DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CVTDQ2PD, MF_NONE, D_U )
+BEGIN_OPCODES()
+    //Note: they're actually 128 bits
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0xE6}, {xmm64, xmm_m64},   D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CVTDQ2PS, MF_NONE, D_U )
+BEGIN_OPCODES()
+    //Note: they're actually 128 bits
+    {OpcodeInfo::all,   {0x0F, 0x5B, _r},   {xmm32, xmm_m32},   D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CVTTPD2DQ, MF_NONE, D_U )
+BEGIN_OPCODES()
+    //Note: they're actually 128 bits
+    {OpcodeInfo::all,   {0x66, 0x0F, 0xE6}, {xmm64, xmm_m64},   D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CVTTPS2DQ, MF_NONE, D_U )
+BEGIN_OPCODES()
+    //Note: they're actually 128 bits
+    {OpcodeInfo::all,   {0xF3, 0x0F, 0x5B, _r},   {xmm32, xmm_m32},   D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+//
+// String operations
+//
+BEGIN_MNEMONIC(STD, MF_AFFECTS_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xFD},         {},     N },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CLD, MF_AFFECTS_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xFC},         {},     N },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(SCAS, MF_AFFECTS_FLAGS, N)
+// to be symmetric, this mnemonic must have either m32 or RegName_EAX
+// but as long, as Jitrino's CG does not use the mnemonic, leaving it
+// in its natural form
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xAF},         {},     N },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(STOS, MF_AFFECTS_FLAGS, DU_DU_U)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0xAB},         {EDI, ECX, EAX},   DU_DU_U },
+    {OpcodeInfo::all,   {0xAA},         {EDI, ECX, AL},    DU_DU_U },
+    {OpcodeInfo::em64t, {REX_W, 0xAB},  {RDI, RCX, RAX},   DU_DU_U },
+END_OPCODES()
+END_MNEMONIC()
+
+/*
+MOVS and CMPS are the special cases.
+Most the code in both CG and Encoder do not expect 2 memory operands.
+Also, they are not supposed to setup constrains on which register the
+memory reference must reside - m8,m8 or m32,m32 is not the choice.
+We can't use r8,r8 either - will have problem with 8bit EDI, ESI.
+So, as the workaround we do r32,r32 and specify size of the operand through
+the specific mnemonic - the same is in the codegen.
+*/
+BEGIN_MNEMONIC(MOVS8, MF_NONE, DU_DU_DU)
+BEGIN_OPCODES()
+    {OpcodeInfo::ia32,  {0xA4},         {r32,r32,ECX},    DU_DU_DU },
+    {OpcodeInfo::em64t, {0xA4},         {r64,r64,RCX},    DU_DU_DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MOVS16, MF_NONE, DU_DU_DU)
+BEGIN_OPCODES()
+    {OpcodeInfo::ia32,  {Size16, 0xA5}, {r32,r32,ECX},  DU_DU_DU },
+    {OpcodeInfo::em64t, {Size16, 0xA5}, {r64,r64,RCX},  DU_DU_DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MOVS32, MF_NONE, DU_DU_DU)
+BEGIN_OPCODES()
+    {OpcodeInfo::ia32,  {0xA5},         {r32,r32,ECX},  DU_DU_DU },
+    {OpcodeInfo::em64t, {0xA5},         {r64,r64,RCX},  DU_DU_DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(MOVS64, MF_NONE, DU_DU_DU)
+BEGIN_OPCODES()
+    {OpcodeInfo::em64t, {REX_W,0xA5},   {r64,r64,RCX},  DU_DU_DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CMPSB, MF_AFFECTS_FLAGS, DU_DU_DU)
+BEGIN_OPCODES()
+    {OpcodeInfo::ia32,  {0xA6},         {ESI,EDI,ECX},    DU_DU_DU },
+    {OpcodeInfo::em64t, {0xA6},         {RSI,RDI,RCX},    DU_DU_DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CMPSW, MF_AFFECTS_FLAGS, DU_DU_DU)
+BEGIN_OPCODES()
+    {OpcodeInfo::ia32,  {Size16, 0xA7}, {ESI,EDI,ECX},  DU_DU_DU },
+    {OpcodeInfo::em64t, {Size16, 0xA7}, {RSI,RDI,RCX},  DU_DU_DU },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(CMPSD, MF_AFFECTS_FLAGS, DU_DU_DU)
+BEGIN_OPCODES()
+    {OpcodeInfo::ia32,  {0xA7},         {ESI,EDI,ECX},  DU_DU_DU },
+    {OpcodeInfo::em64t, {0xA7},         {RSI,RDI,RCX},  DU_DU_DU },
+END_OPCODES()
+END_MNEMONIC()
+
+
+BEGIN_MNEMONIC(WAIT, MF_AFFECTS_FLAGS, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x9B},         {},       N },
+END_OPCODES()
+END_MNEMONIC()
+
+//
+// ~String operations
+//
+
+//
+//Note: the instructions below added for the sake of disassembling routine.
+// They need to have flags, params and params usage to be defined more precisely.
+//
+BEGIN_MNEMONIC(LEAVE, MF_NONE, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::decoder,   {0xC9},         {},       N },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(ENTER, MF_NONE, N)
+BEGIN_OPCODES()
+    {OpcodeInfo::decoder,   {0xC8, iw, ib},           {imm16, imm8},  N },
+END_OPCODES()
+END_MNEMONIC()
+
+};      // ~masterEncodingTable[]
+
+ENCODER_NAMESPACE_END
+
+ENCODER_NAMESPACE_START
+
+static int compareMnemonicInfo(const void* info1, const void* info2)
+{
+    Mnemonic id1, id2;
+
+    id1 = ((const MnemonicInfo*) info1)->mn;
+    id2 = ((const MnemonicInfo*) info2)->mn;
+    if (id1 < id2)
+        return -1;
+    if (id1 > id2)
+        return 1;
+    return 0;
+}
+
+int EncoderBase::buildTable(void)
+{
+    // A check: all mnemonics must be covered
+    assert(COUNTOF(masterEncodingTable) == Mnemonic_Count);
+
+    // sort out the mnemonics so the list become ordered
+    qsort(masterEncodingTable, Mnemonic_Count, sizeof(MnemonicInfo), compareMnemonicInfo);
+
+    //
+    // clear the things
+    //
+    memset(opcodesHashMap, NOHASH, sizeof(opcodesHashMap));
+    memset(opcodes, 0, sizeof(opcodes));
+    //
+    // and, finally, build it
+    for (unsigned i=0; i<Mnemonic_Count; i++) {
+        assert((Mnemonic)i == (masterEncodingTable + i)->mn);
+        buildMnemonicDesc(masterEncodingTable+i);
+    }
+    return 0;
+}
+
+void EncoderBase::buildMnemonicDesc(const MnemonicInfo * minfo)
+{
+    MnemonicDesc& mdesc = mnemonics[minfo->mn];
+    mdesc.mn = minfo->mn;
+    mdesc.flags = minfo->flags;
+    mdesc.roles = minfo->roles;
+    mdesc.name = minfo->name;
+
+    //
+    // fill the used opcodes
+    //
+    for (unsigned i=0, oindex=0; i<COUNTOF(minfo->opcodes); i++) {
+
+        const OpcodeInfo& oinfo = minfo->opcodes[i];
+        OpcodeDesc& odesc = opcodes[minfo->mn][oindex];
+        // last opcode ?
+        if (oinfo.opcode[0] == OpcodeByteKind_LAST) {
+            // mark the opcode 'last', exit
+            odesc.opcode_len = 0;
+            odesc.last = 1;
+            break;
+        }
+        odesc.last = 0;
+#ifdef _EM64T_
+        if (oinfo.platf == OpcodeInfo::ia32) { continue; }
+        if (oinfo.platf == OpcodeInfo::decoder32) { continue; }
+#else
+        if (oinfo.platf == OpcodeInfo::em64t) { continue; }
+        if (oinfo.platf == OpcodeInfo::decoder64) { continue; }
+#endif
+        if (oinfo.platf == OpcodeInfo::decoder64 ||
+            oinfo.platf == OpcodeInfo::decoder32) {
+             odesc.platf = OpcodeInfo::decoder;
+        }
+        else {
+            odesc.platf = (char)oinfo.platf;
+        }
+        //
+        // fill out opcodes
+        //
+        unsigned j = 0;
+        odesc.opcode_len = 0;
+        for(; oinfo.opcode[j]; j++) {
+            unsigned opcod = oinfo.opcode[j];
+            unsigned kind = opcod&OpcodeByteKind_KindMask;
+            if (kind == OpcodeByteKind_REX_W) {
+                odesc.opcode[odesc.opcode_len++] = (unsigned char)0x48;
+                continue;
+            }
+            else if(kind != 0 && kind != OpcodeByteKind_ZeroOpcodeByte) {
+                break;
+            }
+            unsigned lowByte = (opcod & OpcodeByteKind_OpcodeMask);
+            odesc.opcode[odesc.opcode_len++] = (unsigned char)lowByte;
+        }
+        assert(odesc.opcode_len<5);
+        odesc.aux0 = odesc.aux1 = 0;
+        if (oinfo.opcode[j] != 0) {
+            odesc.aux0 = oinfo.opcode[j];
+            assert((odesc.aux0 & OpcodeByteKind_KindMask) != 0);
+            ++j;
+            if(oinfo.opcode[j] != 0) {
+                odesc.aux1 = oinfo.opcode[j];
+                assert((odesc.aux1 & OpcodeByteKind_KindMask) != 0);
+            }
+        }
+        else if (oinfo.roles.count>=2) {
+            if (((oinfo.opnds[0].kind&OpndKind_Mem) &&
+                 (isRegKind(oinfo.opnds[1].kind))) ||
+                ((oinfo.opnds[1].kind&OpndKind_Mem) &&
+                 (isRegKind(oinfo.opnds[0].kind)))) {
+                 // Example: MOVQ xmm1, xmm/m64 has only opcodes
+                 // same with SHRD
+                 // Adding fake /r
+                 odesc.aux0 = _r;
+            }
+        }
+        else if (oinfo.roles.count==1) {
+            if (oinfo.opnds[0].kind&OpndKind_Mem) {
+                 // Example: SETcc r/m8, adding fake /0
+                 odesc.aux0 = _0;
+            }
+        }
+        // check imm
+        if (oinfo.roles.count > 0 &&
+            (oinfo.opnds[0].kind == OpndKind_Imm ||
+            oinfo.opnds[oinfo.roles.count-1].kind == OpndKind_Imm)) {
+            // Example: CALL cd, PUSH imm32 - they fit both opnds[0] and
+            // opnds[oinfo.roles.count-1].
+            // The A3 opcode fits only opnds[0] - it's currently have
+            // MOV imm32, EAX. Looks ridiculous, but this is how the
+            // moffset is currently implemented. Will need to fix together
+            // with other usages of moff.
+            // adding fake /cd or fake /id
+            unsigned imm_opnd_index =
+                oinfo.opnds[0].kind == OpndKind_Imm ? 0 : oinfo.roles.count-1;
+            OpndSize sz = oinfo.opnds[imm_opnd_index].size;
+            unsigned imm_encode, coff_encode;
+            if (sz==OpndSize_8) {imm_encode = ib; coff_encode=cb; }
+            else if (sz==OpndSize_16) {imm_encode = iw; coff_encode=cw;}
+            else if (sz==OpndSize_32) {imm_encode = id; coff_encode=cd; }
+            else if (sz==OpndSize_64) {imm_encode = io; coff_encode=0xCC; }
+            else { assert(false); imm_encode=0xCC; coff_encode=0xCC; }
+            if (odesc.aux1 == 0) {
+                if (odesc.aux0==0) {
+                    odesc.aux0 = imm_encode;
+                }
+                else {
+                    if (odesc.aux0 != imm_encode && odesc.aux0 != coff_encode) {
+                        odesc.aux1 = imm_encode;
+                    }
+                }
+            }
+            else {
+                assert(odesc.aux1==imm_encode);
+            }
+
+        }
+
+        assert(sizeof(odesc.opnds) == sizeof(oinfo.opnds));
+        memcpy(odesc.opnds, oinfo.opnds,
+                sizeof(EncoderBase::OpndDesc)
+                        * EncoderBase::MAX_NUM_OPCODE_OPERANDS);
+        odesc.roles = oinfo.roles;
+        odesc.first_opnd = 0;
+        if (odesc.opnds[0].reg != RegName_Null) {
+            ++odesc.first_opnd;
+            if (odesc.opnds[1].reg != RegName_Null) {
+                ++odesc.first_opnd;
+            }
+        }
+
+        if (odesc.platf == OpcodeInfo::decoder) {
+            // if the opcode is only for decoding info, then do not hash it.
+            ++oindex;
+            continue;
+        }
+
+        //
+        // check whether the operand info is a mask (i.e. r_m*).
+        // in this case, split the info to have separate entries for 'r'
+        // and for 'm'.
+        // the good news is that there can be only one such operand.
+        //
+        int opnd2split = -1;
+        for (unsigned k=0; k<oinfo.roles.count; k++) {
+            if ((oinfo.opnds[k].kind & OpndKind_Mem) &&
+                (OpndKind_Mem != oinfo.opnds[k].kind)) {
+                opnd2split = k;
+                break;
+            }
+        };
+
+        if (opnd2split == -1) {
+            // not a mask, hash it, store it, continue.
+            unsigned short hash = getHash(&oinfo);
+            opcodesHashMap[minfo->mn][hash] = (unsigned char)oindex;
+            ++oindex;
+            continue;
+        };
+
+        OpcodeInfo storeItem = oinfo;
+        unsigned short hash;
+
+        // remove the memory part of the mask, and store only 'r' part
+        storeItem.opnds[opnd2split].kind = (OpndKind)(storeItem.opnds[opnd2split].kind & ~OpndKind_Mem);
+        hash = getHash(&storeItem);
+        if (opcodesHashMap[minfo->mn][hash] == NOHASH) {
+            opcodesHashMap[minfo->mn][hash] = (unsigned char)oindex;
+        }
+        // else {
+        // do not overwrite if there is something there, just check that operands match
+        // the reason is that for some instructions there are several possibilities:
+        // say 'DEC r' may be encode as either '48+r' or 'FF /1', and I believe
+        // the first one is better for 'dec r'.
+        // as we're currently processing an opcode with memory part in operand,
+        // leave already filled items intact, so if there is 'OP reg' there, this
+        // better choice will be left in the table instead of 'OP r_m'
+        // }
+
+        // compute hash of memory-based operand, 'm' part in 'r_m'
+        storeItem.opnds[opnd2split].kind = OpndKind_Mem;
+        hash = getHash(&storeItem);
+        // should not happen: for the r_m opcodes, there is a possibility
+        // that hash value of 'r' part intersects with 'OP r' value, but it's
+        // impossible for 'm' part.
+        assert(opcodesHashMap[minfo->mn][hash] == NOHASH);
+        opcodesHashMap[minfo->mn][hash] = (unsigned char)oindex;
+
+        ++oindex;
+    }
+}
+
+ENCODER_NAMESPACE_END
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.cpp b/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.cpp
new file mode 100644
index 0000000..49ce6e0
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.cpp
@@ -0,0 +1,757 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <stdio.h>
+#include <assert.h>
+#include <limits.h>
+#include "enc_base.h"
+#include "enc_wrapper.h"
+#include "dec_base.h"
+#include "utils/Log.h"
+
+//#define PRINT_ENCODER_STREAM
+bool dump_x86_inst = false;
+
+/**
+ * @brief Provides mapping between PhysicalReg and RegName used by encoder
+ * @param physicalReg The physical register
+ * @return Returns encoder's register name
+ */
+static RegName mapFromPhysicalReg (int physicalReg)
+{
+    RegName reg = RegName_Null;
+
+    //Get mapping between PhysicalReg and RegName
+    switch (physicalReg)
+    {
+        case PhysicalReg_EAX:
+            reg = RegName_EAX;
+            break;
+        case PhysicalReg_EBX:
+            reg = RegName_EBX;
+            break;
+        case PhysicalReg_ECX:
+            reg = RegName_ECX;
+            break;
+        case PhysicalReg_EDX:
+            reg = RegName_EDX;
+            break;
+        case PhysicalReg_EDI:
+            reg = RegName_EDI;
+            break;
+        case PhysicalReg_ESI:
+            reg = RegName_ESI;
+            break;
+        case PhysicalReg_ESP:
+            reg = RegName_ESP;
+            break;
+        case PhysicalReg_EBP:
+            reg = RegName_EBP;
+            break;
+        case PhysicalReg_XMM0:
+            reg = RegName_XMM0;
+            break;
+        case PhysicalReg_XMM1:
+            reg = RegName_XMM1;
+            break;
+        case PhysicalReg_XMM2:
+            reg = RegName_XMM2;
+            break;
+        case PhysicalReg_XMM3:
+            reg = RegName_XMM3;
+            break;
+        case PhysicalReg_XMM4:
+            reg = RegName_XMM4;
+            break;
+        case PhysicalReg_XMM5:
+            reg = RegName_XMM5;
+            break;
+        case PhysicalReg_XMM6:
+            reg = RegName_XMM6;
+            break;
+        case PhysicalReg_XMM7:
+            reg = RegName_XMM7;
+            break;
+        default:
+            //We have no mapping
+            reg = RegName_Null;
+            break;
+    }
+
+    return reg;
+}
+
+//getRegSize, getAliasReg:
+//OpndSize, RegName, OpndExt: enum enc_defs.h
+inline void add_r(EncoderBase::Operands & args, int physicalReg, OpndSize sz, OpndExt ext = OpndExt_None) {
+    RegName reg = mapFromPhysicalReg (physicalReg);
+    if (sz != getRegSize(reg)) {
+       reg = getAliasReg(reg, sz);
+    }
+    args.add(EncoderBase::Operand(reg, ext));
+}
+inline void add_m(EncoderBase::Operands & args, int baseReg, int disp, OpndSize sz, OpndExt ext = OpndExt_None) {
+    args.add(EncoderBase::Operand(sz,
+                                  mapFromPhysicalReg (baseReg),
+                                  RegName_Null, 0,
+                                  disp, ext));
+}
+inline void add_m_scale(EncoderBase::Operands & args, int baseReg, int indexReg, int scale,
+                        OpndSize sz, OpndExt ext = OpndExt_None) {
+    args.add(EncoderBase::Operand(sz,
+                                  mapFromPhysicalReg (baseReg),
+                                  mapFromPhysicalReg (indexReg), scale,
+                                  0, ext));
+}
+inline void add_m_disp_scale(EncoderBase::Operands & args, int baseReg, int disp, int indexReg, int scale,
+                        OpndSize sz, OpndExt ext = OpndExt_None) {
+    args.add(EncoderBase::Operand(sz,
+                                  mapFromPhysicalReg (baseReg),
+                                  mapFromPhysicalReg (indexReg), scale,
+                                  disp, ext));
+}
+
+inline void add_fp(EncoderBase::Operands & args, unsigned i, bool dbl) {
+    return args.add((RegName)( (dbl ? RegName_FP0D : RegName_FP0S) + i));
+}
+inline void add_imm(EncoderBase::Operands & args, OpndSize sz, int value, bool is_signed) {
+    //assert(n_size != imm.get_size());
+    args.add(EncoderBase::Operand(sz, value,
+             is_signed ? OpndExt_Signed : OpndExt_Zero));
+}
+
+#define MAX_DECODED_STRING_LEN 1024
+char tmpBuffer[MAX_DECODED_STRING_LEN];
+
+void printOperand(const EncoderBase::Operand & opnd) {
+    unsigned int sz;
+    if(!dump_x86_inst) return;
+    sz = strlen(tmpBuffer);
+    if(opnd.size() != OpndSize_32) {
+        const char * opndSizeString = getOpndSizeString(opnd.size());
+
+        if (opndSizeString == NULL) {
+            // If the string that represents operand size is null it means that
+            // the operand size is an invalid value. Although this could be a
+            // problem if instruction is corrupted, technically failing to
+            // disassemble is not fatal. Thus, let's warn but proceed with using
+            // an empty string.
+            ALOGW("JIT-WARNING: Cannot decode instruction operand size.");
+            opndSizeString = "";
+        }
+
+        sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN - sz, "%s ",
+                opndSizeString);
+    }
+    if(opnd.is_mem()) {
+        if(opnd.scale() != 0) {
+            sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz,
+                           "%d(%s,%s,%d)", opnd.disp(),
+                           getRegNameString(opnd.base()),
+                           getRegNameString(opnd.index()), opnd.scale());
+        } else {
+            sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, "%d(%s)",
+                           opnd.disp(), getRegNameString(opnd.base()));
+        }
+    }
+    if(opnd.is_imm()) {
+        sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, "#%x",
+                       (int)opnd.imm());
+    }
+    if(opnd.is_reg()) {
+        sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, "%s",
+                       getRegNameString(opnd.reg()));
+    }
+}
+//TODO: the order of operands
+//to make the printout have the same order as assembly in .S
+//I reverse the order here
+void printDecoderInst(Inst & decInst) {
+    unsigned int sz;
+    if(!dump_x86_inst) return;
+    sz = strlen(tmpBuffer);
+    sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, "%s ",
+                   EncoderBase::toStr(decInst.mn));
+    for(unsigned int k = 0; k < decInst.argc; k++) {
+        if(k > 0) {
+            sz = strlen(tmpBuffer);
+            sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, ", ");
+        }
+        printOperand(decInst.operands[decInst.argc-1-k]);
+    }
+    ALOGE("%s", tmpBuffer);
+}
+void printOperands(EncoderBase::Operands& opnds) {
+    unsigned int sz;
+    if(!dump_x86_inst) return;
+    for(unsigned int k = 0; k < opnds.count(); k++) {
+        if(k > 0) {
+            sz = strlen(tmpBuffer);
+            sz += snprintf(&tmpBuffer[sz], MAX_DECODED_STRING_LEN-sz, ", ");
+        }
+        printOperand(opnds[opnds.count()-1-k]);
+    }
+}
+void printEncoderInst(Mnemonic m, EncoderBase::Operands& opnds) {
+    if(!dump_x86_inst) return;
+    snprintf(tmpBuffer, MAX_DECODED_STRING_LEN, "--- ENC %s ",
+             EncoderBase::toStr(m));
+    printOperands(opnds);
+    ALOGE("%s", tmpBuffer);
+}
+int decodeThenPrint(char* stream_start) {
+    if(!dump_x86_inst) return 0;
+    snprintf(tmpBuffer, MAX_DECODED_STRING_LEN, "--- INST @ %p: ",
+             stream_start);
+    Inst decInst;
+    unsigned numBytes = DecoderBase::decode(stream_start, &decInst);
+    printDecoderInst(decInst);
+    return numBytes;
+}
+
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_imm(Mnemonic m, OpndSize size, int imm, char * stream) {
+    EncoderBase::Operands args;
+    //assert(imm.get_size() == size_32);
+    add_imm(args, size, imm, true/*is_signed*/);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT unsigned encoder_get_inst_size(char * stream) {
+    Inst decInst;
+    unsigned numBytes = DecoderBase::decode(stream, &decInst);
+    return numBytes;
+}
+
+extern "C" ENCODER_DECLARE_EXPORT unsigned encoder_get_cur_operand_offset(int opnd_id)
+{
+    return (unsigned)EncoderBase::getOpndLocation(opnd_id);
+}
+
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_update_imm(int imm, char * stream) {
+    Inst decInst;
+    EncoderBase::Operands args;
+
+    //Decode the instruction
+    DecoderBase::decode(stream, &decInst);
+
+    add_imm(args, decInst.operands[0].size(), imm, true/*is_signed*/);
+    char* stream_next = (char *)EncoderBase::encode(stream, decInst.mn, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(decInst.mn, args);
+    decodeThenPrint(stream);
+#endif
+    return stream_next;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem(Mnemonic m, OpndSize size,
+               int disp, int base_reg, bool isBasePhysical, char * stream) {
+    EncoderBase::Operands args;
+    add_m(args, base_reg, disp, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg(Mnemonic m, OpndSize size,
+               int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    if(m == Mnemonic_DIV || m == Mnemonic_IDIV || m == Mnemonic_MUL || m == Mnemonic_IMUL) {
+      add_r(args, 0/*eax*/, size);
+      add_r(args, 3/*edx*/, size);
+    }
+    add_r(args, reg, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+//! \brief Allows for different operand sizes
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_reg_diff_sizes(Mnemonic m, OpndSize srcOpndSize,
+                   int reg, bool isPhysical, OpndSize destOpndSize,
+                   int reg2, bool isPhysical2, LowOpndRegType type, char * stream) {
+    if((m == Mnemonic_MOV || m == Mnemonic_MOVQ || m == Mnemonic_MOVD) && reg == reg2) return stream;
+    EncoderBase::Operands args;
+    add_r(args, reg2, destOpndSize); //destination
+    if(m == Mnemonic_SAL || m == Mnemonic_SHR || m == Mnemonic_SHL || m == Mnemonic_SAR)
+      add_r(args, reg, OpndSize_8);
+    else
+      add_r(args, reg, srcOpndSize);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+//both operands have same size
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_reg(Mnemonic m, OpndSize size,
+                   int reg, bool isPhysical,
+                   int reg2, bool isPhysical2, LowOpndRegType type, char * stream) {
+    return encoder_reg_reg_diff_sizes(m, size, reg, isPhysical, size, reg2, isPhysical2, type, stream);
+}
+//! \brief Allows for different operand sizes
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_to_reg_diff_sizes(Mnemonic m, OpndSize memOpndSize,
+                   int disp, int base_reg, bool isBasePhysical, OpndSize regOpndSize,
+                   int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    add_r(args, reg, regOpndSize);
+    add_m(args, base_reg, disp, memOpndSize);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_reg(Mnemonic m, OpndSize size,
+                   int disp, int base_reg, bool isBasePhysical,
+                   int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    return encoder_mem_to_reg_diff_sizes(m, size, disp, base_reg, isBasePhysical, size, reg, isPhysical, type, stream);
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_scale_reg(Mnemonic m, OpndSize size,
+                         int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
+                         int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    add_r(args, reg, size);
+    add_m_scale(args, base_reg, index_reg, scale, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_mem_scale(Mnemonic m, OpndSize size,
+                         int reg, bool isPhysical,
+                         int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
+                         LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    add_m_scale(args, base_reg, index_reg, scale, size);
+    add_r(args, reg, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+//! \brief Allows for different operand sizes
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_to_reg_diff_sizes(Mnemonic m, OpndSize memOpndSize,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         OpndSize regOpndSize, int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    add_r(args, reg, regOpndSize);
+    add_m_disp_scale(args, base_reg, disp, index_reg, scale, memOpndSize);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_reg(Mnemonic m, OpndSize size,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    return encoder_mem_disp_scale_to_reg_diff_sizes(m, size, base_reg, isBasePhysical,
+            disp, index_reg, isIndexPhysical, scale, size, reg, isPhysical,
+            type, stream);
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_movzs_mem_disp_scale_reg(Mnemonic m, OpndSize size,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    add_r(args, reg, OpndSize_32);
+    add_m_disp_scale(args, base_reg, disp, index_reg, scale, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char* encoder_reg_mem_disp_scale(Mnemonic m, OpndSize size,
+                         int reg, bool isPhysical,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         LowOpndRegType type, char* stream) {
+    EncoderBase::Operands args;
+    add_m_disp_scale(args, base_reg, disp, index_reg, scale, size);
+    add_r(args, reg, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_mem(Mnemonic m, OpndSize size,
+                   int reg, bool isPhysical,
+                   int disp, int base_reg, bool isBasePhysical, LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    add_m(args, base_reg, disp, size);
+    add_r(args, reg, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    if (m == Mnemonic_CMPXCHG ){
+       //CMPXCHG require EAX as args
+       add_r(args,PhysicalReg_EAX,size);
+       //Add lock prefix for CMPXCHG, guarantee the atomic of CMPXCHG in multi-core platform
+       stream = (char *)EncoderBase::prefix(stream, InstPrefix_LOCK);
+    }
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_imm_reg(Mnemonic m, OpndSize size,
+                   int imm, int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    return encoder_imm_reg_diff_sizes(m, size, imm, size, reg, isPhysical, type, stream);
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_imm_reg_diff_sizes(Mnemonic m, OpndSize sizeImm,
+                   int imm, OpndSize sizeReg, int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    add_r(args, reg, sizeReg); //dst
+    if(m == Mnemonic_IMUL) add_r(args, reg, sizeReg); //src CHECK
+    if(m == Mnemonic_SAL || m == Mnemonic_SHR || m == Mnemonic_SHL
+       || m == Mnemonic_SAR || m == Mnemonic_ROR || m == Mnemonic_PSLLQ || m == Mnemonic_PSRLQ)  //fix for shift opcodes
+      add_imm(args, OpndSize_8, imm, true/*is_signed*/);
+    else
+      add_imm(args, sizeImm, imm, true/*is_signed*/);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_update_imm_rm(int imm, char * stream) {
+    Inst decInst;
+    EncoderBase::Operands args;
+
+    //Decode the instruction
+    DecoderBase::decode(stream, &decInst);
+
+    args.add(decInst.operands[0]);
+    add_imm(args, decInst.operands[1].size(), imm, true/*is_signed*/);
+    char* stream_next = (char *)EncoderBase::encode(stream, decInst.mn, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(decInst.mn, args);
+    decodeThenPrint(stream);
+#endif
+    return stream_next;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_imm_mem(Mnemonic m, OpndSize size,
+                   int imm,
+                   int disp, int base_reg, bool isBasePhysical, char * stream) {
+    EncoderBase::Operands args;
+    add_m(args, base_reg, disp, size);
+    if (m == Mnemonic_SAL || m == Mnemonic_SHR || m == Mnemonic_SHL
+        || m == Mnemonic_SAR || m == Mnemonic_ROR)
+        size = OpndSize_8;
+    add_imm(args, size, imm, true);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_fp_mem(Mnemonic m, OpndSize size, int reg,
+                  int disp, int base_reg, bool isBasePhysical, char * stream) {
+    EncoderBase::Operands args;
+    add_m(args, base_reg, disp, size);
+    // a fake FP register as operand
+    add_fp(args, reg, size == OpndSize_64/*is_double*/);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_fp(Mnemonic m, OpndSize size,
+                  int disp, int base_reg, bool isBasePhysical,
+                  int reg, char * stream) {
+    EncoderBase::Operands args;
+    // a fake FP register as operand
+    add_fp(args, reg, size == OpndSize_64/*is_double*/);
+    add_m(args, base_reg, disp, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_return(char * stream) {
+    EncoderBase::Operands args;
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, Mnemonic_RET, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(Mnemonic_RET, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_compare_fp_stack(bool pop, int reg, bool isDouble, char * stream) {
+    //Mnemonic m = pop ? Mnemonic_FUCOMP : Mnemonic_FUCOM;
+    Mnemonic m = pop ? Mnemonic_FUCOMIP : Mnemonic_FUCOMI;
+    //a single operand or 2 operands?
+    //FST ST(i) has a single operand in encoder.inl?
+    EncoderBase::Operands args;
+    add_fp(args, reg, isDouble);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, m, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(m, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_movez_mem_to_reg(OpndSize size,
+                      int disp, int base_reg, bool isBasePhysical,
+                      int reg, bool isPhysical, char * stream) {
+    EncoderBase::Operands args;
+    add_r(args, reg, OpndSize_32);
+    add_m(args, base_reg, disp, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, Mnemonic_MOVZX, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(Mnemonic_MOVZX, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_moves_mem_to_reg(OpndSize size,
+                      int disp, int base_reg, bool isBasePhysical,
+                      int reg, bool isPhysical, char * stream) {
+    EncoderBase::Operands args;
+    add_r(args, reg, OpndSize_32);
+    add_m(args, base_reg, disp, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, Mnemonic_MOVSX, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(Mnemonic_MOVSX, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_movez_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical, int reg2,
+                      bool isPhysical2, LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    add_r(args, reg2, OpndSize_32); //destination
+    add_r(args, reg, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, Mnemonic_MOVZX, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(Mnemonic_MOVZX, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_moves_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical,int reg2,
+                      bool isPhysical2, LowOpndRegType type, char * stream) {
+    EncoderBase::Operands args;
+    add_r(args, reg2, OpndSize_32); //destination
+    add_r(args, reg, size);
+#ifdef PRINT_ENCODER_STREAM
+    char* stream_start = stream;
+#endif
+    stream = (char *)EncoderBase::encode(stream, Mnemonic_MOVSX, args);
+#ifdef PRINT_ENCODER_STREAM
+    printEncoderInst(Mnemonic_MOVSX, args);
+    decodeThenPrint(stream_start);
+#endif
+    return stream;
+}
+
+/**
+ * @brief Generates variable sized nop instructions.
+ * @param numBytes Number of bytes for the nop instruction. If this value is
+ * larger than 9 bytes, more than one nop instruction will be generated.
+ * @param stream Instruction stream where to place the nops
+ * @return Updated instruction stream pointer after generating the nops
+ */
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_nops(unsigned numBytes, char * stream) {
+    return EncoderBase::nops(stream, numBytes);
+}
+
+// Disassemble the operand "opnd" and put the readable format in "strbuf"
+// up to a string length of "len".
+unsigned int DisassembleOperandToBuf(const EncoderBase::Operand& opnd, char* strbuf, unsigned int len)
+{
+    unsigned int sz = 0;
+    if(opnd.size() != OpndSize_32) {
+        const char * opndSizeString = getOpndSizeString(opnd.size());
+
+        if (opndSizeString == NULL) {
+            // If the string that represents operand size is null it means that
+            // the operand size is an invalid value. Although this could be a
+            // problem if instruction is corrupted, technically failing to
+            // disassemble is not fatal. Thus, let's warn but proceed with using
+            // an empty string.
+            ALOGW("JIT-WARNING: Cannot decode instruction operand size.");
+            opndSizeString = "";
+        }
+
+        sz += snprintf(&strbuf[sz], len-sz, "%s ", opndSizeString);
+    }
+    if(opnd.is_mem()) {
+        if(opnd.scale() != 0) {
+            sz += snprintf(&strbuf[sz], len-sz, "%d(%s,%s,%d)", opnd.disp(),
+                           getRegNameString(opnd.base()),
+                           getRegNameString(opnd.index()), opnd.scale());
+        } else {
+            sz += snprintf(&strbuf[sz], len-sz, "%d(%s)",
+                           opnd.disp(), getRegNameString(opnd.base()));
+        }
+    } else if(opnd.is_imm()) {
+        sz += snprintf(&strbuf[sz], len-sz, "#%x", (int)opnd.imm());
+    } else if(opnd.is_reg()) {
+        sz += snprintf(&strbuf[sz], len-sz, "%s",
+                       getRegNameString(opnd.reg()));
+    }
+    return sz;
+}
+
+// Disassemble the instruction "decInst" and put the readable format
+// in "strbuf" up to a string length of "len".
+void DisassembleInstToBuf(Inst& decInst, char* strbuf, unsigned int len)
+{
+    unsigned int sz = 0;
+    int k;
+    sz += snprintf(&strbuf[sz], len-sz, "%s ", EncoderBase::toStr(decInst.mn));
+    if (decInst.argc > 0) {
+        sz += DisassembleOperandToBuf(decInst.operands[decInst.argc-1],
+                                 &strbuf[sz], len-sz);
+        for(k = decInst.argc-2; k >= 0; k--) {
+            sz += snprintf(&strbuf[sz], len-sz, ", ");
+            sz += DisassembleOperandToBuf(decInst.operands[k], &strbuf[sz], len-sz);
+        }
+    }
+}
+
+// Disassmble the x86 instruction pointed to by code pointer "stream."
+// Put the disassemble text in the "strbuf" up to string length "len".
+// Return the code pointer after the disassemble x86 instruction.
+extern "C" ENCODER_DECLARE_EXPORT
+char* decoder_disassemble_instr(char* stream, char* strbuf, unsigned int len)
+{
+    Inst decInst;
+    unsigned numBytes = DecoderBase::decode(stream, &decInst);
+    DisassembleInstToBuf(decInst, strbuf, len);
+    return (stream + numBytes);
+}
+
+/**
+ * @brief Physical register char* counterparts
+ */
+static const char * PhysicalRegString[] = { "eax", "ebx", "ecx", "edx", "edi",
+        "esi", "esp", "ebp", "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "xmm5",
+        "xmm6", "xmm7", "st0", "st1", "st2", "st3", "st4", "st5", "st6", "st7",
+        "null"
+        };
+
+/**
+ * @brief Scratch register char* counterparts
+ */
+static const char * ScratchRegString[] = { "scratch1", "scratch2", "scratch3",
+        "scratch4", "scratch5", "scratch6", "scratch7", "scratch8", "scratch9",
+        "scratch10" };
+
+extern "C" ENCODER_DECLARE_EXPORT
+/**
+ * @brief Transform a physical register into its char* counterpart
+ * @param reg the PhysicalReg we want to have a char* equivalent
+ * @return the register reg in char* form
+ */
+const char * physicalRegToString(PhysicalReg reg)
+{
+    if (reg < PhysicalReg_Null) {
+        return PhysicalRegString[reg];
+    } else if (reg >= PhysicalReg_SCRATCH_1 && reg <= PhysicalReg_SCRATCH_10) {
+        return ScratchRegString[reg - PhysicalReg_SCRATCH_1];
+    } else if (reg == PhysicalReg_Null) {
+        return "null";
+    } else {
+        return "corrupted-data";
+    }
+}
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.h b/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.h
new file mode 100644
index 0000000..727159d
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.h
@@ -0,0 +1,279 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef _VM_ENC_WRAPPER_H_
+#define _VM_ENC_WRAPPER_H_
+
+#include "enc_defs_ext.h"
+
+extern bool dump_x86_inst;
+typedef enum PhysicalReg {
+  // Currently initializing StartOfGPMarker to be 0 in order to match
+  // register index in Reg_No. However, ideally PhysicalReg_Null should
+  // be 0 and the rest moved over.
+  PhysicalReg_StartOfGPMarker = 0,
+  PhysicalReg_EAX = PhysicalReg_StartOfGPMarker,
+  PhysicalReg_EBX, PhysicalReg_ECX, PhysicalReg_EDX,
+  PhysicalReg_EDI, PhysicalReg_ESI, PhysicalReg_ESP, PhysicalReg_EBP,
+  PhysicalReg_EndOfGPMarker = PhysicalReg_EBP,
+
+  PhysicalReg_StartOfXmmMarker,
+  PhysicalReg_XMM0 = PhysicalReg_StartOfXmmMarker,
+  PhysicalReg_XMM1, PhysicalReg_XMM2, PhysicalReg_XMM3,
+  PhysicalReg_XMM4, PhysicalReg_XMM5, PhysicalReg_XMM6, PhysicalReg_XMM7,
+  PhysicalReg_EndOfXmmMarker = PhysicalReg_XMM7,
+
+  PhysicalReg_StartOfX87Marker,
+  PhysicalReg_ST0 = PhysicalReg_StartOfX87Marker,  PhysicalReg_ST1,
+  PhysicalReg_ST2, PhysicalReg_ST3, PhysicalReg_ST4, PhysicalReg_ST5,
+  PhysicalReg_ST6, PhysicalReg_ST7,
+  PhysicalReg_EndOfX87Marker = PhysicalReg_ST7,
+
+  PhysicalReg_Null,
+  //used as scratch logical register in NCG O1
+  //should not overlap with regular logical register, start from 100
+  PhysicalReg_SCRATCH_1 = 100, PhysicalReg_SCRATCH_2, PhysicalReg_SCRATCH_3, PhysicalReg_SCRATCH_4,
+  PhysicalReg_SCRATCH_5, PhysicalReg_SCRATCH_6, PhysicalReg_SCRATCH_7, PhysicalReg_SCRATCH_8,
+  PhysicalReg_SCRATCH_9, PhysicalReg_SCRATCH_10,
+
+  //This should be the last entry
+  PhysicalReg_Last = PhysicalReg_SCRATCH_10
+} PhysicalReg;
+
+typedef enum Reg_No {
+#ifdef _EM64T_
+    rax_reg = 0,rbx_reg,    rcx_reg,    rdx_reg,
+    rdi_reg,    rsi_reg,    rsp_reg,    rbp_reg,
+    r8_reg,     r9_reg,     r10_reg,    r11_reg,
+    r12_reg,    r13_reg,    r14_reg,    r15_reg,
+    xmm0_reg,   xmm1_reg,   xmm2_reg,   xmm3_reg,
+    xmm4_reg,   xmm5_reg,   xmm6_reg,   xmm7_reg,
+    xmm8_reg,   xmm9_reg,   xmm10_reg,  xmm11_reg,
+    xmm12_reg,  xmm13_reg,  xmm14_reg,  xmm15_reg,
+
+#else   // !defined(_EM64T_)
+
+    eax_reg = 0,ebx_reg,    ecx_reg,    edx_reg,
+    edi_reg,    esi_reg,    esp_reg,    ebp_reg,
+    xmm0_reg,   xmm1_reg,   xmm2_reg,   xmm3_reg,
+    xmm4_reg,   xmm5_reg,   xmm6_reg,   xmm7_reg,
+    fs_reg,
+#endif
+    /** @brief Total number of registers.*/
+    n_reg
+} Reg_No;
+//
+// instruction operand sizes: 8,16,32,64 bits
+//
+typedef enum Opnd_Size {
+    size_8 = 0,
+    size_16,
+    size_32,
+    size_64,
+    n_size,
+#ifdef _EM64T_
+    size_platf = size_64
+#else
+    size_platf = size_32
+#endif
+} Opnd_Size;
+
+//
+// opcodes for alu instructions
+//
+typedef enum ALU_Opcode {
+    add_opc = 0,or_opc,     adc_opc,    sbb_opc,
+    and_opc,    sub_opc,    xor_opc,    cmp_opc,
+    mul_opc,    imul_opc,   div_opc,    idiv_opc,
+    sll_opc,    srl_opc,    sra_opc, //shift right arithmetic
+    shl_opc,    shr_opc,
+    sal_opc,    sar_opc,
+    neg_opc,    not_opc,    andn_opc,
+    n_alu
+} ALU_Opcode;
+
+typedef enum ConditionCode {
+    Condition_O     = 0,
+    Condition_NO    = 1,
+    Condition_B     = 2,
+    Condition_NAE   = Condition_B,
+    Condition_C     = Condition_B,
+    Condition_NB    = 3,
+    Condition_AE    = Condition_NB,
+    Condition_NC    = Condition_NB,
+    Condition_Z     = 4,
+    Condition_E     = Condition_Z,
+    Condition_NZ    = 5,
+    Condition_NE    = Condition_NZ,
+    Condition_BE    = 6,
+    Condition_NA    = Condition_BE,
+    Condition_NBE   = 7,
+    Condition_A     = Condition_NBE,
+
+    Condition_S     = 8,
+    Condition_NS    = 9,
+    Condition_P     = 10,
+    Condition_PE    = Condition_P,
+    Condition_NP    = 11,
+    Condition_PO    = Condition_NP,
+    Condition_L     = 12,
+    Condition_NGE   = Condition_L,
+    Condition_NL    = 13,
+    Condition_GE    = Condition_NL,
+    Condition_LE    = 14,
+    Condition_NG    = Condition_LE,
+    Condition_NLE   = 15,
+    Condition_G     = Condition_NLE,
+    Condition_Count = 16
+} ConditionCode;
+
+//
+// prefix code
+//
+typedef enum InstrPrefix {
+    no_prefix,
+    lock_prefix                     = 0xF0,
+    hint_branch_taken_prefix        = 0x2E,
+    hint_branch_not_taken_prefix    = 0x3E,
+    prefix_repne                    = 0xF2,
+    prefix_repnz                    = prefix_repne,
+    prefix_repe                     = 0xF3,
+    prefix_repz                     = prefix_repe,
+    prefix_rep                      = 0xF3,
+    prefix_cs                       = 0x2E,
+    prefix_ss                       = 0x36,
+    prefix_ds                       = 0x3E,
+    prefix_es                       = 0x26,
+    prefix_fs                       = 0x64,
+    prefix_gs                       = 0x65
+} InstrPrefix;
+
+enum LowOpndRegType
+{
+    LowOpndRegType_gp = 0,
+    LowOpndRegType_fs = 1,
+    LowOpndRegType_xmm = 2,
+    LowOpndRegType_fs_s = 3,
+    LowOpndRegType_ss = 4,
+    LowOpndRegType_invalid = 256,
+};
+
+enum LogicalRegType
+{
+    LogicalType_invalid = 0,
+    LowOpndRegType_scratch = 8,
+    LowOpndRegType_temp = 16,
+    LowOpndRegType_hard = 32,
+    LowOpndRegType_virtual = 64,
+    LowOpndRegType_glue = 128,
+};
+
+//if inline, separte enc_wrapper.cpp into two files, one of them is .inl
+//           enc_wrapper.cpp needs to handle both cases
+#ifdef ENCODER_INLINE
+    #define ENCODER_DECLARE_EXPORT inline
+    #include "enc_wrapper.inl"
+#else
+    #define ENCODER_DECLARE_EXPORT
+#endif
+
+#ifdef __cplusplus
+extern "C"
+{
+#endif
+ENCODER_DECLARE_EXPORT char* encoder_imm(Mnemonic m, OpndSize size,
+                  int imm, char* stream);
+ENCODER_DECLARE_EXPORT unsigned encoder_get_inst_size(char * stream);
+ENCODER_DECLARE_EXPORT char* encoder_update_imm(int imm, char * stream);
+ENCODER_DECLARE_EXPORT char* encoder_mem(Mnemonic m, OpndSize size,
+               int disp, int base_reg, bool isBasePhysical, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_reg(Mnemonic m, OpndSize size,
+               int reg, bool isPhysical, LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_reg_reg(Mnemonic m, OpndSize size,
+                   int reg, bool isPhysical,
+                   int reg2, bool isPhysical2, LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_reg_reg_diff_sizes(Mnemonic m, OpndSize srcOpndSize,
+                   int reg, bool isPhysical, OpndSize destOpndSize,
+                   int reg2, bool isPhysical2, LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_mem_reg(Mnemonic m, OpndSize size,
+                   int disp, int base_reg, bool isBasePhysical,
+                   int reg, bool isPhysical, LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_mem_to_reg_diff_sizes(Mnemonic m, OpndSize memOpndSize,
+                   int disp, int base_reg, bool isBasePhysical, OpndSize regOpndSize,
+                   int reg, bool isPhysical, LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_mem_scale_reg(Mnemonic m, OpndSize size,
+                         int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
+                         int reg, bool isPhysical, LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_reg_mem_scale(Mnemonic m, OpndSize size,
+                         int reg, bool isPhysical,
+                         int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
+                         LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_reg(Mnemonic m, OpndSize size,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         int reg, bool isPhysical, LowOpndRegType type, char * stream);
+ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_to_reg_diff_sizes(Mnemonic m, OpndSize memOpndSize,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         OpndSize regOpndSize, int reg, bool isPhysical, LowOpndRegType type, char * stream);
+ENCODER_DECLARE_EXPORT char * encoder_movzs_mem_disp_scale_reg(Mnemonic m, OpndSize size,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         int reg, bool isPhysical, LowOpndRegType type, char * stream);
+ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_to_reg_2(Mnemonic m, OpndSize memOpndSize,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         OpndSize regOpndSize, int reg, bool isPhysical, LowOpndRegType type, char * stream);
+ENCODER_DECLARE_EXPORT char* encoder_reg_mem_disp_scale(Mnemonic m, OpndSize size,
+                         int reg, bool isPhysical,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_reg_mem(Mnemonic m, OpndSize size,
+                   int reg, bool isPhysical,
+                   int disp, int base_reg, bool isBasePhysical, LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_imm_reg(Mnemonic m, OpndSize size,
+                   int imm, int reg, bool isPhysical, LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char * encoder_imm_reg_diff_sizes(Mnemonic m, OpndSize sizeImm,
+                   int imm, OpndSize sizeReg, int reg, bool isPhysical, LowOpndRegType type, char * stream);
+ENCODER_DECLARE_EXPORT char * encoder_update_imm_rm(int imm, char * stream);
+ENCODER_DECLARE_EXPORT char* encoder_imm_mem(Mnemonic m, OpndSize size,
+                   int imm,
+                   int disp, int base_reg, bool isBasePhysical, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_fp_mem(Mnemonic m, OpndSize size, int reg,
+                  int disp, int base_reg, bool isBasePhysical, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_mem_fp(Mnemonic m, OpndSize size,
+                  int disp, int base_reg, bool isBasePhysical,
+                  int reg, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_return(char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_compare_fp_stack(bool pop, int reg, bool isDouble, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_movez_mem_to_reg(OpndSize size,
+                      int disp, int base_reg, bool isBasePhysical,
+                      int reg, bool isPhysical, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_moves_mem_to_reg(OpndSize size,
+                      int disp, int base_reg, bool isBasePhysical,
+                      int reg, bool isPhysical, char* stream);
+ENCODER_DECLARE_EXPORT char * encoder_movez_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical, int reg2,
+                      bool isPhysical2, LowOpndRegType type, char * stream);
+ENCODER_DECLARE_EXPORT char * encoder_moves_reg_to_reg(OpndSize size,
+                      int reg, bool isPhysical, int reg2,
+                      bool isPhysical2, LowOpndRegType type, char * stream);
+ENCODER_DECLARE_EXPORT char * encoder_nops(unsigned numBytes, char * stream);
+ENCODER_DECLARE_EXPORT int decodeThenPrint(char* stream_start);
+ENCODER_DECLARE_EXPORT char* decoder_disassemble_instr(char* stream, char* strbuf, unsigned int len);
+
+//Provide a char* equivalent to a PhysicalReg type
+ENCODER_DECLARE_EXPORT const char * physicalRegToString(PhysicalReg reg);
+#ifdef __cplusplus
+}
+#endif
+#endif // _VM_ENC_WRAPPER_H_
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/encoder.cpp b/vm/compiler/codegen/x86/lightcg/libenc/encoder.cpp
new file mode 100644
index 0000000..ef08a4d
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/encoder.cpp
@@ -0,0 +1,155 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+#include <stdio.h>
+#include <assert.h>
+
+#include <limits.h>
+
+#include "enc_base.h"
+
+#ifdef NO_ENCODER_INLINE
+    #include "encoder.h"
+    #include "encoder.inl"
+#else
+    #define NO_ENCODER_INLINE
+    #include "encoder.h"
+    #undef NO_ENCODER_INLINE
+#endif
+
+
+
+#ifdef _EM64T_
+
+R_Opnd rax_opnd(rax_reg);
+R_Opnd rcx_opnd(rcx_reg);
+R_Opnd rdx_opnd(rdx_reg);
+R_Opnd rbx_opnd(rbx_reg);
+R_Opnd rsp_opnd(rsp_reg);
+R_Opnd rbp_opnd(rbp_reg);
+R_Opnd rsi_opnd(rsi_reg);
+R_Opnd rdi_opnd(rdi_reg);
+
+R_Opnd r8_opnd(r8_reg);
+R_Opnd r9_opnd(r9_reg);
+R_Opnd r10_opnd(r10_reg);
+R_Opnd r11_opnd(r11_reg);
+R_Opnd r12_opnd(r12_reg);
+R_Opnd r13_opnd(r13_reg);
+R_Opnd r14_opnd(r14_reg);
+R_Opnd r15_opnd(r15_reg);
+
+XMM_Opnd xmm8_opnd(xmm8_reg);
+XMM_Opnd xmm9_opnd(xmm9_reg);
+XMM_Opnd xmm10_opnd(xmm10_reg);
+XMM_Opnd xmm11_opnd(xmm11_reg);
+XMM_Opnd xmm12_opnd(xmm12_reg);
+XMM_Opnd xmm13_opnd(xmm13_reg);
+XMM_Opnd xmm14_opnd(xmm14_reg);
+XMM_Opnd xmm15_opnd(xmm15_reg);
+
+#else
+
+R_Opnd eax_opnd(eax_reg);
+R_Opnd ecx_opnd(ecx_reg);
+R_Opnd edx_opnd(edx_reg);
+R_Opnd ebx_opnd(ebx_reg);
+R_Opnd esp_opnd(esp_reg);
+R_Opnd ebp_opnd(ebp_reg);
+R_Opnd esi_opnd(esi_reg);
+R_Opnd edi_opnd(edi_reg);
+
+#endif //_EM64T_
+
+XMM_Opnd xmm0_opnd(xmm0_reg);
+XMM_Opnd xmm1_opnd(xmm1_reg);
+XMM_Opnd xmm2_opnd(xmm2_reg);
+XMM_Opnd xmm3_opnd(xmm3_reg);
+XMM_Opnd xmm4_opnd(xmm4_reg);
+XMM_Opnd xmm5_opnd(xmm5_reg);
+XMM_Opnd xmm6_opnd(xmm6_reg);
+XMM_Opnd xmm7_opnd(xmm7_reg);
+
+
+#define countof(a)      (sizeof(a)/sizeof(a[0]))
+
+extern const RegName map_of_regno_2_regname[];
+extern const OpndSize map_of_EncoderOpndSize_2_RealOpndSize[];
+extern const Mnemonic map_of_alu_opcode_2_mnemonic[];
+extern const Mnemonic map_of_shift_opcode_2_mnemonic[];
+
+const RegName map_of_regno_2_regname [] = {
+#ifdef _EM64T_
+    RegName_RAX,    RegName_RBX,    RegName_RCX,    RegName_RDX,
+    RegName_RDI,    RegName_RSI,    RegName_RSP,    RegName_RBP,
+    RegName_R8,     RegName_R9,     RegName_R10,    RegName_R11,
+    RegName_R12,    RegName_R13,    RegName_R14,    RegName_R15,
+    RegName_XMM0,   RegName_XMM1,   RegName_XMM2,   RegName_XMM3,
+    RegName_XMM4,   RegName_XMM5,   RegName_XMM6,   RegName_XMM7,
+    RegName_XMM8,   RegName_XMM9,   RegName_XMM10,  RegName_XMM11,
+    RegName_XMM12,  RegName_XMM13,   RegName_XMM14, RegName_XMM15,
+
+#else
+    RegName_EAX,    RegName_EBX,    RegName_ECX,    RegName_EDX,
+    RegName_EDI,    RegName_ESI,    RegName_ESP,    RegName_EBP,
+    RegName_XMM0,   RegName_XMM1,   RegName_XMM2,   RegName_XMM3,
+    RegName_XMM4,   RegName_XMM5,   RegName_XMM6,   RegName_XMM7,
+    RegName_FS,
+#endif  // _EM64T_
+
+    RegName_Null,
+};
+
+const OpndSize map_of_EncoderOpndSize_2_RealOpndSize[] = {
+    OpndSize_8, OpndSize_16, OpndSize_32, OpndSize_64, OpndSize_Any
+};
+
+const Mnemonic map_of_alu_opcode_2_mnemonic[] = {
+    //add_opc=0,  or_opc,           adc_opc,        sbb_opc,
+    //and_opc,      sub_opc,        xor_opc,        cmp_opc,
+    //n_alu
+    Mnemonic_ADD,   Mnemonic_OR,    Mnemonic_ADC,   Mnemonic_SBB,
+    Mnemonic_AND,   Mnemonic_SUB,   Mnemonic_XOR,   Mnemonic_CMP,
+};
+
+const Mnemonic map_of_shift_opcode_2_mnemonic[] = {
+    //shld_opc, shrd_opc,
+    // shl_opc, shr_opc, sar_opc, ror_opc, max_shift_opcode=6,
+    // n_shift = 6
+    Mnemonic_SHLD,  Mnemonic_SHRD,
+    Mnemonic_SHL,   Mnemonic_SHR,   Mnemonic_SAR, Mnemonic_ROR
+};
+
+#ifdef _DEBUG
+
+static int debug_check() {
+    // Checks some assumptions.
+
+    // 1. all items of Encoder.h:enum Reg_No  must be mapped plus n_reg->RegName_Null
+    assert(countof(map_of_regno_2_regname) == n_reg + 1);
+    assert(countof(map_of_alu_opcode_2_mnemonic) == n_alu);
+    assert(countof(map_of_shift_opcode_2_mnemonic) == n_shift);
+    return 0;
+}
+
+static int dummy = debug_check();
+
+// can have this - initialization order problems.... static int dummy_run_the_debug_test = debug_check();
+
+#endif
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/encoder.h b/vm/compiler/codegen/x86/lightcg/libenc/encoder.h
new file mode 100644
index 0000000..9ac0219
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/encoder.h
@@ -0,0 +1,717 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+/**
+ * @file
+ * @brief Simple interface for generating processor instructions.
+ *
+ * The interface works for both IA32 and EM64T. By default, only IA32
+ * capabilities are presented. To enable EM64T feature, the _EM64T_ macro
+ * must be defined (and, of course, a proper library version to be used).
+ *
+ * The interface is based on the original ia32.h encoder interface,
+ * with some simplifications and add-ons - EM64T-specific, SSE and SSE2.
+ *
+ * The interface mostly intended for existing legacy code like LIL code
+ * generator. From the implementation point of view, it's just a wrapper
+ * around the EncoderBase functionality.
+ */
+
+#ifndef _VM_ENCODER_H_
+#define _VM_ENCODER_H_
+
+#include <limits.h>
+#include "enc_base.h"
+//#include "open/types.h"
+
+#ifdef _EM64T_
+// size of general-purpose value on the stack in bytes
+#define GR_STACK_SIZE 8
+// size of floating-point value on the stack in bytes
+#define FR_STACK_SIZE 8
+
+#if defined(WIN32) || defined(_WIN64)
+    // maximum number of GP registers for inputs
+    const int MAX_GR = 4;
+    // maximum number of FP registers for inputs
+    const int MAX_FR = 4;
+    // WIN64 reserves 4 words for shadow space
+    const int SHADOW = 4 * GR_STACK_SIZE;
+#else
+    // maximum number of GP registers for inputs
+    const int MAX_GR = 6;
+    // maximum number of FP registers for inputs
+    const int MAX_FR = 8;
+    // Linux x64 doesn't reserve shadow space
+    const int SHADOW = 0;
+#endif
+
+#else
+// size of general-purpose value on the stack in bytes
+#define GR_STACK_SIZE 4
+// size of general-purpose value on the stack in bytes
+#define FR_STACK_SIZE 8
+
+// maximum number of GP registers for inputs
+const int MAX_GR = 0;
+// maximum number of FP registers for inputs
+const int MAX_FR = 0;
+#endif
+
+typedef enum Reg_No {
+#ifdef _EM64T_
+    rax_reg = 0,rbx_reg,    rcx_reg,    rdx_reg,
+    rdi_reg,    rsi_reg,    rsp_reg,    rbp_reg,
+    r8_reg,     r9_reg,     r10_reg,    r11_reg,
+    r12_reg,    r13_reg,    r14_reg,    r15_reg,
+    xmm0_reg,   xmm1_reg,   xmm2_reg,   xmm3_reg,
+    xmm4_reg,   xmm5_reg,   xmm6_reg,   xmm7_reg,
+    xmm8_reg,   xmm9_reg,   xmm10_reg,  xmm11_reg,
+    xmm12_reg,  xmm13_reg,  xmm14_reg,  xmm15_reg,
+
+#else   // !defined(_EM64T_)
+
+    eax_reg = 0,ebx_reg,    ecx_reg,    edx_reg,
+    edi_reg,    esi_reg,    esp_reg,    ebp_reg,
+    xmm0_reg,   xmm1_reg,   xmm2_reg,   xmm3_reg,
+    xmm4_reg,   xmm5_reg,   xmm6_reg,   xmm7_reg,
+    fs_reg,
+#endif
+    /** @brief Total number of registers.*/
+    n_reg
+} Reg_No;
+//
+// instruction operand sizes: 8,16,32,64 bits
+//
+typedef enum Opnd_Size {
+    size_8 = 0,
+    size_16,
+    size_32,
+    size_64,
+    n_size,
+#ifdef _EM64T_
+    size_platf = size_64
+#else
+    size_platf = size_32
+#endif
+} Opnd_Size;
+
+//
+// opcodes for alu instructions
+//
+typedef enum ALU_Opcode {
+    add_opc = 0,or_opc,     adc_opc,    sbb_opc,
+    and_opc,    sub_opc,    xor_opc,    cmp_opc,
+    n_alu
+} ALU_Opcode;
+
+//
+// opcodes for shift instructions
+//
+typedef enum Shift_Opcode {
+    shld_opc,   shrd_opc,   shl_opc,    shr_opc,
+    sar_opc,    ror_opc, max_shift_opcode=6,     n_shift = 6
+} Shift_Opcode;
+
+typedef enum ConditionCode {
+    Condition_O     = 0,
+    Condition_NO    = 1,
+    Condition_B     = 2,
+    Condition_NAE   = Condition_B,
+    Condition_C     = Condition_B,
+    Condition_NB    = 3,
+    Condition_AE    = Condition_NB,
+    Condition_NC    = Condition_NB,
+    Condition_Z     = 4,
+    Condition_E     = Condition_Z,
+    Condition_NZ    = 5,
+    Condition_NE    = Condition_NZ,
+    Condition_BE    = 6,
+    Condition_NA    = Condition_BE,
+    Condition_NBE   = 7,
+    Condition_A     = Condition_NBE,
+
+    Condition_S     = 8,
+    Condition_NS    = 9,
+    Condition_P     = 10,
+    Condition_PE    = Condition_P,
+    Condition_NP    = 11,
+    Condition_PO    = Condition_NP,
+    Condition_L     = 12,
+    Condition_NGE   = Condition_L,
+    Condition_NL    = 13,
+    Condition_GE    = Condition_NL,
+    Condition_LE    = 14,
+    Condition_NG    = Condition_LE,
+    Condition_NLE   = 15,
+    Condition_G     = Condition_NLE,
+    Condition_Count = 16
+} ConditionCode;
+
+//
+// prefix code
+//
+typedef enum InstrPrefix {
+    no_prefix,
+    lock_prefix                     = 0xF0,
+    hint_branch_taken_prefix        = 0x2E,
+    hint_branch_not_taken_prefix    = 0x3E,
+    prefix_repne                    = 0xF2,
+    prefix_repnz                    = prefix_repne,
+    prefix_repe                     = 0xF3,
+    prefix_repz                     = prefix_repe,
+    prefix_rep                      = 0xF3,
+    prefix_cs                       = 0x2E,
+    prefix_ss                       = 0x36,
+    prefix_ds                       = 0x3E,
+    prefix_es                       = 0x26,
+    prefix_fs                       = 0x64,
+    prefix_gs                       = 0x65
+} InstrPrefix;
+
+
+//
+// an instruction operand
+//
+class Opnd {
+
+protected:
+    enum Tag { SignedImm, UnsignedImm, Reg, Mem, FP, XMM };
+
+    const Tag  tag;
+
+    Opnd(Tag t): tag(t) {}
+
+public:
+    void * operator new(size_t, void * mem) {
+        return mem;
+    }
+
+    void operator delete(void *) {}
+
+    void operator delete(void *, void *) {}
+
+private:
+    // disallow copying
+    Opnd(const Opnd &): tag(Mem) { assert(false); }
+    Opnd& operator=(const Opnd &) { assert(false); return *this; }
+};
+typedef int I_32;
+class Imm_Opnd: public Opnd {
+
+protected:
+    union {
+#ifdef _EM64T_
+        int64           value;
+        unsigned char   bytes[8];
+#else
+        I_32           value;
+        unsigned char   bytes[4];
+#endif
+    };
+    Opnd_Size           size;
+
+public:
+    Imm_Opnd(I_32 val, bool isSigned = true):
+        Opnd(isSigned ? SignedImm : UnsignedImm), value(val), size(size_32) {
+        if (isSigned) {
+            if (CHAR_MIN <= val && val <= CHAR_MAX) {
+                size = size_8;
+            } else if (SHRT_MIN <= val && val <= SHRT_MAX) {
+                size = size_16;
+            }
+        } else {
+            assert(val >= 0);
+            if (val <= UCHAR_MAX) {
+                size = size_8;
+            } else if (val <= USHRT_MAX) {
+                size = size_16;
+            }
+        }
+    }
+    Imm_Opnd(const Imm_Opnd& that): Opnd(that.tag), value(that.value), size(that.size) {};
+
+#ifdef _EM64T_
+    Imm_Opnd(Opnd_Size sz, int64 val, bool isSigned = true):
+        Opnd(isSigned ? SignedImm : UnsignedImm), value(val), size(sz) {
+#ifndef NDEBUG
+        switch (size) {
+        case size_8:
+            assert(val == (int64)(I_8)val);
+            break;
+        case size_16:
+            assert(val == (int64)(int16)val);
+            break;
+        case size_32:
+            assert(val == (int64)(I_32)val);
+            break;
+        case size_64:
+            break;
+        case n_size:
+            assert(false);
+            break;
+        }
+#endif // NDEBUG
+    }
+
+    int64 get_value() const { return value; }
+
+#else
+
+    Imm_Opnd(Opnd_Size sz, I_32 val, int isSigned = true):
+        Opnd(isSigned ? SignedImm : UnsignedImm), value(val), size(sz) {
+#ifndef NDEBUG
+        switch (size) {
+        case size_8:
+            assert((I_32)val == (I_32)(I_8)val);
+            break;
+        case size_16:
+            assert((I_32)val == (I_32)(int16)val);
+            break;
+        case size_32:
+            break;
+        case size_64:
+        case n_size:
+            assert(false);
+            break;
+        }
+#endif // NDEBUG
+    }
+
+    I_32 get_value() const { return value; }
+
+#endif
+    Opnd_Size get_size() const { return size; }
+    bool      is_signed() const { return tag == SignedImm; }
+};
+
+class RM_Opnd: public Opnd {
+
+public:
+    bool is_reg() const { return tag != SignedImm && tag != UnsignedImm && tag != Mem; }
+
+protected:
+    RM_Opnd(Tag t): Opnd(t) {}
+
+private:
+    // disallow copying
+    RM_Opnd(const RM_Opnd &): Opnd(Reg) { assert(false); }
+};
+
+class R_Opnd: public RM_Opnd {
+
+protected:
+    Reg_No      _reg_no;
+
+public:
+    R_Opnd(Reg_No r): RM_Opnd(Reg), _reg_no(r) {}
+    Reg_No  reg_no() const { return _reg_no; }
+
+private:
+    // disallow copying
+    R_Opnd(const R_Opnd &): RM_Opnd(Reg) { assert(false); }
+};
+
+//
+// a memory operand with displacement
+// Can also serve as a full memory operand with base,index, displacement and scale.
+// Use n_reg to specify 'no register', say, for index.
+class M_Opnd: public RM_Opnd {
+
+protected:
+    Imm_Opnd        m_disp;
+    Imm_Opnd        m_scale;
+    R_Opnd          m_index;
+    R_Opnd          m_base;
+
+public:
+    //M_Opnd(Opnd_Size sz): RM_Opnd(Mem, K_M, sz), m_disp(0), m_scale(0), m_index(n_reg), m_base(n_reg) {}
+    M_Opnd(I_32 disp):
+        RM_Opnd(Mem), m_disp(disp), m_scale(0), m_index(n_reg), m_base(n_reg) {}
+    M_Opnd(Reg_No rbase, I_32 rdisp):
+        RM_Opnd(Mem), m_disp(rdisp), m_scale(0), m_index(n_reg), m_base(rbase) {}
+    M_Opnd(I_32 disp, Reg_No rbase, Reg_No rindex, unsigned scale):
+        RM_Opnd(Mem), m_disp(disp), m_scale(scale), m_index(rindex), m_base(rbase) {}
+    M_Opnd(const M_Opnd & that) : RM_Opnd(Mem),
+        m_disp((int)that.m_disp.get_value()), m_scale((int)that.m_scale.get_value()),
+        m_index(that.m_index.reg_no()), m_base(that.m_base.reg_no())
+        {}
+    //
+    inline const R_Opnd & base(void) const { return m_base; }
+    inline const R_Opnd & index(void) const { return m_index; }
+    inline const Imm_Opnd & scale(void) const { return m_scale; }
+    inline const Imm_Opnd & disp(void) const { return m_disp; }
+};
+
+//
+//  a memory operand with base register and displacement
+//
+class M_Base_Opnd: public M_Opnd {
+
+public:
+    M_Base_Opnd(Reg_No base, I_32 disp) : M_Opnd(disp, base, n_reg, 0) {}
+
+private:
+    // disallow copying - but it leads to ICC errors #734 in encoder.inl
+    // M_Base_Opnd(const M_Base_Opnd &): M_Opnd(0) { assert(false); }
+};
+
+//
+//  a memory operand with base register, scaled index register
+//  and displacement.
+//
+class M_Index_Opnd : public M_Opnd {
+
+public:
+    M_Index_Opnd(Reg_No base, Reg_No index, I_32 disp, unsigned scale):
+        M_Opnd(disp, base, index, scale) {}
+
+private:
+    // disallow copying - but it leads to ICC errors #734 in encoder.inl
+    // M_Index_Opnd(const M_Index_Opnd &): M_Opnd(0) { assert(false); }
+};
+
+class XMM_Opnd : public Opnd {
+
+protected:
+    unsigned        m_idx;
+
+public:
+    XMM_Opnd(unsigned _idx): Opnd(XMM), m_idx(_idx) {};
+    unsigned get_idx( void ) const { return m_idx; };
+
+private:
+    // disallow copying
+    XMM_Opnd(const XMM_Opnd &): Opnd(XMM) { assert(false); }
+};
+
+//
+// operand structures for ia32 registers
+//
+#ifdef _EM64T_
+
+extern R_Opnd rax_opnd;
+extern R_Opnd rcx_opnd;
+extern R_Opnd rdx_opnd;
+extern R_Opnd rbx_opnd;
+extern R_Opnd rdi_opnd;
+extern R_Opnd rsi_opnd;
+extern R_Opnd rsp_opnd;
+extern R_Opnd rbp_opnd;
+
+extern R_Opnd r8_opnd;
+extern R_Opnd r9_opnd;
+extern R_Opnd r10_opnd;
+extern R_Opnd r11_opnd;
+extern R_Opnd r12_opnd;
+extern R_Opnd r13_opnd;
+extern R_Opnd r14_opnd;
+extern R_Opnd r15_opnd;
+
+extern XMM_Opnd xmm8_opnd;
+extern XMM_Opnd xmm9_opnd;
+extern XMM_Opnd xmm10_opnd;
+extern XMM_Opnd xmm11_opnd;
+extern XMM_Opnd xmm12_opnd;
+extern XMM_Opnd xmm13_opnd;
+extern XMM_Opnd xmm14_opnd;
+extern XMM_Opnd xmm15_opnd;
+#else
+
+extern R_Opnd eax_opnd;
+extern R_Opnd ecx_opnd;
+extern R_Opnd edx_opnd;
+extern R_Opnd ebx_opnd;
+extern R_Opnd esp_opnd;
+extern R_Opnd ebp_opnd;
+extern R_Opnd esi_opnd;
+extern R_Opnd edi_opnd;
+
+#endif // _EM64T_
+
+extern XMM_Opnd xmm0_opnd;
+extern XMM_Opnd xmm1_opnd;
+extern XMM_Opnd xmm2_opnd;
+extern XMM_Opnd xmm3_opnd;
+extern XMM_Opnd xmm4_opnd;
+extern XMM_Opnd xmm5_opnd;
+extern XMM_Opnd xmm6_opnd;
+extern XMM_Opnd xmm7_opnd;
+
+#ifdef NO_ENCODER_INLINE
+    #define ENCODER_DECLARE_EXPORT
+#else
+    #define ENCODER_DECLARE_EXPORT inline
+    #include "encoder.inl"
+#endif
+
+// prefix
+ENCODER_DECLARE_EXPORT char * prefix(char * stream, InstrPrefix p);
+
+// stack push and pop instructions
+ENCODER_DECLARE_EXPORT char * push(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * push(char * stream, const Imm_Opnd & imm);
+ENCODER_DECLARE_EXPORT char * pop(char * stream,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
+
+// cmpxchg or xchg
+ENCODER_DECLARE_EXPORT char * cmpxchg(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * xchg(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz = size_platf);
+
+// inc(rement), dec(rement), not, neg(ate) instructions
+ENCODER_DECLARE_EXPORT char * inc(char * stream,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * dec(char * stream,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * _not(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * neg(char * stream,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * nop(char * stream);
+ENCODER_DECLARE_EXPORT char * int3(char * stream);
+
+// alu instructions: add, or, adc, sbb, and, sub, xor, cmp
+ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const M_Opnd & m, const R_Opnd & r, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+
+// test instruction
+ENCODER_DECLARE_EXPORT char * test(char * stream, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * test(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz = size_platf);
+
+// shift instructions: shl, shr, sar, shld, shrd, ror
+ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode opc, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode opc, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode opc, const RM_Opnd & rm, const R_Opnd & r, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode opc, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz = size_platf);
+
+// multiply instructions: mul, imul
+ENCODER_DECLARE_EXPORT char * mul(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const RM_Opnd & rm, const Imm_Opnd& imm, Opnd_Size sz = size_platf);
+
+// divide instructions: div, idiv
+ENCODER_DECLARE_EXPORT char * idiv(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * div(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+
+// data movement: mov
+ENCODER_DECLARE_EXPORT char * mov(char * stream, const M_Opnd & m,  const R_Opnd & r, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * mov(char * stream, const R_Opnd & r,  const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * mov(char * stream, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz = size_platf);
+
+ENCODER_DECLARE_EXPORT char * movsx( char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * movzx( char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+
+ENCODER_DECLARE_EXPORT char * movd(char * stream, const RM_Opnd & rm, const XMM_Opnd & xmm);
+ENCODER_DECLARE_EXPORT char * movd(char * stream, const XMM_Opnd & xmm, const RM_Opnd & rm);
+ENCODER_DECLARE_EXPORT char * movq(char * stream, const RM_Opnd & rm, const XMM_Opnd & xmm);
+ENCODER_DECLARE_EXPORT char * movq(char * stream, const XMM_Opnd & xmm, const RM_Opnd & rm);
+
+// sse mov
+ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const M_Opnd & mem, const XMM_Opnd & xmm, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
+
+// sse add, sub, mul, div
+ENCODER_DECLARE_EXPORT char * sse_add(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_add(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
+
+ENCODER_DECLARE_EXPORT char * sse_sub(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_sub(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
+
+ENCODER_DECLARE_EXPORT char * sse_mul(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_mul(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
+
+ENCODER_DECLARE_EXPORT char * sse_div(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_div(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
+
+// xor, compare
+ENCODER_DECLARE_EXPORT char * sse_xor(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1);
+
+ENCODER_DECLARE_EXPORT char * sse_compare(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_compare(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem, bool dbl);
+
+// sse conversions
+ENCODER_DECLARE_EXPORT char * sse_cvt_si(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_cvtt2si(char * stream, const R_Opnd & reg, const M_Opnd & mem, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_cvtt2si(char * stream, const R_Opnd & reg, const XMM_Opnd & xmm, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_cvt_fp2dq(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_cvt_dq2fp(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl);
+ENCODER_DECLARE_EXPORT char * sse_d2s(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem64);
+ENCODER_DECLARE_EXPORT char * sse_d2s(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1);
+ENCODER_DECLARE_EXPORT char * sse_s2d(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem32);
+ENCODER_DECLARE_EXPORT char * sse_s2d(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1);
+
+// condition operations
+ENCODER_DECLARE_EXPORT char * cmov(char * stream, ConditionCode cc, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * setcc(char * stream, ConditionCode cc, const RM_Opnd & rm8);
+
+// load effective address: lea
+ENCODER_DECLARE_EXPORT char * lea(char * stream, const R_Opnd & r, const M_Opnd & m, Opnd_Size sz = size_platf);
+ENCODER_DECLARE_EXPORT char * cdq(char * stream);
+ENCODER_DECLARE_EXPORT char * wait(char * stream);
+
+// control-flow instructions
+ENCODER_DECLARE_EXPORT char * loop(char * stream, const Imm_Opnd & imm);
+
+// jump with 8-bit relative
+ENCODER_DECLARE_EXPORT char * jump8(char * stream, const Imm_Opnd & imm);
+
+// jump with 32-bit relative
+ENCODER_DECLARE_EXPORT char * jump32(char * stream, const Imm_Opnd & imm);
+
+// register indirect jump
+ENCODER_DECLARE_EXPORT char * jump(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+
+// jump to target address
+ENCODER_DECLARE_EXPORT char *jump(char * stream, char *target);
+
+// jump with displacement
+//char * jump(char * stream, I_32 disp);
+
+// conditional branch with 8-bit branch offset
+ENCODER_DECLARE_EXPORT char * branch8(char * stream, ConditionCode cc, const Imm_Opnd & imm, InstrPrefix prefix = no_prefix);
+
+// conditional branch with 32-bit branch offset
+ENCODER_DECLARE_EXPORT char * branch32(char * stream, ConditionCode cc, const Imm_Opnd & imm, InstrPrefix prefix = no_prefix);
+
+// conditional branch with target label address
+//char * branch(char * stream, ConditionCode cc, const char * target, InstrPrefix prefix = no_prefix);
+
+// conditional branch with displacement immediate
+ENCODER_DECLARE_EXPORT char * branch(char * stream, ConditionCode cc, I_32 disp, InstrPrefix prefix = no_prefix);
+
+// call with displacement
+ENCODER_DECLARE_EXPORT char * call(char * stream, const Imm_Opnd & imm);
+
+// indirect call through register or memory location
+ENCODER_DECLARE_EXPORT char * call(char * stream, const RM_Opnd & rm, Opnd_Size sz = size_platf);
+
+// call target address
+ENCODER_DECLARE_EXPORT char * call(char * stream, const char * target);
+
+// return instruction
+ENCODER_DECLARE_EXPORT char * ret(char * stream);
+ENCODER_DECLARE_EXPORT char * ret(char * stream, unsigned short pop);
+ENCODER_DECLARE_EXPORT char * ret(char * stream, const Imm_Opnd & imm);
+
+// string operations
+ENCODER_DECLARE_EXPORT char * set_d(char * stream, bool set);
+ENCODER_DECLARE_EXPORT char * scas(char * stream, unsigned char prefix);
+ENCODER_DECLARE_EXPORT char * stos(char * stream, unsigned char prefix);
+
+// floating-point instructions
+
+// st(0) = st(0) fp_op m{32,64}real
+//!char * fp_op_mem(char * stream, FP_Opcode opc,const M_Opnd& mem,int is_double);
+
+// st(0) = st(0) fp_op st(i)
+//!char *fp_op(char * stream, FP_Opcode opc,unsigned i);
+
+// st(i) = st(i) fp_op st(0)    ; optionally pop stack
+//!char * fp_op(char * stream, FP_Opcode opc,unsigned i,unsigned pop_stk);
+
+// compare st(0),st(1) and pop stack twice
+//!char * fcompp(char * stream);
+ENCODER_DECLARE_EXPORT char * fldcw(char * stream, const M_Opnd & mem);
+ENCODER_DECLARE_EXPORT char * fnstcw(char * stream, const M_Opnd & mem);
+ENCODER_DECLARE_EXPORT char * fnstsw(char * stream);
+//!char * fchs(char * stream);
+//!char * frem(char * stream);
+//!char * fxch(char * stream,unsigned i);
+//!char * fcomip(char * stream, unsigned i);
+
+// load from memory (as fp) into fp register stack
+ENCODER_DECLARE_EXPORT char * fld(char * stream, const M_Opnd & m, bool is_double);
+//!char *fld80(char * stream,const M_Opnd& mem);
+
+// load from memory (as int) into fp register stack
+//!char * fild(char * stream,const M_Opnd& mem,int is_long);
+
+// push st(i) onto fp register stack
+//!char * fld(char * stream,unsigned i);
+
+// push the constants 0.0 and 1.0 onto the fp register stack
+//!char * fldz(char * stream);
+//!char * fld1(char * stream);
+
+// store stack to memory (as int), always popping the stack
+ENCODER_DECLARE_EXPORT char * fist(char * stream, const M_Opnd & mem, bool is_long, bool pop_stk);
+// store stack to to memory (as fp), optionally popping the stack
+ENCODER_DECLARE_EXPORT char * fst(char * stream, const M_Opnd & m, bool is_double, bool pop_stk);
+// store ST(0) to ST(i), optionally popping the stack. Takes 1 clock
+ENCODER_DECLARE_EXPORT char * fst(char * stream, unsigned i, bool pop_stk);
+
+//!char * pushad(char * stream);
+//!char * pushfd(char * stream);
+//!char * popad(char * stream);
+//!char * popfd(char * stream);
+
+// stack frame allocation instructions: enter & leave
+//
+//    enter frame_size
+//
+//    is equivalent to:
+//
+//    push    ebp
+//    mov     ebp,esp
+//    sub     esp,frame_size
+//
+//!char *enter(char * stream,const Imm_Opnd& imm);
+
+// leave
+// is equivalent to:
+//
+// mov        esp,ebp
+// pop        ebp
+//!char *leave(char * stream);
+
+// sahf  loads SF, ZF, AF, PF, and CF flags from eax
+//!char *sahf(char * stream);
+
+// Intrinsic FP math functions
+
+//!char *math_fsin(char * stream);
+//!char *math_fcos(char * stream);
+//!char *math_fabs(char * stream);
+//!char *math_fpatan(char * stream);
+ENCODER_DECLARE_EXPORT char * fprem(char * stream);
+ENCODER_DECLARE_EXPORT char * fprem1(char * stream);
+//!char *math_frndint(char * stream);
+//!char *math_fptan(char * stream);
+
+//
+// Add 1-7 bytes padding, with as few instructions as possible,
+// with no effect on the processor state (e.g., registers, flags)
+//
+//!char *padding(char * stream, unsigned num);
+
+// prolog and epilog code generation
+//- char *prolog(char * stream,unsigned frame_size,unsigned reg_save_mask);
+//- char *epilog(char * stream,unsigned reg_save_mask);
+
+//!extern R_Opnd reg_operand_array[];
+
+// fsave and frstor
+//!char *fsave(char * stream);
+//!char *frstor(char * stream);
+
+// lahf : Load Status Flags into AH Register
+//!char *lahf(char * stream);
+
+// mfence : Memory Fence
+//!char *mfence(char * stream);
+
+#endif // _VM_ENCODER_H_
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/encoder.inl b/vm/compiler/codegen/x86/lightcg/libenc/encoder.inl
new file mode 100644
index 0000000..ec72097
--- /dev/null
+++ b/vm/compiler/codegen/x86/lightcg/libenc/encoder.inl
@@ -0,0 +1,863 @@
+/*
+ *  Licensed to the Apache Software Foundation (ASF) under one or more
+ *  contributor license agreements.  See the NOTICE file distributed with
+ *  this work for additional information regarding copyright ownership.
+ *  The ASF licenses this file to You under the Apache License, Version 2.0
+ *  (the "License"); you may not use this file except in compliance with
+ *  the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ *  Unless required by applicable law or agreed to in writing, software
+ *  distributed under the License is distributed on an "AS IS" BASIS,
+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ *  See the License for the specific language governing permissions and
+ *  limitations under the License.
+ */
+/**
+ * @author Alexander V. Astapchuk
+ */
+#include <stdio.h>
+#include <assert.h>
+#include <limits.h>
+
+extern const RegName map_of_regno_2_regname[];
+extern const OpndSize map_of_EncoderOpndSize_2_RealOpndSize[];
+extern const Mnemonic map_of_alu_opcode_2_mnemonic[];
+extern const Mnemonic map_of_shift_opcode_2_mnemonic[];
+
+// S_ stands for 'Signed'
+extern const Mnemonic S_map_of_condition_code_2_branch_mnemonic[];
+// U_ stands for 'Unsigned'
+extern const Mnemonic U_map_of_condition_code_2_branch_mnemonic[];
+
+inline static RegName map_reg(Reg_No r) {
+    assert(r >= 0 && r <= n_reg);
+    return map_of_regno_2_regname[r];
+}
+
+inline static OpndSize map_size(Opnd_Size o_size) {
+    assert(o_size >= 0 && o_size <= n_size);
+    return map_of_EncoderOpndSize_2_RealOpndSize[o_size];
+}
+
+inline static Mnemonic map_alu(ALU_Opcode alu) {
+    assert(alu >= 0 && alu < n_alu);
+    return map_of_alu_opcode_2_mnemonic[alu];
+}
+
+inline static Mnemonic map_shift(Shift_Opcode shc) {
+    assert(shc >= 0 && shc < n_shift);
+    return map_of_shift_opcode_2_mnemonic[shc];
+}
+
+inline bool fit8(int64 val) {
+    return (CHAR_MIN <= val) && (val <= CHAR_MAX);
+}
+
+inline bool fit32(int64 val) {
+    return (INT_MIN <= val) && (val <= INT_MAX);
+}
+
+inline static void add_r(EncoderBase::Operands & args, const R_Opnd & r, Opnd_Size sz, OpndExt ext = OpndExt_None) {
+    RegName reg = map_reg(r.reg_no());
+    if (sz != n_size) {
+        OpndSize size = map_size(sz);
+        if (size != getRegSize(reg)) {
+            reg = getAliasReg(reg, size);
+        }
+    }
+    args.add(EncoderBase::Operand(reg, ext));
+}
+
+inline static void add_m(EncoderBase::Operands & args, const M_Opnd & m, Opnd_Size sz, OpndExt ext = OpndExt_None) {
+        assert(n_size != sz);
+        args.add(EncoderBase::Operand(map_size(sz),
+            map_reg(m.base().reg_no()), map_reg(m.index().reg_no()),
+            (unsigned)m.scale().get_value(), (int)m.disp().get_value(), ext));
+}
+
+inline static void add_rm(EncoderBase::Operands & args, const RM_Opnd & rm, Opnd_Size sz, OpndExt ext = OpndExt_None) {
+    rm.is_reg() ? add_r(args, (R_Opnd &)rm, sz, ext) : add_m(args, (M_Opnd &)rm, sz, ext);
+}
+
+inline static void add_xmm(EncoderBase::Operands & args, const XMM_Opnd & xmm, bool dbl) {
+    // Gregory -
+    // XMM registers indexes in Reg_No enum are shifted by xmm0_reg, their indexes
+    // don't start with 0, so it is necessary to subtract xmm0_reg index from
+    // xmm.get_idx() value
+    assert(xmm.get_idx() >= xmm0_reg);
+    return args.add((RegName)( (dbl ? RegName_XMM0D : RegName_XMM0S) + xmm.get_idx() -
+            xmm0_reg));
+}
+
+inline static void add_fp(EncoderBase::Operands & args, unsigned i, bool dbl) {
+    return args.add((RegName)( (dbl ? RegName_FP0D : RegName_FP0S) + i));
+}
+
+inline static void add_imm(EncoderBase::Operands & args, const Imm_Opnd & imm) {
+    assert(n_size != imm.get_size());
+    args.add(EncoderBase::Operand(map_size(imm.get_size()), imm.get_value(),
+        imm.is_signed() ? OpndExt_Signed : OpndExt_Zero));
+}
+
+ENCODER_DECLARE_EXPORT char * prefix(char * stream, InstrPrefix p) {
+    *stream = (char)p;
+    return stream + 1;
+}
+
+// stack push and pop instructions
+ENCODER_DECLARE_EXPORT char * push(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_PUSH, args);
+}
+
+ENCODER_DECLARE_EXPORT char * push(char * stream, const Imm_Opnd & imm) {
+    EncoderBase::Operands args;
+#ifdef _EM64T_
+    add_imm(args, imm);
+#else
+    // we need this workaround to be compatible with the former ia32 encoder implementation
+    add_imm(args, Imm_Opnd(size_32, imm.get_value()));
+#endif
+    return EncoderBase::encode(stream, Mnemonic_PUSH, args);
+}
+
+ENCODER_DECLARE_EXPORT char * pop(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_POP, args);
+}
+
+// cmpxchg or xchg
+ENCODER_DECLARE_EXPORT char * cmpxchg(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    add_r(args, r, sz);
+    RegName implicitReg = getAliasReg(RegName_EAX, map_size(sz));
+    args.add(implicitReg);
+    return (char*)EncoderBase::encode(stream, Mnemonic_CMPXCHG, args);
+}
+
+ENCODER_DECLARE_EXPORT char * xchg(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    add_r(args, r, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_XCHG, args);
+}
+
+// inc(rement), dec(rement), not, neg(ate) instructions
+ENCODER_DECLARE_EXPORT char * inc(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_INC, args);
+}
+
+ENCODER_DECLARE_EXPORT char * dec(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_DEC, args);
+}
+
+ENCODER_DECLARE_EXPORT char * _not(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_NOT, args);
+}
+
+ENCODER_DECLARE_EXPORT char * neg(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_NEG, args);
+}
+
+ENCODER_DECLARE_EXPORT char * nop(char * stream) {
+    EncoderBase::Operands args;
+    return (char*)EncoderBase::encode(stream, Mnemonic_NOP, args);
+}
+
+ENCODER_DECLARE_EXPORT char * int3(char * stream) {
+    EncoderBase::Operands args;
+    return (char*)EncoderBase::encode(stream, Mnemonic_INT3, args);
+}
+
+// alu instructions: add, or, adc, sbb, and, sub, xor, cmp
+ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, map_alu(opc), args);
+};
+
+ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const M_Opnd & m, const R_Opnd & r, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, m, sz);
+    add_rm(args, r, sz);
+    return (char*)EncoderBase::encode(stream, map_alu(opc), args);
+}
+
+ENCODER_DECLARE_EXPORT char * alu(char * stream, ALU_Opcode opc, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, r, sz);
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, map_alu(opc), args);
+}
+
+// test instruction
+ENCODER_DECLARE_EXPORT char * test(char * stream, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    assert(imm.get_size() <= sz);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, Mnemonic_TEST, args);
+}
+
+ENCODER_DECLARE_EXPORT char * test(char * stream, const RM_Opnd & rm, const R_Opnd & r, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    add_r(args, r, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_TEST, args);
+}
+
+// shift instructions: shl, shr, sar, shld, shrd
+ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode shc, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, map_shift(shc), args);
+}
+
+ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode shc, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    args.add(RegName_CL);
+    return (char*)EncoderBase::encode(stream, map_shift(shc), args);
+}
+
+ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode shc, const RM_Opnd & rm,
+                            const R_Opnd & r, const Imm_Opnd & imm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    assert(shc == shld_opc || shc == shrd_opc);
+    add_rm(args, rm, sz);
+    add_r(args, r, sz);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, map_shift(shc), args);
+}
+
+ENCODER_DECLARE_EXPORT char * shift(char * stream, Shift_Opcode shc, const RM_Opnd & rm,
+                            const R_Opnd & r, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    assert(shc == shld_opc || shc == shrd_opc);
+    add_rm(args, rm, sz);
+    add_r(args, r, sz);
+    args.add(RegName_CL);
+    return (char*)EncoderBase::encode(stream, map_shift(shc), args);
+}
+
+// multiply instructions: mul, imul
+ENCODER_DECLARE_EXPORT char * mul(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    args.add(RegName_EDX);
+    args.add(RegName_EAX);
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MUL, args);
+}
+
+ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_r(args, r, sz);
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_IMUL, args);
+}
+
+ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const Imm_Opnd & imm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_r(args, r, sz);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, Mnemonic_IMUL, args);
+}
+
+ENCODER_DECLARE_EXPORT char * imul(char * stream, const R_Opnd & r, const RM_Opnd & rm,
+                           const Imm_Opnd & imm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_r(args, r, sz);
+    add_rm(args, rm, sz);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, Mnemonic_IMUL, args);
+}
+
+// divide instructions: div, idiv
+ENCODER_DECLARE_EXPORT char * idiv(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+#ifdef _EM64T_
+    add_r(args, rdx_opnd, sz);
+    add_r(args, rax_opnd, sz);
+#else
+    add_r(args, edx_opnd, sz);
+    add_r(args, eax_opnd, sz);
+#endif
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_IDIV, args);
+}
+
+ENCODER_DECLARE_EXPORT char * div(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+#ifdef _EM64T_
+    add_r(args, rdx_opnd, sz);
+    add_r(args, rax_opnd, sz);
+#else
+    add_r(args, edx_opnd, sz);
+    add_r(args, eax_opnd, sz);
+#endif
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_DIV, args);
+}
+
+// data movement: mov
+ENCODER_DECLARE_EXPORT char * mov(char * stream, const M_Opnd & m, const R_Opnd & r, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_m(args, m, sz);
+    add_r(args, r, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MOV, args);
+}
+
+ENCODER_DECLARE_EXPORT char * mov(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_r(args, r, sz);
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MOV, args);
+}
+
+ENCODER_DECLARE_EXPORT char * mov(char * stream, const RM_Opnd & rm, const Imm_Opnd & imm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MOV, args);
+}
+
+ENCODER_DECLARE_EXPORT char * movd(char * stream, const RM_Opnd & rm, const XMM_Opnd & xmm) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, size_32);
+    add_xmm(args, xmm, false);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MOVD, args);
+}
+
+ENCODER_DECLARE_EXPORT char * movd(char * stream, const XMM_Opnd & xmm, const RM_Opnd & rm) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm, false);
+    add_rm(args, rm, size_32);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MOVD, args);
+}
+
+ENCODER_DECLARE_EXPORT char * movq(char * stream, const RM_Opnd & rm, const XMM_Opnd & xmm) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, size_64);
+    add_xmm(args, xmm, true);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MOVQ, args);
+}
+
+ENCODER_DECLARE_EXPORT char * movq(char * stream, const XMM_Opnd & xmm, const RM_Opnd & rm) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm, true);
+    add_rm(args, rm, size_64);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MOVQ, args);
+}
+
+ENCODER_DECLARE_EXPORT char * movsx(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_r(args, r, n_size);
+    add_rm(args, rm, sz, OpndExt_Signed);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MOVSX, args);
+}
+
+ENCODER_DECLARE_EXPORT char * movzx(char * stream, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_r(args, r, n_size);
+    // movzx r64, r/m32 is not available on em64t
+    // mov r32, r/m32 should zero out upper bytes
+    assert(sz <= size_16);
+    add_rm(args, rm, sz, OpndExt_Zero);
+    return (char*)EncoderBase::encode(stream, Mnemonic_MOVZX, args);
+}
+
+// sse mov
+ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm, dbl);
+    add_m(args, mem, dbl ? size_64 : size_32);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MOVSD : Mnemonic_MOVSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const M_Opnd &  mem, const XMM_Opnd & xmm, bool dbl) {
+    EncoderBase::Operands args;
+    add_m(args, mem, dbl ? size_64 : size_32);
+    add_xmm(args, xmm, dbl);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MOVSD : Mnemonic_MOVSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_mov(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, dbl);
+    add_xmm(args, xmm1, dbl);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MOVSD : Mnemonic_MOVSS, args );
+}
+
+// sse add, sub, mul, div
+ENCODER_DECLARE_EXPORT char * sse_add(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm, dbl);
+    add_m(args, mem, dbl ? size_64 : size_32);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_ADDSD : Mnemonic_ADDSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_add(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, dbl);
+    add_xmm(args, xmm1, dbl);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_ADDSD : Mnemonic_ADDSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_sub(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm, dbl);
+    add_m(args, mem, dbl ? size_64 : size_32);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_SUBSD : Mnemonic_SUBSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_sub(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, dbl);
+    add_xmm(args, xmm1, dbl);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_SUBSD : Mnemonic_SUBSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_mul( char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm, dbl);
+    add_m(args, mem, dbl ? size_64 : size_32);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MULSD : Mnemonic_MULSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_mul(char * stream, const XMM_Opnd& xmm0, const XMM_Opnd& xmm1, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args,  xmm0, dbl);
+    add_xmm(args,  xmm1, dbl);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_MULSD : Mnemonic_MULSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_div(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm, dbl);
+    add_m(args, mem, dbl ? size_64 : size_32);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_DIVSD : Mnemonic_DIVSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_div(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, dbl);
+    add_xmm(args, xmm1, dbl);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_DIVSD : Mnemonic_DIVSS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_xor(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, true);
+    add_xmm(args, xmm1, true);
+    return (char*)EncoderBase::encode(stream, Mnemonic_PXOR, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_compare(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, true);
+    add_xmm(args, xmm1, true);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_COMISD : Mnemonic_COMISS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_compare(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, dbl);
+    add_m(args, mem, dbl ? size_64 : size_32);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_COMISD : Mnemonic_COMISS, args);
+}
+
+// sse conversions
+ENCODER_DECLARE_EXPORT char * sse_cvt_si(char * stream, const XMM_Opnd & xmm, const M_Opnd & mem, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm, dbl);
+    add_m(args, mem, size_32);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_CVTSI2SD : Mnemonic_CVTSI2SS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_cvtt2si(char * stream, const R_Opnd & reg, const M_Opnd & mem, bool dbl) {
+    EncoderBase::Operands args;
+    add_rm(args, reg, size_32);
+    add_m(args, mem, dbl ? size_64 : size_32);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_CVTTSD2SI : Mnemonic_CVTTSS2SI, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_cvtt2si(char * stream, const R_Opnd & reg, const XMM_Opnd & xmm, bool dbl) {
+    EncoderBase::Operands args;
+    add_rm(args, reg, size_32);
+    add_xmm(args, xmm, dbl);
+    return (char*)EncoderBase::encode(stream, dbl ? Mnemonic_CVTTSD2SI : Mnemonic_CVTTSS2SI, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_cvt_fp2dq(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, dbl);
+    add_xmm(args, xmm1, dbl);
+    return (char*)EncoderBase::encode(stream, dbl ?  Mnemonic_CVTTPD2DQ : Mnemonic_CVTTPS2DQ, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_cvt_dq2fp(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1, bool dbl) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, dbl);
+    add_xmm(args, xmm1, dbl);
+    return (char*)EncoderBase::encode(stream, dbl ?  Mnemonic_CVTDQ2PD : Mnemonic_CVTDQ2PS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_d2s(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem64) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, false);
+    add_m(args, mem64, size_64);
+    return (char*)EncoderBase::encode(stream, Mnemonic_CVTSD2SS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_d2s(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, false);
+    add_xmm(args, xmm1, true);
+    return (char*)EncoderBase::encode(stream, Mnemonic_CVTSD2SS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_s2d(char * stream, const XMM_Opnd & xmm0, const M_Opnd & mem32) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, true);
+    add_m(args, mem32, size_32);
+    return (char*)EncoderBase::encode(stream, Mnemonic_CVTSS2SD, args);
+}
+
+ENCODER_DECLARE_EXPORT char * sse_s2d(char * stream, const XMM_Opnd & xmm0, const XMM_Opnd & xmm1) {
+    EncoderBase::Operands args;
+    add_xmm(args, xmm0, true);
+    add_xmm(args, xmm1, false);
+    return (char*)EncoderBase::encode(stream, Mnemonic_CVTSS2SD, args);
+}
+
+// condition operations
+ENCODER_DECLARE_EXPORT char *cmov(char * stream, ConditionCode cc, const R_Opnd & r, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_r(args, r, sz);
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, (Mnemonic)(Mnemonic_CMOVcc + cc), args);
+}
+
+ENCODER_DECLARE_EXPORT char * setcc(char * stream, ConditionCode cc, const RM_Opnd & rm8) {
+    EncoderBase::Operands args;
+    add_rm(args, rm8, size_8);
+    return (char*)EncoderBase::encode(stream, (Mnemonic)(Mnemonic_SETcc + cc), args);
+}
+
+// load effective address: lea
+ENCODER_DECLARE_EXPORT char * lea(char * stream, const R_Opnd & r, const M_Opnd & m, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_r(args, r, sz);
+    add_m(args, m, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_LEA, args);
+}
+
+ENCODER_DECLARE_EXPORT char * cdq(char * stream) {
+    EncoderBase::Operands args;
+    args.add(RegName_EDX);
+    args.add(RegName_EAX);
+    return (char*)EncoderBase::encode(stream, Mnemonic_CDQ, args);
+}
+
+ENCODER_DECLARE_EXPORT char * wait(char * stream) {
+    return (char*)EncoderBase::encode(stream, Mnemonic_WAIT, EncoderBase::Operands());
+}
+
+// control-flow instructions
+
+// loop
+ENCODER_DECLARE_EXPORT char * loop(char * stream, const Imm_Opnd & imm) {
+    EncoderBase::Operands args;
+    assert(imm.get_size() == size_8);
+    args.add(RegName_ECX);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, Mnemonic_LOOP, args);
+}
+
+// jump
+ENCODER_DECLARE_EXPORT char * jump8(char * stream, const Imm_Opnd & imm) {
+    EncoderBase::Operands args;
+    assert(imm.get_size() == size_8);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, Mnemonic_JMP, args);
+}
+
+ENCODER_DECLARE_EXPORT char * jump32(char * stream, const Imm_Opnd & imm) {
+    EncoderBase::Operands args;
+    assert(imm.get_size() == size_32);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, Mnemonic_JMP, args);
+}
+
+ENCODER_DECLARE_EXPORT char * jump(char * stream, const RM_Opnd & rm, Opnd_Size sz) {
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_JMP, args);
+}
+
+/**
+ * @note On EM64T: if target lies beyond 2G (does not fit into 32 bit
+ *       offset) then generates indirect jump using RAX (whose content is
+ *       destroyed).
+ */
+ENCODER_DECLARE_EXPORT char * jump(char * stream, char * target) {
+#ifdef _EM64T_
+    int64 offset = target - stream;
+    // sub 2 bytes for the short version
+    offset -= 2;
+    if (fit8(offset)) {
+        // use 8-bit signed relative form
+        return jump8(stream, Imm_Opnd(size_8, offset));
+    } else if (fit32(offset)) {
+        // sub 5 (3 + 2)bytes for the long version
+        offset -= 3;
+        // use 32-bit signed relative form
+        return jump32(stream, Imm_Opnd(size_32, offset));
+    }
+    // need to use absolute indirect jump
+    stream = mov(stream, rax_opnd, Imm_Opnd(size_64, (int64)target), size_64);
+    return jump(stream, rax_opnd, size_64);
+#else
+    I_32 offset = target - stream;
+    // sub 2 bytes for the short version
+    offset -= 2;
+    if (fit8(offset)) {
+        // use 8-bit signed relative form
+        return jump8(stream, Imm_Opnd(size_8, offset));
+    }
+    // sub 5 (3 + 2) bytes for the long version
+    offset -= 3;
+    // use 32-bit signed relative form
+    return jump32(stream, Imm_Opnd(size_32, offset));
+#endif
+}
+
+// branch
+ENCODER_DECLARE_EXPORT char * branch8(char * stream, ConditionCode cond,
+                                      const Imm_Opnd & imm,
+                                      InstrPrefix pref)
+{
+    if (pref != no_prefix) {
+        assert(pref == hint_branch_taken_prefix || pref == hint_branch_taken_prefix);
+        stream = prefix(stream, pref);
+    }
+    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cond);
+    EncoderBase::Operands args;
+    assert(imm.get_size() == size_8);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, m, args);
+}
+
+ENCODER_DECLARE_EXPORT char * branch32(char * stream, ConditionCode cond,
+                                       const Imm_Opnd & imm,
+                                       InstrPrefix pref)
+{
+    if (pref != no_prefix) {
+        assert(pref == hint_branch_taken_prefix || pref == hint_branch_taken_prefix);
+        stream = prefix(stream, pref);
+    }
+    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cond);
+    EncoderBase::Operands args;
+    assert(imm.get_size() == size_32);
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, m, args);
+}
+
+/*
+ENCODER_DECLARE_EXPORT char * branch(char * stream, ConditionCode cc, const char * target, InstrPrefix prefix) {
+// sub 2 bytes for the short version
+int64 offset = stream-target-2;
+if( fit8(offset) ) {
+return branch8(stream, cc, Imm_Opnd(size_8, (char)offset), is_signed);
+}
+return branch32(stream, cc, Imm_Opnd(size_32, (int)offset), is_signed);
+}
+*/
+
+// call
+ENCODER_DECLARE_EXPORT char * call(char * stream, const Imm_Opnd & imm)
+{
+    EncoderBase::Operands args;
+    add_imm(args, imm);
+    return (char*)EncoderBase::encode(stream, Mnemonic_CALL, args);
+}
+
+ENCODER_DECLARE_EXPORT char * call(char * stream, const RM_Opnd & rm,
+                                   Opnd_Size sz)
+{
+    EncoderBase::Operands args;
+    add_rm(args, rm, sz);
+    return (char*)EncoderBase::encode(stream, Mnemonic_CALL, args);
+}
+
+/**
+* @note On EM64T: if target lies beyond 2G (does not fit into 32 bit
+*       offset) then generates indirect jump using RAX (whose content is
+*       destroyed).
+*/
+ENCODER_DECLARE_EXPORT char * call(char * stream, const char * target)
+{
+#ifdef _EM64T_
+    int64 offset = target - stream;
+    if (fit32(offset)) {
+        offset -= 5; // sub 5 bytes for this instruction
+        Imm_Opnd imm(size_32, offset);
+        return call(stream, imm);
+    }
+    // need to use absolute indirect call
+    stream = mov(stream, rax_opnd, Imm_Opnd(size_64, (int64)target), size_64);
+    return call(stream, rax_opnd, size_64);
+#else
+    I_32 offset = target - stream;
+    offset -= 5; // sub 5 bytes for this instruction
+    Imm_Opnd imm(size_32, offset);
+    return call(stream, imm);
+#endif
+}
+
+// return instruction
+ENCODER_DECLARE_EXPORT char * ret(char * stream)
+{
+    EncoderBase::Operands args;
+    return (char*)EncoderBase::encode(stream, Mnemonic_RET, args);
+}
+
+ENCODER_DECLARE_EXPORT char * ret(char * stream, const Imm_Opnd & imm)
+{
+    EncoderBase::Operands args;
+    // TheManual says imm can be 16-bit only
+    //assert(imm.get_size() <= size_16);
+    args.add(EncoderBase::Operand(map_size(size_16), imm.get_value()));
+    return (char*)EncoderBase::encode(stream, Mnemonic_RET, args);
+}
+
+ENCODER_DECLARE_EXPORT char * ret(char * stream, unsigned short pop)
+{
+    // TheManual says it can only be imm16
+    EncoderBase::Operands args(EncoderBase::Operand(OpndSize_16, pop, OpndExt_Zero));
+    return (char*)EncoderBase::encode(stream, Mnemonic_RET, args);
+}
+
+// floating-point instructions
+ENCODER_DECLARE_EXPORT char * fld(char * stream, const M_Opnd & m,
+                                  bool is_double) {
+    EncoderBase::Operands args;
+    // a fake FP register as operand
+    add_fp(args, 0, is_double);
+    add_m(args, m, is_double ? size_64 : size_32);
+    return (char*)EncoderBase::encode(stream, Mnemonic_FLD, args);
+}
+
+ENCODER_DECLARE_EXPORT char * fist(char * stream, const M_Opnd & mem,
+                                   bool is_long, bool pop_stk)
+{
+    EncoderBase::Operands args;
+    if (pop_stk) {
+        add_m(args, mem, is_long ? size_64 : size_32);
+        // a fake FP register as operand
+        add_fp(args, 0, is_long);
+        return (char*)EncoderBase::encode(stream,  Mnemonic_FISTP, args);
+    }
+    // only 32-bit operands are supported
+    assert(is_long == false);
+    add_m(args, mem, size_32);
+    add_fp(args, 0, false);
+    return (char*)EncoderBase::encode(stream,  Mnemonic_FIST, args);
+}
+
+ENCODER_DECLARE_EXPORT char * fst(char * stream, const M_Opnd & m,
+                                  bool is_double, bool pop_stk)
+{
+    EncoderBase::Operands args;
+    add_m(args, m, is_double ? size_64 : size_32);
+    // a fake FP register as operand
+    add_fp(args, 0, is_double);
+    return (char*)EncoderBase::encode(stream,
+                                    pop_stk ? Mnemonic_FSTP : Mnemonic_FST,
+                                    args);
+}
+
+ENCODER_DECLARE_EXPORT char * fst(char * stream, unsigned i, bool pop_stk)
+{
+    EncoderBase::Operands args;
+    add_fp(args, i, true);
+    return (char*)EncoderBase::encode(stream,
+                                    pop_stk ? Mnemonic_FSTP : Mnemonic_FST,
+                                    args);
+}
+
+ENCODER_DECLARE_EXPORT char * fldcw(char * stream, const M_Opnd & mem) {
+    EncoderBase::Operands args;
+    add_m(args, mem, size_16);
+    return (char*)EncoderBase::encode(stream, Mnemonic_FLDCW, args);
+}
+
+ENCODER_DECLARE_EXPORT char * fnstcw(char * stream, const M_Opnd & mem) {
+    EncoderBase::Operands args;
+    add_m(args, mem, size_16);
+    return (char*)EncoderBase::encode(stream, Mnemonic_FNSTCW, args);
+}
+
+ENCODER_DECLARE_EXPORT char * fnstsw(char * stream)
+{
+    return (char*)EncoderBase::encode(stream, Mnemonic_FNSTCW,
+                                      EncoderBase::Operands());
+}
+
+// string operations
+ENCODER_DECLARE_EXPORT char * set_d(char * stream, bool set) {
+    EncoderBase::Operands args;
+    return (char*)EncoderBase::encode(stream,
+                                      set ? Mnemonic_STD : Mnemonic_CLD,
+                                      args);
+}
+
+ENCODER_DECLARE_EXPORT char * scas(char * stream, unsigned char prefix)
+{
+	EncoderBase::Operands args;
+    if (prefix != no_prefix) {
+        assert(prefix == prefix_repnz || prefix == prefix_repz);
+        *stream = prefix;
+        ++stream;
+    }
+    return (char*)EncoderBase::encode(stream, Mnemonic_SCAS, args);
+}
+
+ENCODER_DECLARE_EXPORT char * stos(char * stream, unsigned char prefix)
+{
+    if (prefix != no_prefix) {
+        assert(prefix == prefix_rep);
+        *stream = prefix;
+        ++stream;
+    }
+
+	EncoderBase::Operands args;
+	return (char*)EncoderBase::encode(stream, Mnemonic_STOS, args);
+}
+
+// Intrinsic FP math functions
+
+ENCODER_DECLARE_EXPORT char * fprem(char * stream) {
+    return (char*)EncoderBase::encode(stream, Mnemonic_FPREM,
+                                      EncoderBase::Operands());
+}
+
+ENCODER_DECLARE_EXPORT char * fprem1(char * stream) {
+    return (char*)EncoderBase::encode(stream, Mnemonic_FPREM1,
+                                      EncoderBase::Operands());
+}
diff --git a/vm/compiler/codegen/x86/pcg/Android.mk b/vm/compiler/codegen/x86/pcg/Android.mk
index 2605c99..f5dc00f 100644
--- a/vm/compiler/codegen/x86/pcg/Android.mk
+++ b/vm/compiler/codegen/x86/pcg/Android.mk
@@ -67,7 +67,8 @@ LOCAL_C_INCLUDES += \
 	dalvik/vm \
 	dalvik/vm/compiler \
 	dalvik/vm/compiler/codegen/x86 \
-    dalvik/vm/compiler/codegen/x86/libenc \
+    dalvik/vm/compiler/codegen/x86/lightcg \
+    dalvik/vm/compiler/codegen/x86/lightcg/libenc \
     prebuilts/ndk/8/sources/cxx-stl/gnu-libstdc++/include \
     prebuilts/ndk/8/sources/cxx-stl/gnu-libstdc++/libs/x86/include \
     bionic \
@@ -123,7 +124,8 @@ ifeq ($(WITH_HOST_DALVIK),true)
         dalvik/vm \
         dalvik/vm/compiler \
         dalvik/vm/compiler/codegen/x86 \
-        dalvik/vm/compiler/codegen/x86/libenc \
+        dalvik/vm/compiler/codegen/x86/lightcg \
+        dalvik/vm/compiler/codegen/x86/lightcg/libenc \
         prebuilts/ndk/8/sources/cxx-stl/gnu-libstdc++/include \
         prebuilts/ndk/8/sources/cxx-stl/gnu-libstdc++/libs/x86/include \
         bionic \
-- 
1.7.4.1

