From 8e9b38e72e4542117c9f73b6adc4e8610399848d Mon Sep 17 00:00:00 2001
From: Razvan A Lupusoru <razvan.a.lupusoru@intel.com>
Date: Mon, 15 Jul 2013 15:48:15 -0700
Subject: Dalvik: Support inlining of non-throwing leaf methods

BZ: 118165

Inlining of non-throwing leaves is now supported. This is done by creating frame
for callee without fully initializing it. This allows a home location for all
virtual registers in the callee. However, registers of callee and caller must be
distinguished. For example, it is not possible for both callee and caller to have
a v0 because there is an implicit assumption everywhere that v0 always points to
same memory location. Additionally, VRs should never alias and all should point
to a different memory location. As a result, register window shift concept is
introduced which allows the ME to rename all virtual registers. All virtual
registers in the Compilation Unit are renamed to so that they are relative to
the register window shift needed. This works due to understanding of stack
layout which allows us to have all registers named relative to callee FP. All
renamed MIRs have a tag when printed that says "renamed:#".

Only LCG backend is supported. PCG backend has full implementation for inlining
but there are outstanding test failures. Thus complicated method inlining is
disabled for PCG.

The following command line options can be used to control inlining:
-Xjitdisableinlining - Used to disable method inlining
-Xjitinliningmethodsizemax:<value> - Used to specify the maximum number of
bytecodes for method to be considered for inlining.

Category: device-enablement
Domain: AOSP-Dalvik-Compiler-CG; AOSP-Dalvik-Compiler-ME; AOSP-Dalvik-Runtime
Origin: internal
Upstream-Candidate: no, needs rework

Change-Id: Ic1f0adf5b096342aa705c0dea2608701d7caa6ff
Orig-MCG-Change-Id: Ia45a71d3ad1500b3b4e9383063362a51fde55330
Signed-off-by: Razvan A Lupusoru <razvan.a.lupusoru@intel.com>
Reviewed-on: http://android.intel.com:8080/114912
Reviewed-by: Banerji, Udayan <udayan.banerji@intel.com>
Reviewed-by: Beyler, Jean Christophe <jean.christophe.beyler@intel.com>
Reviewed-by: Hartley, Timothy D <timothy.d.hartley@intel.com>
Reviewed-by: Dittert, Eric <eric.dittert@intel.com>
Reviewed-by: Chen, Dong-Yuan <dong-yuan.chen@intel.com>
Tested-by: Chen, Dong-Yuan <dong-yuan.chen@intel.com>
Reviewed-by: cactus <cactus@intel.com>
Tested-by: cactus <cactus@intel.com>
Signed-off-by: Serguei Katkov <serguei.i.katkov@intel.com>
---
 vm/Globals.h                                       |    8 +
 vm/Init.cpp                                        |   25 +-
 vm/compiler/BBOptimization.h                       |    2 +-
 vm/compiler/Checks.cpp                             |    9 +-
 vm/compiler/Compiler.h                             |   11 +-
 vm/compiler/CompilerIR.h                           |   84 ++-
 vm/compiler/Dataflow.cpp                           |  233 +++--
 vm/compiler/Dataflow.h                             |    2 +-
 vm/compiler/Frontend.cpp                           |   77 ++-
 vm/compiler/InlineTransformation.cpp               |  965 ++++++++++++++++----
 vm/compiler/IntermediateRep.cpp                    |  247 ++++-
 vm/compiler/Loop.cpp                               |   34 +-
 vm/compiler/LoopInformation.cpp                    |    2 +-
 vm/compiler/RegisterizationME.cpp                  |    2 +-
 vm/compiler/SSATransformation.cpp                  |   27 +-
 vm/compiler/StackExtension.h                       |    4 +-
 vm/compiler/Utility.cpp                            |  142 +++-
 vm/compiler/Utility.h                              |   17 +
 vm/compiler/codegen/CodegenFactory.cpp             |   24 +
 vm/compiler/codegen/CompilerCodegen.h              |    6 +
 vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp     |   36 +-
 .../codegen/x86/lightcg/BytecodeVisitor.cpp        |   15 +
 .../codegen/x86/lightcg/CodegenInterface.cpp       |  116 ++--
 .../codegen/x86/lightcg/CompilationUnit.cpp        |   13 +
 vm/compiler/codegen/x86/lightcg/CompilationUnit.h  |    6 +
 vm/compiler/codegen/x86/lightcg/CompileTable.cpp   |   30 +
 vm/compiler/codegen/x86/lightcg/CompileTable.h     |   31 +-
 .../codegen/x86/lightcg/ExceptionHandling.cpp      |    2 +-
 .../codegen/x86/lightcg/InstructionGeneration.cpp  |   50 +-
 .../codegen/x86/lightcg/InstructionGeneration.h    |    9 +
 vm/compiler/codegen/x86/lightcg/Lower.cpp          |    4 +-
 vm/compiler/codegen/x86/lightcg/Lower.h            |   65 +-
 vm/compiler/codegen/x86/lightcg/LowerAlu.cpp       |    3 +-
 vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp    |    2 +-
 vm/compiler/codegen/x86/lightcg/LowerHelper.cpp    |  136 ++-
 vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp    |   67 +-
 vm/compiler/codegen/x86/lightcg/LowerJump.cpp      |  271 +++---
 vm/compiler/codegen/x86/lightcg/LowerObject.cpp    |   19 +-
 vm/compiler/codegen/x86/lightcg/LowerReturn.cpp    |   18 +-
 vm/compiler/codegen/x86/lightcg/NcgAot.cpp         |   57 +-
 vm/compiler/codegen/x86/lightcg/NcgAot.h           |    6 +-
 .../codegen/x86/lightcg/RegisterizationBE.cpp      |    5 +-
 vm/compiler/codegen/x86/lightcg/Scheduler.cpp      |    3 +
 .../codegen/x86/lightcg/StackExtensionX86.cpp      |    9 +-
 .../codegen/x86/lightcg/libenc/enc_defs_ext.h      |    2 +
 .../codegen/x86/lightcg/libenc/enc_tabl.cpp        |   13 +
 .../codegen/x86/lightcg/libenc/enc_wrapper.h       |    1 -
 vm/compiler/codegen/x86/pcg/Analysis.cpp           |    4 +
 .../codegen/x86/pcg/ChainingCellException.cpp      |    3 +-
 vm/compiler/codegen/x86/pcg/CodeGeneration.cpp     |   10 +-
 vm/compiler/codegen/x86/pcg/LowerExtended.cpp      |   65 ++
 vm/compiler/codegen/x86/pcg/LowerExtended.h        |   16 +
 vm/compiler/codegen/x86/pcg/LowerMemory.cpp        |    4 +-
 vm/compiler/codegen/x86/pcg/LowerOther.cpp         |   34 -
 vm/compiler/codegen/x86/pcg/LowerOther.h           |    8 -
 vm/compiler/codegen/x86/pcg/PcgInterface.cpp       |    1 +
 vm/compiler/codegen/x86/pcg/UtilityPCG.cpp         |   71 +-
 vm/compiler/codegen/x86/pcg/UtilityPCG.h           |   15 +
 58 files changed, 2338 insertions(+), 803 deletions(-)

diff --git a/vm/Globals.h b/vm/Globals.h
index ed3b87f..3b20ff7 100644
--- a/vm/Globals.h
+++ b/vm/Globals.h
@@ -834,6 +834,9 @@ typedef struct sJitFramework
     /** @brief Invoke handler */
     const char* (*backEndInvokeArgsDone) (int);
 
+    /** @brief Used to check whether backend supports extended opcode */
+    bool (*backendSupportExtendedOp) (int);
+
     /** @brief Back-end callback to add a symbol at a specific location in the JIT code cache */
     void (*backEndSymbolCreationCallback) (const char *, void *);
 
@@ -1041,6 +1044,11 @@ struct DvmJitGlobals {
      */
     unsigned int maximumRegisterization;
 
+    /*
+     * Used to determine the maximum number of bytecode when considering method to inline
+     */
+    unsigned int maximumInliningNumBytecodes;
+
     /* Map to handle options for the backend */
     std::map<std::string, std::string> backendOptions;
 
diff --git a/vm/Init.cpp b/vm/Init.cpp
index 74711c0..0d3ad3e 100644
--- a/vm/Init.cpp
+++ b/vm/Init.cpp
@@ -182,7 +182,8 @@ static void usage(const char* progName)
     dvmFprintf(stderr, "  -Xjitdebugallpasses (Used to enable verbosity for all optimization passes)\n");
     dvmFprintf(stderr, "  -Xjitlooppasses (Prints the loop passes available)\n");
     dvmFprintf(stderr, "  -Xjitregisterization:<value> Request a maximum of registerization requests\n");
-    dvmFprintf(stderr, "  -Xjitdisableinlining Disable method inlining\n");
+    dvmFprintf(stderr, "  -Xjitdisableinlining Disables all method inlining\n");
+    dvmFprintf(stderr, "  -Xjitinliningmethodsizemax:<value> The maximum number of bytecodes a method can have to be considered for inlining\n");
 #if defined(VTUNE_DALVIK)
     dvmFprintf(stderr, "  -Xjitsepdalvik\n");
     dvmFprintf(stderr, "  -Xjitvtuneinfo:{none,jit,dex,src}\n");
@@ -1609,6 +1610,23 @@ int processOptions(int argc, const char* const argv[], bool ignoreUnrecognized)
             }
         } else if (strncmp(argv[i], "-Xjitdisableinlining", 20) == 0) {
             gDvmJit.disableOpt |= 1 << kMethodInlining;
+        } else if (strncmp(argv[i], "-Xjitinliningmethodsizemax:", strlen ("-Xjitinliningmethodsizemax:")) == 0) {
+            const unsigned int sizeOfOption = strlen ("-Xjitinliningmethodsizemax:");
+            char *endptr = NULL;
+
+            long maxBytecodes = strtoul (argv[i] + sizeOfOption, &endptr, 0);
+
+            if (endptr != NULL && *endptr == '\0' && maxBytecodes > 0 && errno != ERANGE)
+            {
+                gDvmJit.maximumInliningNumBytecodes = maxBytecodes;
+                dvmFprintf (stderr, "Set inlining max number of bytecodes to: %u\n",
+                        gDvmJit.maximumInliningNumBytecodes);
+            }
+            else
+            {
+                dvmFprintf (stderr, "Refusing option for %s, it is not a valid number: "
+                        "must be only a strictly positive number\n", argv[i]);
+            }
         } else if (strncmp(argv[i], "-Xjitoldloops", 13) == 0) {
             gDvmJit.oldLoopDetection = true;
         } else if (strncmp(argv[i], "-Xjitignorepasses:", strlen ("-Xjitignorepasses:")) == 0) {
@@ -1858,6 +1876,7 @@ static void setJitFramework ()
     jitFramework.backEndDumpSpecificBB = x86StandAloneArchSpecificDumpBB;
     jitFramework.backEndBasicBlockAllocation = x86StandAloneArchSpecificNewBB;
     jitFramework.backEndInvokeArgsDone = dvmCompilerHandleInvokeArgsHeader;
+    jitFramework.backendSupportExtendedOp = dvmCompilerArchSupportsExtendedOp;
 }
 #endif
 
@@ -1939,6 +1958,10 @@ static void setCommandLineDefaults()
     //Minimum vectorized iterations or we won't vectorize
     gDvmJit.minVectorizedIterations = 3;
 
+    //By default only inline methods that meet certain size requirements.
+    //This is configurable via command line.
+    gDvmJit.maximumInliningNumBytecodes = 20;
+
     gDvmJit.ignorePasses = 0;
     gDvmJit.debugPasses = 0;
     gDvmJit.debugAllPasses = false;
diff --git a/vm/compiler/BBOptimization.h b/vm/compiler/BBOptimization.h
index af573a2..1f4b8bf 100644
--- a/vm/compiler/BBOptimization.h
+++ b/vm/compiler/BBOptimization.h
@@ -90,7 +90,7 @@ bool dvmCompilerConvert2addr (CompilationUnit *cUnit, BasicBlock *bb);
 bool dvmCompilerAddInvokeSupportBlocks (CompilationUnit *cUnit, BasicBlock *bb);
 
 /**
- * @brief Goes through the given basic blocks and try to inline invokes
+ * @brief Goes through the given basic block and tries to inline invokes
  * @param cUnit The compilation unit
  * @param bb The basic block through which to look for invokes
  * @return Returns true if any method inlining was done
diff --git a/vm/compiler/Checks.cpp b/vm/compiler/Checks.cpp
index ab41d91..5e6e07d 100644
--- a/vm/compiler/Checks.cpp
+++ b/vm/compiler/Checks.cpp
@@ -991,11 +991,13 @@ bool dvmCompilerGenerateNullCheckHoist (BasicBlock *hoistToBB, int objectReg)
     //Check if we have a BB to hoist to
     if (hoistToBB != 0)
     {
+        const MIR *firstMir = hoistToBB->fallThrough->firstMIRInsn;
+
         //Now check if we can determine PC in case of exception
-        if (hoistToBB->fallThrough != 0 && hoistToBB->fallThrough->firstMIRInsn != 0)
+        if (hoistToBB->fallThrough != 0 && firstMir != 0)
         {
             //Do a sanity check on the block offset before we hoist. It should match the offset of first instruction
-            if (hoistToBB->fallThrough->startOffset == hoistToBB->fallThrough->firstMIRInsn->offset)
+            if (hoistToBB->fallThrough->startOffset == firstMir->offset)
             {
                 MIR *nullCheck = dvmCompilerNewMIR ();
 
@@ -1006,6 +1008,9 @@ bool dvmCompilerGenerateNullCheckHoist (BasicBlock *hoistToBB, int objectReg)
                 //For exception purpose, we set the offset to match the offset in the block following entry
                 nullCheck->offset = hoistToBB->fallThrough->startOffset;
 
+                //We also make sure that it has the same nesting information
+                nullCheck->nesting = firstMir->nesting;
+
                 //Now append the MIR to the BB
                 dvmCompilerAppendMIR (hoistToBB, nullCheck);
 
diff --git a/vm/compiler/Compiler.h b/vm/compiler/Compiler.h
index 49b7e6d..066c223 100644
--- a/vm/compiler/Compiler.h
+++ b/vm/compiler/Compiler.h
@@ -186,11 +186,12 @@ typedef enum DataFlowAnalysisMode {
 } DataFlowAnalysisMode;
 
 typedef struct CompilerMethodStats {
-    const Method *method;       // Used as hash entry signature
-    int dalvikSize;             // # of bytes for dalvik bytecodes
-    int compiledDalvikSize;     // # of compiled dalvik bytecodes
-    int nativeSize;             // # of bytes for produced native code
-    int attributes;             // attribute vector
+    const Method *method;          // Used as hash entry signature
+    int dalvikSize;                // # of bytes for dalvik bytecodes
+    int compiledDalvikSize;        // # of compiled dalvik bytecodes
+    int nativeSize;                // # of bytes for produced native code
+    int attributes;                // attribute vector
+    unsigned int numBytecodes;     // # of dalvik bytecodes
 } CompilerMethodStats;
 
 struct CompilationUnit;
diff --git a/vm/compiler/CompilerIR.h b/vm/compiler/CompilerIR.h
index 56e549d..6ad2f20 100644
--- a/vm/compiler/CompilerIR.h
+++ b/vm/compiler/CompilerIR.h
@@ -168,6 +168,12 @@ enum ExtendedMIROpcode {
      */
     kMirOpPackedSet,
 
+    /**
+     * @brief Check if creating frame for target method will cause a stack overflow.
+     * @details vB holds size of managed frame for target method.
+     */
+    kMirOpCheckStackOverflow,
+
     /** @brief Last enumeration: not used except for array bounds */
     kMirOpLast,
 };
@@ -214,6 +220,23 @@ typedef struct sInstructionColor {
     MIR *next;                          /**< @brief Next instruction in the color */
 }SInstructionColor;
 
+/**
+ * @brief Used to keep track of nesting level of a bytecode
+ */
+typedef struct NestedMethod {
+    /**
+     * @brief Constructor for nested method information. Sets up parent as null.
+     * @param source The source method
+     */
+    NestedMethod (const Method *source) :
+            parent (0), sourceMethod (source)
+    {
+    }
+
+    NestedMethod *parent;       //!< The nesting information of parent. If 0, the sourceMethod matches cUnit's method
+    const Method *sourceMethod; //!< The source method of the bytecode
+} NestedMethod;
+
 typedef struct MIR {
     DecodedInstruction dalvikInsn;
     unsigned int width;
@@ -229,6 +252,21 @@ typedef struct MIR {
     struct SSARepresentation *ssaRep;
     int OptimizationFlags;
     int seqNum;
+
+    /**
+     * @brief Used to keep track of the nesting level of the MIR
+     */
+    NestedMethod nesting;
+
+    /**
+     * @brief Used to keep track of renaming offset.
+     * @details For example if v1 was renamed to v3, this holds value of 2 (3 - 1).
+     * This field is similar to CompilationUnit::registerWindowShift in that it captures renaming
+     * of virtual register. However, this field is specific to the instruction itself because
+     * depending on source method, different MIRs get renamed differently.
+     */
+    int virtualRegRenameOffset;
+
     union {
         // Used by the inlined insn from the callee to find the mother method
         const Method *calleeMethod;
@@ -440,7 +478,13 @@ typedef struct CompilationUnit {
 
     JitMode jitMode;
     int numReachableBlocks;
-    int numDalvikRegisters;             // method->registersSize + inlined
+
+    /**
+     * @brief Keeps track of number of registers in the cUnit.
+     * @details Takes into account method->registersSize, inlined registers, and scratch registers.
+     */
+    int numDalvikRegisters;
+
     BasicBlock *entryBlock;
     BasicBlock *exitBlock;
     BasicBlock *puntBlock;              // punting to interp for exceptions
@@ -453,9 +497,10 @@ typedef struct CompilationUnit {
     int defBlockMatrixSize;             // Size of defBlockMatrix
     BitVector **defBlockMatrix;         // numDalvikRegister x numBlocks
     BitVector *tempBlockV;
-    BitVector *tempDalvikRegisterV;
-    BitVector *tempSSARegisterV;        // numSSARegs
-    BitVector *usedVRs;                 //Which VRs are being used
+
+    BitVector *tempDalvikRegisterV;     //!< Temporary vector used during dataflow to store dalvik registers
+    BitVector *tempSSARegisterV;        //!< Temporary vector used during dataflow to store SSA registers
+    BitVector *usedVRs;                 //!< Which VRs are being used
 
     SPhiVectors phi;
 
@@ -464,6 +509,20 @@ typedef struct CompilationUnit {
     bool quitLoopMode;                  // cold path/complex bytecode
 
     std::map<int, int> *constantValues; // Constant values map using the ssa register as a key
+
+    /**
+     * @brief All virtual registers in compilation unit are relative to frame pointer modified with this offset.
+     * @details The frame pointer is designed to point to place on stack where a method's virtual registers
+     * exist. But when inlining methods, we bring in even more virtual registers. Since we create a frame for
+     * callee as well, those VRs are negative direction (in direction of stack growth). But in order to allow
+     * all algorithms to work, we rename all VRs to have positive values relative to frame pointer. But we need
+     * to specify the modified on the frame pointer itself. For example if this holds a value of 2, if you see v3
+     * in the compilation unit after rename, then if the MIR's source method is not inlined, then v3 was actually
+     * originally v1. The MIR::virtualRegRenameOffset would capture 2 as well in this case. But for MIRs from inlined
+     * methods, the MIR::virtualRegRenameOffset may be different. The distinction is that registerWindowShift captures
+     * the global renaming property for the frame pointer which applies for all virtual registers in cUnit.
+     */
+    int registerWindowShift;
 }CompilationUnit;
 
 #if defined(WITH_SELF_VERIFICATION)
@@ -611,6 +670,13 @@ void dvmCompilerUpdatePredecessors(BasicBlock *parent, BasicBlock *oldChild, Bas
 MIR *dvmCompilerCopyMIR (MIR *orig);
 
 /**
+ * @brief Get the dalvik PC for a MIR
+ * @param mir The bytecode whose PC to determine
+ * @return Returns the dalvik PC if the bytecode originates from dex file. Otherwise returns 0.
+ */
+u2 * dvmCompilerGetDalvikPC (const MIR *mir);
+
+/**
  * @brief Determines if ssa reg define is live out of current basic block.
  * @param cUnit The compilation unit.
  * @param bb The basic block we want to look at.
@@ -666,6 +732,16 @@ bool dvmCompilerRewriteMirDef (MIR *mir, int oldVR, int newVR, bool shouldRewrit
 bool dvmCompilerRewriteMirUses (MIR *mir, int oldVR, int newVR);
 
 /**
+ * @brief Used to rewrite the virtual register numbers of a MIR
+ * @details This is guaranteed to not rewrite unless it will succeed.
+ * @param dalvikInsn The dalvik instruction to update
+ * @param oldToNew The map of old to new virtual registers
+ * @param onlyUses Flag on whether only uses should be updated
+ * @return Returns whether the rewrite was successful
+ */
+bool dvmCompilerRewriteMirVRs (DecodedInstruction &dalvikInsn, const std::map<int, int> &oldToNew, bool onlyUses = true);
+
+/**
  * @brief Given a dalvik Opcode or an extended opcode, it returns the flags.
  * @param opcode The opcode for which to get flags
  * @return Returns the flags for the specified opcode.
diff --git a/vm/compiler/Dataflow.cpp b/vm/compiler/Dataflow.cpp
index 02126e3..bf2af24 100644
--- a/vm/compiler/Dataflow.cpp
+++ b/vm/compiler/Dataflow.cpp
@@ -821,10 +821,10 @@ int dvmCompilerDataFlowAttributes[kMirOpLast] = {
     DF_FORMAT_EXT_OP,
 
     //kMirOpPunt
-    DF_FORMAT_EXT_OP,
+    DF_NOP,
 
     //kMirOpCheckInlinePrediction
-    DF_FORMAT_EXT_OP,
+    DF_UC | DF_NULL_OBJECT_CHECK_0,
 
     //kMirOpNullCheck
     DF_FORMAT_EXT_OP | DF_NULL_OBJECT_CHECK_0,
@@ -852,6 +852,9 @@ int dvmCompilerDataFlowAttributes[kMirOpLast] = {
 
     //kMirOpPackedSet,
     DF_UB,
+
+    //kMirOpCheckStackOverflow
+    DF_NOP,
 };
 
 /* Return the Dalvik register/subscript pair of a given SSA register */
@@ -1086,8 +1089,20 @@ void dvmCompilerExtendedDisassembler (const CompilationUnit *cUnit,
             }
         }
 
+        const unsigned int renameNoteSize = 32;
+        char renamingOffsetNote[renameNoteSize];
+
+        if (mir->virtualRegRenameOffset != 0)
+        {
+            snprintf (renamingOffsetNote, renameNoteSize, " (renamed: %d)", mir->virtualRegRenameOffset);
+        }
+        else
+        {
+            renamingOffsetNote[0] = 0;
+        }
+
         //Now actually put everything in the desired buffer
-        snprintf (buffer, len, "%s%s%s", decodedInstruction, checkEliminationNote, inliningNote);
+        snprintf (buffer, len, "%s%s%s%s", decodedInstruction, renamingOffsetNote, checkEliminationNote, inliningNote);
 
         return;
     }
@@ -1269,6 +1284,9 @@ void dvmCompilerExtendedDisassembler (const CompilationUnit *cUnit,
                 snprintf (buffer, len, "kMirOpNullCheck v%d", insn->vA);
             }
             break;
+        case kMirOpCheckStackOverflow:
+            snprintf (buffer, len, "kMirOpCheckStackOverflow #%d", insn->vB);
+            break;
         default:
             snprintf (buffer, len, "Unknown Extended Opcode");
             break;
@@ -1475,9 +1493,6 @@ void handleExtOpUses (BitVector *useV, BitVector *defV, MIR *mir)
         case kMirOpPhi:
             //Phi nodes shouldn't be updating the ssa so we make it seem that it has no uses
             break;
-        case kMirOpPunt:
-            //No uses
-            break;
         case kMirOpNullNRangeUpCheck:
         case kMirOpNullNRangeDownCheck:
             //vA holds the array pointer register
@@ -1489,10 +1504,6 @@ void handleExtOpUses (BitVector *useV, BitVector *defV, MIR *mir)
             //vA holds the index register
             handleUse (useV, defV, dInsn->vA);
             break;
-        case kMirOpCheckInlinePrediction:
-            //vC holds the reference register
-            handleUse (useV, defV, dInsn->vC);
-            break;
         case kMirOpNullCheck:
             //We only reference the register if we need to do a null check
             if ((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
@@ -1538,8 +1549,6 @@ void handleExtOpDefs (BitVector *defV, DecodedInstruction* dInsn)
         case kMirOpNullNRangeUpCheck:
         case kMirOpNullNRangeDownCheck:
         case kMirOpLowerBound:
-        case kMirOpPunt:
-        case kMirOpCheckInlinePrediction:
         case kMirOpNullCheck:
         case kMirOpBoundCheck:
         case kMirOpRegisterize:
@@ -1560,93 +1569,123 @@ void handleExtOpDefs (BitVector *defV, DecodedInstruction* dInsn)
  */
 bool dvmCompilerFindLocalLiveIn(CompilationUnit *cUnit, BasicBlock *bb)
 {
-    MIR *mir;
-    BitVector *useV, *defV;
-
     if (bb->dataFlowInfo == NULL) return false;
 
     //If not allocated yet
     if (bb->dataFlowInfo->useV == 0)
     {
-        bb->dataFlowInfo->useV =
-            dvmCompilerAllocBitVector(cUnit->numDalvikRegisters, false);
+        bb->dataFlowInfo->useV = dvmCompilerAllocBitVector (cUnit->numDalvikRegisters, true);
     }
     else
     {
+        //Ensure that BitVector grows to the number of dalvik registers
+        dvmCompilerSetBit (bb->dataFlowInfo->useV, cUnit->numDalvikRegisters - 1);
         dvmClearAllBits (bb->dataFlowInfo->useV);
     }
 
-    //Get local version
-    useV = bb->dataFlowInfo->useV;
-
     //If not allocated yet
     if (bb->dataFlowInfo->defV == 0)
     {
-        bb->dataFlowInfo->defV =
-            dvmCompilerAllocBitVector(cUnit->numDalvikRegisters, false);
+        bb->dataFlowInfo->defV = dvmCompilerAllocBitVector (cUnit->numDalvikRegisters, true);
     }
     else
     {
+        //Ensure that BitVector grows to the number of dalvik registers
+        dvmCompilerSetBit (bb->dataFlowInfo->defV, cUnit->numDalvikRegisters - 1);
         dvmClearAllBits (bb->dataFlowInfo->defV);
     }
 
-    //Get local version
-    defV = bb->dataFlowInfo->defV;
-
     //If not allocated yet
     if (bb->dataFlowInfo->liveInV == 0)
     {
-        bb->dataFlowInfo->liveInV =
-            dvmCompilerAllocBitVector(cUnit->numDalvikRegisters, false);
+        bb->dataFlowInfo->liveInV = dvmCompilerAllocBitVector (cUnit->numDalvikRegisters, true);
     }
     else
     {
+        //Ensure that BitVector grows to the number of dalvik registers
+        dvmCompilerSetBit (bb->dataFlowInfo->liveInV, cUnit->numDalvikRegisters - 1);
         dvmClearAllBits (bb->dataFlowInfo->liveInV);
     }
 
     //If not allocated yet
     if (bb->dataFlowInfo->liveOutV == 0)
     {
-        bb->dataFlowInfo->liveOutV =
-            dvmCompilerAllocBitVector(cUnit->numDalvikRegisters, false);
+        bb->dataFlowInfo->liveOutV = dvmCompilerAllocBitVector (cUnit->numDalvikRegisters, true);
     }
     else
     {
+        //Ensure that BitVector grows to the number of dalvik registers
+        dvmCompilerSetBit (bb->dataFlowInfo->liveOutV, cUnit->numDalvikRegisters - 1);
         dvmClearAllBits (bb->dataFlowInfo->liveOutV);
     }
 
-    for (mir = bb->firstMIRInsn; mir; mir = mir->next) {
+    //Get local versions
+    BitVector *defV = bb->dataFlowInfo->defV;
+    BitVector *useV = bb->dataFlowInfo->useV;
+
+    //When handling uses of registers for exceptions, we want to skip the scratch registers
+    //because they are not live outside of trace.
+    std::set<int> scratchRegisters;
+    for (unsigned int scratch = 0; scratch < dvmArchSpecGetNumberOfScratch (); scratch++)
+    {
+        scratchRegisters.insert (dvmArchSpecGetScratchRegister (cUnit->method, scratch, cUnit->registerWindowShift));
+    }
+
+    for (MIR *mir = bb->firstMIRInsn; mir; mir = mir->next) {
         int dfAttributes =
             dvmCompilerDataFlowAttributes[mir->dalvikInsn.opcode];
         DecodedInstruction *dInsn = &mir->dalvikInsn;
 
-        if (backendCanBailOut(cUnit, mir) == true) {
-            for (int i = 0; i < cUnit->method->registersSize; i++) {
-                handleUse(useV, defV, i);
+        //If backend can bail out, ensure that all reaching definitions are uses
+        if (backendCanBailOut (cUnit, mir) == true)
+        {
+            //At this point we could actually compute reaching definitions but let's just be
+            //conservative that all registers in cUnit are uses. However, we can skip some
+            //registers if they are from inlined method and we are in caller.
+
+            int start = 0;
+            if (mir->nesting.parent == 0)
+            {
+                //We are not in nested method and therefore we only need to consider caller registers.
+                //The reason for this is that when we have an inlined body we have the whole scope so
+                //we have no registers that are live from outside trace. Thus when we are going through
+                //mir that is in caller, the callee frame doesn't even exist.
+                start = cUnit->registerWindowShift;
             }
-        } else {
-            if (dfAttributes & DF_HAS_USES) {
-                if (dfAttributes & DF_UA) {
-                    handleUse(useV, defV, dInsn->vA);
-                } else if (dfAttributes & DF_UA_WIDE) {
-                    handleUse(useV, defV, dInsn->vA);
-                    handleUse(useV, defV, dInsn->vA + 1);
-                }
-                if (dfAttributes & DF_UB) {
-                    handleUse(useV, defV, dInsn->vB);
-                } else if (dfAttributes & DF_UB_WIDE) {
-                    handleUse(useV, defV, dInsn->vB);
-                    handleUse(useV, defV, dInsn->vB + 1);
-                }
-                if (dfAttributes & DF_UC) {
-                    handleUse(useV, defV, dInsn->vC);
-                } else if (dfAttributes & DF_UC_WIDE) {
-                    handleUse(useV, defV, dInsn->vC);
-                    handleUse(useV, defV, dInsn->vC + 1);
+
+            //Go through the dalvik registers and add them as explicit uses
+            for (int i = start; i < cUnit->numDalvikRegisters; i++)
+            {
+                //Now check if the register we are looking at now is scratch.
+                //If it is, then we don't add a use for it.
+                if (scratchRegisters.find (i) == scratchRegisters.end ())
+                {
+                    handleUse (useV, defV, i);
                 }
             }
         }
 
+        if (dfAttributes & DF_HAS_USES) {
+            if (dfAttributes & DF_UA) {
+                handleUse(useV, defV, dInsn->vA);
+            } else if (dfAttributes & DF_UA_WIDE) {
+                handleUse(useV, defV, dInsn->vA);
+                handleUse(useV, defV, dInsn->vA + 1);
+            }
+            if (dfAttributes & DF_UB) {
+                handleUse(useV, defV, dInsn->vB);
+            } else if (dfAttributes & DF_UB_WIDE) {
+                handleUse(useV, defV, dInsn->vB);
+                handleUse(useV, defV, dInsn->vB + 1);
+            }
+            if (dfAttributes & DF_UC) {
+                handleUse(useV, defV, dInsn->vC);
+            } else if (dfAttributes & DF_UC_WIDE) {
+                handleUse(useV, defV, dInsn->vC);
+                handleUse(useV, defV, dInsn->vC + 1);
+            }
+        }
+
         if (dfAttributes & DF_HAS_DEFS) {
             handleDef(defV, dInsn->vA);
             if (dfAttributes & DF_DA_WIDE) {
@@ -1899,9 +1938,6 @@ static void dataFlowSSAFormatExtendedOp(CompilationUnit *cUnit, MIR *mir)
 
     switch (static_cast<int> (mir->dalvikInsn.opcode))
     {
-        case kMirOpPunt:
-            //No uses or defs
-            break;
         case kMirOpPhi:
             handleSSADef (cUnit, defs, mir->dalvikInsn.vA, numDefs);
             numDefs++;
@@ -1917,10 +1953,6 @@ static void dataFlowSSAFormatExtendedOp(CompilationUnit *cUnit, MIR *mir)
             handleSSAUse(cUnit, uses, mir->dalvikInsn.vA, numUses);
             numUses++;
             break;
-        case kMirOpCheckInlinePrediction:
-            handleSSAUse(cUnit, uses, mir->dalvikInsn.vC, numUses);
-            numUses++;
-            break;
         case kMirOpNullCheck:
             //We only have a use if we need to do a null check
             if ((mir->OptimizationFlags & MIR_IGNORE_NULL_CHECK) == 0)
@@ -2012,15 +2044,32 @@ bool dvmCompilerDoSSAConversion(CompilationUnit *cUnit, BasicBlock *bb)
     }
     bb->visited = true;
 
-    //Next remember SSA state at entrance
-    if (bb->dataFlowInfo->dalvikToSSAMapEntrance == 0)
+    const unsigned int numDalvikRegisters = cUnit->numDalvikRegisters;
+
+    //We want to remember state at entrance into BB but we need space to store it.
+    //In case the BB's dalvikToSSAMap isn't allocated yet or we need larger size, we allocate it now.
+    if (bb->dataFlowInfo->dalvikToSSAMapEntrance == 0 || numDalvikRegisters != bb->dataFlowInfo->numEntriesDalvikToSSAMap)
+    {
+        bb->dataFlowInfo->dalvikToSSAMapEntrance = static_cast<int *> (dvmCompilerNew (
+                sizeof (* (bb->dataFlowInfo->dalvikToSSAMapEntrance)) * numDalvikRegisters, true));
+    }
+
+    //Do the same for exit as we did for entrance
+    if (bb->dataFlowInfo->dalvikToSSAMapExit == 0 || numDalvikRegisters != bb->dataFlowInfo->numEntriesDalvikToSSAMap)
     {
-        bb->dataFlowInfo->dalvikToSSAMapEntrance = static_cast<int *> (dvmCompilerNew (sizeof (* (bb->dataFlowInfo->dalvikToSSAMapEntrance)) * cUnit->numDalvikRegisters, true));
+        bb->dataFlowInfo->dalvikToSSAMapExit = static_cast<int *> (dvmCompilerNew (
+                sizeof (* (bb->dataFlowInfo->dalvikToSSAMapExit)) * numDalvikRegisters, true));
     }
 
+    //Number of entries in the dalvikToSSAMaps now matches number of registers
+    bb->dataFlowInfo->numEntriesDalvikToSSAMap = numDalvikRegisters;
+
+    //For ensuring sanity of memcpy, we check that type matches because both structures should be same size
+    assert (sizeof (*(cUnit->dalvikToSSAMap)) == sizeof (*(bb->dataFlowInfo->dalvikToSSAMapEntrance)));
+
     //Remember the state we were at when starting the BasicBlock
     memcpy(bb->dataFlowInfo->dalvikToSSAMapEntrance, cUnit->dalvikToSSAMap,
-           sizeof (* (bb->dataFlowInfo->dalvikToSSAMapEntrance)) * cUnit->numDalvikRegisters);
+           sizeof (* (bb->dataFlowInfo->dalvikToSSAMapEntrance)) * numDalvikRegisters);
 
     for (mir = bb->firstMIRInsn; mir; mir = mir->next) {
         //If not yet generated
@@ -2147,22 +2196,16 @@ bool dvmCompilerDoSSAConversion(CompilationUnit *cUnit, BasicBlock *bb)
         dvmCompilerDoSSAConversion (cUnit, bb->fallThrough);
     }
 
-
-    //Do we have an exit?
-    if (bb->dataFlowInfo->dalvikToSSAMapExit == 0)
-    {
-        bb->dataFlowInfo->dalvikToSSAMapExit = static_cast<int *> (dvmCompilerNew (sizeof (* (bb->dataFlowInfo->dalvikToSSAMapExit)) * cUnit->numDalvikRegisters, true));
-    }
+    //For ensuring sanity of memcpy, we check that type matches because both structures should be same size
+    assert (sizeof (*(cUnit->dalvikToSSAMap)) == sizeof (*(bb->dataFlowInfo->dalvikToSSAMapExit)));
 
     //Copy the state also to exit, this is used by any PHI operand calculation
     memcpy(bb->dataFlowInfo->dalvikToSSAMapExit, cUnit->dalvikToSSAMap,
-           sizeof (* (bb->dataFlowInfo->dalvikToSSAMapExit)) * cUnit->numDalvikRegisters);
-
+           sizeof (* (bb->dataFlowInfo->dalvikToSSAMapExit)) * numDalvikRegisters);
 
     //Copy the entrance back to cUnit, this is used to know the SSA registers associated to VRs at the entrance of a BB
     memcpy (cUnit->dalvikToSSAMap, bb->dataFlowInfo->dalvikToSSAMapEntrance,
-           sizeof (* (bb->dataFlowInfo->dalvikToSSAMapEntrance)) * cUnit->numDalvikRegisters);
-
+           sizeof (* (bb->dataFlowInfo->dalvikToSSAMapEntrance)) * numDalvikRegisters);
 
     return true;
 }
@@ -2961,24 +3004,8 @@ void dvmCompilerFindInductionVariables(CompilationUnit *cUnit,
 /* Setup the basic data structures for SSA conversion */
 void dvmInitializeSSAConversion(CompilationUnit *cUnit)
 {
-    int i;
     int numDalvikReg = cUnit->numDalvikRegisters;
 
-    //If ever the new number is not the same as before, invalidate all dataflow and ssa structures
-    if (numDalvikReg > cUnit->numSSARegs)
-    {
-        cUnit->ssaToDalvikMap = 0;
-        cUnit->dalvikToSSAMap = 0;
-        cUnit->defBlockMatrix = 0;
-        cUnit->ssaSubScripts = 0;
-
-        //For tempDalvikRegisterV, we don't have to set it to 0, we can just expand it
-        if (cUnit->tempDalvikRegisterV != 0)
-        {
-            dvmSetBit (cUnit->tempDalvikRegisterV, numDalvikReg - 1);
-        }
-    }
-
     if (cUnit->ssaToDalvikMap == 0)
     {
         cUnit->ssaToDalvikMap = static_cast<GrowableList *> (dvmCompilerNew (sizeof (* (cUnit->ssaToDalvikMap)), false));
@@ -2995,7 +3022,7 @@ void dvmInitializeSSAConversion(CompilationUnit *cUnit)
      * the subscript is 0 so we use the ENCODE_REG_SUB macro to encode the value
      * into "(0 << 16) | i"
      */
-    for (i = 0; i < numDalvikReg; i++) {
+    for (int i = 0; i < numDalvikReg; i++) {
         dvmInsertGrowableList(cUnit->ssaToDalvikMap, ENCODE_REG_SUB(i, 0));
     }
 
@@ -3009,6 +3036,10 @@ void dvmInitializeSSAConversion(CompilationUnit *cUnit)
         cUnit->dalvikToSSAMap = static_cast<int *> (dvmCompilerNew (sizeof (* (cUnit->dalvikToSSAMap)) * numDalvikReg, false));
     }
 
+    for (int i = 0; i < numDalvikReg; i++) {
+        cUnit->dalvikToSSAMap[i] = i;
+    }
+
     /**
      * Initialize the SSA subscript array. This provides a means to get a unique subscript
      * for each register and start them all at 0. A unique counter is also possible but it
@@ -3021,27 +3052,27 @@ void dvmInitializeSSAConversion(CompilationUnit *cUnit)
     else
     {
         //Otherwise set it back to 0
-        for (i = 0; i < numDalvikReg; i++) {
+        for (int i = 0; i < numDalvikReg; i++) {
             cUnit->ssaSubScripts[i] = 0;
         }
     }
 
+    // Constant propagation: allocate the vector if required
+    if (cUnit->isConstantV == 0)
+    {
+        cUnit->isConstantV = dvmCompilerAllocBitVector (numDalvikReg, true);
+    }
+    else
+    {
+        dvmClearAllBits (cUnit->isConstantV);
+    }
+
     /*
      * Initial number of SSA registers is equal to the number of Dalvik
      * registers.
      */
     cUnit->numSSARegs = numDalvikReg;
 
-    for (i = 0; i < numDalvikReg; i++) {
-        cUnit->dalvikToSSAMap[i] = i;
-    }
-
-    // Constant propagation: allocate the vector if required
-    if (cUnit->isConstantV == 0)
-    {
-        cUnit->isConstantV = dvmCompilerAllocBitVector(cUnit->numSSARegs, true);
-    }
-
     /*
      * Allocate the BasicBlockDataFlow structure for the entry and code blocks
      */
diff --git a/vm/compiler/Dataflow.h b/vm/compiler/Dataflow.h
index ec3c900..37efedf 100644
--- a/vm/compiler/Dataflow.h
+++ b/vm/compiler/Dataflow.h
@@ -117,9 +117,9 @@ typedef struct BasicBlockDataFlow {
     BitVector *defV;
     BitVector *liveInV;
     BitVector *liveOutV;
-    BitVector *phiV;
     int *dalvikToSSAMapExit;
     int *dalvikToSSAMapEntrance;
+    unsigned int numEntriesDalvikToSSAMap; //!< Represents number of entries in each of the dalvikToSSAMap
 } BasicBlockDataFlow;
 
 /**
diff --git a/vm/compiler/Frontend.cpp b/vm/compiler/Frontend.cpp
index 9e4665a..1d732c0 100644
--- a/vm/compiler/Frontend.cpp
+++ b/vm/compiler/Frontend.cpp
@@ -306,6 +306,7 @@ CompilerMethodStats *dvmCompilerAnalyzeMethodBody(const Method *method,
     const u2 *codeEnd = dexCode->insns + dexCode->insnsSize;
     int insnSize = 0;
     int hashValue = dvmComputeUtf8Hash(method->name);
+    unsigned int numBytecodes = 0;
 
     CompilerMethodStats dummyMethodEntry; // For hash table lookup
     CompilerMethodStats *realMethodEntry; // For hash table storage
@@ -363,6 +364,9 @@ CompilerMethodStats *dvmCompilerAnalyzeMethodBody(const Method *method,
         if (width == 0)
             break;
 
+        /* We have a valid instruction so increment number of instructions */
+        numBytecodes++;
+
         if (isCallee) {
             attributes = analyzeInlineTarget(&dalvikInsn, attributes, insnSize);
         }
@@ -380,7 +384,16 @@ CompilerMethodStats *dvmCompilerAnalyzeMethodBody(const Method *method,
         attributes &= ~(METHOD_IS_GETTER | METHOD_IS_SETTER);
     }
 
-    realMethodEntry->dalvikSize = insnSize * 2;
+    /*
+     * Each bytecode unit is 2 bytes large so to get the total size we multiply
+     * number of bytecode units by size of bytecode unit.
+     */
+    realMethodEntry->dalvikSize = insnSize * sizeof (u2);
+
+    /* Keep track of number of bytecodes in method */
+    realMethodEntry->numBytecodes = numBytecodes;
+
+    /* Set the method attributes */
     realMethodEntry->attributes |= attributes;
 
 #if 0
@@ -1674,15 +1687,23 @@ bool dvmCompilerBuildCFG (const Method *method, GrowableList *blockList, BasicBl
 
     /* Parse all instructions and put them into containing basic blocks */
     while (codePtr < codeEnd) {
-        MIR *insn = dvmCompilerNewMIR ();
-        insn->offset = curOffset;
-        int width = parseInsn (codePtr, &insn->dalvikInsn, false);
-        insn->width = width;
+        /* Parse instruction */
+        DecodedInstruction dalvikInsn;
+        int width = parseInsn (codePtr, &dalvikInsn, false);
 
         /* Terminate when the data section is seen */
         if (width == 0)
             break;
 
+        /* Set up MIR */
+        MIR *insn = dvmCompilerNewMIR ();
+        insn->dalvikInsn = dalvikInsn;
+        insn->offset = curOffset;
+        insn->width = width;
+
+        /* Keep track which method this MIR is from. Initially we assume that there is no method nesting caused by inlining */
+        insn->nesting.sourceMethod = method;
+
         if (bytecodeGate != 0)
         {
             bool accept = bytecodeGate (method, &insn->dalvikInsn, 0);
@@ -1820,9 +1841,8 @@ bool dvmCompileMethod(const Method *method, JitTranslationInfo *info)
     /* Now that we finished inserting blocks, let's update the number of blocks in cUnit */
     cUnit.numBlocks = dvmGrowableListSize (&cUnit.blockList);
 
-    /* Adjust this value accordingly once inlining is performed */
-    cUnit.numDalvikRegisters = cUnit.method->registersSize
-            + dvmArchSpecGetNumberOfScratch();
+    const int numDalvikRegisters = cUnit.method->registersSize + dvmArchSpecGetNumberOfScratch ();
+    dvmCompilerUpdateCUnitNumDalvikRegisters (&cUnit, numDalvikRegisters);
 
     /* Verify if all blocks are connected as claimed */
     /* FIXME - to be disabled in the future */
@@ -1908,15 +1928,24 @@ static bool exhaustTrace(CompilationUnit *cUnit, BasicBlock *curBlock)
           return changed;
     }
     while (true) {
-        MIR *insn = (MIR *) dvmCompilerNew(sizeof(MIR), true);
-        insn->offset = curOffset;
-        int width = parseInsn(codePtr, &insn->dalvikInsn, false);
-        insn->width = width;
+        /* Parse instruction */
+        DecodedInstruction dalvikInsn;
+        int width = parseInsn (codePtr, &dalvikInsn, false);
 
         /* Terminate when the data section is seen */
         if (width == 0)
             break;
 
+        /* Set up MIR */
+        MIR *insn = dvmCompilerNewMIR ();
+        insn->dalvikInsn = dalvikInsn;
+        insn->offset = curOffset;
+        insn->width = width;
+
+        /* Keep track which method this MIR is from. Initially we assume that there is no method nesting caused by inlining */
+        insn->nesting.sourceMethod = cUnit->method;
+
+        /* Add it to current BB */
         dvmCompilerAppendMIR(curBlock, insn);
 
         codePtr += width;
@@ -2049,6 +2078,7 @@ static bool compileLoop(CompilationUnit *cUnit, unsigned int startOffset,
                         JitTranslationInfo *info, jmp_buf *bailPtr,
                         int optHints)
 {
+    int numDalvikRegisters = 0;
     unsigned int curOffset = startOffset;
     bool changed;
 #if defined(WITH_JIT_TUNING)
@@ -2122,8 +2152,8 @@ static bool compileLoop(CompilationUnit *cUnit, unsigned int startOffset,
     }
 #endif
 
-    cUnit->numDalvikRegisters = cUnit->method->registersSize
-            + dvmArchSpecGetNumberOfScratch();
+    numDalvikRegisters = cUnit->method->registersSize + dvmArchSpecGetNumberOfScratch ();
+    dvmCompilerUpdateCUnitNumDalvikRegisters (cUnit, numDalvikRegisters);
 
     /* Verify if all blocks are connected as claimed */
     /* FIXME - to be disabled in the future */
@@ -2568,13 +2598,17 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
      * of Dalvik instructions into the IR.
      */
     while (1) {
-        MIR *insn;
-        int width;
-        insn = (MIR *)dvmCompilerNew(sizeof(MIR), true);
+        /* Create and set up MIR */
+        MIR *insn = dvmCompilerNewMIR ();
         insn->offset = curOffset;
-        width = parseInsn(codePtr, &insn->dalvikInsn, cUnit.printMe);
 
-        /* The trace should never incude instruction data */
+        /* Keep track which method this MIR is from. Initially we assume that there is no method nesting caused by inlining */
+        insn->nesting.sourceMethod = cUnit.method;
+
+        /* Parse the instruction */
+        int width = parseInsn(codePtr, &insn->dalvikInsn, cUnit.printMe);
+
+        /* The trace should never include instruction data */
         assert(width);
         insn->width = width;
         traceSize += width;
@@ -2853,6 +2887,9 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
     /* Set the instruction set to use (NOTE: later components may change it) */
     cUnit.instructionSet = dvmCompilerInstructionSet();
 
+    const int numDalvikRegisters = cUnit.method->registersSize + dvmArchSpecGetNumberOfScratch ();
+    dvmCompilerUpdateCUnitNumDalvikRegisters (&cUnit, numDalvikRegisters);
+
 #ifndef ARCH_IA32
     //Try to inline invokes. For x86, the loop framework has inlining pass so we do not do the inlining here.
     if (cUnit.hasInvoke == true)
@@ -2861,8 +2898,6 @@ bool dvmCompileTrace(JitTraceDescription *desc, int numMaxInsts,
     }
 #endif
 
-    cUnit.numDalvikRegisters = cUnit.method->registersSize
-            + dvmArchSpecGetNumberOfScratch();
 
 #ifndef ARCH_IA32
     /* Preparation for SSA conversion */
diff --git a/vm/compiler/InlineTransformation.cpp b/vm/compiler/InlineTransformation.cpp
index 71c3364..62afd66 100755
--- a/vm/compiler/InlineTransformation.cpp
+++ b/vm/compiler/InlineTransformation.cpp
@@ -14,6 +14,7 @@
  * limitations under the License.
  */
 
+#include <map>
 #include "Dalvik.h"
 #include "Dataflow.h"
 #include "libdex/DexOpcodes.h"
@@ -28,9 +29,7 @@ enum InliningFailure
     kInliningSuccess = kInliningNoError, //!< @brief Inlining success is the same as having no inlining error
     kInliningMirRemovalFailed,           //!< @brief Used when removal of an MIR fails
     kInliningInvokeBBNoChild,            //!< @brief Used when the invoke's BB does not have a child
-    kInliningOverlapUseWithDef,          //!< @brief Used when there is rewriting and there is overlapping use with def
     kInliningBadCalleeCFG,               //!< @brief Used when the callee CFG created is bad
-    kInliningCalleeCFGHasLoops,          //!< @brief Used when the callee CFG has loops
     kInliningUnsupportedBytecodes,       //!< @brief Used when CFG building fails because of unsupported bytecodes for inlining
     kInliningCannotFindReturn,           //!< @brief Used when return bytecode from callee cannot be found
     kInliningCannotFindMoveResult,       //!< @brief Used when move-result bytecode from caller cannot be found
@@ -39,7 +38,6 @@ enum InliningFailure
     kInliningNativeMethod,               //!< @brief Used when for inlining is native
     kInliningFailedBefore,               //!< @brief Used when we've tried inlining before and failed for same method
     kInliningNoVirtualSupport,           //!< @brief Used when backend does not support devirtualization
-    kInliningMethodComplicated,          //!< @brief Used when method is too complicated for inliner
     kInliningDisabled,                   //!< @brief Used when inlining is disabled
     kInliningMethodTraceEnabled,         //!< @brief Used when method trace is enabled because inlining cannot happen
     kInliningSingleStepInvoke,           //!< @brief Used when invoke is selected for single stepping and thus inlining cannot happen
@@ -48,6 +46,17 @@ enum InliningFailure
     kInliningUnknownMethod,              //!< @brief Used when it is not know which method to inline
     kInliningMoreThanOneBytecode,        //!< @brief Used when we are trying to inline a method with one bytecode and we find more than one
     kInliningAlreadyInlined,             //!< @brief Used when we have already inlined the invoke
+    kInliningNoDefButMoveResult,         //!< @brief Used when caller has move-result but callee doesn't define anything
+    kInliningDefNoMatchReturn,           //!< @brief Used when callee has a def that doesn't match the VR returned
+    kInliningRewriteFailed,              //!< @brief Used when virtual register rewriting fails
+    kInliningUnrecoverableRewrite,       //!< @brief Used when there is a rewriting failure that is not recoverable
+    kInliningCalleeHasLoops,             //!< @brief Used when callee has loops
+    kInliningVirtualRegNumberTooLarge,   //!< @brief Used when the renamed VR is larger than 16-bits
+    kInliningNoBackendExtendedOpSupport, //!< @brief Used when backend does not support one of the generated extended ops
+    kInliningCalleeMayThrow,             //!< @brief Used when callee may throw
+    kInliningCalleeTooLarge,             //!< @brief Used when callee is too large
+    kInliningCalleeNotLeaf,              //!< @brief Used when callee is not a leaf
+    kInliningMethodComplicated,          //!< @brief Used when method is too complicated for inliner
 };
 
 /**
@@ -69,15 +78,9 @@ static const char *getFailureMessage (InliningFailure failure)
         case kInliningInvokeBBNoChild:
             message = "invoke's basic block does not have a child basic block";
             break;
-        case kInliningOverlapUseWithDef:
-            message = "cannot handle rewriting when there is overlapping use with def";
-            break;
         case kInliningBadCalleeCFG:
             message = "the callee method CFG has unexpected shape";
             break;
-        case kInliningCalleeCFGHasLoops:
-            message = "the CFG of callee method has loops and those are not yet supported";
-            break;
         case kInliningUnsupportedBytecodes:
             message = "during building of callee CFG, unsupported bytecodes were found";
             break;
@@ -102,9 +105,6 @@ static const char *getFailureMessage (InliningFailure failure)
         case kInliningNoVirtualSupport:
             message = "backend does not support devirtualization so we cannot inline virtual invokes";
             break;
-        case kInliningMethodComplicated:
-            message = "method is too complicated for inliner";
-            break;
         case kInliningDisabled:
             message = "inlining is disabled";
             break;
@@ -129,6 +129,37 @@ static const char *getFailureMessage (InliningFailure failure)
         case kInliningAlreadyInlined:
             message = "already inlined invoke";
             break;
+        case kInliningNoDefButMoveResult:
+            message = "we have a move-result but inlined MIR doesn't define anything";
+            break;
+        case kInliningDefNoMatchReturn:
+            message = "define of inlineable instruction does not match return";
+            break;
+        case kInliningRewriteFailed:
+        case kInliningUnrecoverableRewrite:
+            message = "virtual register rewriting failed";
+            break;
+        case kInliningCalleeHasLoops:
+            message = "the CFG of callee method has loops and those are not yet supported";
+            break;
+        case kInliningVirtualRegNumberTooLarge:
+            message = "register window shift causes virtual register number to exceed 16-bits";
+            break;
+        case kInliningNoBackendExtendedOpSupport:
+            message = "backend does not support extended MIR needed for inlining";
+            break;
+        case kInliningCalleeMayThrow:
+            message = "callee method has potential to throw exceptions";
+            break;
+        case kInliningCalleeNotLeaf:
+            message = "callee method is not a leaf method";
+            break;
+        case kInliningCalleeTooLarge:
+            message = "callee method exceeds number of maximum bytecodes";
+            break;
+        case kInliningMethodComplicated:
+            message = "method is too complicated for inliner";
+            break;
         default:
             break;
     }
@@ -152,6 +183,11 @@ static bool isInliningFailureFatal (InliningFailure failure)
         case kInliningInvokeBBNoChild:
         //MIR removal can only fail if there is a bad CFG when we cannot find a MIR in its BB
         case kInliningMirRemovalFailed:
+        //Register window shift causes virtual register number to exceed 16-bits
+        //Since this error can happen after inlining, we cannot safely recover.
+        case kInliningVirtualRegNumberTooLarge:
+        //The rewrite is unrecoverable thus fatal
+        case kInliningUnrecoverableRewrite:
             isFatal = true;
             break;
         default:
@@ -200,6 +236,33 @@ static bool canInlineBytecode (const Method *method, const DecodedInstruction *i
             }
             return false;
 
+        case OP_NEW_INSTANCE:
+        case OP_CHECK_CAST:
+        case OP_FILLED_NEW_ARRAY:
+        case OP_FILLED_NEW_ARRAY_RANGE:
+        case OP_CONST_CLASS:
+        case OP_NEW_ARRAY:
+        case OP_INSTANCE_OF:
+            //The reason we reject these is because the backends assume that the cUnit's method
+            //is the one that needs to be use when looking for class. However, instead the MIR's
+            //method should be used because we may be inlining a method that comes from a different
+            //dex and thus class resolution won't be correct.
+            if (failureMessage != 0)
+            {
+                *failureMessage = "backends need to support looking at class from MIR not class from cUnit";
+            }
+            return false;
+
+        case OP_FILL_ARRAY_DATA:
+            //When inlined as a single bytecode (without creating callee frame), the implementation of fill-array
+            //assumes that the data it needs to load is at end of current method when in reality it needs to load
+            //data from its original method.
+            if (failureMessage != 0)
+            {
+                *failureMessage = "fill-array may try to load data from wrong location";
+            }
+            return false;
+
         case OP_IGET_VOLATILE:
         case OP_IPUT_VOLATILE:
         case OP_SGET_VOLATILE:
@@ -227,20 +290,119 @@ static bool canInlineBytecode (const Method *method, const DecodedInstruction *i
     return true;
 }
 
-/* Convert the reg id from the callee to the original id passed by the caller */
-static inline u4 convertRegId(const DecodedInstruction *invoke,
-                              const Method *calleeMethod,
-                              int calleeRegId, bool isRange)
+/**
+ * @brief Checks if we have a very simple method: empty, getter, setter, or single bytecode
+ * @details For a method to be single bytecode (including getters and setters), the return
+ * bytecode is not counted towards this. Namely a "single bytecode" method always has two
+ * bytecodes when counting the return.
+ * @param methodStats The method stats of the method we care about
+ * @return Returns whether we have a very simple method
+ */
+static bool isVerySimpleMethod (CompilerMethodStats *methodStats)
 {
-    /* The order in the original arg passing list */
-    int rank = calleeRegId -
-               (calleeMethod->registersSize - calleeMethod->insSize);
-    assert(rank >= 0);
-    if (!isRange) {
-        return invoke->arg[rank];
-    } else {
-        return invoke->vC + rank;
+    //If empty then it is very simple
+    if ((methodStats->attributes & METHOD_IS_EMPTY) != 0)
+    {
+        return true;
+    }
+
+    //If getter/setter then it is very simple
+    if ((methodStats->attributes & METHOD_IS_GETTER) != 0
+            || (methodStats->attributes & METHOD_IS_SETTER) != 0)
+    {
+        return true;
     }
+
+    //All methods must have return so we check if they have one additional bytecode
+    if ((methodStats->attributes & METHOD_IS_LEAF) != 0 && methodStats->numBytecodes == 2)
+    {
+        return true;
+    }
+
+    //Not very simple
+    return false;
+}
+
+/**
+ * @brief Checks if method is throw-free, leaf, and fewer than 20 instructions.
+ * @param methodStats The statics on method we are trying to inline
+ * @return Returns kInliningNoError if method is a small throw free leaf. Otherwise
+ * returns the inlining error message saying what the limitation is.
+ */
+static InliningFailure isSmallThrowFreeLeaf (CompilerMethodStats *methodStats)
+{
+    //Check if method is leaf
+    bool methodIsLeaf = (methodStats->attributes & METHOD_IS_LEAF) != 0;
+
+    if (methodIsLeaf == false)
+    {
+        //Reject non-leaf methods
+        return kInliningCalleeNotLeaf;
+    }
+
+    //Check if method can throw
+    bool methodThrowFree = (methodStats->attributes & METHOD_IS_THROW_FREE) != 0;
+
+    if (methodThrowFree == false)
+    {
+        //Reject methods that can throw
+        return kInliningCalleeMayThrow;
+    }
+
+    //Check number of bytecodes
+    unsigned int numBytecodes = methodStats->numBytecodes;
+
+    if (numBytecodes > gDvmJit.maximumInliningNumBytecodes)
+    {
+        //Reject if we have more than the allowed number of instructions
+        return kInliningCalleeTooLarge;
+    }
+
+    return kInliningNoError;
+}
+
+
+
+/**
+ * @brief Used to determine the register window shift required to uniquely name all VRs for multiple levels of nesting
+ * @param calleeMethod The callee method that is being invoked
+ * @param invokeNesting The nesting information from the invoke bytecode
+ * @return Returns the register window shift required to support inlining callee method
+ */
+static int determineRegisterWindowShift (const Method *calleeMethod, const NestedMethod *invokeNesting)
+{
+    assert (calleeMethod != 0);
+
+    //Basically the approach to computing the offset is
+    // 1) Fist we look at method being inlined and count its registers and StackSaveArea
+    // 2) Then we start walking through the invoke's nesting information to count additional levels of nesting
+    int registerWindowShift = 0;
+
+    do
+    {
+        //Include caller's stack save area in calculation of offset
+        registerWindowShift += (sizeof(StackSaveArea) / sizeof(u4));
+
+        //Now count callee's registers in offset calculation
+        registerWindowShift += calleeMethod->registersSize;
+
+        //We check if invoke has nesting information and that it actually is in the middle of nesting.
+        if (invokeNesting != 0 && invokeNesting->parent != 0)
+        {
+            calleeMethod = invokeNesting->sourceMethod;
+        }
+        else
+        {
+            //We don't have nesting so we have no other method to include in our calculation
+            calleeMethod = 0;
+        }
+
+        //We want to look at next level
+        invokeNesting = invokeNesting->parent;
+
+    } while (calleeMethod != 0);
+
+    return registerWindowShift;
 }
 
 /**
@@ -270,6 +432,76 @@ static inline bool isRangeInvoke (Opcode opcode)
 }
 
 /**
+ * @brief Used to check whether invoke is virtual or interface
+ * @param opcode The opcode to check
+ * @return Returns whether the invoke is virtual or interface
+ */
+static inline bool isVirtualInvoke (Opcode opcode)
+{
+    switch (opcode)
+    {
+        //Return true for all virtual/interface invokes
+        case OP_INVOKE_VIRTUAL:
+        case OP_INVOKE_VIRTUAL_QUICK:
+        case OP_INVOKE_INTERFACE:
+        case OP_INVOKE_VIRTUAL_RANGE:
+        case OP_INVOKE_VIRTUAL_QUICK_RANGE:
+        case OP_INVOKE_INTERFACE_RANGE:
+            return true;
+        default:
+            break;
+    }
+
+    //If we get here we do not have a virtual invoke
+    return false;
+
+}
+
+/**
+ * @brief Determines the mapping between caller's and callee's virtual registers.
+ * @param invoke The decoded instruction for the invoke we are trying to inline.
+ * @param invokedMethod Information about the method we are invoking.
+ * @param calleeToCaller Updated by function to contain map of callee's virtual
+ * registers to caller's virtual registers.
+ */
+static void determineRegisterMapping (DecodedInstruction &invoke,
+                                      const Method *invokedMethod,
+                                      std::map<int, int> &calleeToCaller)
+{
+   bool isRange = isRangeInvoke (invoke.opcode);
+
+   //When setting up stack the ins follow the locals for the callee. So lets determine
+   //the virtual register number of the first in.
+   int firstIn = invokedMethod->registersSize - invokedMethod->insSize;
+
+   //For invokes, vA holds the argument count
+   unsigned int argumentCount = invoke.vA;
+
+   //Now walk through the arguments of this invoke
+   for (unsigned int rank = 0; rank < argumentCount; rank++)
+   {
+       //The callee ins are in order from the first in
+       int calleeReg = firstIn + rank;
+
+       //Now determine the virtual register used for current invoke parameter
+       int callerReg;
+       if (isRange == false)
+       {
+           //For non-range versions, the arguments are in order in the arg field
+           callerReg = invoke.arg[rank];
+       }
+       else
+       {
+           //vC holds the first register to use
+           callerReg = invoke.vC + rank;
+       }
+
+       //Add this entry to the map
+       calleeToCaller[calleeReg] = callerReg;
+   }
+}
+
+/**
  * @brief Used to check whether an opcode is a return.
  * @param opcode The opcode to check
  * @return Returns whether the opcode is a return
@@ -450,7 +682,7 @@ static InliningFailure removeInvokeAndMoveResult (GrowableList &callerBasicBlock
  */
 static MIR *createPredictionCheck (MIR *invoke)
 {
-    //Make a copy of the invoke
+    //Make a copy of the invoke. This includes copying the invoke nesting information.
     MIR *checkPrediction = dvmCompilerCopyMIR (invoke);
 
     //Get reference to decoded instruction so we can update it
@@ -608,6 +840,34 @@ static Opcode findMatchingMoveResult (Opcode returnOpcode)
 }
 
 /**
+ * @brief Given a type of move-result bytecode, finds a matching move.
+ * @param moveResultOpcode The opcode of move-result bytecode
+ * @return Returns the matching move which reads from VR not from Thread.retval
+ */
+static Opcode findMatchingMove (Opcode moveResultOpcode)
+{
+    Opcode moveOpcode;
+
+    switch (moveResultOpcode)
+    {
+        case OP_MOVE_RESULT:
+            moveOpcode = OP_MOVE;
+            break;
+        case OP_MOVE_RESULT_OBJECT:
+            moveOpcode = OP_MOVE_OBJECT;
+            break;
+        case OP_MOVE_RESULT_WIDE:
+            moveOpcode = OP_MOVE_WIDE;
+            break;
+        default:
+            moveOpcode = OP_NOP;
+            break;
+    }
+
+    return moveOpcode;
+}
+
+/**
  * @brief Tags an MIR as being inlined
  * @param mir The MIR to tag
  * @param sourceMethod The source method for this MIR
@@ -632,9 +892,11 @@ static inline void tagMirInlined (MIR *mir, const Method *sourceMethod)
  * @param calleeEntry The callee's entry basic block
  * @param calleeExit The callee's exit basic block
  * @param calleeBasicBlocks The list of callee basic blocks
+ * @param invoke The invoke bytecode
  */
 static void insertCalleeBetweenBasicBlocks (GrowableList &callerBasicBlocks, const Method *method, BasicBlock *topBB,
-        BasicBlock *bottomBB, BasicBlock *calleeEntry, BasicBlock *calleeExit, GrowableList &calleeBasicBlocks)
+        BasicBlock *bottomBB, BasicBlock *calleeEntry, BasicBlock *calleeExit, GrowableList &calleeBasicBlocks,
+        MIR *invoke)
 {
     //In case the entry and exit are not bytecode blocks, we make them that now since we are inserting in middle of trace
     calleeEntry->blockType = kDalvikByteCode;
@@ -667,10 +929,6 @@ static void insertCalleeBetweenBasicBlocks (GrowableList &callerBasicBlocks, con
         //Add it to the cUnit
         dvmInsertGrowableList (&callerBasicBlocks, (intptr_t) bb);
 
-        //The start offset of this block does not make sense in context of inlining because it is
-        //relative to the inlined method. In order to early catch errors due to this, we set an invalid offset
-        bb->startOffset = 0xdeadc0de;
-
         //Update the method for this BB
         bb->containingMethod = method;
 
@@ -678,6 +936,17 @@ static void insertCalleeBetweenBasicBlocks (GrowableList &callerBasicBlocks, con
         for (MIR *mir = bb->firstMIRInsn; mir != 0; mir = mir->next)
         {
             tagMirInlined (mir, method);
+
+            //Here we check if the inlined MIR has same source method and offset as the invoke.
+            //The reason we do this check is that inlining has an optimization for methods that are one bytecode.
+            //It does the inlining by rewriting the single MIR and then setting the PC of bytecode to that of invoke.
+            //If we find that case, we don't update nesting information because all registers it uses are caller
+            //registers and not callee registers.
+            if (mir->nesting.sourceMethod != invoke->nesting.sourceMethod || mir->offset != invoke->offset)
+            {
+                //Set the nesting chain of invoke as parent of current MIR
+                mir->nesting.parent = &(invoke->nesting);
+            }
         }
     }
 }
@@ -760,7 +1029,7 @@ static InliningFailure insertMethodBodyIntoCFG (GrowableList &callerBasicBlocks,
 
     //Now we insert the callee CFG between the two blocks we have decided on
     insertCalleeBetweenBasicBlocks (callerBasicBlocks, method, invokeBB, afterInvokeBB, calleeEntry, calleeExit,
-            calleeBasicBlocks);
+            calleeBasicBlocks, invoke);
 
     //If we had a singleton chaining cell with the invoke then we need to remove it right now since we got rid of the invoke
     if (singletonCC != 0)
@@ -777,7 +1046,7 @@ static InliningFailure insertMethodBodyIntoCFG (GrowableList &callerBasicBlocks,
  * @param invoke The MIR for the invoke.
  * @return Returns pointer to MIR representing the move-result. Returns 0 if none is found.
  */
-static MIR *findMoveResult (MIR *invoke)
+static MIR *findMoveResult (const MIR *invoke)
 {
     //We must have an invoke
     assert(invoke != 0 && invoke->bb != 0);
@@ -814,12 +1083,12 @@ static MIR *findMoveResult (MIR *invoke)
  * @param exit The single basic block that represents exit
  * @return Returns the MIR if a return bytecode is found.
  */
-static MIR *findReturn (GrowableList &blockList, BasicBlock *exit)
+static MIR *findReturn (GrowableList &blockList, const BasicBlock *exit)
 {
     //Paranoid
     assert (exit != 0);
 
-    BasicBlock *bbToSearch = exit;
+    const BasicBlock *bbToSearch = exit;
 
     //If the exit block has no MIRs, we search its predecessor
     if (exit->lastMIRInsn == 0)
@@ -835,7 +1104,7 @@ static MIR *findReturn (GrowableList &blockList, BasicBlock *exit)
 
         //Get the basic block that is predecessor of exit block
         int blockIdx = dvmHighestBitSet (exit->predecessors);
-        bbToSearch = reinterpret_cast<BasicBlock *> (dvmGrowableListGetElement (&blockList, blockIdx));
+        bbToSearch = reinterpret_cast<const BasicBlock *> (dvmGrowableListGetElement (&blockList, blockIdx));
 
         assert (bbToSearch != 0);
     }
@@ -859,90 +1128,463 @@ static MIR *findReturn (GrowableList &blockList, BasicBlock *exit)
 
 /**
  * @brief For one bytecode short methods, this is used to rewrite the virtual registers.
- * @param calleeMethod The method we are inlining.
- * @param invoke The MIR for the invoke.
- * @param moveResult The MIR for the move-result.
  * @param newMir The MIR that we are inlining
+ * @param moveResult The MIR for the move-result.
+ * @param returnMir The MIR for the return
+ * @param calleeToCaller Map of virtual registers of callee to that of caller
  * @return Returns inlining success/failure
  */
-static InliningFailure rewriteInlinedMIR (const Method *calleeMethod, MIR *invoke, MIR *moveResult, MIR *newMir)
+static InliningFailure rewriteSingleInlinedMIR (MIR *newMir, const MIR *moveResult, const MIR *returnMir,
+        const std::map<int, int> &calleeToCaller)
 {
-    //Determine if the current invoke is a range variant
-    bool isRange = isRangeInvoke (invoke->dalvikInsn.opcode);
+    //Copy the decoded instruction
+    DecodedInstruction newInsn = newMir->dalvikInsn;
+
+    //If we have a move-result then something is being returned by function
+    if (moveResult != 0)
+    {
+        //Get dataflow flags
+        int newMirFlags = dvmCompilerDataFlowAttributes[newInsn.opcode];
 
-    //Get reference to the decoded instruction
-    DecodedInstruction &newInsn = newMir->dalvikInsn;
+        //Make sure that return has a use in vA and move-result has define in vA
+        assert ((dvmCompilerDataFlowAttributes[returnMir->dalvikInsn.opcode] & DF_A_IS_USED_REG) != 0);
+        assert ((dvmCompilerDataFlowAttributes[moveResult->dalvikInsn.opcode] & DF_A_IS_DEFINED_REG) != 0);
 
-    //Get the dataflow flags for the instruction we are inlining
-    int dfFlags = dvmCompilerDataFlowAttributes[newInsn.opcode];
+        //Check to make sure that our new MIR has a define because we have a move-result
+        if ((newMirFlags & DF_A_IS_DEFINED_REG) == 0)
+        {
+            //TODO Since we have no define but we have a move-result and a return, it must be the case that
+            //caller passed argument to callee that it is returning. So what we need to do for this case
+            //is generate a move from register caller passed to register that move-result is moving to.
+            //However, for now we reject.
+            return kInliningNoDefButMoveResult;
+        }
+        else
+        {
+            //We have a define but let's make sure that return VR matches our define
+            if (returnMir->dalvikInsn.vA != newInsn.vA)
+            {
+                return kInliningDefNoMatchReturn;
+            }
+            else
+            {
+                //We write directly into register desired by move-result
+                newInsn.vA = moveResult->dalvikInsn.vA;
+            }
+        }
+    }
 
-    //If we have an overlapping use with define, then don't inline
-    if ((dfFlags & DF_A_IS_USED_REG) != 0 && (dfFlags & DF_A_IS_DEFINED_REG) != 0)
+    //Try to rewrite the uses now
+    if (dvmCompilerRewriteMirVRs (newInsn, calleeToCaller, true) == false)
     {
-        return kInliningOverlapUseWithDef;
+        return kInliningRewriteFailed;
     }
 
-    //Now rewrite the virtual registers
-    if ((dfFlags & DF_A_IS_USED_REG) != 0)
+    //When we get here everything went well so copy back the modified dalvik instruction
+    newMir->dalvikInsn = newInsn;
+
+    //We finished initializing the new MIR
+    return kInliningNoError;
+}
+
+/**
+ * @brief Used to locate and rewrite the single MIR in a very simple method
+ * @param calleeBasicBlocks The basic blocks of callee
+ * @param calleeToCaller The map of registers from callee to caller
+ * @param invoke The invoke whose method we are inlining
+ * @param returnMir The return bytecode of the callee method
+ * @param moveResult The move-result following invoke
+ * @return Returns inlining success/failure depending on whether rewriting went well
+ */
+static InliningFailure locateAndRewriteSingleMIR (GrowableList &calleeBasicBlocks,
+        const std::map<int, int> &calleeToCaller, const MIR *invoke, const MIR *returnMir, const MIR *moveResult)
+{
+    MIR *mirToInline = 0;
+
+    //Search the callee CFG for the MIR to rewrite
+    GrowableListIterator calleeIter;
+    dvmGrowableListIteratorInit (&calleeBasicBlocks, &calleeIter);
+
+    while (true)
     {
-        newInsn.vA = convertRegId (&invoke->dalvikInsn, calleeMethod, newInsn.vA, isRange);
+        //Get the basic block
+        BasicBlock *bb = reinterpret_cast<BasicBlock *> (dvmGrowableListIteratorNext (&calleeIter));
+
+        //When we reach the end we stop
+        if (bb == 0)
+        {
+            break;
+        }
+
+        //We found a bytecode
+        if (bb->firstMIRInsn != 0)
+        {
+            if (mirToInline != 0)
+            {
+                return kInliningMoreThanOneBytecode;
+            }
+
+            mirToInline = bb->firstMIRInsn;
+
+            if (mirToInline->next != 0)
+            {
+                return kInliningMoreThanOneBytecode;
+            }
+        }
     }
-    else if ((dfFlags & DF_A_IS_DEFINED_REG) != 0)
+
+    if (mirToInline != 0)
     {
-        //Since we have a single bytecode with a definition, it must the case that we have a move-result.
-        //So we assert it right now.
-        assert (moveResult != 0);
+        //Now we need to rewrite the VRs for the MIR we wish to inline
+        InliningFailure rewriting  = rewriteSingleInlinedMIR (mirToInline, moveResult, returnMir, calleeToCaller);
 
-        //When not in assert world we need to actually check that we have a move-result
-        if (moveResult != 0)
+        if (rewriting != kInliningNoError)
+        {
+            //If we fail, then don't inline
+            return rewriting;
+        }
+
+        //If the instruction is about to raise any exception, we want to punt to the interpreter
+        //and re-execute the invoke. Thus we set the newMir's offset to match the invoke's offset.
+        mirToInline->offset = invoke->offset;
+
+        //Make sure that the new MIR has same nesting information as the invoke
+        mirToInline->nesting = invoke->nesting;
+    }
+
+    //If we made it here we have no error
+    return kInliningNoError;
+}
+
+/**
+ * @brief Used to create MIR that does the stack overflow check
+ * @param inlinedInvoke The invoke that was inlined for which we need to check stack overflow at entry
+ * @param inlinedMethod The method that was inlined
+ * @return Returns the MIR which has extended opcode kMirOpCheckStackOverflow
+ */
+static MIR *createStackOverflowCheck (const MIR *inlinedInvoke, const Method *inlinedMethod)
+{
+    MIR *stackOverflowCheck = dvmCompilerNewMIR();
+
+    stackOverflowCheck->dalvikInsn.opcode = static_cast<Opcode> (kMirOpCheckStackOverflow);
+
+    //We can determine required register window shift for this invoke and method it calls
+    int registerWindowShift = determineRegisterWindowShift (inlinedMethod, &inlinedInvoke->nesting);
+
+    //Basically to compute the space require we need to multiply the register window shift by sizeof (u4)
+    //and then add a StackSaveArea space which is required for callee
+    unsigned int stackSpaceRequired = registerWindowShift * sizeof (u4) + sizeof (StackSaveArea);
+
+    //Store the stack space required in vB
+    stackOverflowCheck->dalvikInsn.vB = stackSpaceRequired;
+
+    //Now copy the offset and the nesting information
+    stackOverflowCheck->offset = inlinedInvoke->offset;
+    stackOverflowCheck->nesting = inlinedInvoke->nesting;
+
+    return stackOverflowCheck;
+}
+
+/**
+ * @brief Used to perform a register window shift by selectively rewriting MIRs from the CFG
+ * @param callerBasicBlocks The basic blocks of caller after callee has merged
+ * @param calleeMethod The callee method we are inlining
+ * @param moveResult The caller's move result matched with callee invoke
+ * @param updatedMoveResult Updated by the function to true if the moveResult mir is rewritten
+ * @param renameCallee Whether callee should be renamed. If false, caller registers are renamed.
+ * @param renameOffset The offset to use for renaming virtual registers.
+ * @param oldToNew Map of register names to the new number they should be renamed to
+ * @return Returns inlining success/failure depending on whether the renaming is successful
+ */
+static InliningFailure handleRenamingAfterShift (GrowableList &callerBasicBlocks, const Method *calleeMethod,
+        const MIR *moveResult, bool &updatedMoveResult, const bool renameCallee, const int renameOffset,
+        const std::map<int, int> &oldToNew)
+{
+    GrowableListIterator blockIter;
+    dvmGrowableListIteratorInit (&callerBasicBlocks, &blockIter);
+
+    while (true)
+    {
+        BasicBlock *bb = reinterpret_cast<BasicBlock *> (dvmGrowableListIteratorNext (&blockIter));
+
+        //Bail at the end
+        if (bb == 0)
         {
-            //Because we are looking at a simple bytecode, we write directly into register desired by move-result
-            newInsn.vA = moveResult->dalvikInsn.vA;
+            break;
+        }
+
+        //Walk through the MIRs so we can rewrite them
+        for (MIR *mir = bb->firstMIRInsn; mir != 0; mir = mir->next)
+        {
+            //Depending on the rewriting logic which has happened, we need to modify either
+            //MIRs from the callee method or all of the other MIRs.
+            // 1) So if renameCallee is false, then we only need to rewrite if the MIR doesn't
+            //come from callee method.
+            // 2) If renameCalleee is true, then we only rewrite the MIR if it comes from callee method.
+            if ((renameCallee == false && mir->nesting.sourceMethod != calleeMethod)
+                    || (renameCallee == true && mir->nesting.sourceMethod == calleeMethod))
+            {
+                bool rewritten = dvmCompilerRewriteMirVRs (mir->dalvikInsn, oldToNew, false);
+
+                if (rewritten == false)
+                {
+                    //We have already updated several MIRs and thus we cannot revert the damage
+                    //we have done.
+                    return kInliningUnrecoverableRewrite;
+                }
+
+                //We want to keep track of the renaming offset. However, this is only useful for MIRs
+                //that come from a method and are not artificially generated.
+                if (mir->nesting.sourceMethod != 0)
+                {
+                    mir->virtualRegRenameOffset += renameOffset;
+                }
+
+                //Invalidate the SSA representation in case someone prints out the CFG during inlining
+                mir->ssaRep = 0;
+
+                if (mir == moveResult)
+                {
+                    updatedMoveResult = true;
+                }
+            }
+        }
+    }
+
+    return kInliningNoError;
+}
+
+/**
+ * @brief Used to generate and insert the MIRs for doing caller to callee moves to replace argument
+ * passing of the invoke.
+ * @param calleeToCaller Map of virtual registers of callee to that of caller
+ * @param calleeEntry The entry block into the callee body
+ * @param renameCallee Whether callee should be renamed. If false, caller registers are renamed.
+ * @param renameOffset The offset to use for renaming virtual registers.
+ */
+static void insertCallerToCalleeMoves (const std::map<int, int> &calleeToCaller, BasicBlock *calleeEntry,
+        const bool renameCallee, const int renameOffset)
+{
+    //Go through the callee to caller mapping in order to insert moves
+    for (std::map<int, int>::const_reverse_iterator regIter = calleeToCaller.rbegin ();
+            regIter != calleeToCaller.rend (); regIter++)
+    {
+        //Get the original source and destination VRs
+        int sourceVR = regIter->second;
+        int destVR = regIter->first;
+
+        //Depending on how renaming was done, we need to include the renaming offset
+        if (renameCallee == true)
+        {
+            destVR += renameOffset;
         }
         else
         {
-            //We were expecting to have a move-result but we didn't find it
-            return kInliningCannotFindMoveResult;
+            sourceVR += renameOffset;
         }
+
+        //Create the move itself.
+        MIR *moveMir = dvmCompilerNewMoveMir (sourceVR, destVR, false);
+
+        //Now add the move MIR to the callee entry block
+        dvmCompilerPrependMIR (calleeEntry, moveMir);
     }
+}
+
+/**
+ * @brief Used to generate and insert the MIR for doing callee to caller move to replace return and move-result.
+ * @param calleeExit The callee method's exit block
+ * @param moveResult The caller's move result matched with callee invoke
+ * @param returnMir The return bytecode from callee
+ * @param updatedMoveResult Whether the move-result has been renamed already
+ * @param renameCallee Whether callee should be renamed. If false, caller registers are renamed.
+ * @param renameOffset The offset to use for renaming virtual registers.
+ */
+static void insertCalleeToCallerMove (BasicBlock *calleeExit, const MIR *moveResult, const MIR *returnMir,
+        const bool updatedMoveResult, const bool renameCallee, const int renameOffset)
+{
+    //When we have a move-result we surely have a return
+    assert (returnMir != 0);
+
+    //Make sure that return has a use in vA and move-result has define in vA
+    assert ((dvmCompilerDataFlowAttributes[returnMir->dalvikInsn.opcode] & DF_A_IS_USED_REG) != 0);
+    assert ((dvmCompilerDataFlowAttributes[moveResult->dalvikInsn.opcode] & DF_A_IS_DEFINED_REG) != 0);
 
-    if ((dfFlags & DF_B_IS_REG) != 0)
+    int sourceVR = returnMir->dalvikInsn.vA;
+    int destVR = moveResult->dalvikInsn.vA;
+
+    //Depending on how renaming was done, we need to include the renaming offset
+    if (renameCallee == false)
+    {
+        if (updatedMoveResult == false)
+        {
+            destVR += renameOffset;
+        }
+    }
+    else
     {
-        newInsn.vB = convertRegId (&invoke->dalvikInsn, calleeMethod, newInsn.vB, isRange);
+        sourceVR += renameOffset;
+    }
+
+    //Get the opcode we need for the move
+    Opcode moveOpcode = findMatchingMove (moveResult->dalvikInsn.opcode);
+
+    //Create the move itself.
+    MIR *moveMir = dvmCompilerNewMoveMir (sourceVR, destVR, moveOpcode == OP_MOVE_WIDE ? true : false);
+
+    //Now add the move MIR to the callee exit block.
+    dvmCompilerAppendMIR (calleeExit, moveMir);
+}
+
+/**
+ * @brief Used to shift the register window of the cUnit and create the moves from callee to caller regs and vice versa
+ * @param cUnit The compilation unit
+ * @param callerBasicBlocks The list of basic blocks for caller
+ * @param calleeToCaller Map of virtual registers of callee to that of caller
+ * @param calleeMethod The method that was inlined
+ * @param calleeEntry The entry block into the callee body
+ * @param calleeExit The exit block from the callee body
+ * @param invoke The invoke whose method was inlined
+ * @param moveResult The move-result following invoke
+ * @param returnMir The return bytecode of callee
+ * @return Returns inlining success/failure
+ */
+static InliningFailure shiftRegisterWindow (CompilationUnit *cUnit, GrowableList &callerBasicBlocks,
+        const std::map<int, int> &calleeToCaller, const Method *calleeMethod, BasicBlock *calleeEntry,
+        BasicBlock *calleeExit, const MIR *invoke, const MIR *moveResult, const MIR *returnMir)
+{
 
+    std::map<int, int> oldToNew;
+
+    //If false we rename all VRs in cUnit excluding callee. Otherwise we update only callee.
+    bool needRenaming = true;
+    bool renameCallee = false;
+    int renameOffset = 0;
+
+    //Determine the register window shift
+    int registerWindowShift = determineRegisterWindowShift (calleeMethod, &invoke->nesting);
+
+    //From this point we need to handle 3 possible different cases where the register window
+    //shift of the cUnit needs to be synchronized with what we're currently inlining.
+    // 1) cUnit->registerWindowShift == registerWindowShift
+    //     - This case can happen when we're inlining a method that has the same shift
+    //     as a method already inlined. This means that both callee methods use the same number
+    //     of virtual registers and thus we have already shifted the window of registers in the
+    //     caller method.
+    // 2) cUnit->registerWindowShift < registerWindowShift
+    //     - This case happens when we're inlining a method with more virtual registers than one
+    //     we've already inlined or if this is the first not very simple method we've inlined.
+    //     This means that we need to rename registers in caller so that we can accommodate a frame
+    //     pointer shift so that callee's virtual registers stay unchanged.
+    // 3) cUnit->registerWindowShift > registerWindowShift
+    //     - This case happens when we're inlining a method with fewer virtual registers that
+    //     method we've already inlined. All of the virtual registers in the cUnit are relative
+    //     the shift we've already done. This means that we need to rename callee's registers
+    //     so that they are relative to the frame pointer which has already been shifted. Caller's
+    //     registers and all other inlined registers remain unchanged.
+
+    //Check if we have already done a register window shift by the same amount
+    if (cUnit->registerWindowShift == registerWindowShift)
+    {
+        //We don't need to do any renaming but we still need to generate all support instructions
+        needRenaming = false;
     }
+    else if (cUnit->registerWindowShift < registerWindowShift)
+    {
+        //In this situation we need to reshift by a greater amount so we rename everything but callee
+        renameCallee = false;
+
+        renameOffset = registerWindowShift - cUnit->registerWindowShift;
+
+        for (int reg = 0; reg < cUnit->numDalvikRegisters; reg++)
+        {
+            int newRegName = reg + renameOffset;
+
+            //We cannot exceed a register name greater than 16-bits due to how SSA is represented
+            if (newRegName >= (1 << 16))
+            {
+                return kInliningVirtualRegNumberTooLarge;
+            }
+
+            oldToNew[reg] = newRegName;
+        }
 
-    if ((dfFlags & DF_C_IS_REG) != 0)
+        dvmCompilerUpdateCUnitNumDalvikRegisters (cUnit, cUnit->numDalvikRegisters + renameOffset);
+        cUnit->registerWindowShift = registerWindowShift;
+    }
+    else if (cUnit->registerWindowShift > registerWindowShift)
     {
-        newInsn.vC = convertRegId (&invoke->dalvikInsn, calleeMethod, newInsn.vC, isRange);
+        //In this situation the shift is greater than we expect so we need to rename only callee
+        renameCallee = true;
+
+        //As noted in approach above, we do this because the cUnit already had a bigger shift done
+        //so we need to synchronize all callee registers to match this shift already done.
+        renameOffset = cUnit->registerWindowShift - registerWindowShift;
+
+        for (int reg = 0; reg < calleeMethod->registersSize; reg++)
+        {
+            oldToNew[reg] = reg + renameOffset;
+        }
     }
 
-    //If the getter instruction is about to raise any exception, we want to punt to the interpreter
-    //and re-execute the invoke. Thus we set the newMir's offset to match the invoke's offset.
-    newMir->offset = invoke->offset;
+    bool updatedMoveResult = false;
 
-    //We finished initializing the new MIR
+    //Before we start renaming we check if actually needs to be done
+    if (needRenaming == true)
+    {
+        InliningFailure renaming = handleRenamingAfterShift (callerBasicBlocks, calleeMethod, moveResult,
+                updatedMoveResult, renameCallee, renameOffset, oldToNew);
+
+        //If we had an error during renaming, simply pass it along
+        if (renaming != kInliningNoError)
+        {
+            return renaming;
+        }
+    }
+
+    //Now create and insert the caller to callee moves
+    insertCallerToCalleeMoves (calleeToCaller, calleeEntry, renameCallee, renameOffset);
+
+    //Now insert the stack overflow check
+    MIR *stackOverflowCheck = createStackOverflowCheck (invoke, calleeMethod);
+
+    //Prepend it to the callee entry basic block
+    dvmCompilerPrependMIR (calleeEntry, stackOverflowCheck);
+
+    //If we have a move-result it means we are returning a value so create that move right now
+    if (moveResult != 0)
+    {
+        insertCalleeToCallerMove (calleeExit, moveResult, returnMir, updatedMoveResult, renameCallee, renameOffset);
+    }
+
+    //If we made it here we successfully did register window shifting
     return kInliningNoError;
 }
 
 /**
- * @brief Used to inline small methods (one bytecode).
- * @param callerBasicBlocks The basic blocks from caller.
+ * @brief Performs the inlining work
+ * @param cUnit The compilation unit
  * @param calleeMethod The method being invoked.
  * @param invoke The MIR for the invoke
  * @param isPredicted Whether method being invoked is predicted
+ * @param isVerySimple The inlined method only has one bytecode to inline
  * @return Returns inlining success/failure
  */
-static InliningFailure doInline (GrowableList &callerBasicBlocks, const Method *calleeMethod, MIR *invoke,
-        bool isPredicted)
+static InliningFailure doInline (CompilationUnit *cUnit, const Method *calleeMethod, MIR *invoke,
+        bool isPredicted, bool isVerySimple)
 {
     //Keep track of BBs of callee
     BasicBlock *calleeEntry = 0, *calleeExit = 0;
 
+    //Get the caller basic blocks
+    GrowableList &callerBasicBlocks = cUnit->blockList;
+
+    //We expect that we have at least 3 basic blocks: one for entry, one for exit, and one or more for body of method
+    const unsigned int fewestExpectedBlocks = 3;
+
     //We set up a growable list that can be used to insert the blocks
-    const int initialSize = 3;
     GrowableList calleeBasicBlocks;
-    dvmInitGrowableList (&calleeBasicBlocks, initialSize);
+    dvmInitGrowableList (&calleeBasicBlocks, fewestExpectedBlocks);
 
     //We create the CFG for the method
     bool didCreateCfg = dvmCompilerBuildCFG (calleeMethod, &calleeBasicBlocks, &calleeEntry, &calleeExit, 0,
@@ -961,6 +1603,12 @@ static InliningFailure doInline (GrowableList &callerBasicBlocks, const Method *
         return kInliningBadCalleeCFG;
     }
 
+    //Currently we do not support methods with loops
+    if (dvmCompilerDoesContainLoop (calleeBasicBlocks, calleeEntry) == true)
+    {
+        return kInliningCalleeHasLoops;
+    }
+
     MIR *returnMir = findReturn (calleeBasicBlocks, calleeExit);
 
     //If we do not have a return, we cannot inline
@@ -998,49 +1646,20 @@ static InliningFailure doInline (GrowableList &callerBasicBlocks, const Method *
         return kInliningMirRemovalFailed;
     }
 
-    MIR *mirToInline = 0;
-
-    //Search the callee CFG for the MIR to rewrite
-    GrowableListIterator calleeIter;
-    dvmGrowableListIteratorInit (&calleeBasicBlocks, &calleeIter);
+    //Determine the mapping between callee's ins to caller's outs
+    std::map<int, int> calleeToCaller;
+    determineRegisterMapping (invoke->dalvikInsn, calleeMethod, calleeToCaller);
 
-    while (true)
+    //If method is very simple we won't be creating a callee frame so simply rewrite MIR so that it
+    //it is using caller's virtual registers.
+    if (isVerySimple == true)
     {
-        //Get the basic block
-        BasicBlock *bb = reinterpret_cast<BasicBlock *> (dvmGrowableListIteratorNext (&calleeIter));
+        InliningFailure rewritten = locateAndRewriteSingleMIR (calleeBasicBlocks, calleeToCaller, invoke, returnMir,
+                moveResult);
 
-        //When we reach the end we stop
-        if (bb == 0)
-        {
-            break;
-        }
-
-        //We found a bytecode
-        if (bb->firstMIRInsn != 0)
+        if (rewritten != kInliningNoError)
         {
-            if (mirToInline != 0)
-            {
-                return kInliningMoreThanOneBytecode;
-            }
-
-            mirToInline = bb->firstMIRInsn;
-
-            if (mirToInline->next != 0)
-            {
-                return kInliningMoreThanOneBytecode;
-            }
-        }
-    }
-
-    if (mirToInline != 0)
-    {
-        //Now we need to rewrite the VRs for the MIR we wish to inline
-        InliningFailure rewriting = rewriteInlinedMIR (calleeMethod, invoke, moveResult, mirToInline);
-
-        if (rewriting != kInliningNoError)
-        {
-            //If we fail, then don't inline
-            return rewriting;
+            return rewritten;
         }
     }
 
@@ -1048,6 +1667,14 @@ static InliningFailure doInline (GrowableList &callerBasicBlocks, const Method *
     InliningFailure inlined = insertMethodBodyIntoCFG (callerBasicBlocks, calleeMethod, invoke, moveResult, calleeEntry,
             calleeExit, calleeBasicBlocks, isPredicted);
 
+    //If we inlined and we don't have a very simple case, then we need to do some general virtual register renaming
+    //by shifting the cUnit's register window
+    if (isVerySimple == false && inlined == kInliningNoError)
+    {
+        inlined = shiftRegisterWindow (cUnit, callerBasicBlocks, calleeToCaller, calleeMethod, calleeEntry, calleeExit, invoke,
+                moveResult, returnMir);
+    }
+
     //Return the result of the inlining
     return inlined;
 }
@@ -1072,6 +1699,17 @@ static InliningFailure tryInline (CompilationUnit *cUnit, const Method *calleeMe
         return kInliningNativeMethod;
     }
 
+    //Get backend checker whether extended MIR is supported
+    bool (*backendSupportsExtended) (int) = gDvmJit.jitFramework.backendSupportExtendedOp;
+
+    //If we have predicted invoke, check whether backend supports devirtualization check
+    if (backendSupportsExtended == 0 ||
+            (isPredicted == true && backendSupportsExtended != 0
+                    && backendSupportsExtended (kMirOpCheckInlinePrediction) == false))
+    {
+        return kInliningNoBackendExtendedOpSupport;
+    }
+
     //Analyze the body of the method
     CompilerMethodStats *methodStats = dvmCompilerAnalyzeMethodBody (calleeMethod, true);
 
@@ -1082,16 +1720,39 @@ static InliningFailure tryInline (CompilationUnit *cUnit, const Method *calleeMe
     {
         inlined = kInliningFailedBefore;
     }
-    else if ((methodStats->attributes & METHOD_IS_GETTER) != 0
-            || (methodStats->attributes & METHOD_IS_SETTER) != 0
-            || (methodStats->attributes & METHOD_IS_EMPTY) != 0)
+    else if (isVerySimpleMethod (methodStats) == true)
+    {
+        const bool isVerySimple = true;
+        inlined = doInline (cUnit, calleeMethod, invoke, isPredicted, isVerySimple);
+    }
+#ifdef ARCH_IA32
+    //Only x86 has register window shift implementation so we don't inline small methods for anyone else
+    else if ((inlined = isSmallThrowFreeLeaf (methodStats)) == kInliningNoError)
+    {
+        //For methods that are not very simple, we need to make sure we don't overflow
+        if (backendSupportsExtended != 0 && backendSupportsExtended (kMirOpCheckStackOverflow) == true)
+        {
+            const bool isVerySimple = false;
+            inlined = doInline (cUnit, calleeMethod, invoke, isPredicted, isVerySimple);
+        }
+        else
+        {
+            inlined = kInliningNoBackendExtendedOpSupport;
+        }
+    }
+#endif
+
+    //We may have inserted basic blocks so update cUnit's value right now
+    cUnit->numBlocks = dvmGrowableListSize (&cUnit->blockList);
+
+    //If we have inlined then we may have added new basic blocks so calculate predecessors
+    if (inlined == kInliningSuccess)
     {
-        //Getters and setters have a single bytecode and a return
-        inlined = doInline (cUnit->blockList, calleeMethod, invoke, isPredicted);
+        dvmCompilerCalculatePredecessors (cUnit);
 
 #if defined(WITH_JIT_TUNING)
         //When we are trying to tune JIT, keep track of how many getters/setters we inlined
-        if (inlined == kInliningSuccess && (methodStats->attributes & METHOD_IS_GETTER) != 0)
+        if ((methodStats->attributes & METHOD_IS_GETTER) != 0)
         {
             if (isPredicted == true)
             {
@@ -1103,7 +1764,7 @@ static InliningFailure tryInline (CompilationUnit *cUnit, const Method *calleeMe
             }
 
         }
-        else if (inlined == kInliningSuccess && (methodStats->attributes & METHOD_IS_SETTER) != 0)
+        else if ((methodStats->attributes & METHOD_IS_SETTER) != 0)
         {
             if (isPredicted == true)
             {
@@ -1116,15 +1777,6 @@ static InliningFailure tryInline (CompilationUnit *cUnit, const Method *calleeMe
         }
 #endif
     }
-
-    //We may have inserted basic blocks so update cUnit's value right now
-    cUnit->numBlocks = dvmGrowableListSize (&cUnit->blockList);
-
-    //If we have inlined then we may have added new basic blocks so calculate predecessors
-    if (inlined == kInliningSuccess)
-    {
-        dvmCompilerCalculatePredecessors (cUnit);
-    }
     //If we failed to inline then remember it so we don't retry in future
     else
     {
@@ -1178,7 +1830,7 @@ static bool handleInlining (CompilationUnit *cUnit, JitTranslationInfo *info, Ba
     {
        inlined = kInliningInvokeBBProblem;
     }
-    else if ((invoke->OptimizationFlags & MIR_CALLEE) != 0)
+    else if (invoke->nesting.parent != 0 || (invoke->OptimizationFlags & MIR_CALLEE) != 0)
     {
         //For now we only accept one level of inlining so do not accept invokes that come from callee methods
         inlined = kInliningNestedInlining;
@@ -1192,9 +1844,6 @@ static bool handleInlining (CompilationUnit *cUnit, JitTranslationInfo *info, Ba
     //If we haven't found a problem yet, then we continue with trying to inline
     if (inlined == kInliningNoError)
     {
-        //Keep track of whether the invoke is of polymorphic method
-        bool isPredicted = false;
-
         //If we have callsite information from the trace building, then we try to use that.
         //For virtual and interface invokes, the callsite information will be the method called
         //during building so it is just a guess.
@@ -1203,42 +1852,17 @@ static bool handleInlining (CompilationUnit *cUnit, JitTranslationInfo *info, Ba
             calleeMethod = invoke->meta.callsiteInfo->method;
         }
 
-        //Now see if there is anything else we can do depending on the invoke.
-        switch (opcode) {
-            case OP_INVOKE_DIRECT:
-            case OP_INVOKE_STATIC:
-            case OP_INVOKE_DIRECT_RANGE:
-            case OP_INVOKE_STATIC_RANGE:
-            case OP_INVOKE_OBJECT_INIT_RANGE:
-                //We can try to resolve the method if needed since it is non-virtual
-                if (calleeMethod == 0)
-                {
-                    //Method index is in vB
-                    u4 methodIdx = invoke->dalvikInsn.vB;
+        //Keep track of whether the invoke is of polymorphic method
+        bool isPredicted = isVirtualInvoke (opcode);
 
-                    //Paranoid
-                    if (cUnit->method != 0 && cUnit->method->clazz != 0 && cUnit->method->clazz->pDvmDex != 0)
-                    {
-                        //Try to resolve the method
-                        calleeMethod = dvmDexGetResolvedMethod (cUnit->method->clazz->pDvmDex, methodIdx);
-                    }
-                }
-                break;
-            case OP_INVOKE_VIRTUAL:
-            case OP_INVOKE_INTERFACE:
-            case OP_INVOKE_VIRTUAL_RANGE:
-            case OP_INVOKE_INTERFACE_RANGE:
-            case OP_INVOKE_VIRTUAL_QUICK:
-            case OP_INVOKE_VIRTUAL_QUICK_RANGE:
-                //We have a polymorphic call so we must make a prediction. However, for now we just use the
-                //callsite information.
-                isPredicted = true;
-
-                //If logic is added here to try to resolve a guess in case we don't have callsite information,
-                //then "quick" versions should be excluded.
-                break;
-            default:
-                break;
+        //If we do not have a predicted invoke and we still don't know method, we try to resolve it
+        if (isPredicted == false && calleeMethod == 0)
+        {
+            //Get the method from which the invoke is originally from
+            const Method *invokeSourceMethod = invoke->nesting.sourceMethod;
+
+            //Method may not be resolved but for inlining we will try to resolve it
+            calleeMethod = dvmCompilerCheckResolvedMethod (invokeSourceMethod, &(invoke->dalvikInsn));
         }
 
         if (calleeMethod != 0)
@@ -1290,7 +1914,7 @@ static bool handleInlining (CompilationUnit *cUnit, JitTranslationInfo *info, Ba
     }
 
     //If we have have verbosity enabled then we print a message
-    if (cUnit->printMe == true)
+    if (cUnit->printPass == true || cUnit->printMe == true)
     {
         //Decode the MIR
         char *decoded = dvmCompilerGetDalvikDisassembly (&(invoke->dalvikInsn), 0);
@@ -1358,6 +1982,7 @@ void dvmCompilerInlineMIR (CompilationUnit *cUnit, JitTranslationInfo *info)
     }
 }
 
+/* Goes through the given basic block and tries to inline invokes */
 bool dvmCompilerMethodInlining (CompilationUnit *cUnit, BasicBlock *bb)
 {
     //Start off with assumption we won't inline anything
diff --git a/vm/compiler/IntermediateRep.cpp b/vm/compiler/IntermediateRep.cpp
index 470c296..fc5b002 100644
--- a/vm/compiler/IntermediateRep.cpp
+++ b/vm/compiler/IntermediateRep.cpp
@@ -928,6 +928,29 @@ MIR *dvmCompilerCopyMIR (MIR *orig)
     return copy;
 }
 
+u2 * dvmCompilerGetDalvikPC (const MIR *mir)
+{
+    //If we are not provided a MIR, then we cannot figure out a PC
+    if (mir == 0)
+    {
+        return 0;
+    }
+
+    //The MIR knows its own method
+    const Method *sourceMethod = mir->nesting.sourceMethod;
+
+    //If the MIR had no source method, then we cannot figure out a valid PC for it
+    if (sourceMethod == 0)
+    {
+        return 0;
+    }
+
+    //The dalvik PC is the pointer to instructions of method plus the offset
+    u2 *dalvikPC = const_cast<u2 *> (sourceMethod->insns) + mir->offset;
+
+    return dalvikPC;
+}
+
 /**
  * @brief Find the highest MIR in the color
  * @param elem an instruction
@@ -1099,14 +1122,123 @@ bool dvmCompilerCheckVariant (MIR *elem, BitVector *variants, int skipUses)
 }
 
 /**
- * @brief Rewrite the uses of a Dalvik instruction structure.
+ * @brief Used to rewrite instructions in 3rc format.
+ * @param dalvikInsn The dalvik instruction to update. Guaranteed to not be updated if function returns false.
+ * @param oldToNew The mapping of old VRs to the new ones
+ * @param foundOperand Updated by function if operand is found
+ * @return Returns false if problem is encountered.
+ */
+static bool rewrite3rc (DecodedInstruction &dalvikInsn, const std::map<int, int> &oldToNew, bool &foundOperand)
+{
+    //The number of arguments is guaranteed to be in vA for this format
+    u4 count = dalvikInsn.vA;
+
+    //vC holds the start register. The range uses registers from vC to (vC + vA - 1)
+    u4 vC = dalvikInsn.vC;
+
+    u4 newVC = 0;
+    {
+        std::map<int, int>::const_iterator iter = oldToNew.find (vC);
+
+        if (iter == oldToNew.end ())
+        {
+            //We have no new mapping for vC so we cannot rewrite anything
+            return false;
+        }
+        else
+        {
+            newVC = iter->second;
+        }
+    }
+
+    //Now check that all VRs have a consistent old to new mapping
+    for (unsigned int vR = vC + 1; vR < vC + count; vR++)
+    {
+        std::map<int, int>::const_iterator iter = oldToNew.find (vR);
+
+        if (iter == oldToNew.end ())
+        {
+            //We don't have an entry and therefore we don't know the new
+            return false;
+        }
+        else
+        {
+            u4 newVR = iter->second;
+
+            //Now check that the range difference is the same
+            if ((vR - vC) != (newVR - newVC))
+            {
+                //Range is not the same
+                return false;
+            }
+        }
+    }
+
+    //If we made it to this point, all the checks passed and therefore we can update vC
+    dalvikInsn.vC = newVC;
+    foundOperand = true;
+
+    return true;
+}
+
+/**
+ * @brief Used to rewrite instructions in 35c format.
  * @param dalvikInsn The dalvik instruction to update.
- * @param oldVR the old VR that we want to rewrite
- * @param newVR the new VR we want to use
- * @return whether the rewrite was successful
+ * @param oldToNew The mapping of old VRs to the new ones
+ * @param foundOperand Updated by function if operand is found
+ */
+static void rewrite35c (DecodedInstruction &dalvikInsn, const std::map<int, int> &oldToNew, bool &foundOperand)
+{
+    //The number of arguments is guaranteed to be in vA for this format
+    u4 count = dalvikInsn.vA;
+
+    //Go through each of the operands to look for a match for the old VR.
+    for (u4 operand = 0; operand < count; operand++)
+    {
+        //Iterate through the mapping of old to new VRs
+        for (std::map<int, int>::const_iterator iter = oldToNew.begin (); iter != oldToNew.end (); iter++)
+        {
+            u4 oldVR = iter->first;
+            u4 newVR = iter->second;
+
+            //If we found a match, update it now
+            if (dalvikInsn.arg[operand] == oldVR)
+            {
+                //Update the operand and mark it as found
+                dalvikInsn.arg[operand] = newVR;
+                foundOperand = true;
+                break;
+            }
+        }
+    }
+}
+
+/**
+ * @brief Used to rename a single virtual register
+ * @details The user passes a reference to the register to update and this function looks
+ * in the oldToNew mapping for entry with key that has old register
+ * @param oldToNew The mapping of old VRs to the new ones
+ * @param regToRewrite Reference to the register name which needs updated
+ * @param foundOperand Updated by function if operand is found
  */
-static bool rewriteUses (DecodedInstruction &dalvikInsn, int oldVR,
-        int newVR)
+static void rewriteVR (const std::map<int, int> &oldToNew, u4 &regToRewrite, bool &foundOperand)
+{
+    //Iterate through the mapping of old to new VRs
+    for (std::map<int, int>::const_iterator iter = oldToNew.begin (); iter != oldToNew.end (); iter++)
+    {
+        u4 oldVR = iter->first;
+        u4 newVR = iter->second;
+
+        if (regToRewrite == oldVR)
+        {
+            regToRewrite = newVR;
+            foundOperand = true;
+            break;
+        }
+    }
+}
+
+bool dvmCompilerRewriteMirVRs (DecodedInstruction &dalvikInsn, const std::map<int, int> &oldToNew, bool onlyUses)
 {
     //Phi nodes get recomputed automatically and thus we don't need to rewrite the uses.
     if (static_cast<ExtendedMIROpcode> (dalvikInsn.opcode) == kMirOpPhi)
@@ -1118,81 +1250,97 @@ static bool rewriteUses (DecodedInstruction &dalvikInsn, int oldVR,
     //Get dataflow flags
     int dfAttributes = dvmCompilerDataFlowAttributes[dalvikInsn.opcode];
 
-    u4 oldVRU4 = oldVR;
-    u4 newVRU4 = newVR;
-
-    //If we have a instruction that is range form, we cannot rewrite the
-    //operand and therefore we fail immediately
-    if ((dfAttributes & DF_FORMAT_3RC) != 0)
-    {
-        return false;
-    }
-
     //If we have an extended MIR then reject because rewriting support has not been added
     if ((dfAttributes & DF_FORMAT_EXT_OP) != 0)
     {
         return false;
     }
 
-    //Check if the first use overlaps the def for this MIR. If it does
-    //and the VR we are rewriting is this operand, we should reject.
-    if ((dfAttributes & (DF_UA | DF_UA_WIDE)) != 0
-            && (dfAttributes & DF_HAS_DEFS) != 0
-            && dalvikInsn.vA == oldVRU4)
+    //If we are rewriting only uses, then we cannot have overlapping use and def because we are updating the use and not the def
+    if (onlyUses == true)
     {
-        return false;
+        //Check if we have overlap
+        if ((dfAttributes & DF_A_IS_DEFINED_REG) != 0 && (dfAttributes & DF_A_IS_USED_REG) != 0)
+        {
+            return false;
+        }
     }
 
     //Be pessimistic and assume we won't find operand
     bool foundOperand = false;
 
+    //Check if we have instruction that is in range form
+    if ((dfAttributes & DF_FORMAT_3RC) != 0)
+    {
+        if (rewrite3rc (dalvikInsn, oldToNew, foundOperand) == false)
+        {
+            //Something went wrong so we just pass along the error
+            return false;
+        }
+    }
+
     //Now check if it is format35c which may have multiple uses.
     bool format35c = ((dfAttributes & DF_FORMAT_35C) != 0);
 
     if (format35c == true)
     {
-        //The number of arguments is guaranteed to be in vA for this format
-        u4 count = dalvikInsn.vA;
-
-        //Go through each of the operands to look for a match for the old VR.
-        for (u4 operand = 0; operand < count; operand++)
-        {
-            //If we found a match, update it now
-            if (dalvikInsn.arg[operand] == oldVRU4)
-            {
-                //Update the operand and mark it as found
-                dalvikInsn.arg[operand] = newVRU4;
-                foundOperand = true;
-            }
-        }
+        rewrite35c (dalvikInsn, oldToNew, foundOperand);
     }
 
-    //Check if vA matches operand we are looking for.
-    if ( ( (dfAttributes & DF_UA) || (dfAttributes & DF_UA_WIDE)) && (dalvikInsn.vA == oldVRU4))
+    //We write to vA if it is a register and we're not just rewriting uses, or if vA is used and not defined
+    bool writevA = (onlyUses == false && (dfAttributes & DF_A_IS_REG) != 0)
+            || ((dfAttributes & DF_A_IS_USED_REG) != 0 && (dfAttributes & DF_A_IS_DEFINED_REG) == 0);
+
+    //Check if vA matches operand we are looking for
+    if (writevA == true)
     {
-        dalvikInsn.vA = newVRU4;
-        foundOperand = true;
+        rewriteVR (oldToNew, dalvikInsn.vA, foundOperand);
     }
 
     //Check if vB matches operand we are looking for.
-    if ((dfAttributes & DF_B_IS_REG) != 0 && dalvikInsn.vB == oldVRU4)
+    if ((dfAttributes & DF_B_IS_REG) != 0)
     {
-        dalvikInsn.vB = newVRU4;
-        foundOperand = true;
+        rewriteVR (oldToNew, dalvikInsn.vB, foundOperand);
     }
 
     //Check if vC matches operand we are looking for. If we have 35c form,
     //the "this" pointer might be in vC and also in arg[0]. Since we don't
     //know who will use this decoded instruction, we try to update vC as well.
-    if (((dfAttributes & DF_C_IS_REG) != 0 || format35c == true)
-            && dalvikInsn.vC == oldVRU4)
+    if ((dfAttributes & DF_C_IS_REG) != 0 || format35c == true)
+    {
+        rewriteVR (oldToNew, dalvikInsn.vC, foundOperand);
+    }
+
+    //If we are rewriting only uses, then we expect to find the operand.
+    if (onlyUses == true)
+    {
+        //If we did not find operand we failed the rewrite
+        return foundOperand;
+    }
+    else
     {
-        dalvikInsn.vC = newVRU4;
-        foundOperand = true;
+        //If we aren't rewriting only uses, then we are generically doing VR rewriting.
+        //Thus if we make it to this point all rewriting went fine. We don't check foundOperand because
+        //some bytecodes (like gotos) have no virtual register usages/defines.
+        return true;
     }
+}
+
+/**
+ * @brief Rewrite the uses of a Dalvik instruction structure.
+ * @param dalvikInsn The dalvik instruction to update.
+ * @param oldVR the old VR that we want to rewrite
+ * @param newVR the new VR we want to use
+ * @return whether the rewrite was successful
+ */
+static bool rewriteUses (DecodedInstruction &dalvikInsn, int oldVR, int newVR)
+{
+    //Create a map of old to new VR
+    std::map<int, int> oldToNew;
+    oldToNew[oldVR] = newVR;
 
-    //If we did not find operand we failed the rewrite
-    return foundOperand;
+    //Ask for MIR rewriting for just the uses
+    return dvmCompilerRewriteMirVRs (dalvikInsn, oldToNew, true);
 }
 
 /**
@@ -1383,6 +1531,7 @@ OpcodeFlags dvmCompilerGetOpcodeFlags (int opcode)
             case kMirOpLowerBound:
             case kMirOpNullCheck:
             case kMirOpBoundCheck:
+            case kMirOpCheckStackOverflow:
                 //Instruction can continue or it may throw
                 flags = kInstrCanContinue | kInstrCanThrow;
                 break;
diff --git a/vm/compiler/Loop.cpp b/vm/compiler/Loop.cpp
index 1de6f22..5c8582c 100644
--- a/vm/compiler/Loop.cpp
+++ b/vm/compiler/Loop.cpp
@@ -950,6 +950,15 @@ void dvmCompilerGenHoistedChecks(CompilationUnit *cUnit, Pass* pass)
     /* Should be loop invariant */
     int idxReg = 0;
 
+    //TODO The offset in entry->offset may not be correct to use. The offset for exception
+    //must use the offset of the first instruction in block before heavy optimizations are
+    //applied like invariant hoisting. The same applies for the parent method for these
+    //extended MIRs. They technically have no source method but the one matching the first
+    //instruction in loop should be assigned. This ensures that correct exit PC is set
+    //in case these checks lead to exception.
+    const unsigned int offsetForException = entry->startOffset;
+    NestedMethod nesting (cUnit->method);
+
     // Go through array access elements and generate range checks
     // Range check in the current implemntation is the upper border of the loop
     // E.g. for count down loops it is lowest index
@@ -980,7 +989,8 @@ void dvmCompilerGenHoistedChecks(CompilationUnit *cUnit, Pass* pass)
 
         // set offset to the start offset of entry block
         // this will set rPC in case of bail to interpreter
-        rangeCheckMIR->offset = entry->startOffset;
+        rangeCheckMIR->offset = offsetForException;
+        rangeCheckMIR->nesting = nesting;
         dvmCompilerAppendMIR(entry, rangeCheckMIR);
         // To do bound check we need to know globalMaxC/globalMinC value
         // as soon as we're limited with just one BIV globalMaxC will contain
@@ -998,24 +1008,26 @@ void dvmCompilerGenHoistedChecks(CompilationUnit *cUnit, Pass* pass)
     // Bound check values should be adjusted to meet loop branch condition
     if (loopInfo->getArrayAccessInfo()->numUsed != 0) {
         if (loopInfo->isCountUpLoop()) {
-            MIR *boundCheckMIR = (MIR *)dvmCompilerNew(sizeof(MIR), true);
+            MIR *boundCheckMIR = dvmCompilerNewMIR ();
             boundCheckMIR->dalvikInsn.opcode = (Opcode)kMirOpLowerBound;
             boundCheckMIR->dalvikInsn.vA = idxReg;
             boundCheckMIR->dalvikInsn.vB = globalMinC;
             // set offset to the start offset of entry block
             // this will set rPC in case of bail to interpreter
-            boundCheckMIR->offset = entry->startOffset;
+            boundCheckMIR->offset = offsetForException;
+            boundCheckMIR->nesting = nesting;
             dvmCompilerAppendMIR(entry, boundCheckMIR);
      } else {
             if (loopInfo->getLoopBranchOpcode() == OP_IF_LT ||
                 loopInfo->getLoopBranchOpcode() == OP_IF_LE) {
-                MIR *boundCheckMIR = (MIR *)dvmCompilerNew(sizeof(MIR), true);
+                MIR *boundCheckMIR = dvmCompilerNewMIR ();
                 boundCheckMIR->dalvikInsn.opcode = (Opcode)kMirOpLowerBound;
                 boundCheckMIR->dalvikInsn.vA = loopInfo->getEndConditionReg();
                 boundCheckMIR->dalvikInsn.vB = globalMinC;
                 // set offset to the start offset of entry block
                 // this will set rPC in case of bail to interpreter
-                boundCheckMIR->offset = entry->startOffset;
+                boundCheckMIR->offset = offsetForException;
+                boundCheckMIR->nesting = nesting;
                 /*
                  * If the end condition is ">" in the source, the check in the
                  * Dalvik bytecode is OP_IF_LE. In this case add 1 back to the
@@ -1029,23 +1041,23 @@ void dvmCompilerGenHoistedChecks(CompilationUnit *cUnit, Pass* pass)
             } else if (loopInfo->getLoopBranchOpcode() == OP_IF_LTZ) {
                 /* Array index will fall below 0 */
                 if (globalMinC < 0) {
-                    MIR *boundCheckMIR = (MIR *)dvmCompilerNew(sizeof(MIR),
-                                                               true);
+                    MIR *boundCheckMIR = dvmCompilerNewMIR ();
                     boundCheckMIR->dalvikInsn.opcode = (Opcode)kMirOpPunt;
                     // set offset to the start offset of entry block
                     // this will set rPC in case of bail to interpreter
-                    boundCheckMIR->offset = entry->startOffset;
+                    boundCheckMIR->offset = offsetForException;
+                    boundCheckMIR->nesting = nesting;
                     dvmCompilerAppendMIR(entry, boundCheckMIR);
                 }
             } else if (loopInfo->getLoopBranchOpcode() == OP_IF_LEZ) {
                 /* Array index will fall below 0 */
                 if (globalMinC < -1) {
-                    MIR *boundCheckMIR = (MIR *)dvmCompilerNew(sizeof(MIR),
-                                                               true);
+                    MIR *boundCheckMIR = dvmCompilerNewMIR ();
                     boundCheckMIR->dalvikInsn.opcode = (Opcode)kMirOpPunt;
                     // set offset to the start offset of entry block
                     // this will set rPC in case of bail to interpreter
-                    boundCheckMIR->offset = entry->startOffset;
+                    boundCheckMIR->offset = offsetForException;
+                    boundCheckMIR->nesting = nesting;
                     dvmCompilerAppendMIR(entry, boundCheckMIR);
                 }
             } else {
diff --git a/vm/compiler/LoopInformation.cpp b/vm/compiler/LoopInformation.cpp
index 90587d1..510774b 100644
--- a/vm/compiler/LoopInformation.cpp
+++ b/vm/compiler/LoopInformation.cpp
@@ -1406,7 +1406,7 @@ int LoopInformation::getFreeScratchRegisters (CompilationUnit *cUnit, int consec
         setScratchRegisters (res + consecutives);
 
         //Get the actual scratch register
-        res = dvmArchSpecGetScratchRegister (cUnit->method, res);
+        res = dvmArchSpecGetScratchRegister (cUnit->method, res, cUnit->registerWindowShift);
 
         //Return first register
         return res;
diff --git a/vm/compiler/RegisterizationME.cpp b/vm/compiler/RegisterizationME.cpp
index 3e9b46d..1cea93a 100644
--- a/vm/compiler/RegisterizationME.cpp
+++ b/vm/compiler/RegisterizationME.cpp
@@ -214,7 +214,7 @@ static void handleWriteBackRequestsPostLoop (const CompilationUnit *cUnit, LoopI
         //For the moment, we are being simple, exiting the loop, we request write backs of
         //every register in the method. Note that we do not request writebacks of
         //temporaries and thus we do not want to use cUnit->numDalvikRegisters
-        unsigned int size = cUnit->method->registersSize;
+        unsigned int size = cUnit->numDalvikRegisters;
         dvmSetInitialBits (bb->requestWriteBack, size);
     }
 }
diff --git a/vm/compiler/SSATransformation.cpp b/vm/compiler/SSATransformation.cpp
index 6c65030..4c76d42 100644
--- a/vm/compiler/SSATransformation.cpp
+++ b/vm/compiler/SSATransformation.cpp
@@ -95,8 +95,10 @@ static void computeDefBlockMatrix(CompilationUnit *cUnit)
 {
     int numRegisters = cUnit->numDalvikRegisters;
 
-    /* Allocate numDalvikRegisters bit vector pointers */
-    if (cUnit->defBlockMatrixSize < numRegisters)
+    //At this point we need to determine if we need to allocate the defBlockMatrix.
+    //Since the size of it is always set to numRegisters + 1, in our comparison we subtract one
+    //in order to determine if it actually needs resized.
+    if (cUnit->defBlockMatrixSize - 1 < numRegisters)
     {
         cUnit->defBlockMatrixSize = numRegisters + 1;
         cUnit->defBlockMatrix = static_cast<BitVector **> (dvmCompilerNew(sizeof(BitVector *) * cUnit->defBlockMatrixSize, true));
@@ -522,10 +524,14 @@ static void insertPhiNodes(CompilationUnit *cUnit)
 
     if (cUnit->tempDalvikRegisterV == 0)
     {
-        cUnit->tempDalvikRegisterV =
-            dvmCompilerAllocBitVector(cUnit->numDalvikRegisters, false);
+        cUnit->tempDalvikRegisterV = dvmCompilerAllocBitVector (cUnit->numDalvikRegisters, true);
+    }
+    else
+    {
+        //Ensure that BitVector grows to the number of dalvik registers
+        dvmCompilerSetBit (cUnit->tempDalvikRegisterV, cUnit->numDalvikRegisters - 1);
+        dvmClearAllBits (cUnit->tempDalvikRegisterV);
     }
-    dvmClearAllBits (cUnit->tempDalvikRegisterV);
 
     dvmCompilerDataFlowAnalysisDispatcher(cUnit, computeBlockLiveIns,
                                           kPostOrderDFSTraversal,
@@ -1022,8 +1028,11 @@ bool dvmCompilerCalculateBasicBlockInformation (CompilationUnit *cUnit, bool fil
     {
         cUnit->usedVRs = dvmCompilerAllocBitVector (1, true);
     }
-    //Clear it
-    dvmClearAllBits (cUnit->usedVRs);
+    else
+    {
+        //If already allocated then we just need to clear it because we fill it below
+        dvmClearAllBits (cUnit->usedVRs);
+    }
 
     //Now dispatch
     dvmCompilerDataFlowAnalysisDispatcher(cUnit, dvmCompilerFillUsedVector, kAllNodes, false);
@@ -1067,10 +1076,12 @@ bool dvmCompilerCalculateBasicBlockInformation (CompilationUnit *cUnit, bool fil
      */
     if (cUnit->tempSSARegisterV == 0)
     {
-        cUnit->tempSSARegisterV = dvmCompilerAllocBitVector(cUnit->numSSARegs, true);
+        cUnit->tempSSARegisterV = dvmCompilerAllocBitVector (cUnit->numSSARegs, true);
     }
     else
     {
+        //Set the highest SSA reg in order to expand the vector
+        dvmSetBit (cUnit->tempSSARegisterV, cUnit->numSSARegs - 1);
         dvmClearAllBits (cUnit->tempSSARegisterV);
     }
 
diff --git a/vm/compiler/StackExtension.h b/vm/compiler/StackExtension.h
index 177142f..5321773 100644
--- a/vm/compiler/StackExtension.h
+++ b/vm/compiler/StackExtension.h
@@ -31,7 +31,7 @@ unsigned int dvmArchSpecGetNumberOfScratch (void);
  * for which we want to use scratch register. Whenever a scratch register
  * with that index is not available, the return value will be -1.
  */
-int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx);
+int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx, int registerWindowShift);
 
 #ifdef ARCH_IA32
 
@@ -45,7 +45,7 @@ unsigned int dvmArchSpecGetNumberOfScratch (void)
     return 0;
 }
 
-int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx)
+int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx, int registerWindowShift)
 {
     //Non-x86 don't have scratch registers in their frames
     return -1;
diff --git a/vm/compiler/Utility.cpp b/vm/compiler/Utility.cpp
index c5a34d3..c0222d3 100644
--- a/vm/compiler/Utility.cpp
+++ b/vm/compiler/Utility.cpp
@@ -328,7 +328,6 @@ void dvmCompilerDumpCompilationUnit(CompilationUnit *cUnit)
         "Exception Handling",
         "Catch Entry",
         "PreBackward Block",
-        "Code Block with Vectorized MIRs",
         "From Interpreter",
     };
 
@@ -883,3 +882,144 @@ bool dvmCompilerCheckResolvedReferences (const Method *method, const DecodedInst
     //If we get here, everything went okay
     return true;
 }
+
+void dvmCompilerUpdateCUnitNumDalvikRegisters (CompilationUnit *cUnit, int newNumberDalvikRegisters)
+{
+    //We only need to update data structures if the new number of dalvik registers is greater than before
+    if (newNumberDalvikRegisters > cUnit->numDalvikRegisters)
+    {
+        //Invalidate all structures that are size of numDalvikRegisters
+        cUnit->dalvikToSSAMap = 0;
+        cUnit->defBlockMatrix = 0;
+        cUnit->ssaSubScripts = 0;
+
+        //We do not need to invalidate the isConstantV because it is an expandable BitVector
+        if (cUnit->isConstantV != 0)
+        {
+            //Let's go ahead and clear it because we may have different constant information when we update
+            dvmClearAllBits(cUnit->isConstantV);
+        }
+
+        //We also do not need to invalidate ssaToDalvikMap because it is a GrowableList
+        if (cUnit->ssaToDalvikMap != 0)
+        {
+            //The dalvik registers have changed so the map doesn't hold good content
+            dvmClearGrowableList (cUnit->ssaToDalvikMap);
+        }
+
+        //For usedVRs we need to clear it because its possible we changed register names
+        if (cUnit->usedVRs != 0)
+        {
+            dvmClearAllBits (cUnit->usedVRs);
+        }
+
+        //For tempDalvikRegisterV and tempSSARegisterV, we don't have to set it to 0, we can just clear all bits
+        if (cUnit->tempDalvikRegisterV != 0)
+        {
+            //Ensure that BitVector grows to the number of dalvik registers
+            dvmCompilerSetBit (cUnit->tempDalvikRegisterV, newNumberDalvikRegisters - 1);
+            dvmClearAllBits (cUnit->tempDalvikRegisterV);
+        }
+        if (cUnit->tempSSARegisterV != 0)
+        {
+            //Ensure that BitVector grows to the number of dalvik registers
+            dvmCompilerSetBit (cUnit->tempSSARegisterV, newNumberDalvikRegisters - 1);
+            dvmClearAllBits (cUnit->tempSSARegisterV);
+        }
+
+        cUnit->numDalvikRegisters = newNumberDalvikRegisters;
+    }
+}
+
+/**
+ * @brief Used to color during DFS traversal of CFG
+ */
+enum VisitingColor
+{
+    BeingVisited,  //!< Node is being visited
+    DoneVisiting,  //!< Node has already been visited
+};
+
+/**
+ * @brief Helper method which looks for a loop
+ * @param bb The basic block being visited
+ * @param visited Map which holds visited block and their visiting color
+ * @return Returns whether a loop has been found
+ */
+static bool lookForLoop (BasicBlock *bb, std::map<BasicBlock *, VisitingColor> &visited)
+{
+    //If we have already visited it and we're also in the middle of visiting it and its children then we found loop
+    if (visited.find (bb) != visited.end ())
+    {
+        VisitingColor color = visited[bb];
+
+        if (color == BeingVisited)
+        {
+            return true;
+        }
+        else if (color == DoneVisiting)
+        {
+            return false;
+        }
+    }
+
+    //Insert current BB to visited
+    visited[bb] = BeingVisited;
+
+    bool foundLoop = false;
+
+    //Create iterator for visiting children
+    ChildBlockIterator childIter (bb);
+
+    //Now iterate through the children to visit each of them first
+    for (BasicBlock **childPtr = childIter.getNextChildPtr (); childPtr != 0 && foundLoop == false;
+            childPtr = childIter.getNextChildPtr ())
+    {
+        BasicBlock *child = *childPtr;
+
+        assert (child != 0);
+
+        //Doing a DFS so we look for loop in child first
+        foundLoop = lookForLoop (child, visited);
+    }
+
+    //Done visiting
+    visited[bb] = DoneVisiting;
+
+    return foundLoop;
+}
+
+bool dvmCompilerDoesContainLoop (GrowableList &blockList, BasicBlock *entry)
+{
+    bool foundLoop = false;
+    std::map<BasicBlock *, VisitingColor> visited;
+
+    if (entry != 0)
+    {
+        //Keep track of visited blocks and start from entry
+        foundLoop = lookForLoop (entry, visited);
+    }
+
+    //We'd like to assume that we visited all blocks by going from entry but now we check to make sure
+    GrowableListIterator iterator;
+    dvmGrowableListIteratorInit (&blockList, &iterator);
+
+    while (foundLoop == false)
+    {
+        BasicBlock *bb = reinterpret_cast<BasicBlock *> (dvmGrowableListIteratorNext (&iterator));
+
+        //We break out of loop if we don't have anymore BBs
+        if (bb == 0)
+        {
+            break;
+        }
+
+        //Check if we visited this already
+        if (visited.find (bb) == visited.end())
+        {
+            foundLoop = lookForLoop (bb, visited);
+        }
+    }
+
+    return foundLoop;
+}
diff --git a/vm/compiler/Utility.h b/vm/compiler/Utility.h
index 7137793..a882539 100644
--- a/vm/compiler/Utility.h
+++ b/vm/compiler/Utility.h
@@ -59,4 +59,21 @@ const Method *dvmCompilerCheckResolvedMethod (const Method *methodContainingInvo
  */
 bool dvmCompilerCheckResolvedReferences (const Method *method, const DecodedInstruction *insn, bool tryToResolve = false);
 
+/**
+ * @brief Used to update the number of dalvik registers in a cUnit.
+ * @details The update is only done if the new number is larger than old number of dalvik registers.
+ * Function ensures that all structures dependent on this are invalidated.
+ * @param cUnit The compilation unit
+ * @param newNumberDalvikRegisters The new number of dalvik registers to use
+ */
+void dvmCompilerUpdateCUnitNumDalvikRegisters (CompilationUnit *cUnit, int newNumberDalvikRegisters);
+
+/**
+ * @brief Returns whether there is a loop in the CFG
+ * @param blockList The blocks in the CFG
+ * @param entry The entry block into CFG
+ * @return Returns whether a loop exists
+ */
+bool dvmCompilerDoesContainLoop (GrowableList &blockList, BasicBlock *entry);
+
 #endif
diff --git a/vm/compiler/codegen/CodegenFactory.cpp b/vm/compiler/codegen/CodegenFactory.cpp
index 02282cd..32260b9 100644
--- a/vm/compiler/codegen/CodegenFactory.cpp
+++ b/vm/compiler/codegen/CodegenFactory.cpp
@@ -324,4 +324,28 @@ bool dvmCompilerArchSupportsVectorizedPackedSize (unsigned int size)
     //No vectorization support for other architectures unless encoder implements it
     return false;
 }
+
+/**
+ * @brief Used to check whether the architecture specific portion supports extended opcode
+ * @param extendedOpcode The opcode to check
+ * @return Returns whether the extended opcode is supported
+ */
+bool dvmCompilerArchSupportsExtendedOp (int extendedOpcode)
+{
+    switch (extendedOpcode)
+    {
+        case kMirOpPhi:
+        case kMirOpNullNRangeUpCheck:
+        case kMirOpNullNRangeDownCheck:
+        case kMirOpLowerBound:
+        case kMirOpPunt:
+        case kMirOpCheckInlinePrediction:
+            return true;
+        default:
+            break;
+    }
+
+    //If we get here it is not supported
+    return false;
+}
 #endif
diff --git a/vm/compiler/codegen/CompilerCodegen.h b/vm/compiler/codegen/CompilerCodegen.h
index 36c6a7c..ec15313 100644
--- a/vm/compiler/codegen/CompilerCodegen.h
+++ b/vm/compiler/codegen/CompilerCodegen.h
@@ -109,4 +109,10 @@ bool backendCanBailOut(CompilationUnit *cUnit, MIR *mir);
  */
 bool dvmCompilerArchSupportsVectorizedPackedSize (unsigned int size);
 
+/*
+ * Implemented in codegen/<target>/CodegenDriver.cpp
+ * Used to check whether the architecture specific portion supports extended opcode
+ */
+bool dvmCompilerArchSupportsExtendedOp (int extendedOpcode);
+
 #endif  // DALVIK_VM_COMPILERCODEGEN_H_
diff --git a/vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp b/vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp
index 0ad40db..0b78e9c 100644
--- a/vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp
+++ b/vm/compiler/codegen/x86/lightcg/AnalysisO1.cpp
@@ -76,7 +76,10 @@ bool isVirtualReg(int type) {
  */
 bool isTemporary (int type, int regNum)
 {
-    return (isVirtualReg (type) == false);
+    //Create a compile entry in order to ask it if we have a temporary
+    CompileTableEntry compileEntry (regNum, type);
+
+    return compileEntry.isTemporary ();
 }
 
 /**
@@ -1265,6 +1268,7 @@ bool skipExtendedMir(Opcode opc) {
         case kMirOpPackedAddReduce:
         case kMirOpPackedSet:
         case kMirOpNullCheck:
+        case kMirOpCheckStackOverflow:
             return false;
         default:
             return true;
@@ -1487,7 +1491,9 @@ int codeGenBasicBlock(const Method* method, BasicBlock_O1* bb, CompilationUnit_O
     {
         int k;
         offsetPC = mir->seqNum;
-        rPC = const_cast<u2 *>(method->insns) + mir->offset;
+
+        //Update rPC to contain the dalvik PC for this bytecode
+        rPC = dvmCompilerGetDalvikPC (mir);
 
         //Skip mirs tagged as being no-ops
         if ((mir->OptimizationFlags & MIR_INLINED) != 0)
@@ -2934,11 +2940,16 @@ void writeBackVR(int vR, LowOpndRegType type, int physicalReg) {
 
     // Handle writing back different types of VRs
     if (physicalType == LowOpndRegType_gp || physicalType == LowOpndRegType_xmm)
+    {
         set_virtual_reg_noalloc(vR, getRegSize(physicalType), physicalReg,
                 true);
+    }
     if (physicalType == LowOpndRegType_ss)
-        move_ss_reg_to_mem_noalloc(physicalReg, true, 4 * vR, PhysicalReg_FP,
+    {
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vR);
+        move_ss_reg_to_mem_noalloc(physicalReg, true, vrOffset, PhysicalReg_FP,
                 true, MemoryAccess_VR, vR);
+    }
 
     // Mark it in memory because we have written it back
     setVRMemoryState (vR, getRegSize (physicalType), true);
@@ -2967,8 +2978,9 @@ void dumpPartToMem(int reg /*xmm physical reg*/, int vA, bool isLow) {
     }
     //move part to vA or vA+1
     if(isLow) {
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
         move_ss_reg_to_mem_noalloc(reg, true,
-                                   4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
+                                   vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA);
     } else {
 #ifdef SPILL_IN_THREAD
         int k = getSpillIndex (OpndSize_64);
@@ -2985,7 +2997,8 @@ void dumpPartToMem(int reg /*xmm physical reg*/, int vA, bool isLow) {
         dump_imm_reg_noalloc_alu(Mnemonic_PSRLQ, OpndSize_64, 32, reg, true, LowOpndRegType_xmm);
 #endif
         //move low 32 bits of xmm reg to vA+1
-        move_ss_reg_to_mem_noalloc(reg, true, 4*(vA+1), PhysicalReg_FP, true, MemoryAccess_VR, vA+1);
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA + 1);
+        move_ss_reg_to_mem_noalloc(reg, true, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA+1);
     }
 
     if (isLow)
@@ -4282,8 +4295,9 @@ void updateVRAtUse(int reg, LowOpndRegType pType, int regAll) {
 
             // Load from memory into the physical register
             if (loadFromMemory) {
+                const int vrOffset = getVirtualRegOffsetRelativeToFP (currentBB->xferPoints[k].regNum);
                 move_mem_to_reg_noalloc(OpndSize_64,
-                        4 * currentBB->xferPoints[k].regNum, PhysicalReg_FP,
+                        vrOffset, PhysicalReg_FP,
                         true, MemoryAccess_VR, currentBB->xferPoints[k].regNum,
                         regAll, true);
             }
@@ -6189,15 +6203,21 @@ void transferToState(int stateNum) {
                 bool isSS = ((compileTable[k].physicalType & MASK_FOR_TYPE) == LowOpndRegType_ss);
                 if(compileTable[k].physicalReg == PhysicalReg_Null && targetReg != PhysicalReg_Null) {
                     if(isSS)
-                        move_ss_mem_to_reg_noalloc(4*compileTable[k].regNum,
+                    {
+                        const int vrOffset = getVirtualRegOffsetRelativeToFP (compileTable[k].regNum);
+                        move_ss_mem_to_reg_noalloc(vrOffset,
                                                    PhysicalReg_FP, true,
                                                    MemoryAccess_VR, compileTable[k].regNum,
                                                    targetReg, true);
+                    }
                     else
-                        move_mem_to_reg_noalloc(oSize, 4*compileTable[k].regNum,
+                    {
+                        const int vrOffset = getVirtualRegOffsetRelativeToFP (compileTable[k].regNum);
+                        move_mem_to_reg_noalloc(oSize, vrOffset,
                                                 PhysicalReg_FP, true,
                                                 MemoryAccess_VR, compileTable[k].regNum,
                                                 targetReg, true);
+                    }
                 }
                 if(compileTable[k].physicalReg != PhysicalReg_Null && targetReg != PhysicalReg_Null) {
                     move_reg_to_reg_noalloc((isSS ? OpndSize_64 : oSize),
diff --git a/vm/compiler/codegen/x86/lightcg/BytecodeVisitor.cpp b/vm/compiler/codegen/x86/lightcg/BytecodeVisitor.cpp
index de72b77..d3e014e 100644
--- a/vm/compiler/codegen/x86/lightcg/BytecodeVisitor.cpp
+++ b/vm/compiler/codegen/x86/lightcg/BytecodeVisitor.cpp
@@ -1576,6 +1576,9 @@ int getVirtualRegInfo (VirtualRegInfo* infoArray, const MIR * currentMIR, bool u
                 }
 
                 break;
+            case kMirOpCheckStackOverflow:
+                num_regs_per_bytecode = 0;
+                break;
             default:
                 ALOGI("JIT_INFO: Extended MIR not supported in getVirtualRegInfo");
                 SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
@@ -3876,6 +3879,18 @@ int getTempRegInfo(TempRegInfo* infoArray, const MIR * currentMIR, const u2* dal
                 //Return the number of temps being used
                 return tempRegCount;
             }
+            case kMirOpCheckStackOverflow:
+                //Temp1 is used for loading self pointer
+                infoArray[0].regNum = 1;
+                infoArray[0].refCount = 2;
+                infoArray[0].physicalType = LowOpndRegType_gp;
+
+                //Temp2 is used for storing calculations on FP for overflow
+                infoArray[1].regNum = 2;
+                infoArray[1].refCount = 3;
+                infoArray[1].physicalType = LowOpndRegType_gp;
+
+                return 2;
             default:
                 ALOGI("JIT_INFO: Extended MIR not supported in getTempRegInfo");
                 SET_JIT_ERROR(kJitErrorUnsupportedBytecode);
diff --git a/vm/compiler/codegen/x86/lightcg/CodegenInterface.cpp b/vm/compiler/codegen/x86/lightcg/CodegenInterface.cpp
index f563417..fa53889 100644
--- a/vm/compiler/codegen/x86/lightcg/CodegenInterface.cpp
+++ b/vm/compiler/codegen/x86/lightcg/CodegenInterface.cpp
@@ -413,6 +413,39 @@ bool dvmCompilerArchSupportsVectorizedPackedSize (unsigned int size)
     return false;
 }
 
+/**
+ * @brief Used to check whether the architecture specific portion supports extended opcode
+ * @param extendedOpcode The opcode to check
+ * @return Returns whether the extended opcode is supported
+ */
+bool dvmCompilerArchSupportsExtendedOp (int extendedOpcode)
+{
+    switch (extendedOpcode)
+    {
+        case kMirOpPhi:
+        case kMirOpNullCheck:
+        case kMirOpBoundCheck:
+        case kMirOpNullNRangeUpCheck:
+        case kMirOpNullNRangeDownCheck:
+        case kMirOpLowerBound:
+        case kMirOpCheckInlinePrediction:
+        case kMirOpRegisterize:
+        case kMirOpMove128b:
+        case kMirOpPackedAddition:
+        case kMirOpPackedMultiply:
+        case kMirOpPackedAddReduce:
+        case kMirOpConst128b:
+        case kMirOpPackedSet:
+        case kMirOpCheckStackOverflow:
+            return true;
+        default:
+            break;
+    }
+
+    //If we get here it is not supported
+    return false;
+}
+
 void dvmCompilerPatchInlineCache(void)
 {
     int i;
@@ -525,67 +558,6 @@ void dvmJitScanAllClassPointers(void (*callback)(void *))
 {
 }
 
-/**
- * @brief Generates a jump with 32-bit relative immediate that jumps
- * to the target.
- * @details Updates the instruction stream with the jump.
- * @param target absolute address of target.
- */
-void unconditional_jump_rel32(void * target) {
-    // We will need to figure out the immediate to use for the relative
-    // jump, so we need to flush scheduler so that stream is updated.
-    // In most cases this won't affect the schedule since the jump would've
-    // ended the native BB anyway and would've been scheduled last.
-    if(gDvmJit.scheduling)
-        singletonPtr<Scheduler>()->signalEndOfNativeBasicBlock();
-
-    // Calculate the address offset between the destination of jump and the
-    // function we are jumping to.
-    int relOffset = reinterpret_cast<int>(target)
-            - reinterpret_cast<int>(stream);
-
-    // Since instruction pointer will already be updated when executing this,
-    // subtract size of jump instruction
-    relOffset -= getJmpCallInstSize(OpndSize_32, JmpCall_uncond);
-
-    // Generate the unconditional jump now
-    unconditional_jump_int(relOffset, OpndSize_32);
-}
-
-// works whether instructions for target basic block are generated or not
-LowOp* jumpToBasicBlock(char* instAddr, int targetId,
-        bool immediateNeedsAligned) {
-    stream = instAddr;
-    bool unknown;
-    OpndSize size;
-    if(gDvmJit.scheduling) {
-        // If target is chaining cell, we must align the immediate
-        unconditional_jump_block(targetId, immediateNeedsAligned);
-    } else {
-        if (immediateNeedsAligned == true) {
-            alignOffset(1);
-        }
-        int relativeNCG = getRelativeNCG(targetId, JmpCall_uncond, &unknown, &size);
-        unconditional_jump_int(relativeNCG, size);
-    }
-    return NULL;
-}
-
-LowOp* condJumpToBasicBlock(char* instAddr, ConditionCode cc, int targetId,
-        bool immediateNeedsAligned) {
-    stream = instAddr;
-    bool unknown;
-    OpndSize size;
-    if(gDvmJit.scheduling) {
-        // If target is chaining cell, we must align the immediate
-        conditional_jump_block(cc, targetId, immediateNeedsAligned);
-    } else {
-        int relativeNCG = getRelativeNCG(targetId, JmpCall_cond, &unknown, &size);
-        conditional_jump_int(cc, relativeNCG, size);
-    }
-    return NULL;
-}
-
 /*
  * Attempt to enqueue a work order to patch an inline cache for a predicted
  * chaining cell for virtual/interface calls.
@@ -1550,6 +1522,12 @@ bool handleExtendedMIR (CompilationUnit *cUnit, BasicBlock_O1 *bb, MIR *mir)
             result = genPackedSet (cUnit, mir);
             break;
         }
+        case kMirOpCheckStackOverflow:
+        {
+            gDvm.executionMode = origMode;
+            genCheckStackOverflow (cUnit, mir);
+            break;
+        }
         default:
         {
             char * decodedString = dvmCompilerGetDalvikDisassembly(&mir->dalvikInsn, NULL);
@@ -1851,7 +1829,7 @@ static void handleFallThroughBranch (CompilationUnit *cUnit, BasicBlock *bb, Bas
     if (needFallThroughBranch == true)
     {
         //Generate the jump now
-        jumpToBasicBlock (stream, nextFallThrough->id, jumpNeedsAlignment);
+        jumpToBasicBlock (nextFallThrough->id, jumpNeedsAlignment);
     }
 
     //Clear it
@@ -2013,7 +1991,7 @@ static bool generateCode (CompilationUnit_O1 *cUnit, BasicBlock *bb, BasicBlock
     {
         char blockName[BLOCK_NAME_LEN];
         dvmGetBlockName (bb, blockName);
-        ALOGD ("Get ready to handle BB%d type:%s hidden:%s @%p", bb->id, blockName, bb->hidden ? "yes" : "no", stream);
+        ALOGD ("LOWER BB%d type:%s hidden:%s @%p", bb->id, blockName, bb->hidden ? "yes" : "no", stream);
     }
 
     /* We want to update the stream start to remember it for future backward chaining cells */
@@ -2270,9 +2248,21 @@ static void compilerMIR2LIRJit(CompilationUnit_O1 *cUnit, JitTranslationInfo *in
             case kExceptionHandling:
                 //First handle fallthrough branch
                 handleFallThroughBranch (cUnit, bbO1, &nextFallThrough);
+
+                //Update the offset of the block
                 bbO1->label->lop.generic.offset = (stream - streamMethodStart);
+
+                //Now generate any code for this BB
+                if (generateCode (cUnit, bbO1, &nextFallThrough) == false)
+                {
+                    //Generate code set an error for the jit, we can just return
+                    return;
+                }
+
+                //Finally generate a jump to dvmJitToInterpPunt using eax as scratch register
                 scratchRegs[0] = PhysicalReg_EAX;
                 jumpToInterpPunt();
+
                 break;
             case kChainingCellBackwardBranch:
                 /* Handle the codegen later */
diff --git a/vm/compiler/codegen/x86/lightcg/CompilationUnit.cpp b/vm/compiler/codegen/x86/lightcg/CompilationUnit.cpp
index 5faea03..9a687be 100644
--- a/vm/compiler/codegen/x86/lightcg/CompilationUnit.cpp
+++ b/vm/compiler/codegen/x86/lightcg/CompilationUnit.cpp
@@ -43,3 +43,16 @@ bool CompilationUnit_O1::setCanSpillRegister (int reg, bool value)
     //Update succeeded
     return true;
 }
+
+int CompilationUnit_O1::getFPAdjustment (void)
+{
+    //In order to get adjustment we multiply the window shift by size of VR
+    int adjustment = registerWindowShift * sizeof (u4);
+
+    //Stack grows in a negative direction and when we have a register window shift we push the
+    //stack up. Thus taking that into account, the shift is negative.
+    //Namely desiredFP = actualFP - adjustment
+    adjustment = adjustment * (-1);
+
+    return adjustment;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/CompilationUnit.h b/vm/compiler/codegen/x86/lightcg/CompilationUnit.h
index 9915284..d9c4537 100644
--- a/vm/compiler/codegen/x86/lightcg/CompilationUnit.h
+++ b/vm/compiler/codegen/x86/lightcg/CompilationUnit.h
@@ -75,6 +75,12 @@ class CompilationUnit_O1: public CompilationUnit
                 canSpillRegister[k] = true;
             }
         }
+
+        /**
+         * @brief If the compilation unit has a register window shift, it returns the relative change of FP
+         * @return The frame pointer adjustment
+         */
+        int getFPAdjustment (void);
 };
 
 #endif
diff --git a/vm/compiler/codegen/x86/lightcg/CompileTable.cpp b/vm/compiler/codegen/x86/lightcg/CompileTable.cpp
index 341f670..23a0e81 100644
--- a/vm/compiler/codegen/x86/lightcg/CompileTable.cpp
+++ b/vm/compiler/codegen/x86/lightcg/CompileTable.cpp
@@ -70,6 +70,22 @@ bool CompileTableEntry::goToState (int stateNum)
     return true;
 }
 
+bool CompileTableEntry::isTemporary (void) const
+{
+    //If we do not have a logical type simply assume we have a temporary
+    if (logicalType == 0)
+    {
+        return true;
+    }
+
+    bool isHardcoded = ((logicalType & LowOpndRegType_hard) != 0);
+    bool isScratch = ((logicalType & LowOpndRegType_scratch) != 0);
+    bool isTemp = ((logicalType & LowOpndRegType_temp) != 0);
+
+    //We have a temporary if hardcoded reg, scratch, or temp
+    return (isHardcoded == true || isScratch == true || isTemp == true);
+}
+
 CompileTable::iterator CompileTable::find (int regNum, int physicalType)
 {
     CompileTableEntry lookupEntry (regNum, physicalType);
@@ -84,6 +100,20 @@ CompileTable::const_iterator CompileTable::find (int regNum, int physicalType) c
     return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
 }
 
+CompileTable::iterator CompileTable::find (int regNum, LowOpndRegType physicalType, LogicalRegType logicalType)
+{
+    CompileTableEntry lookupEntry (regNum, physicalType, logicalType);
+
+    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
+}
+
+CompileTable::const_iterator CompileTable::find (int regNum, LowOpndRegType physicalType, LogicalRegType logicalType) const
+{
+    CompileTableEntry lookupEntry (regNum, physicalType, logicalType);
+
+    return std::find (compileTable.begin (), compileTable.end (), lookupEntry);
+}
+
 CompileTable::iterator CompileTable::findVirtualRegister (int regNum, LowOpndRegType physicalType)
 {
     CompileTableEntry lookupEntry (regNum, LowOpndRegType_virtual | physicalType);
diff --git a/vm/compiler/codegen/x86/lightcg/CompileTable.h b/vm/compiler/codegen/x86/lightcg/CompileTable.h
index 2465b8e..db164c1 100644
--- a/vm/compiler/codegen/x86/lightcg/CompileTable.h
+++ b/vm/compiler/codegen/x86/lightcg/CompileTable.h
@@ -234,10 +234,7 @@ public:
      * @brief Checks if this is a backend temporary used during bytecode generation.
      * @return Returns whether this entry represent a backend temporary.
      */
-    bool isTemporary (void) const
-    {
-        return (isVirtualReg () == false);
-    }
+    bool isTemporary (void) const;
 
     /**
      * @brief Links a temporary to a corresponding virtual register.
@@ -334,12 +331,12 @@ public:
 
 private:
     /**
-     * @brief Used to save the sate of register allocator.
+     * @brief Used to save the state of register allocator.
      */
     struct RegisterState
     {
-        int spill_loc_index;
-        int physicalReg;
+        int spill_loc_index; //!< @brief Keeps track of CompileTableEntry::spill_loc_index
+        int physicalReg;     //!< @brief Keeps track of CompileTableEntry::physicalReg
     };
 
     /**
@@ -446,6 +443,26 @@ public:
     const_iterator find (int regNum, int physicalType) const;
 
     /**
+     * @brief Used to get an iterator pointing to the entry matching number and type.
+     * @param regNum The register number (can be temp, virtual, or hardcoded)
+     * @param physicalType The physical type of entry
+     * @param logicalType The logical type (virtual, temp, etc)
+     * @return Returns iterator pointing to the desired entry. If one is not found, it
+     * returns the past-the-end iterator.
+     */
+    iterator find (int regNum, LowOpndRegType physicalType, LogicalRegType logicalType);
+
+    /**
+     * @brief Used to get a const iterator pointing to the entry matching number and type.
+     * @param regNum The register number (can be temp, virtual, or hardcoded)
+     * @param physicalType The physical type of entry
+     * @param logicalType The logical type (virtual, temp, etc)
+     * @return Returns const iterator pointing to the desired entry. If one is not found, it
+     * returns the past-the-end const iterator.
+     */
+    const_iterator find (int regNum, LowOpndRegType physicalType, LogicalRegType logicalType) const;
+
+    /**
      * @brief Used to get an iterator pointing to the virtual register whose physical type matches.
      * @param regNum The virtual register number.
      * @param physicalType The physical type of the virtual register.
diff --git a/vm/compiler/codegen/x86/lightcg/ExceptionHandling.cpp b/vm/compiler/codegen/x86/lightcg/ExceptionHandling.cpp
index 4c7ca3c..049c17f 100644
--- a/vm/compiler/codegen/x86/lightcg/ExceptionHandling.cpp
+++ b/vm/compiler/codegen/x86/lightcg/ExceptionHandling.cpp
@@ -141,7 +141,7 @@ void ExceptionHandlingRestoreState::dumpAllExceptionHandlingRestoreState(void) {
         stream = stream + sizeOfExceptionRestore;
 
         // Jump to the target error label
-        unconditional_jump_global_API(targetLabel, false);
+        unconditional_jump (targetLabel, false);
     }
 
     // Since we dumped to code stream, we can clear out the data structures
diff --git a/vm/compiler/codegen/x86/lightcg/InstructionGeneration.cpp b/vm/compiler/codegen/x86/lightcg/InstructionGeneration.cpp
index e8b81b3..2dc238f 100644
--- a/vm/compiler/codegen/x86/lightcg/InstructionGeneration.cpp
+++ b/vm/compiler/codegen/x86/lightcg/InstructionGeneration.cpp
@@ -54,10 +54,10 @@ void genHoistedNullCheck (CompilationUnit *cUnit, MIR *mir)
     }
     else
     {
-        get_virtual_reg (mir->dalvikInsn.vA, OpndSize_32, P_GPR_1, true);
-        export_pc ();
-        compare_imm_reg (OpndSize_32, 0, P_GPR_1, true);
-        condJumpToBasicBlock (stream, Condition_E, cUnit->exceptionBlockId);
+        get_virtual_reg(mir->dalvikInsn.vA, OpndSize_32, P_GPR_1, true);
+        export_pc();
+        compare_imm_reg(OpndSize_32, 0, P_GPR_1, true);
+        condJumpToBasicBlock (Condition_E, cUnit->exceptionBlockId);
     }
 }
 
@@ -91,12 +91,12 @@ void genHoistedBoundCheck (CompilationUnit *cUnit, MIR *mir)
     //Compare array length with index value
     compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), P_GPR_1, true, P_GPR_2, true);
     //Jump to exception block if array.length <= index
-    condJumpToBasicBlock(stream, Condition_LE, cUnit->exceptionBlockId);
+    condJumpToBasicBlock (Condition_LE, cUnit->exceptionBlockId);
 
     //Now, compare to 0
     compare_imm_reg(OpndSize_32, 0, P_GPR_2, true);
     //Jump to exception if index < 0
-    condJumpToBasicBlock(stream, Condition_L, cUnit->exceptionBlockId);
+    condJumpToBasicBlock (Condition_L, cUnit->exceptionBlockId);
 }
 
 //use O0 code generator for hoisted checks outside of the loop
@@ -141,7 +141,7 @@ void genHoistedChecksForCountUpLoop(CompilationUnit *cUnit, MIR *mir)
         alu_binary_imm_reg(OpndSize_32, add_opc, delta, P_GPR_2, true);
     }
     compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), P_GPR_1, true, P_GPR_2, true);
-    condJumpToBasicBlock(stream, Condition_NC, cUnit->exceptionBlockId);
+    condJumpToBasicBlock (Condition_NC, cUnit->exceptionBlockId);
 }
 
 /*
@@ -170,7 +170,7 @@ void genHoistedChecksForCountDownLoop(CompilationUnit *cUnit, MIR *mir)
         alu_binary_imm_reg(OpndSize_32, add_opc, maxC, P_GPR_2, true);
     }
     compare_mem_reg(OpndSize_32, OFFSETOF_MEMBER(ArrayObject, length), P_GPR_1, true, P_GPR_2, true);
-    condJumpToBasicBlock(stream, Condition_NC, cUnit->exceptionBlockId);
+    condJumpToBasicBlock (Condition_NC, cUnit->exceptionBlockId);
 
 }
 
@@ -189,7 +189,7 @@ void genHoistedLowerBoundCheck(CompilationUnit *cUnit, MIR *mir)
     get_virtual_reg(mir->dalvikInsn.vA, OpndSize_32, P_GPR_1, true); //array
     export_pc();
     compare_imm_reg(OpndSize_32, -minC, P_GPR_1, true);
-    condJumpToBasicBlock(stream, Condition_L, cUnit->exceptionBlockId);
+    condJumpToBasicBlock (Condition_L, cUnit->exceptionBlockId);
 }
 #undef P_GPR_1
 
@@ -535,3 +535,35 @@ bool genPackedReduce (CompilationUnit *cUnit, MIR *mir)
     set_virtual_reg (dstVr, OpndSize_32, temp1, false);
     return true;
 }
+
+bool genCheckStackOverflow (CompilationUnit *cUnit, MIR *mir)
+{
+    assert (static_cast<ExtendedMIROpcode> (mir->dalvikInsn.opcode) == kMirOpCheckStackOverflow);
+
+    //Set up some variables to improve readability
+    const int temp1 = 1;
+    const int temp2 = 2;
+    const int exceptionState = 1;
+
+    //Get self pointer and put it in temp1
+    get_self_pointer (temp1, false);
+
+    //Move the frame pointer into temp2
+    move_reg_to_reg (OpndSize_32, PhysicalReg_FP, true, temp2, false);
+
+    //vB holds the size of space of frame needed relative to frame pointer
+    int spaceNeeded = mir->dalvikInsn.vB;
+
+    //Stack grows in negative direction so subtract the size from the frame pointer
+    alu_binary_imm_reg (OpndSize_32, sub_opc, spaceNeeded, temp2, false);
+
+    //Now compare the stack bottom with our expected stack bottom
+    compare_mem_reg (OpndSize_32, OFFSETOF_MEMBER (Thread, interpStackEnd), temp1, false, temp2, false);
+
+    //We want to throw a StackOverflow exception but we don't have the right logic here to do that.
+    //Therefore we simply jump to "common_exception" which in turn generates a jump to exception block.
+    handlePotentialException (Condition_BE, Condition_NBE, exceptionState, "common_exception");
+
+    //If we get here everything went well
+    return true;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/InstructionGeneration.h b/vm/compiler/codegen/x86/lightcg/InstructionGeneration.h
index 99598a6..850282f 100644
--- a/vm/compiler/codegen/x86/lightcg/InstructionGeneration.h
+++ b/vm/compiler/codegen/x86/lightcg/InstructionGeneration.h
@@ -147,4 +147,13 @@ bool genPackedMultiply (CompilationUnit *cUnit, MIR *mir);
  * @return whether the operation was successful
  */
 bool genPackedReduce (CompilationUnit *cUnit, MIR *mir);
+
+/**
+ * @brief Used to generate stack overflow check
+ * @param cUnit The compilation unit
+ * @param mir The MIR with extended opcode kMirOpCheckStackOverflow
+ * @return Returns whether code generation was successful
+ */
+bool genCheckStackOverflow (CompilationUnit *cUnit, MIR *mir);
+
 #endif
diff --git a/vm/compiler/codegen/x86/lightcg/Lower.cpp b/vm/compiler/codegen/x86/lightcg/Lower.cpp
index f99ea0f..2302769 100644
--- a/vm/compiler/codegen/x86/lightcg/Lower.cpp
+++ b/vm/compiler/codegen/x86/lightcg/Lower.cpp
@@ -720,11 +720,9 @@ int lowerByteCode(const Method* method, const MIR * mir, const u2 * dalvikPC, Co
     case OP_THROW_VERIFICATION_ERROR:
         return op_throw_verification_error(mir);
     case OP_GOTO:
-        return op_goto(mir);
     case OP_GOTO_16:
-        return op_goto_16(mir);
     case OP_GOTO_32:
-        return op_goto_32(mir);
+        return op_goto (mir, traceCurrentBB);
     case OP_PACKED_SWITCH:
         return op_packed_switch(mir, dalvikPC, cUnit);
     case OP_SPARSE_SWITCH:
diff --git a/vm/compiler/codegen/x86/lightcg/Lower.h b/vm/compiler/codegen/x86/lightcg/Lower.h
index e406c3a..cf46940 100644
--- a/vm/compiler/codegen/x86/lightcg/Lower.h
+++ b/vm/compiler/codegen/x86/lightcg/Lower.h
@@ -768,10 +768,6 @@ void move_sd_reg_to_mem(LowOp* op, int reg, bool isPhysical,
 
 void conditional_jump(ConditionCode cc, const char* target, bool isShortTerm);
 void unconditional_jump(const char* target, bool isShortTerm);
-void conditional_jump_int(ConditionCode cc, int target, OpndSize size);
-void unconditional_jump_int(int target, OpndSize size);
-void conditional_jump_block(ConditionCode cc, int targetBlockId, bool immediateNeedsAligned = false);
-void unconditional_jump_block(int targetBlockId, bool immediateNeedsAligned = false);
 void unconditional_jump_reg(int reg, bool isPhysical);
 void unconditional_jump_rel32(void * target);
 void call(const char* target);
@@ -921,7 +917,8 @@ LowOpMemReg* move_mem_to_reg_noalloc(OpndSize size,
 
 //////////////////////////////////////////////////////////////
 int insertLabel(const char* label, bool checkDup);
-int export_pc();
+void export_pc (void);
+
 int simpleNullCheck(int reg, bool isPhysical, int vr);
 int nullCheck(int reg, bool isPhysical, int exceptionNum, int vr);
 int handlePotentialException(
@@ -943,7 +940,6 @@ void set_return_value(OpndSize size, int sourceReg, bool isSourcePhysical,
 int clear_exception();
 int get_exception(int reg, bool isPhysical);
 int set_exception(int reg, bool isPhysical);
-int savearea_from_fp(int reg, bool isPhysical);
 
 int call_moddi3();
 int call_divdi3();
@@ -1091,9 +1087,7 @@ int op_filled_new_array_range(const MIR * mir);
 int op_fill_array_data(const MIR * mir, const u2 * dalvikPC);
 int op_throw(const MIR * mir);
 int op_throw_verification_error(const MIR * mir);
-int op_goto(const MIR * mir);
-int op_goto_16(const MIR * mir);
-int op_goto_32(const MIR * mir);
+int op_goto (const MIR * mir, BasicBlock *currentBB);
 int op_packed_switch(const MIR * mir, const u2 * dalvikPC, CompilationUnit_O1* cUnit);
 int op_sparse_switch(const MIR * mir, const u2 * dalvikPC, CompilationUnit_O1* cUnit);
 int op_if_ge(const MIR * mir);
@@ -1409,8 +1403,43 @@ void performWorklistWork (void);
  */
 int generateConditionalJumpToTakenBlock (ConditionCode takenCondition);
 
-LowOp* jumpToBasicBlock(char* instAddr, int targetId, bool targetIsChainingCell = false);
-LowOp* condJumpToBasicBlock(char* instAddr, ConditionCode cc, int targetId, bool immediateNeedsAligned = false);
+/**
+ * @brief Align a pointer to n-bytes aligned
+ * @param addr The address to align
+ * @param n The bytes to align it to (should be power of two)
+ * @return Returns the address n-bytes aligned
+ */
+char* align (char* addr, int n);
+
+/**
+ * @brief Aligns the immediate operand of jmp/jcc and movl within 16B
+ * @details Updates the global stream pointer to ensure that immediate falls in 16B
+ * @param offset The size of mnemonic and arguments excluding the immediate
+ */
+void alignOffset (int offset);
+
+/**
+ * @brief Determines if operand of jump needs alignment
+ * @param bb The basic block being jumped to
+ * @return Whether operand alignment is needed
+ */
+bool doesJumpToBBNeedAlignment (BasicBlock *bb);
+
+/**
+ * @brief Generates jump to basic block. Operand of jump will be initialized when BB is lowered.
+ * @param targetBlockId The id of the basic block we must jump to
+ * @param immediateNeedsAligned Whether the operand of the jump must be aligned within 16 bytes
+ */
+void jumpToBasicBlock (int targetBlockId, bool immediateNeedsAligned = false);
+
+/**
+ * @brief Generates conditional jump to basic block. Operand of jump will be initialized when BB is lowered.
+ * @param cc The condition code for the jump
+ * @param targetBlockId The id of the basic block we must jump to
+ * @param immediateNeedsAligned Whether the operand of the jump must be aligned within 16 bytes
+ */
+void condJumpToBasicBlock (ConditionCode cc, int targetBlockId, bool immediateNeedsAligned = false);
+
 bool jumpToException(const char* target);
 int codeGenBasicBlockJit(const Method* method, BasicBlock* bb, CompilationUnit_O1* cUnit);
 void endOfBasicBlock(struct BasicBlock* bb);
@@ -1420,6 +1449,14 @@ bool handleExtendedMIR (CompilationUnit *cUnit, BasicBlock_O1 *bb, MIR *mir);
 
 int insertChainingWorklist(int bbId, char * codeStart);
 void startOfTraceO1(const Method* method, int exceptionBlockId, CompilationUnit *cUnit);
+
+/**
+ * @brief Used to obtain the offset relative to frame pointer for a given VR
+ * @param vR The virtual register number for which to calculate offset
+ * @return Returns the offset relative to FP.
+ */
+int getVirtualRegOffsetRelativeToFP (int vR);
+
 /** @brief search globalMap to find the entry for the given label */
 char* findCodeForLabel(const char* label);
 /* Find a label offset given a BasicBlock index */
@@ -1458,12 +1495,6 @@ OpndSize estOpndSizeFromImm(int target);
 
 //Preprocess a BasicBlock before being lowered
 int preprocessingBB (CompilationUnit *cUnit, BasicBlock *bb);
-/** @brief align the relative offset of jmp/jcc and movl within 16B */
-void alignOffset(int cond);
-
-/** @brief align a pointer to n-bytes aligned */
-char* align(char* addr, int n);
-bool doesJumpToBBNeedAlignment(BasicBlock * bb);
 
 /**
  * @brief Architecture specific BasicBlock creator
diff --git a/vm/compiler/codegen/x86/lightcg/LowerAlu.cpp b/vm/compiler/codegen/x86/lightcg/LowerAlu.cpp
index fca5504..03af0ec 100644
--- a/vm/compiler/codegen/x86/lightcg/LowerAlu.cpp
+++ b/vm/compiler/codegen/x86/lightcg/LowerAlu.cpp
@@ -1363,8 +1363,7 @@ int common_div_rem_int_lit(bool isRem, int vA, int vB, s2 imm) {
         ALOGI("EXTRA code to handle exception");
 #endif
         beforeCall("exception"); //dump GG, GL VRs
-        unconditional_jump_global_API(
-                          "common_errDivideByZero", false);
+        unconditional_jump ("common_errDivideByZero", false);
 
         return 0;
     }
diff --git a/vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp b/vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp
index e5b8f1d..c8c8145 100644
--- a/vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp
+++ b/vm/compiler/codegen/x86/lightcg/LowerGetPut.cpp
@@ -691,7 +691,7 @@ int op_aput_object(const MIR * mir) { //type checking
     call_dvmCanPutArrayElement(); //scratch??
     load_effective_addr(12, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
     compare_imm_reg(OpndSize_32, 0, PhysicalReg_EAX, true);
-    conditional_jump_global_API(Condition_E, "common_errArrayStore", false);
+    conditional_jump (Condition_E, "common_errArrayStore", false);
 
 #ifndef WITH_SELF_VERIFICATION
     //NOTE: "2, false" is live through function call
diff --git a/vm/compiler/codegen/x86/lightcg/LowerHelper.cpp b/vm/compiler/codegen/x86/lightcg/LowerHelper.cpp
index aada534..25df8fc 100644
--- a/vm/compiler/codegen/x86/lightcg/LowerHelper.cpp
+++ b/vm/compiler/codegen/x86/lightcg/LowerHelper.cpp
@@ -1551,7 +1551,8 @@ void compare_VR_reg_all(OpndSize size,
                                      &(gCompilationUnit->constListHead));
                 } else {
                     writeBackConstVR(vA, tmpValue[0]);
-                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
+                    const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+                    dump_mem_reg(m, ATOM_NORMAL, size, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
                 }
                 return;
             }
@@ -1598,7 +1599,8 @@ void compare_VR_reg_all(OpndSize size,
                 } else {
                     writeBackConstVR(vA, tmpValue[0]);
                     writeBackConstVR(vA+1, tmpValue[1]);
-                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
+                    const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+                    dump_mem_reg(m, ATOM_NORMAL, size, vrOffset, PhysicalReg_FP, true,
                                     MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
                 }
                 return;
@@ -1619,13 +1621,15 @@ void compare_VR_reg_all(OpndSize size,
         }
         else {
             //virtual register is not allocated to a physical register
-            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL, size, vrOffset, PhysicalReg_FP, true,
                 MemoryAccess_VR, vA, reg, isPhysical, pType);
         }
         updateRefCount(vA, type);
         return;
     } else {
-        dump_mem_reg(m, ATOM_NORMAL, size, 4*vA, PhysicalReg_FP, true,
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+        dump_mem_reg(m, ATOM_NORMAL, size, vrOffset, PhysicalReg_FP, true,
             MemoryAccess_VR, vA, reg, isPhysical, pType, NULL);
         return;
     }
@@ -1676,9 +1680,11 @@ void load_fp_stack_VR_all(OpndSize size, int vB, Mnemonic m) {
                     MemoryAccess_VR, vB, getTypeFromIntSize(size));
 #endif
         }
-        dump_mem_fp(m, ATOM_NORMAL, size, 4*vB, PhysicalReg_FP, true, MemoryAccess_VR, vB, 0);
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vB);
+        dump_mem_fp(m, ATOM_NORMAL, size, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vB, 0);
     } else {
-        dump_mem_fp(m, ATOM_NORMAL, size, 4*vB, PhysicalReg_FP, true, MemoryAccess_VR, vB, 0);
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vB);
+        dump_mem_fp(m, ATOM_NORMAL, size, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vB, 0);
     }
 }
 //!load VR(float or double) to stack
@@ -1700,7 +1706,8 @@ void load_int_fp_stack_VR(OpndSize size, int vA) {//fild(ll|l)
 //!
 void store_fp_stack_VR(bool pop, OpndSize size, int vA) {//fst(p)(s|l)
     Mnemonic m = pop ? Mnemonic_FSTP : Mnemonic_FST;
-    dump_fp_mem(m, ATOM_NORMAL, size, 0, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
+    const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+    dump_fp_mem(m, ATOM_NORMAL, size, 0, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA);
     if(gDvm.executionMode == kExecutionModeNcgO1) {
         if(size == OpndSize_32)
             updateVirtualReg(vA, LowOpndRegType_fs_s);
@@ -1713,7 +1720,8 @@ void store_fp_stack_VR(bool pop, OpndSize size, int vA) {//fst(p)(s|l)
 //!
 void store_int_fp_stack_VR(bool pop, OpndSize size, int vA) {//fist(p)(l)
     Mnemonic m = pop ? Mnemonic_FISTP : Mnemonic_FIST;
-    dump_fp_mem(m, ATOM_NORMAL, size, 0, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA);
+    const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+    dump_fp_mem(m, ATOM_NORMAL, size, 0, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA);
     if(gDvm.executionMode == kExecutionModeNcgO1) {
         if(size == OpndSize_32)
             updateVirtualReg(vA, LowOpndRegType_fs_s);
@@ -1748,9 +1756,11 @@ void fpu_VR(ALU_Opcode opc, OpndSize size, int vA) {
             SET_JIT_ERROR(kJitErrorRegAllocFailed);
             return;
         }
-        dump_mem_fp(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, 0);
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+        dump_mem_fp(m, ATOM_NORMAL_ALU, size, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA, 0);
     } else {
-        dump_mem_fp(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, 0);
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+        dump_mem_fp(m, ATOM_NORMAL_ALU, size, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA, 0);
     }
 }
 //! cmp imm reg
@@ -1801,13 +1811,19 @@ void compare_imm_VR(OpndSize size, int imm,
         }
         int regAll = checkVirtualReg(vA, getTypeFromIntSize(size), 0);
         if(regAll != PhysicalReg_Null)
+        {
             dump_imm_reg_noalloc(m, size, imm, regAll, true, LowOpndRegType_gp);
+        }
         else
-            dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true,
+        {
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+            dump_imm_mem_noalloc(m, size, imm, vrOffset, PhysicalReg_FP, true,
                 MemoryAccess_VR, vA, NULL);
+        }
         updateRefCount(vA, getTypeFromIntSize(size));
     } else {
-        dump_imm_mem(m, ATOM_NORMAL, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, false);
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+        dump_imm_mem(m, ATOM_NORMAL, size, imm, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA, false);
     }
 }
 //! cmp reg reg
@@ -2060,7 +2076,8 @@ bool alu_imm_to_VR(OpndSize size, ALU_Opcode opc, int srcVR, int destVR, int imm
                     return setVRToConst(destVR, size, constValue);
                 }
                 else if (caseDest == DEST_IN_MEMORY) {
-                    dump_imm_mem_noalloc(Mnemonic_MOV, size, finalSum, 4*destVR, PhysicalReg_FP, true, MemoryAccess_VR, destVR, NULL);
+                    const int vrOffset = getVirtualRegOffsetRelativeToFP (destVR);
+                    dump_imm_mem_noalloc(Mnemonic_MOV, size, finalSum, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, destVR, NULL);
                     return true; //Successfully updated
                 }
                 else if (caseDest == DEST_IS_ALLOCATED) {
@@ -2115,7 +2132,8 @@ bool alu_imm_to_VR(OpndSize size, ALU_Opcode opc, int srcVR, int destVR, int imm
 
                         // when we reach here, we can use add/sub on memory directly based
                         // on the fact that no uses of the mir's def in adjacent mirs window
-                        dump_imm_mem_noalloc_alu(alu_mn, size, imm, 4*destVR, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
+                        const int vrOffset = getVirtualRegOffsetRelativeToFP (destVR);
+                        dump_imm_mem_noalloc_alu(alu_mn, size, imm, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
 
                         // Successfully updated
                         return true;
@@ -2123,7 +2141,8 @@ bool alu_imm_to_VR(OpndSize size, ALU_Opcode opc, int srcVR, int destVR, int imm
 
                     // for other platforms
                     else {
-                        dump_imm_mem_noalloc_alu(alu_mn, size, imm, 4*destVR, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
+                        const int vrOffset = getVirtualRegOffsetRelativeToFP (destVR);
+                        dump_imm_mem_noalloc_alu(alu_mn, size, imm, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, destVR);
 
                         // Successfully updated
                         return true;
@@ -2136,7 +2155,8 @@ bool alu_imm_to_VR(OpndSize size, ALU_Opcode opc, int srcVR, int destVR, int imm
                 else if (caseDest == DEST_IS_ALLOCATED) {
                     //Load srcVR to regDest, and then add the constant
                     //Note that with MOVE_OPT on, this is as good as get_VR, add / sub , set_VR
-                    dump_mem_reg_noalloc(Mnemonic_MOV, size, 4*srcVR, PhysicalReg_FP, true, MemoryAccess_VR, srcVR, regDest, true, pType);
+                    const int vrOffset = getVirtualRegOffsetRelativeToFP (srcVR);
+                    dump_mem_reg_noalloc(Mnemonic_MOV, size, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, srcVR, regDest, true, pType);
                     dump_imm_reg_noalloc_alu(alu_mn, size, imm, regDest, true, pType);
                     updateRefCount(destVR, pType);
                     updateVirtualReg(destVR, pType);
@@ -2238,7 +2258,8 @@ void alu_sd_binary_VR_reg(ALU_Opcode opc, int vA, int reg, bool isPhysical, bool
                                    &(gCompilationUnit->constListHead));
             } else {
                  writeBackConstVR(vA, tmpValue[0]);
-                 dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_32, 4*vA, PhysicalReg_FP, true,
+                 const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+                 dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_32, vrOffset, PhysicalReg_FP, true,
                                 MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
             }
             return;
@@ -2273,7 +2294,8 @@ void alu_sd_binary_VR_reg(ALU_Opcode opc, int vA, int reg, bool isPhysical, bool
             } else {
                 writeBackConstVR(vA, tmpValue[0]);
                 writeBackConstVR(vA+1, tmpValue[1]);
-                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, 4*vA, PhysicalReg_FP, true,
+                const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, vrOffset, PhysicalReg_FP, true,
                        MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
             }
             return;
@@ -2295,13 +2317,15 @@ void alu_sd_binary_VR_reg(ALU_Opcode opc, int vA, int reg, bool isPhysical, bool
             endNativeCode();
         }
         else {
-            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL_ALU, size, vrOffset, PhysicalReg_FP, true,
                          MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm);
         }
         updateRefCount(vA, type);
     }
     else {
-        dump_mem_reg(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+        dump_mem_reg(m, ATOM_NORMAL_ALU, size, vrOffset, PhysicalReg_FP, true,
                     MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
     }
 }
@@ -2358,7 +2382,9 @@ void alu_binary_VR_reg(OpndSize size, ALU_Opcode opc, int vA, int reg, bool isPh
             } else {
                 writeBackConstVR(vA, tmpValue[0]);
                 writeBackConstVR(vA+1, tmpValue[1]);
-                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, 4*vA, PhysicalReg_FP, true,
+
+                const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+                dump_mem_reg(m, ATOM_NORMAL_ALU, OpndSize_64, vrOffset, PhysicalReg_FP, true,
                        MemoryAccess_VR, vA, reg, isPhysical, LowOpndRegType_xmm, NULL);
             }
             return;
@@ -2379,13 +2405,15 @@ void alu_binary_VR_reg(OpndSize size, ALU_Opcode opc, int vA, int reg, bool isPh
             endNativeCode();
         }
         else {
-            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+            dump_mem_reg_noalloc_mem(m, ATOM_NORMAL_ALU, size, vrOffset, PhysicalReg_FP, true,
                 MemoryAccess_VR, vA, reg, isPhysical, getTypeFromIntSize(size));
         }
         updateRefCount(vA, getTypeFromIntSize(size));
     }
     else {
-        dump_mem_reg(m, ATOM_NORMAL_ALU, size, 4*vA, PhysicalReg_FP, true,
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+        dump_mem_reg(m, ATOM_NORMAL_ALU, size, vrOffset, PhysicalReg_FP, true,
             MemoryAccess_VR, vA, reg, isPhysical, getTypeFromIntSize(size), NULL);
     }
 }
@@ -2691,7 +2719,8 @@ void set_VR_to_imm(int vA, OpndSize size, int imm) {
         freeReg(false);
         regAll = registerAlloc(LowOpndRegType_virtual | getTypeFromIntSize(size), vA, false/*dummy*/, true, true);
         if(regAll == PhysicalReg_Null) {
-            dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, NULL);
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+            dump_imm_mem_noalloc(m, size, imm, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA, NULL);
             return;
         }
 
@@ -2699,7 +2728,8 @@ void set_VR_to_imm(int vA, OpndSize size, int imm) {
         updateVirtualReg(vA, getTypeFromIntSize(size));
     }
     else {
-        dump_imm_mem(m, ATOM_NORMAL, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, false);
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+        dump_imm_mem(m, ATOM_NORMAL, size, imm, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA, false);
     }
 }
 void set_VR_to_imm_noupdateref(LowOp* op, int vA, OpndSize size, int imm) {
@@ -2716,7 +2746,9 @@ void set_VR_to_imm_noalloc(int vA, OpndSize size, int imm) {
         return;
     }
     Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_imm_mem_noalloc(m, size, imm, 4*vA, PhysicalReg_FP, true, MemoryAccess_VR, vA, NULL);
+
+    const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+    dump_imm_mem_noalloc(m, size, imm, vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, vA, NULL);
 }
 
 void move_chain_to_reg(OpndSize size, int imm, int reg, bool isPhysical) {
@@ -2840,7 +2872,8 @@ void get_virtual_reg_all(int vR, OpndSize size, int reg, bool isPhysical, Mnemon
                     //VR is not mapped to a register but in memory
                     writeBackConstVR(vR, tmpValue[0]);
                     //temporary reg has "pType" (which is xmm)
-                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
+                    const int vrOffset = getVirtualRegOffsetRelativeToFP (vR);
+                    dump_mem_reg(m, ATOM_NORMAL, size, vrOffset, PhysicalReg_FP, true,
                                        MemoryAccess_VR, vR, reg, isPhysical, pType, NULL);
                 }
                 return;
@@ -2877,7 +2910,8 @@ void get_virtual_reg_all(int vR, OpndSize size, int reg, bool isPhysical, Mnemon
                     //VR is not mapped to a register but in memory
                     writeBackConstVR(vR, tmpValue[0]);
                     writeBackConstVR(vR+1, tmpValue[1]);
-                    dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
+                    const int vrOffset = getVirtualRegOffsetRelativeToFP (vR);
+                    dump_mem_reg(m, ATOM_NORMAL, size, vrOffset, PhysicalReg_FP, true,
                                        MemoryAccess_VR, vR, reg, isPhysical, pType, NULL);
                 }
                 return;
@@ -2921,7 +2955,8 @@ void get_virtual_reg_all(int vR, OpndSize size, int reg, bool isPhysical, Mnemon
         // If we still have no physical register for the VR, then use it as
         // a memory operand
         if(physRegForVR == PhysicalReg_Null) {
-            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (vR);
+            dump_mem_reg_noalloc(m, size, vrOffset, PhysicalReg_FP, true,
                 MemoryAccess_VR, vR, reg, isPhysical, pType);
             return;
         }
@@ -2930,13 +2965,16 @@ void get_virtual_reg_all(int vR, OpndSize size, int reg, bool isPhysical, Mnemon
         // Check to see if the temp can share same physical register.
         if(checkTempReg2(reg, pType, isPhysical, physRegForVR, vR)) {
             registerAllocMove(reg, pType, isPhysical, physRegForVR);
-            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
+
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (vR);
+            dump_mem_reg_noalloc(m, size, vrOffset, PhysicalReg_FP, true,
                 MemoryAccess_VR, vR, physRegForVR, true, pType);
             updateRefCount(vR, type);
             return;
         }
         else {
-            dump_mem_reg_noalloc(m, size, 4*vR, PhysicalReg_FP, true,
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (vR);
+            dump_mem_reg_noalloc(m, size, vrOffset, PhysicalReg_FP, true,
                 MemoryAccess_VR, vR, physRegForVR, true, pType);
             //xmm with 32 bits
             startNativeCode(vR, type);
@@ -2951,7 +2989,8 @@ void get_virtual_reg_all(int vR, OpndSize size, int reg, bool isPhysical, Mnemon
         }
     }
     else {
-        dump_mem_reg(m, ATOM_NORMAL, size, 4*vR, PhysicalReg_FP, true,
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vR);
+        dump_mem_reg(m, ATOM_NORMAL, size, vrOffset, PhysicalReg_FP, true,
             MemoryAccess_VR, vR, reg, isPhysical, pType, NULL);
     }
 }
@@ -2961,7 +3000,8 @@ void get_virtual_reg(int vB, OpndSize size, int reg, bool isPhysical) {
 }
 void get_virtual_reg_noalloc(int vB, OpndSize size, int reg, bool isPhysical) {
     Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_mem_reg_noalloc(m, size, 4*vB, PhysicalReg_FP, true,
+    const int vrOffset = getVirtualRegOffsetRelativeToFP (vB);
+    dump_mem_reg_noalloc(m, size, vrOffset, PhysicalReg_FP, true,
         MemoryAccess_VR, vB, reg, isPhysical, getTypeFromIntSize(size));
 }
 //3 cases: gp, xmm, ss
@@ -3015,7 +3055,8 @@ void set_virtual_reg_all(int vA, OpndSize size, int reg, bool isPhysical, Mnemon
         //case 3
         regAll = registerAlloc(LowOpndRegType_virtual | type, vA, false/*dummy*/, false, true);
         if(regAll == PhysicalReg_Null) {
-            dump_reg_mem_noalloc(m, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+            dump_reg_mem_noalloc(m, size, reg, isPhysical, vrOffset, PhysicalReg_FP, true,
                 MemoryAccess_VR, vA, pType);
             return;
         }
@@ -3031,7 +3072,8 @@ void set_virtual_reg_all(int vA, OpndSize size, int reg, bool isPhysical, Mnemon
         updateVirtualReg(vA, type);
     }
     else {
-        dump_reg_mem(m, ATOM_NORMAL, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+        dump_reg_mem(m, ATOM_NORMAL, size, reg, isPhysical, vrOffset, PhysicalReg_FP, true,
             MemoryAccess_VR, vA, pType);
     }
 }
@@ -3041,7 +3083,8 @@ void set_virtual_reg(int vA, OpndSize size, int reg, bool isPhysical) {
 }
 void set_virtual_reg_noalloc(int vA, OpndSize size, int reg, bool isPhysical) {
     Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
-    dump_reg_mem_noalloc(m, size, reg, isPhysical, 4*vA, PhysicalReg_FP, true,
+    const int vrOffset = getVirtualRegOffsetRelativeToFP (vA);
+    dump_reg_mem_noalloc(m, size, reg, isPhysical, vrOffset, PhysicalReg_FP, true,
         MemoryAccess_VR, vA, getTypeFromIntSize(size));
 }
 void get_VR_ss(int vB, int reg, bool isPhysical) {
@@ -3087,7 +3130,7 @@ int simpleNullCheck(int reg, bool isPhysical, int vr) {
         return 0;
     }
     compare_imm_reg(OpndSize_32, 0, reg, isPhysical);
-    conditional_jump_global_API(Condition_E, "common_errNullObject", false);
+    conditional_jump (Condition_E, "common_errNullObject", false);
     int retCode = setVRNullCheck(vr, OpndSize_32);
     if (retCode < 0)
         return retCode;
@@ -3360,15 +3403,6 @@ int set_exception(int reg, bool isPhysical) {
     return 0;
 }
 
-//! get SaveArea pointer
-
-//!
-int savearea_from_fp(int reg, bool isPhysical) {
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
-    load_effective_addr(-sizeofStackSaveArea, PhysicalReg_FP, true, reg, isPhysical);
-    return 0;
-}
-
 #ifdef DEBUG_CALL_STACK3
 int call_debug_dumpSwitch() {
     typedef void (*vmHelper)(int);
@@ -4678,3 +4712,13 @@ bool vec_extract_imm_reg_reg (int count, int srcReg, bool isSrcPhysical, int des
     return true;
 }
 
+int getVirtualRegOffsetRelativeToFP (int vR)
+{
+    //Each virtual register is 32-bit and thus we multiply its size with the VR number
+    int offset = vR * sizeof (u4);
+
+    //We may have had a frame pointer change for our compilation unit so we need to take into account
+    offset += gCompilationUnit->getFPAdjustment ();
+
+    return offset;
+}
diff --git a/vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp b/vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp
index bbddf9d..cd3aa68 100644
--- a/vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp
+++ b/vm/compiler/codegen/x86/lightcg/LowerInvoke.cpp
@@ -607,15 +607,23 @@ int common_invokeMethodNoRange_noJmp(const DecodedInstruction &decodedInst) {
     int startStreamPtr = (int)stream;
 #endif
     u2 count = decodedInst.vA;
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
-    int offsetFromSaveArea = -4 * count;
+
+    //The outs of caller (ins of callee) are at a lower address than StackSaveArea of caller
+    //So to calculate the beginning of the ins we multiply size of VR with the number of arguments
+    //and we negate that because stack grows in a negative direction.
+    int offsetFromSaveArea = -sizeof(u4) * count;
+
+    //The stack save area is in negative direction relative to frame pointer so we calculate displacement now
+    //and load up the address into temp21
+    const int saveAreaDisp = -(sizeof(StackSaveArea));
+
     int numQuad = 0; // keeping track of xmm moves
     int numMov = 0; // keeping track of gp moves
     for (int vrNum = 0; vrNum < count; vrNum++) {
         if (vrNum != 0 && (vrNum + 1 < count) && decodedInst.arg[vrNum] + 1 == decodedInst.arg[vrNum + 1]) {
             // move 64 bit values from VR to memory if consecutive VRs are to be copied to memory
             get_virtual_reg(decodedInst.arg[vrNum], OpndSize_64, 22, false);
-            move_reg_to_mem(OpndSize_64, 22, false, offsetFromSaveArea - sizeofStackSaveArea,
+            move_reg_to_mem(OpndSize_64, 22, false, offsetFromSaveArea + saveAreaDisp,
                             PhysicalReg_FP, true);
             vrNum++;
             numQuad++;               // keep track of number of 64 bit moves
@@ -625,35 +633,35 @@ int common_invokeMethodNoRange_noJmp(const DecodedInstruction &decodedInst) {
             // We need to use separate temp reg for each case.
             if (numMov == 4){
                 get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 27, false);
-                move_reg_to_mem(OpndSize_32, 27, false, offsetFromSaveArea - sizeofStackSaveArea,
+                move_reg_to_mem(OpndSize_32, 27, false, offsetFromSaveArea + saveAreaDisp,
                                 PhysicalReg_FP, true);
                 offsetFromSaveArea += 4;
                 numMov++;
             }
             if (numMov == 3){
                 get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 26, false);
-                move_reg_to_mem(OpndSize_32, 26, false, offsetFromSaveArea - sizeofStackSaveArea,
+                move_reg_to_mem(OpndSize_32, 26, false, offsetFromSaveArea + saveAreaDisp,
                                 PhysicalReg_FP, true);
                 offsetFromSaveArea += 4;
                 numMov++;
             }
             if (numMov == 2){
                 get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 25, false);
-                move_reg_to_mem(OpndSize_32, 25, false, offsetFromSaveArea - sizeofStackSaveArea,
+                move_reg_to_mem(OpndSize_32, 25, false, offsetFromSaveArea + saveAreaDisp,
                                 PhysicalReg_FP, true);
                 offsetFromSaveArea += 4;
                 numMov++;
             }
             if (numMov == 1){
                 get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 24, false);
-                move_reg_to_mem(OpndSize_32, 24, false, offsetFromSaveArea - sizeofStackSaveArea,
+                move_reg_to_mem(OpndSize_32, 24, false, offsetFromSaveArea + saveAreaDisp,
                                 PhysicalReg_FP, true);
                 offsetFromSaveArea += 4;
                 numMov++;
             }
             if (numMov == 0){
                 get_virtual_reg(decodedInst.arg[vrNum], OpndSize_32, 23, false);
-                move_reg_to_mem(OpndSize_32, 23, false, offsetFromSaveArea - sizeofStackSaveArea,
+                move_reg_to_mem(OpndSize_32, 23, false, offsetFromSaveArea + saveAreaDisp,
                                 PhysicalReg_FP, true);
                 offsetFromSaveArea += 4;
                 numMov++;
@@ -702,12 +710,30 @@ int common_invokeMethod_Jmp(ArgsDoneType form) {
     }
     int takenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
     move_chain_to_mem(OpndSize_32, takenId, 0, PhysicalReg_ESP, true);
+
+    //keep ecx live, if ecx was spilled, it is loaded here
+    touchEcx();
+
+    //Determine the target of this invoke
+    const char* target;
     if(form == ArgsDone_Full)
-        unconditional_jump_global_API(".invokeArgsDone_jit", false);
+    {
+        target = ".invokeArgsDone_jit";
+    }
     else if(form == ArgsDone_Native)
-        unconditional_jump_global_API(".invokeArgsDone_native", false);
+    {
+        target = ".invokeArgsDone_native";
+    }
     else
-        unconditional_jump_global_API(".invokeArgsDone_normal", false);
+    {
+        target = ".invokeArgsDone_normal";
+    }
+
+    //Do work needed before calling specific target like writing back VRs
+    beforeCall (target);
+
+    //Unconditionally jump to the common invokeArgsDone
+    unconditional_jump (target, false);
 
 #if defined VTUNE_DALVIK
     if(gDvmJit.vtuneInfo != kVTuneInfoDisabled) {
@@ -756,7 +782,15 @@ int common_invokeMethodRange_noJmp(const DecodedInstruction &decodedInst) {
     u2 count = decodedInst.vA;
     int vD = decodedInst.vC; //the first argument
 
-    savearea_from_fp(21, false);
+    //Use temp21 to keep track of save area pointer
+    const int temp21 = 21;
+    const bool isTemp21Physical = false;
+
+    //The stack save area is in negative direction relative to frame pointer so we calculate displacement now
+    //and load up the address into temp21
+    const int saveAreaDisp = -(sizeof(StackSaveArea));
+    load_effective_addr (saveAreaDisp, PhysicalReg_FP, true, temp21, isTemp21Physical);
+
     //vD to rFP-4*count-20
     //vD+1 to rFP-4*count-20+4 = rFP-20-4*(count-1)
     if(count >= 1 && count <= 10) {
@@ -807,7 +841,8 @@ int common_invokeMethodRange_noJmp(const DecodedInstruction &decodedInst) {
         for(k = 0; k < count; k++) {
             spillVirtualReg(vD+k, LowOpndRegType_gp, true); //will update refCount
         }
-        load_effective_addr(4*vD, PhysicalReg_FP, true, 12, false);
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vD);
+        load_effective_addr(vrOffset, PhysicalReg_FP, true, 12, false);
         alu_binary_imm_reg(OpndSize_32, sub_opc, 4*count, 21, false);
         move_imm_to_reg(OpndSize_32, count, 13, false);
         if (insertLabel(".invokeMethod_1", true) == -1) //if checkDup: will perform work from ShortWorklist
@@ -957,8 +992,8 @@ int common_invokeArgsDone(ArgsDoneType form) {
     // Determine the offset by multiplying size of 4 with how many ins+locals we have
     alu_binary_imm_reg(OpndSize_32, shl_opc, 2, PhysicalReg_EAX, true);
 
-    // Load save area
-    savearea_from_fp(PhysicalReg_ESI, true);
+    // Load save area into %esi
+    load_effective_addr(-sizeof(StackSaveArea), PhysicalReg_FP, true, PhysicalReg_ESI, true);
 
     // Computer the new FP (old save area - regsSize)
     alu_binary_reg_reg(OpndSize_32, sub_opc, PhysicalReg_EAX, true,
@@ -1611,7 +1646,7 @@ int common_invoke_super_quick(bool hasRange, int vD, u2 IMMC,
     beforeCall("exception"); //dump GG, GL VRs
     compare_imm_VR(OpndSize_32, 0, vD);
 
-    conditional_jump_global_API(Condition_E, "common_errNullObject", false);
+    conditional_jump (Condition_E, "common_errNullObject", false);
     /* for trace-based JIT, callee is already resolved */
     int mIndex = IMMC/4;
     const Method *calleeMethod = currentMethod->clazz->super->vtable[mIndex];
diff --git a/vm/compiler/codegen/x86/lightcg/LowerJump.cpp b/vm/compiler/codegen/x86/lightcg/LowerJump.cpp
index 325cdba..73a9a37 100644
--- a/vm/compiler/codegen/x86/lightcg/LowerJump.cpp
+++ b/vm/compiler/codegen/x86/lightcg/LowerJump.cpp
@@ -668,7 +668,7 @@ int getRelativeOffset(const char* target, bool isShortTerm, JmpCall_type type, b
 */
 void conditional_jump(ConditionCode cc, const char* target, bool isShortTerm) {
     if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
-        condJumpToBasicBlock(stream, cc, currentExceptionBlockIdx);
+        condJumpToBasicBlock (cc, currentExceptionBlockIdx);
         return;
     }
     Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
@@ -687,46 +687,35 @@ If the target is ".invokeArgsDone" and mode is NCG O1, extra work is performed t
 */
 void unconditional_jump(const char* target, bool isShortTerm) {
     if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
-        jumpToBasicBlock(stream, currentExceptionBlockIdx);
+        jumpToBasicBlock (currentExceptionBlockIdx);
         return;
     }
     Mnemonic m = Mnemonic_JMP;
     bool unknown;
     OpndSize size = OpndSize_Null;
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        //for other three labels used by JIT: invokeArgsDone_formal, _native, _jit
-        if(!strncmp(target, ".invokeArgsDone", 15)) {
-            touchEcx(); //keep ecx live, if ecx was spilled, it is loaded here
-            beforeCall(target); //
-        }
-        if(!strcmp(target, ".invokeArgsDone")) {
-            nextVersionOfHardReg(PhysicalReg_EDX, 1); //edx will be used in a function
-            call("ncgGetEIP"); //must be immediately before JMP
-        }
-    }
     int imm = 0;
     if(!gDvmJit.scheduling)
         imm = getRelativeOffset(target, isShortTerm, JmpCall_uncond, &unknown, &size);
     dump_label(m, size, imm, target, isShortTerm);
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
-        if(!strncmp(target, ".invokeArgsDone", 15)) {
-            afterCall(target); //un-spill before executing the next bytecode
-        }
-    }
 }
-/*!
-\brief generate a single native instruction "jcc imm"
 
-*/
-void conditional_jump_int(ConditionCode cc, int target, OpndSize size) {
+/**
+ * @brief Generates a single native instruction "jcc imm"
+ * @param cc The condition to take the jump
+ * @param target The immediate representing the relative offset from instruction pointer to jump to.
+ * @param size The size of immediate
+ */
+static void conditional_jump_int(ConditionCode cc, int target, OpndSize size) {
     Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
     dump_imm(m, size, target);
 }
-/*!
-\brief generate a single native instruction "jmp imm"
 
-*/
-void unconditional_jump_int(int target, OpndSize size) {
+/**
+ * @brief Generates a single native instruction "jmp imm"
+ * @param target The immediate representing the relative offset from instruction pointer to jump to.
+ * @param size The size of immediate
+ */
+static void unconditional_jump_int(int target, OpndSize size) {
     Mnemonic m = Mnemonic_JMP;
     dump_imm(m, size, target);
 }
@@ -738,7 +727,7 @@ void unconditional_jump_int(int target, OpndSize size) {
 //! \param targetBlockId id of MIR basic block
 //! \param immediateNeedsAligned Whether the immediate needs to be aligned
 //! within 16-bytes
-void conditional_jump_block(ConditionCode cc, int targetBlockId,
+static void conditional_jump_block(ConditionCode cc, int targetBlockId,
         bool immediateNeedsAligned) {
     Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
     dump_blockid_imm(m, targetBlockId, immediateNeedsAligned);
@@ -750,19 +739,103 @@ void conditional_jump_block(ConditionCode cc, int targetBlockId,
 //! \param targetBlockId id of MIR basic block
 //! \param immediateNeedsAligned Whether the immediate needs to be aligned
 //! within 16-bytes
-void unconditional_jump_block(int targetBlockId, bool immediateNeedsAligned) {
+static void unconditional_jump_block(int targetBlockId, bool immediateNeedsAligned) {
     Mnemonic m = Mnemonic_JMP;
     dump_blockid_imm(m, targetBlockId, immediateNeedsAligned);
 }
 
-/*!
-\brief generate a single native instruction "jmp reg"
-
-*/
+/**
+ * @brief Generates a single native instruction "jmp reg"
+ * @param reg The register to use for jump
+ * @param isPhysical Whether the register is physical
+ */
 void unconditional_jump_reg(int reg, bool isPhysical) {
     dump_reg(Mnemonic_JMP, ATOM_NORMAL, OpndSize_32, reg, isPhysical, LowOpndRegType_gp);
 }
 
+/**
+ * @brief Generates a jump with 32-bit relative immediate that jumps
+ * to the target.
+ * @details Updates the instruction stream with the jump.
+ * @param target absolute address of target.
+ */
+void unconditional_jump_rel32 (void * target)
+{
+    // We will need to figure out the immediate to use for the relative
+    // jump, so we need to flush scheduler so that stream is updated.
+    // In most cases this won't affect the schedule since the jump would've
+    // ended the native BB anyway and would've been scheduled last.
+    if (gDvmJit.scheduling == true)
+    {
+        singletonPtr<Scheduler> ()->signalEndOfNativeBasicBlock ();
+    }
+
+    // Calculate the address offset between the destination of jump and the
+    // function we are jumping to.
+    int relOffset = reinterpret_cast<int> (target) - reinterpret_cast<int> (stream);
+
+    // Since instruction pointer will already be updated when executing this,
+    // subtract size of jump instruction
+    relOffset -= getJmpCallInstSize (OpndSize_32, JmpCall_uncond);
+
+    // Generate the unconditional jump now
+    unconditional_jump_int (relOffset, OpndSize_32);
+}
+
+void jumpToBasicBlock (int targetBlockId, bool immediateNeedsAligned)
+{
+    //When scheduling is enabled the jump that needs patched may be moved and thus
+    //we cannot assume that current place in code stream is where the jump will be lowered.
+    //For that reason we have two different paths.
+    if (gDvmJit.scheduling == true)
+    {
+        unconditional_jump_block (targetBlockId, immediateNeedsAligned);
+    }
+    else
+    {
+        //If jump needs aligned, then we simply align by 1 since size of encoded jump is 1
+        if (immediateNeedsAligned == true)
+        {
+            alignOffset (1);
+        }
+
+        //Get location of target
+        bool unknown;
+        OpndSize size;
+        int relativeNCG = getRelativeNCG (targetBlockId, JmpCall_uncond, &unknown, &size);
+
+        //Generate unconditional jump
+        unconditional_jump_int (relativeNCG, size);
+    }
+}
+
+void condJumpToBasicBlock (ConditionCode cc, int targetBlockId, bool immediateNeedsAligned)
+{
+    //When scheduling is enabled the jump that needs patched may be moved and thus
+    //we cannot assume that current place in code stream is where the jump will be lowered.
+    //For that reason we have two different paths.
+    if (gDvmJit.scheduling == true)
+    {
+        conditional_jump_block (cc, targetBlockId, immediateNeedsAligned);
+    }
+    else
+    {
+        //If jump needs aligned, then we simply align by 2 since size of encoded conditional jump is 2
+        if (immediateNeedsAligned == true)
+        {
+            alignOffset (2);
+        }
+
+        //Get location of target
+        bool unknown;
+        OpndSize size;
+        int relativeNCG = getRelativeNCG (targetBlockId, JmpCall_cond, &unknown, &size);
+
+        //Generate unconditional jump
+        conditional_jump_int (cc, relativeNCG, size);
+    }
+}
+
 /*!
 \brief generate a single native instruction to call a function
 
@@ -1168,46 +1241,6 @@ int common_if(s4 tmp, ConditionCode cc_next, ConditionCode cc_taken) {
 }
 #else
 
-//! \brief common code to handle GOTO
-//!
-//! \details If it is a backward branch, call common_periodicChecks4
-//! to handle GC request.
-//! Since this is the end of a basic block,
-//! globalVREndOfBB is called right before the jump instruction.
-//! when this is called from JIT, there is no need to check GC
-//!
-//! \param targetBlockId
-//!
-//! \return -1 if error
-int common_goto(s4 targetBlockId) {
-    bool unknown;
-    int retCode = 0;
-    OpndSize size;
-    bool needAlignment = doesJumpToBBNeedAlignment(traceCurrentBB->taken);
-
-    // We call it with true because we want to actually want to update
-    // association tables of children and handle ME spill requests
-    retCode = handleRegistersEndOfBB (true);
-    if (retCode < 0)
-    {
-        //Just pass along error information
-        return retCode;
-    }
-
-    if(gDvmJit.scheduling) {
-        // Assuming that gotos never go to chaining cells because they are not
-        // part of the bytecode and are just for trace transitions
-        unconditional_jump_block((int)targetBlockId, needAlignment);
-    } else {
-        if (needAlignment == true) {
-            alignOffset(1);
-        }
-        int relativeNCG = getRelativeNCG(targetBlockId, JmpCall_uncond, &unknown, &size);
-        unconditional_jump_int(relativeNCG, size);
-    }
-    return 1;
-}
-
 int generateConditionalJumpToTakenBlock (ConditionCode takenCondition)
 {
     // A basic block whose last bytecode is "if" must have two children
@@ -1303,20 +1336,8 @@ int generateConditionalJumpToTakenBlock (ConditionCode takenCondition)
         return -1;
     }
 
-    if (gDvmJit.scheduling)
-    {
-        conditional_jump_block (takenCondition, takenBB->id, doesJumpToBBNeedAlignment (takenBB));
-    }
-    else
-    {
-        //Conditional jumps in x86 are 2 bytes
-        alignOffset (2);
-
-        bool unknown;
-        OpndSize size = OpndSize_Null;
-        int relativeNCG = getRelativeNCG (takenBB->id, JmpCall_cond, &unknown, &size);
-        conditional_jump_int (takenCondition, relativeNCG, size);
-    }
+    //Now generate conditional jump to taken branch
+    condJumpToBasicBlock (takenCondition, takenBB->id, doesJumpToBBNeedAlignment (takenBB));
 
     // Now sync with the fallthrough child
     if (AssociationTable::createOrSyncTable (currentBB, true) == false)
@@ -1605,54 +1626,44 @@ int throw_exception(int exceptionPtrReg, int immReg,
 }
 
 /**
- * @brief Generate native code for bytecode goto
- * @param mir bytecode representation
- * @return value >= 0 when handled
+ * @brief Generates jump for handling goto bytecode. It also ensures that it handles registers since it is end of BB.
+ * @param mir The mir that represents the goto
+ * @param currentBB The current basic block
+ * @return Returns a positive value if successful.
  */
-int op_goto(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_GOTO);
-#if !defined(WITH_JIT)
-    u2 tt = INST_AA(inst);
-    s2 tmp = (s2)((s2)tt << 8) >> 8; //00AA --> AA00 --> xxAA
-#else
-    s2 tmp = traceCurrentBB->taken->id;
-#endif
-    int retval = common_goto(tmp);
-    return retval;
-}
+int op_goto (const MIR * mir, BasicBlock *currentBB)
+{
+    assert (mir->dalvikInsn.opcode == OP_GOTO
+            || mir->dalvikInsn.opcode == OP_GOTO_16
+            || mir->dalvikInsn.opcode == OP_GOTO_32);
 
-/**
- * @brief Generate native code for bytecode goto/16
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_goto_16(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_GOTO_16);
-#if !defined(WITH_JIT)
-    s2 tmp = (s2)FETCH(1);
-#else
-    s2 tmp = traceCurrentBB->taken->id;
-#endif
-    int retval = common_goto(tmp);
-    return retval;
-}
+    BasicBlock *targetBlock = currentBB->taken;
 
-/**
- * @brief Generate native code for bytecode goto/32
- * @param mir bytecode representation
- * @return value >= 0 when handled
- */
-int op_goto_32(const MIR * mir) {
-    assert(mir->dalvikInsn.opcode == OP_GOTO_32);
-#if !defined(WITH_JIT)
-    u4 tmp = (u4)FETCH(1);
-    tmp |= (u4)FETCH(2) << 16;
-#else
-    s2 tmp = traceCurrentBB->taken->id;
-#endif
-    int retval = common_goto((s4)tmp);
-    return retval;
+    //Paranoid
+    if (targetBlock == 0)
+    {
+        return -1;
+    }
+
+    //We call it with true because we want to actually want to update
+    //association tables of children and handle ME spill requests
+    int retCode = handleRegistersEndOfBB (true);
+    if (retCode < 0)
+    {
+        //Just pass along error information
+        return retCode;
+    }
+
+    //Determine if jump needs alignment
+    bool needAlignment = doesJumpToBBNeedAlignment(targetBlock);
+
+    //Generate an unconditional jump to the basic block
+    jumpToBasicBlock (targetBlock->id, needAlignment);
+
+    //We are successful so return positive value
+    return 1;
 }
+
 #define P_GPR_1 PhysicalReg_EBX
 
 /*
diff --git a/vm/compiler/codegen/x86/lightcg/LowerObject.cpp b/vm/compiler/codegen/x86/lightcg/LowerObject.cpp
index 81f0402..59a781f 100644
--- a/vm/compiler/codegen/x86/lightcg/LowerObject.cpp
+++ b/vm/compiler/codegen/x86/lightcg/LowerObject.cpp
@@ -135,7 +135,7 @@ int check_cast_nohelper(int vA, u4 tmp, bool instance, int vDest) {
         nextVersionOfHardReg(PhysicalReg_EDX, 2); //next version has 2 ref count
         export_pc();
 
-        unconditional_jump_global_API("common_throw_message", false);
+        unconditional_jump ("common_throw_message", false);
     }
     //handler for speical case where object reference is null
     if(instance) {
@@ -670,7 +670,7 @@ int op_new_array(const MIR * mir) {
     assert(mir->dalvikInsn.opcode == OP_NEW_ARRAY);
     int vA = mir->dalvikInsn.vA; //destination
     int vB = mir->dalvikInsn.vB; //length
-    u4 tmp = mir->dalvikInsn.vC;
+    u4 classIdx = mir->dalvikInsn.vC;
 #ifdef INC_NCG_O0
     if(gDvm.helper_switch[17]) {
         // .new_array_helper
@@ -699,9 +699,15 @@ int op_new_array(const MIR * mir) {
                                            Condition_S, Condition_NS,
                                            1, "common_errNegArraySize");
 #if defined(WITH_JIT)
-       void *classPtr = (void*)
-            (currentMethod->clazz->pDvmDex->pResClasses[tmp]);
-       assert(classPtr != NULL);
+
+        //If inlined we need to get the right method
+        const Method *method = (mir->OptimizationFlags & MIR_CALLEE) != 0 ? mir->meta.calleeMethod : currentMethod;
+
+        //Get the class pointer from the correct dex
+        ClassObject *classPtr = dvmDexGetResolvedClass (method->clazz->pDvmDex, classIdx);
+
+        //The class pointer should have already been resolved
+        assert (classPtr != NULL);
 #else
         //try to resolve class, if already resolved, jump to resolved
         //if not, call class_resolve
@@ -920,7 +926,8 @@ int op_filled_new_array_range(const MIR * mir) {
             spillVirtualReg(vC+k, LowOpndRegType_gp, true); //will update refCount
         }
         //address of the first virtual register that will be moved to the array object
-        load_effective_addr(vC*4, PhysicalReg_FP, true, 7, false); //addr
+        const int vrOffset = getVirtualRegOffsetRelativeToFP (vC);
+        load_effective_addr(vrOffset, PhysicalReg_FP, true, 7, false); //addr
         //start address for contents of the array object
         load_effective_addr(OFFSETOF_MEMBER(ArrayObject, contents), PhysicalReg_EAX, true, 8, false); //addr
         //loop counter
diff --git a/vm/compiler/codegen/x86/lightcg/LowerReturn.cpp b/vm/compiler/codegen/x86/lightcg/LowerReturn.cpp
index e82772b..034988a 100644
--- a/vm/compiler/codegen/x86/lightcg/LowerReturn.cpp
+++ b/vm/compiler/codegen/x86/lightcg/LowerReturn.cpp
@@ -28,24 +28,26 @@
  * @details Uses one scratch register to make the jump
  * @return value 0 when successful
  */
-inline int jumpTocommon_returnFromMethod() {
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
-
-    void * funcPtr = reinterpret_cast<void *>(dvmJitHelper_returnFromMethod);
+static int jumpTocommon_returnFromMethod (void)
+{
+    //The stack save area is in negative direction relative to frame pointer so we calculate displacement now
+    const int saveAreaDisp = -(sizeof(StackSaveArea));
 
     // Load save area into EDX
-    load_effective_addr(-sizeofStackSaveArea, PhysicalReg_FP, true, PhysicalReg_EDX, true);
+    load_effective_addr (saveAreaDisp, PhysicalReg_FP, true, PhysicalReg_EDX, true);
 
     // We may suffer from agen stall here due if edx is not ready
     // So instead of doing:
     //   movl offStackSaveArea_prevFrame(%edx), rFP
     // We can just compute directly
     //   movl (offStackSaveArea_prevFrame - sizeofStackSaveArea)(rFP), rFP
-    move_mem_to_reg(OpndSize_32,
-            OFFSETOF_MEMBER(StackSaveArea, prevFrame) - sizeofStackSaveArea,
+    move_mem_to_reg(OpndSize_32, OFFSETOF_MEMBER(StackSaveArea, prevFrame) + saveAreaDisp,
             PhysicalReg_FP, true, PhysicalReg_FP, true);
 
-    unconditional_jump_rel32(funcPtr);
+    //Jump to dvmJitHelper_returnFromMethod
+    unconditional_jump_rel32 (reinterpret_cast<void *> (dvmJitHelper_returnFromMethod));
+
+    //We return 0 as per our function definition when successful
     return 0;
 }
 
diff --git a/vm/compiler/codegen/x86/lightcg/NcgAot.cpp b/vm/compiler/codegen/x86/lightcg/NcgAot.cpp
index 75cd822..d4c01f9 100644
--- a/vm/compiler/codegen/x86/lightcg/NcgAot.cpp
+++ b/vm/compiler/codegen/x86/lightcg/NcgAot.cpp
@@ -28,17 +28,34 @@ int get_eip_API() {
     return 1;
 }
 #define NEW_EXPORT_PC
-//!update current PC in the stack frame with %eip
 
-//!
-int export_pc() {
-    /* for trace-based JIT, pc points to bytecode
-       for NCG, pc points to native code */
-    int sizeofStackSaveArea = sizeof(StackSaveArea);
+/**
+ * @brief Updates stack save area to contain the given dalvik PC
+ * @param dalvikPC The dalvik program counter for instruction we want to execute next
+ * @param framePtrRegister The register that holds the frame pointer
+ * @param isFramePtrRegPhysical Whether the register holding frame pointer is physical register
+ */
+static void export_pc (int dalvikPC, int framePtrRegister, bool isFramePtrRegPhysical)
+{
+    //The stack save area is in negative direction relative to frame pointer so we calculate displacement now
+    int displacement = -(sizeof(StackSaveArea)) + OFFSETOF_MEMBER(StackSaveArea, xtra.currentPc);
+
+    //Update the stack save area to contain the proper PC
+    move_imm_to_mem (OpndSize_32, dalvikPC, displacement, framePtrRegister, isFramePtrRegPhysical);
+}
+
+/**
+ * @brief Updates stack save area to contain the current dalvik PC
+ */
+void export_pc (void)
+{
+    //Determine PC and register that contains frame pointer
+    int dalvikPC = reinterpret_cast<int> (rPC);
+    int framePtrRegister = PhysicalReg_FP;
+    bool isFramePtrRegPhysical = true;
 
-    move_imm_to_mem(OpndSize_32, (int)rPC,
-                    -sizeofStackSaveArea+OFFSETOF_MEMBER(StackSaveArea, xtra.currentPc), PhysicalReg_FP, true);
-    return 1; //return number of ops
+    //Now call helper that does the work to store the PC to the save area
+    export_pc (dalvikPC, framePtrRegister, isFramePtrRegPhysical);
 }
 
 /* jump from JIT'ed code to interpreter without chaining */
@@ -132,26 +149,6 @@ bool jumpToException(const char* target) {
     return isException;
 }
 
-int conditional_jump_global_API(
-                                ConditionCode cc, const char* target,
-                                bool isShortTerm) {
-    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
-        condJumpToBasicBlock(stream, cc, currentExceptionBlockIdx);
-        return 1; //return number of ops
-    }
-    conditional_jump(cc, target, isShortTerm);
-    return 1;
-}
-int unconditional_jump_global_API(
-                                  const char* target, bool isShortTerm) {
-    if(jumpToException(target) && currentExceptionBlockIdx >= 0) { //jump to the exceptionThrow block
-        jumpToBasicBlock(stream, currentExceptionBlockIdx);
-        return 1; //return number of ops
-    }
-    unconditional_jump(target, isShortTerm);
-    return 1;
-}
-
 /* @brief Provides address to global constants
  * @details Essentially provides an associative
  * array of global data values indexed by name
diff --git a/vm/compiler/codegen/x86/lightcg/NcgAot.h b/vm/compiler/codegen/x86/lightcg/NcgAot.h
index 4d3d978..6e1343a 100644
--- a/vm/compiler/codegen/x86/lightcg/NcgAot.h
+++ b/vm/compiler/codegen/x86/lightcg/NcgAot.h
@@ -29,11 +29,7 @@ void callFuncPtr(int funcPtr, const char* funcName);
 /** @brief generate a "call imm32"*/
 void callFuncPtrImm(int funcPtr);
 int call_helper_API(const char* helperName);
-int conditional_jump_global_API(
-                                ConditionCode cc, const char* target,
-                                bool isShortTerm);
-int unconditional_jump_global_API(
-                                  const char* target, bool isShortTerm);
+
 int load_imm_global_data_API(const char* dataName,
                              OpndSize size,
                              int reg, bool isPhysical);
diff --git a/vm/compiler/codegen/x86/lightcg/RegisterizationBE.cpp b/vm/compiler/codegen/x86/lightcg/RegisterizationBE.cpp
index 4d96441..da07a8a 100644
--- a/vm/compiler/codegen/x86/lightcg/RegisterizationBE.cpp
+++ b/vm/compiler/codegen/x86/lightcg/RegisterizationBE.cpp
@@ -351,7 +351,7 @@ bool AssociationTable::syncCompileTableWithAssociations(AssociationTable & assoc
         }
 
         // If we did not find an entry, we must insert it
-        if (foundCompileTableEntry == false)
+        if (foundCompileTableEntry == false && associationEntry.isVirtualReg () == true)
         {
             DEBUG_COMPILETABLE_UPDATE(ALOGD("We have not found v%d in compile "
                         "table so we will make a new entry.", assocIter->first));
@@ -1273,7 +1273,8 @@ static bool loadVirtualRegistersForChild (const std::set<int> &virtualRegistersT
         // Load VR into the target physical register
         if (type == LowOpndRegType_ss)
         {
-            move_ss_mem_to_reg_noalloc (4 * VR, PhysicalReg_FP, true, MemoryAccess_VR, VR, targetReg, true);
+            const int vrOffset = getVirtualRegOffsetRelativeToFP (VR);
+            move_ss_mem_to_reg_noalloc (vrOffset, PhysicalReg_FP, true, MemoryAccess_VR, VR, targetReg, true);
         }
         else
         {
diff --git a/vm/compiler/codegen/x86/lightcg/Scheduler.cpp b/vm/compiler/codegen/x86/lightcg/Scheduler.cpp
index 31c3d70..311dedb 100644
--- a/vm/compiler/codegen/x86/lightcg/Scheduler.cpp
+++ b/vm/compiler/codegen/x86/lightcg/Scheduler.cpp
@@ -374,6 +374,9 @@ MachineModelEntry atomMachineModel[Mnemonic_Count*6] = {
     {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,4},{INVP,INVN},{INVP,INVN}, //PEXTRW - 3 operand instruction
     {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,4},{INVP,INVN},{INVP,INVN}, //PEXTRD - SSE4.1 3 operand instruction
     {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //MOVDQA
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SHUFPS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVAPS
 };
 
 //! \brief Get issue port for mnemonic with no operands
diff --git a/vm/compiler/codegen/x86/lightcg/StackExtensionX86.cpp b/vm/compiler/codegen/x86/lightcg/StackExtensionX86.cpp
index 94f6dc8..4e8cedb 100644
--- a/vm/compiler/codegen/x86/lightcg/StackExtensionX86.cpp
+++ b/vm/compiler/codegen/x86/lightcg/StackExtensionX86.cpp
@@ -27,15 +27,17 @@ unsigned int dvmArchSpecGetNumberOfScratch (void)
 }
 
 /**
- * @brief Given a stratch register index, it gives the VR register number.
+ * @brief Given a scratch register index, it gives the VR register number.
  * @param method Method that contains the MIR for which we want to
  * use scratch register.
  * @param idx Index of scratch register. Must be in range [0 .. N-1] where
  * N is the maximum number of scratch registers available.
+ * @param registerWindowShift If compilation unit uses a different register frame pointer
+ * base, it shifts the register window. This is the amount that register window has shifted.
  * @return Return virtual register number when it finds one for the index.
  * Otherwise, it returns -1.
  */
-int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx)
+int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx, int registerWindowShift)
 {
     unsigned int maxScratch = dvmArchSpecGetNumberOfScratch ();
 
@@ -54,5 +56,8 @@ int dvmArchSpecGetScratchRegister (const Method * method, unsigned int idx)
     //Calculate the regnum
     int regnum = idx + numLocals + numIns;
 
+    //Take into account the register window shift
+    regnum += registerWindowShift;
+
     return regnum;
 }
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_defs_ext.h b/vm/compiler/codegen/x86/lightcg/libenc/enc_defs_ext.h
index 8fe8b17..d0425dc 100644
--- a/vm/compiler/codegen/x86/lightcg/libenc/enc_defs_ext.h
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_defs_ext.h
@@ -342,6 +342,8 @@ Mnemonic_PHADDD,
 Mnemonic_PEXTRW,
 Mnemonic_PEXTRD,
 Mnemonic_MOVDQA,
+Mnemonic_SHUFPS,
+Mnemonic_MOVAPS,
 //
 Mnemonic_Count
 } Mnemonic;
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_tabl.cpp b/vm/compiler/codegen/x86/lightcg/libenc/enc_tabl.cpp
index 3d9a958..8e3ca30 100644
--- a/vm/compiler/codegen/x86/lightcg/libenc/enc_tabl.cpp
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_tabl.cpp
@@ -1303,6 +1303,19 @@ BEGIN_OPCODES()
 END_OPCODES()
 END_MNEMONIC()
 
+BEGIN_MNEMONIC(MOVAPS, MF_NONE, D_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all,   {0x0F, 0x28, _r},   {xmm64, xmm_m64},   D_U },
+    {OpcodeInfo::all,   {0x0F, 0x29, _r},   {xmm_m64, xmm64},   D_U },
+END_OPCODES()
+END_MNEMONIC()
+
+BEGIN_MNEMONIC(SHUFPS, MF_NONE, D_U_U )
+BEGIN_OPCODES()
+    {OpcodeInfo::all, {0x0F, 0xC6, _r, ib}, {xmm64, xmm_m64, imm8}, D_U_U },
+END_OPCODES()
+END_MNEMONIC()
+
 
 BEGIN_MNEMONIC(MOVSD, MF_NONE, D_U )
 BEGIN_OPCODES()
diff --git a/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.h b/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.h
index f107ddf..829f575 100644
--- a/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.h
+++ b/vm/compiler/codegen/x86/lightcg/libenc/enc_wrapper.h
@@ -178,7 +178,6 @@ enum LogicalRegType
     LowOpndRegType_temp = 16,
     LowOpndRegType_hard = 32,
     LowOpndRegType_virtual = 64,
-    LowOpndRegType_glue = 128,
 };
 
 //if inline, separte enc_wrapper.cpp into two files, one of them is .inl
diff --git a/vm/compiler/codegen/x86/pcg/Analysis.cpp b/vm/compiler/codegen/x86/pcg/Analysis.cpp
index b3610c7..5b8b337 100644
--- a/vm/compiler/codegen/x86/pcg/Analysis.cpp
+++ b/vm/compiler/codegen/x86/pcg/Analysis.cpp
@@ -1521,6 +1521,10 @@ bool dvmCompilerPcgNewRegisterizeVRAnalysis (CompilationUnitPCG *cUnit)
                     }
                     break;
 
+                case kMirOpCheckStackOverflow:
+                    //No virtual registers are being used
+                    break;
+
                 default:
                     ALOGI ("Unsupported instruction in trace for new registerization:");
                     char mybuf[2048];
diff --git a/vm/compiler/codegen/x86/pcg/ChainingCellException.cpp b/vm/compiler/codegen/x86/pcg/ChainingCellException.cpp
index 2a9fada..6993a93 100644
--- a/vm/compiler/codegen/x86/pcg/ChainingCellException.cpp
+++ b/vm/compiler/codegen/x86/pcg/ChainingCellException.cpp
@@ -850,7 +850,8 @@ void dvmCompilerPcgGenerateSpeculativeNullChecks (CompilationUnitPCG *cUnit)
                 ALOGI ("\n--------- generating speculative null check for SSA:%d.\n", temp);
             }
 
-            CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid, virtualReg * 4);
+            int vrOffset = dvmCompilerPcgGetVROffsetRelativeToVMPtr (cUnit, virtualReg);
+            CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid, vrOffset);
 
             size = dvmCompilerPcgGetOpcodeAndSizeForDtype (dtype, &opcode);
             if (IS_ANY_JIT_ERROR_SET() == true) {
diff --git a/vm/compiler/codegen/x86/pcg/CodeGeneration.cpp b/vm/compiler/codegen/x86/pcg/CodeGeneration.cpp
index 2c9891f..69c84d9 100644
--- a/vm/compiler/codegen/x86/pcg/CodeGeneration.cpp
+++ b/vm/compiler/codegen/x86/pcg/CodeGeneration.cpp
@@ -49,7 +49,9 @@ static bool dvmCompilerPcgTranslateInsn (CompilationUnitPCG *cUnit, MIR *mir)
 {
     bool success = true;
     Opcode dalvikOpCode = mir->dalvikInsn.opcode;
-    rPC = (u2*) (cUnit->method->insns) + mir->offset;
+
+    //Update rPC to contain the dalvik PC for this bytecode
+    rPC = dvmCompilerGetDalvikPC (mir);
 
     //Get the SSARepresentation
     SSARepresentation *ssaRep = mir->ssaRep;
@@ -99,8 +101,12 @@ static bool dvmCompilerPcgTranslateInsn (CompilationUnitPCG *cUnit, MIR *mir)
                 dvmCompilerPcgTranslateLoopChecks (cUnit, mir, false);
                 break;
 
+            case kMirOpCheckStackOverflow:
+                dvmCompilerPcgTranslateCheckStackOverflow (cUnit, mir);
+                break;
+
             default:
-                LOGE ("Jit (PCG): unsupported externded MIR opcode");
+                LOGE ("Jit (PCG): unsupported extended MIR opcode");
                 assert (0);
         }
         return true;
diff --git a/vm/compiler/codegen/x86/pcg/LowerExtended.cpp b/vm/compiler/codegen/x86/pcg/LowerExtended.cpp
index 75f61df..e40034a 100644
--- a/vm/compiler/codegen/x86/pcg/LowerExtended.cpp
+++ b/vm/compiler/codegen/x86/pcg/LowerExtended.cpp
@@ -14,6 +14,7 @@
  * limitations under the License.
  */
 
+#include "BasicBlockPCG.h"
 #include "ChainingCellException.h"
 #include "CompilationUnitPCG.h"
 #include "CompilerIR.h"
@@ -21,6 +22,7 @@
 #include "LowerExtended.h"
 #include "LowerMemory.h"
 #include "UtilityPCG.h"
+#include "LowerJump.h"
 
 /*
  * uses[0] = idxReg;
@@ -124,3 +126,66 @@ void dvmCompilerPcgTranslateLoopChecks (CompilationUnitPCG *cUnit, MIR *mir, boo
     CGInst arrayLength = dvmCompilerPcgCreateSimpleLoad (array, OFFSETOF_MEMBER(ArrayObject, length));
     CGCreateNewInst ("cjcc", "rcrbp", endIndex, "uge", arrayLength, checkFailedLabel, 0);
 }
+
+void dvmCompilerPcgTranslatePredictionInlineCheck (CompilationUnitPCG *cUnit, MIR *mir)
+{
+    //This function should only be called when generating inline prediction
+    assert (static_cast<ExtendedMIROpcode> (mir->dalvikInsn.opcode) == kMirOpCheckInlinePrediction);
+
+    BasicBlockPCG *bb = static_cast<BasicBlockPCG *> (mir->bb);
+
+    //Paranoid
+    assert (bb != 0);
+
+    //Instruction has conditional branching semantics so it should be block ending
+    assert (mir->next == 0 && bb->lastMIRInsn == mir && bb->fallThrough != 0 && bb->taken != 0);
+
+    //Get the SSARepresentation
+    SSARepresentation *ssaRep = mir->ssaRep;
+
+    assert (ssaRep != 0);
+
+    //Get the "this" pointer
+    CGInst thisPtr = dvmCompilerPcgGetVirtualReg (cUnit, ssaRep->uses[0], "mov", 4);
+
+    //Check it for null
+    dvmCompilerPcgGenerateNullCheck (cUnit, thisPtr, mir, ssaRep->uses[0]);
+
+    //The class literal is in vB
+    CGInst clazzLiteral = CGCreateNewInst ("mov", "i", mir->dalvikInsn.vB);
+
+    //Get the class from "this"
+    CGInst clazz = dvmCompilerPcgCreateSimpleLoad (thisPtr, OFFSETOF_MEMBER (Object, clazz));
+
+    //We take the taken branch if the class of this doesn't match our expected class
+    dvmCompilerPcgTranslateConditionalJump (bb, clazz, "ne", clazzLiteral);
+}
+
+void dvmCompilerPcgTranslateCheckStackOverflow (CompilationUnitPCG *cUnit, MIR *mir)
+{
+    assert (static_cast<ExtendedMIROpcode> (mir->dalvikInsn.opcode) == kMirOpCheckStackOverflow);
+
+    //vB holds the size of space of frame needed relative to frame pointer
+    int spaceNeeded = mir->dalvikInsn.vB;
+
+    //Stack grows in negative direction so subtract the size from the frame pointer
+    CGInst stackUsedEnd = CGCreateNewInst ("sub", "ri", cUnit->getVMPtr(), spaceNeeded);
+
+    //Obtain the self pointer
+    CGInst selfPtr = dvmCompilerPcgGetSelfPointer (cUnit);
+
+    //Create label for case when we don't overflow
+    CGLabel noOverflow = CGCreateLabel ();
+
+    //Load the interpStackEnd from thread
+    CGInst interpStackEnd = dvmCompilerPcgCreateSimpleLoad (selfPtr, OFFSETOF_MEMBER (Thread, interpStackEnd));
+
+    //If not below or equal, then we do not overflow. Overflowing is a rare condition.
+    CGCreateNewInst ("cjcc", "rcrbp", stackUsedEnd, "ugt", interpStackEnd, noOverflow, 100);
+
+    //Now generate an exception if we overflow so we can punt
+    dvmCompilerPcgGenerateRaiseException (cUnit);
+
+    //Bind label so we can get here when we don't take the overflow path
+    CGBindLabel (noOverflow);
+}
diff --git a/vm/compiler/codegen/x86/pcg/LowerExtended.h b/vm/compiler/codegen/x86/pcg/LowerExtended.h
index eb19f82..2b812da 100644
--- a/vm/compiler/codegen/x86/pcg/LowerExtended.h
+++ b/vm/compiler/codegen/x86/pcg/LowerExtended.h
@@ -18,6 +18,7 @@
 #define H_LOWEREXTENDED
 
 // Forward declarations
+struct BasicBlockPCG;
 class CompilationUnitPCG;
 struct MIR;
 
@@ -50,4 +51,19 @@ void dvmCompilerPcgTranslateNullCheck (CompilationUnitPCG *cUnit, MIR *mir);
  */
 void dvmCompilerPcgTranslateLoopChecks (CompilationUnitPCG *cUnit, MIR *mir, bool countUp);
 
+/**
+ * @brief Translate the extended prediction inline check MIR
+ * @details Does a class check to verify if the inlined path should be taken or the path with invoke in case of mispredict.
+ * @param cUnit The Compilation Unit
+ * @param mir The MIR with opcode kMirOpCheckInlinePrediction
+ */
+void dvmCompilerPcgTranslatePredictionInlineCheck (CompilationUnitPCG *cUnit, MIR *mir);
+
+/**
+ * @brief Translates the extended MIR used for doing a stack overflow check
+ * @param cUnit The compilation unit
+ * @param mir The MIR with opcode kMirOpCheckStackOverflow
+ */
+void dvmCompilerPcgTranslateCheckStackOverflow (CompilationUnitPCG *cUnit, MIR *mir);
+
 #endif // H_LOWEREXTENDED
diff --git a/vm/compiler/codegen/x86/pcg/LowerMemory.cpp b/vm/compiler/codegen/x86/pcg/LowerMemory.cpp
index c28540c..de84310 100644
--- a/vm/compiler/codegen/x86/pcg/LowerMemory.cpp
+++ b/vm/compiler/codegen/x86/pcg/LowerMemory.cpp
@@ -130,8 +130,8 @@ void dvmCompilerPcgStoreVirtualReg (CompilationUnitPCG *cUnit, int ssaNum, int s
     }
 
     void *handle = dvmCompilerPcgGetVRHandle (virtualReg, storeSize);
-    CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid,
-            virtualReg * 4);
+    int vrOffset = dvmCompilerPcgGetVROffsetRelativeToVMPtr (cUnit, virtualReg);
+    CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid, vrOffset);
 
     CGCreateNewInst (opcode, "mr", addr, storeSize, handle, storeVal);
 }
diff --git a/vm/compiler/codegen/x86/pcg/LowerOther.cpp b/vm/compiler/codegen/x86/pcg/LowerOther.cpp
index 7ae7908..918e8d7 100644
--- a/vm/compiler/codegen/x86/pcg/LowerOther.cpp
+++ b/vm/compiler/codegen/x86/pcg/LowerOther.cpp
@@ -608,37 +608,3 @@ void dvmCompilerPcgAddVRInterfaceCode (CompilationUnitPCG *cUnit)
                 "    ===========================\n");
     }
 }
-
-void dvmCompilerPcgTranslatePredictionInlineCheck (CompilationUnitPCG *cUnit, MIR *mir)
-{
-    //This function should only be called when generating inline prediction
-    assert (static_cast<ExtendedMIROpcode> (mir->dalvikInsn.opcode) == kMirOpCheckInlinePrediction);
-
-    BasicBlockPCG *bb = static_cast<BasicBlockPCG *> (mir->bb);
-
-    //Paranoid
-    assert (bb != 0);
-
-    //Instruction has conditional branching semantics so it should be block ending
-    assert (mir->next == 0 && bb->lastMIRInsn == mir && bb->fallThrough != 0 && bb->taken != 0);
-
-    //Get the SSARepresentation
-    SSARepresentation *ssaRep = mir->ssaRep;
-
-    assert (ssaRep != 0);
-
-    //Get the "this" pointer
-    CGInst thisPtr = dvmCompilerPcgGetVirtualReg (cUnit, ssaRep->uses[0], "mov", 4);
-
-    //Check it for null
-    dvmCompilerPcgGenerateNullCheck (cUnit, thisPtr, mir, ssaRep->uses[0]);
-
-    //The class literal is in vB
-    CGInst clazzLiteral = CGCreateNewInst ("mov", "i", mir->dalvikInsn.vB);
-
-    //Get the class from "this"
-    CGInst clazz = dvmCompilerPcgCreateSimpleLoad (thisPtr, OFFSETOF_MEMBER (Object, clazz));
-
-    //We take the taken branch if the class of this doesn't match our expected class
-    dvmCompilerPcgTranslateConditionalJump (bb, clazz, "ne", clazzLiteral);
-}
diff --git a/vm/compiler/codegen/x86/pcg/LowerOther.h b/vm/compiler/codegen/x86/pcg/LowerOther.h
index b1d0841..a1cb070 100644
--- a/vm/compiler/codegen/x86/pcg/LowerOther.h
+++ b/vm/compiler/codegen/x86/pcg/LowerOther.h
@@ -101,12 +101,4 @@ void dvmCompilerPcgTranslateSparseSwitch (CompilationUnitPCG *cUnit, MIR *mir);
  */
 void dvmCompilerPcgAddVRInterfaceCode (CompilationUnitPCG *cUnit);
 
-/**
- * @brief Translate the extended prediction inline check MIR
- * @details Does a class check to verify if the inlined path should be taken or the path with invoke in case of mispredict.
- * @param cUnit The Compilation Unit
- * @param mir The MIR with opcode kMirOpCheckInlinePrediction
- */
-void dvmCompilerPcgTranslatePredictionInlineCheck (CompilationUnitPCG *cUnit, MIR *mir);
-
 #endif
diff --git a/vm/compiler/codegen/x86/pcg/PcgInterface.cpp b/vm/compiler/codegen/x86/pcg/PcgInterface.cpp
index f53e67a..e758500 100644
--- a/vm/compiler/codegen/x86/pcg/PcgInterface.cpp
+++ b/vm/compiler/codegen/x86/pcg/PcgInterface.cpp
@@ -237,6 +237,7 @@ extern "C" void setupPcgJit (void)
     jitFramework.backEndBasicBlockAllocation = pcgBBAllocator;
     jitFramework.backEndDumpSpecificBB = 0;
     jitFramework.backEndInvokeArgsDone = dvmCompilerPcgHandleInvokeArgsHeader;
+    jitFramework.backendSupportExtendedOp = dvmCompilerPcgSupportsExtendedOp;
 }
 
 /**
diff --git a/vm/compiler/codegen/x86/pcg/UtilityPCG.cpp b/vm/compiler/codegen/x86/pcg/UtilityPCG.cpp
index 6f694e1..13f3df4 100644
--- a/vm/compiler/codegen/x86/pcg/UtilityPCG.cpp
+++ b/vm/compiler/codegen/x86/pcg/UtilityPCG.cpp
@@ -190,8 +190,8 @@ CGInst dvmCompilerPcgGetVirtualReg (CompilationUnitPCG *cUnit, int ssaNum, const
     u2 virtualReg = DECODE_REG (dalvikReg);
 
     void *handle = dvmCompilerPcgGetVRHandle (virtualReg, loadSize);
-    CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid,
-                               virtualReg * 4);
+    int vrOffset = dvmCompilerPcgGetVROffsetRelativeToVMPtr (cUnit, virtualReg);
+    CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid, vrOffset);
 
     return CGCreateNewInst (pcgOpcode, "m", addr, loadSize, handle);
 }
@@ -220,8 +220,8 @@ void dvmCompilerPcgSetVirtualReg (CompilationUnitPCG *cUnit, int ssaNum, const c
     u2 virtualReg = dvmExtractSSARegister (cUnit, ssaNum);
 
     void *handle = dvmCompilerPcgGetVRHandle (virtualReg, storeSize);
-    CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid,
-                               virtualReg * 4);
+    int vrOffset = dvmCompilerPcgGetVROffsetRelativeToVMPtr (cUnit, virtualReg);
+    CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid, vrOffset);
 
     CGCreateNewInst (pcgOpcode, "mr", addr, storeSize, handle, storeVal);
 }
@@ -429,6 +429,27 @@ bool dvmCompilerPcgIsHighDtype (pcgDtype dtype)
     return false;
 }
 
+bool dvmCompilerPcgSupportsExtendedOp (int extendedOpcode)
+{
+    switch (extendedOpcode)
+    {
+        case kMirOpPhi:
+        case kMirOpRegisterize:
+        case kMirOpCheckInlinePrediction:
+        case kMirOpLowerBound:
+        case kMirOpBoundCheck:
+        case kMirOpNullCheck:
+        case kMirOpNullNRangeUpCheck:
+        case kMirOpNullNRangeDownCheck:
+            return true;
+        default:
+            break;
+    }
+
+    //If we get here it is not supported
+    return false;
+}
+
 /**
  * @details Useful interface routine that allows us to selectively use PCG or the existing dalvik JIT.
  */
@@ -492,21 +513,9 @@ bool dvmCompilerPcgSupportTrace (CompilationUnit *cUnit)
 
             if (dalvikOpCode >= kNumPackedOpcodes) {
                 // Use an opt-in approach for extended MIRs.
-                switch ( (ExtendedMIROpcode) dalvikOpCode) {
-                    // The are the PCG-supported extended MIR bytecodes.
-                    case kMirOpPhi:
-                    case kMirOpRegisterize:
-                    case kMirOpCheckInlinePrediction:
-                    case kMirOpLowerBound:
-                    case kMirOpBoundCheck:
-                    case kMirOpNullCheck:
-                    case kMirOpNullNRangeUpCheck:
-                    case kMirOpNullNRangeDownCheck:
-                        break;
-
-                    default:
-                        faultyMIR = mir;
-                        break;
+                if (dvmCompilerPcgSupportsExtendedOp (dalvikOpCode) == false)
+                {
+                    faultyMIR = mir;
                 }
             }
 
@@ -722,7 +731,8 @@ void dvmCompilerPcgHandleInitialLoad (CompilationUnitPCG *cUnit, BasicBlock *bb,
                 DECODE_SUB (dvmConvertSSARegToDalvik (cUnit, ssaNum)));
     }
 
-    CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid, virtualReg * 4);
+    int vrOffset = dvmCompilerPcgGetVROffsetRelativeToVMPtr (cUnit, virtualReg);
+    CGAddr addr = CGCreateAddr (cUnit->getVMPtr (), CGInstInvalid, 0, CGSymbolInvalid, vrOffset);
 
     //Get size and opcode
     const char *pcgOpcode = 0;
@@ -872,3 +882,24 @@ void dvmCompilerPcgRejectLoopsWithInvokes (CompilationUnit *cUnit, Pass *curPass
         }
     }
 }
+
+int dvmCompilerPcgGetVROffsetRelativeToVMPtr (CompilationUnitPCG *cUnit, int vR)
+{
+    int sizeOfVR = sizeof (u4);
+
+    //In order to get adjustment we multiply the window shift by size of VR
+    int adjustment = cUnit->registerWindowShift * sizeOfVR;
+
+    //Stack grows in a negative direction and when we have a register window shift we push the
+    //stack up. Thus taking that into account, the shift is negative.
+    //Namely desiredFP = actualFP - adjustment
+    adjustment = adjustment * (-1);
+
+    //Each virtual register is 32-bit and thus we multiply its size with the VR number
+    int offset = vR * sizeOfVR;
+
+    //Now take into account any FP adjustment
+    offset += adjustment;
+
+    return offset;
+}
diff --git a/vm/compiler/codegen/x86/pcg/UtilityPCG.h b/vm/compiler/codegen/x86/pcg/UtilityPCG.h
index 5ffc433..6e5eef4 100644
--- a/vm/compiler/codegen/x86/pcg/UtilityPCG.h
+++ b/vm/compiler/codegen/x86/pcg/UtilityPCG.h
@@ -27,6 +27,13 @@ struct BasicBlockPCG;
 class CompilationUnitPCG;
 
 /**
+ * @brief Used to check whether PCG supports extended opcode
+ * @param extendedOpcode The opcode to check
+ * @return Returns whether the extended opcode is supported
+ */
+bool dvmCompilerPcgSupportsExtendedOp (int extendedOpcode);
+
+/**
  * @brief Should we be using PCG for the trace
  * @param cUnit the CompilationUnit
  * @return whether we want to handle the trace
@@ -202,4 +209,12 @@ void dvmCompilerPcgRemoveNonPhiNodes (CompilationUnitPCG *cUnit, BitVector *bv,
  */
 void dvmCompilerPcgRejectLoopsWithInvokes (CompilationUnit *cUnit, Pass *curPass);
 
+/**
+ * @brief Used to obtain the offset relative to VM frame pointer for a given VR
+ * @param cUnit The compilation unit
+ * @param vR The virtual register number for which to calculate offset
+ * @return Returns the offset relative to VM frame pointer
+ */
+int dvmCompilerPcgGetVROffsetRelativeToVMPtr (CompilationUnitPCG *cUnit, int vR);
+
 #endif
-- 
1.7.4.1

