From e5f783d4cadf341edcc36c585627603aaa04aebb Mon Sep 17 00:00:00 2001
From: Qiming Shi <qiming.shi@intel.com>
Date: Sat, 1 Jun 2013 14:34:17 +0800
Subject: Dalvik: Instruction Scheduling for Atom JIT

BZ: 58841

Enables instruction scheduling for atom. To turn off scheduling
the global property dalvik.vm.extra-opts must be set to include
value -Xjitnoscheduling

Limitations of design:
-Memory latency is not included in the execute to execute latency
for the Atom machine model.
-Scheduling is done only per native basic block.
-Scheduling is done post register allocation.
-Mapping from bytecode to x86 instructions is less precise since
the native instructions get moved around.

Category: device-enablement
Domain: AOSP-Dalvik-Compiler-CG; AOSP-Dalvik-Runtime
Origin: internal
Upstream-Candidate: no, needs rework

Change-Id: I7af2dfa2e5224ad9a42b305c4a7b5fe95fcf7bba
Orig-MCG-Change-Id: I8bff588304c362ecbd532d3753c4b07c75e4e613
Signed-off-by: Razvan A Lupusoru <razvan.a.lupusoru@intel.com>
Signed-off-by: Qiming Shi <qiming.shi@intel.com>
Signed-off-by: Serguei Katkov <serguei.i.katkov@intel.com>
---
 vm/Init.cpp                                    |   10 +-
 vm/compiler/codegen/x86/AnalysisO1.cpp         |   26 +-
 vm/compiler/codegen/x86/CodegenInterface.cpp   |   49 +-
 vm/compiler/codegen/x86/Lower.cpp              |   23 +-
 vm/compiler/codegen/x86/Lower.h                |  600 +++++----
 vm/compiler/codegen/x86/LowerHelper.cpp        | 1007 +++++++++------
 vm/compiler/codegen/x86/LowerInvoke.cpp        |   27 +-
 vm/compiler/codegen/x86/LowerJump.cpp          |  180 ++-
 vm/compiler/codegen/x86/Schedule.cpp           | 1748 ++++++++++++++++++++++++
 vm/compiler/codegen/x86/Scheduler.h            |  120 ++
 vm/compiler/codegen/x86/doxygen-config-x86-jit |    9 +
 vm/compiler/codegen/x86/libenc/enc_wrapper.cpp |   31 +-
 vm/compiler/codegen/x86/libenc/enc_wrapper.h   |    6 +
 13 files changed, 3134 insertions(+), 702 deletions(-)
 create mode 100644 vm/compiler/codegen/x86/Schedule.cpp
 create mode 100644 vm/compiler/codegen/x86/Scheduler.h
 create mode 100644 vm/compiler/codegen/x86/doxygen-config-x86-jit

diff --git a/vm/Init.cpp b/vm/Init.cpp
index 7111bf7..9d8dd89 100644
--- a/vm/Init.cpp
+++ b/vm/Init.cpp
@@ -150,8 +150,9 @@ static void usage(const char* progName)
     dvmFprintf(stderr, "  -Xjitconfig:filename\n");
     dvmFprintf(stderr, "  -Xjitcheckcg\n");
     dvmFprintf(stderr, "  -Xjitverbose\n");
-    dvmFprintf(stderr, "  -Xjitsepdalvik\n");
+    dvmFprintf(stderr, "  -Xjit[no]scheduling (Turn on/off Atom Instruction Scheduling)\n");
 #if defined(VTUNE_DALVIK)
+    dvmFprintf(stderr, "  -Xjitsepdalvik\n");
     dvmFprintf(stderr, "  -Xjitvtuneinfo:{none,jit,dex,src}\n");
 #endif
     dvmFprintf(stderr, "  -Xjitprofile\n");
@@ -1150,6 +1151,10 @@ static int processOptions(int argc, const char* const argv[],
             gDvmJit.printBinary = true;
         } else if (strncmp(argv[i], "-Xjitverbose", 12) == 0) {
             gDvmJit.printMe = true;
+        } else if (strncmp(argv[i], "-Xjitscheduling", 15) == 0) {
+            gDvmJit.scheduling = true;
+        } else if (strncmp(argv[i], "-Xjitnoscheduling", 17) == 0) {
+            gDvmJit.scheduling = false;
 #if defined(VTUNE_DALVIK)
         } else if (strncmp(argv[i], "-Xjitsepdalvik", 14) == 0) {
             gDvmJit.vtuneInfo = kVTuneInfoNativeCode;
@@ -1307,7 +1312,8 @@ static void setCommandLineDefaults()
 #if defined(WITH_JIT)
     gDvm.executionMode = kExecutionModeJit;
     gDvmJit.num_entries_pcTable = 0;
-    gDvmJit.includeSelectedMethod = false;
+    gDvmJit.scheduling = true;
+    gDvmJit.includeSelectedMethod = false; //uninitialized variable may not be zero
     gDvmJit.includeSelectedOffset = false;
     gDvmJit.methodTable = NULL;
     gDvmJit.classTable = NULL;
diff --git a/vm/compiler/codegen/x86/AnalysisO1.cpp b/vm/compiler/codegen/x86/AnalysisO1.cpp
index 0178d9e..0cbedb3 100644
--- a/vm/compiler/codegen/x86/AnalysisO1.cpp
+++ b/vm/compiler/codegen/x86/AnalysisO1.cpp
@@ -24,6 +24,7 @@
 #include "interp/InterpState.h"
 #include "interp/InterpDefs.h"
 #include "libdex/Leb128.h"
+#include "Scheduler.h"
 
 /* compilation flags to turn on debug printout */
 //#define DEBUG_COMPILE_TABLE
@@ -351,7 +352,9 @@ void syncAllRegs() {
         }
         if(!stillUsed && allRegs[k].isUsed) {
             allRegs[k].isUsed = false;
-            allRegs[k].freeTimeStamp = lowOpTimeStamp;
+            allRegs[k].freeTimeStamp = lowOpTimeStamp; /* TODO Semantics of code that uses freeTimeStamp might
+                                                          not be correct because value of lowOpTimeStamp is not
+                                                          being updated when each x86 instruction is emitted. */
         }
     }
     return;
@@ -626,7 +629,28 @@ int codeGenBasicBlockJit(const Method* method, BasicBlock* bb) {
         if(method_bbs_sorted[k]->jitBasicBlock == bb) {
             lowOpTimeStamp = 0; //reset time stamp at start of a basic block
             currentBB = method_bbs_sorted[k];
+#if 0
+            if(indexForGlue >= 0) {
+                /* at start of a basic block, GLUE is mapped to %ebp */
+                compileTable[indexForGlue].physicalReg = PhysicalReg_EBP;
+                compileTable[indexForGlue].spill_loc_index = -1;
+                if(!currentBB->hasAccessToGlue) {
+                    /* since GLUE is not used in this basic block, spill it to free one GPR */
+                    spillLogicalReg(indexForGlue, true);
+                    syncAllRegs(); /* sync up allRegs with compileTable */
+                }
+            }
+#endif
+            // Basic block here also means new native basic block
+            if (gDvmJit.scheduling)
+                g_SchedulerInstance.signalEndOfNativeBasicBlock();
+
             int cg_ret = codeGenBasicBlock(method, currentBB);
+
+            // End of managed basic block means end of native basic block
+            if (gDvmJit.scheduling)
+                g_SchedulerInstance.signalEndOfNativeBasicBlock();
+
             currentBB = NULL;
             return cg_ret;
         }
diff --git a/vm/compiler/codegen/x86/CodegenInterface.cpp b/vm/compiler/codegen/x86/CodegenInterface.cpp
index 2d31d5a..6dad924 100644
--- a/vm/compiler/codegen/x86/CodegenInterface.cpp
+++ b/vm/compiler/codegen/x86/CodegenInterface.cpp
@@ -341,9 +341,12 @@ LowOp* jumpToBasicBlock(char* instAddr, int targetId) {
     stream = instAddr;
     bool unknown;
     OpndSize size;
-    int relativeNCG = targetId;
-    relativeNCG = getRelativeNCG(targetId, JmpCall_uncond, &unknown, &size);
-    unconditional_jump_int(relativeNCG, size);
+    if(gDvmJit.scheduling) {
+        unconditional_jump_block(targetId);
+    } else {
+        int relativeNCG = getRelativeNCG(targetId, JmpCall_uncond, &unknown, &size);
+        unconditional_jump_int(relativeNCG, size);
+    }
     return NULL;
 }
 
@@ -351,9 +354,12 @@ LowOp* condJumpToBasicBlock(char* instAddr, ConditionCode cc, int targetId) {
     stream = instAddr;
     bool unknown;
     OpndSize size;
-    int relativeNCG = targetId;
-    relativeNCG = getRelativeNCG(targetId, JmpCall_cond, &unknown, &size);
-    conditional_jump_int(cc, relativeNCG, size);
+    if(gDvmJit.scheduling) {
+        conditional_jump_block(cc, targetId);
+    } else {
+        int relativeNCG = getRelativeNCG(targetId, JmpCall_cond, &unknown, &size);
+        conditional_jump_int(cc, relativeNCG, size);
+    }
     return NULL;
 }
 
@@ -539,8 +545,11 @@ const Method *dvmJitToPatchPredictedChain(const Method *method,
     COMPILER_TRACE_CHAINING(
             ALOGI("inlineCachePatchEnqueue chain %p to method %s%s inst size %d",
                   cell, method->clazz->descriptor, method->name, jumpSize));
-    //can't use stream here since it is used by the compilation thread
-    dump_imm_with_codeaddr(Mnemonic_JMP, immSize, relOffset, (char*) (&newCell)); //update newCell.branch
+
+    // This does not need to go through lowering interface and can encode directly
+    // at address because it does not actually update code stream until safe point.
+    // Can't use stream here since it is used by the compilation thread.
+    encoder_imm(Mnemonic_JMP, immSize, relOffset, (char*) (&newCell)); //update newCell.branch
 
     newCell.clazz = clazz;
     newCell.method = method;
@@ -1144,7 +1153,6 @@ void dvmCompilerMIR2LIR(CompilationUnit *cUnit, JitTranslationInfo *info)
     /* Used to hold the labels of each block */
     LowOpBlockLabel *labelList =
         (LowOpBlockLabel *)dvmCompilerNew(sizeof(LowOpBlockLabel) * cUnit->numBlocks, true); //Utility.c
-    LowOp *headLIR = NULL;
     GrowableList chainingListByType[kChainingCellLast];
     unsigned int i, padding;
 
@@ -1356,11 +1364,18 @@ void dvmCompilerMIR2LIR(CompilationUnit *cUnit, JitTranslationInfo *info)
                 dexGetFormatFromOpcode(dalvikOpCode);
             ALOGV("ready to handle bytecode at offset %x: opcode %d format %d",
                   mir->offset, dalvikOpCode, dalvikFormat);
-            LowOpImm *boundaryLIR = dump_special(ATOM_PSEUDO_DALVIK_BYTECODE_BOUNDARY, mir->offset);
-            /* Remember the first LIR for this block */
-            if (headLIR == NULL) {
-                headLIR = (LowOp*)boundaryLIR;
-            }
+
+            // Before: A boundary LIR with Atom pseudo-mnemonic named
+            //      ATOM_PSEUDO_DALVIK_BYTECODE_BOUNDARY was being created
+            //      at this point. The allocation of the Atom LIR used to
+            //      update the global variable named lowOpTimeStamp.
+            // After: LIRs are now only allocated through the Instruction
+            //      scheduling interface and LIRs with only pseudo-mnemonics
+            //      are not supported. In order to keep semantics, the
+            //      timestamp will be updated here manually since it affects
+            //      register allocation.
+            lowOpTimeStamp++;
+
             bool notHandled = true;
             /*
              * Debugging: screen the opcode first to see if it is in the
@@ -1403,7 +1418,6 @@ void dvmCompilerMIR2LIR(CompilationUnit *cUnit, JitTranslationInfo *info)
         dvmCompilerApplyLocalOptimizations(cUnit, (LIR *) headLIR,
                                            cUnit->lastLIRInsn);
 #endif
-        if (headLIR) headLIR = NULL;
 gen_fallthrough:
         /*
          * Check if the block is terminated due to trace length constraint -
@@ -1568,11 +1582,12 @@ void* dvmJitChain(void* tgtAddr, u4* branchAddr)
         /* Hard coded the jump opnd size to 32 bits, This instruction will replace the "jump 0" in
          * the original code sequence.
          */
-        OpndSize immSize = OpndSize_32;
         relOffset -= 5;
         //can't use stream here since it is used by the compilation thread
         UNPROTECT_CODE_CACHE(branchAddr, sizeof(*branchAddr));
-        dump_imm_with_codeaddr(Mnemonic_JMP, immSize, relOffset, (char*)branchAddr); //dump to branchAddr
+        dump_imm_update(relOffset, (char*) branchAddr, false); // An update is done instead of an encode
+                                                                // because the Jmp instruction is already
+                                                                // part of chaining cell.
         PROTECT_CODE_CACHE(branchAddr, sizeof(*branchAddr));
 
         gDvmJit.hasNewChain = true;
diff --git a/vm/compiler/codegen/x86/Lower.cpp b/vm/compiler/codegen/x86/Lower.cpp
index d43185a..af394b8 100644
--- a/vm/compiler/codegen/x86/Lower.cpp
+++ b/vm/compiler/codegen/x86/Lower.cpp
@@ -59,8 +59,6 @@ MIR* traceCurrentMIR = NULL;
 JitMode traceMode = kJitTrace;
 bool branchInLoop = false;
 
-bool scheduling_is_on = false;
-
 int common_invokeMethodNoRange();
 int common_invokeMethodRange();
 int common_invokeArgsDone(ArgsDoneType, bool);
@@ -353,7 +351,23 @@ void init_common(const char* curFileName, DvmDex *pDvmDex, bool forNCG) {
 }
 
 void initGlobalMethods() {
-    dump_x86_inst = false; /* DEBUG */
+    bool old_dump_x86_inst = dump_x86_inst;
+    bool old_scheduling = gDvmJit.scheduling;
+    dump_x86_inst = false; // Enable this to debug common section
+
+    //! \warning Scheduling should be turned off when creating common section
+    //! because it relies on the fact the register allocation has already been
+    //! done (either via register allocator or via hardcoded registers). But,
+    //! when we get to this point, the execution mode is Jit instead of either
+    //! NcgO1 or NcgO0, which leads to the unintended consequence that NcgO0
+    //! path is taken, but logical registers are used instead of physical
+    //! registers and thus relies on encoder to do the mapping, which the
+    //! scheduler cannot predict for dependency graph creation.
+    //! \todo The reason "logical" registers are used is because variable
+    //! isScratchPhysical is set to false even when a physical register is
+    //! used. This issue should be addressed at some point.
+    gDvmJit.scheduling = false;
+
     // generate native code for function ncgGetEIP
     insertLabel("ncgGetEIP", false);
     move_mem_to_reg(OpndSize_32, 0, PhysicalReg_ESP, true, PhysicalReg_EDX, true);
@@ -406,7 +420,8 @@ void initGlobalMethods() {
     performCGWorklist(); //generate code for helper functions
     performLabelWorklist(); //it is likely that the common labels will jump to other common labels
 
-    dump_x86_inst = false;
+    gDvmJit.scheduling = old_scheduling;
+    dump_x86_inst = old_dump_x86_inst;
 }
 
 ExecutionMode origMode;
diff --git a/vm/compiler/codegen/x86/Lower.h b/vm/compiler/codegen/x86/Lower.h
index efa1127..50b0c2a 100644
--- a/vm/compiler/codegen/x86/Lower.h
+++ b/vm/compiler/codegen/x86/Lower.h
@@ -15,8 +15,8 @@
  */
 
 
-/*! \file lower.h
-    \brief A header file to define interface between lowering and register allocator
+/*! \file Lower.h
+    \brief A header file to define interface between lowering, register allocator, and scheduling
 */
 
 #ifndef _DALVIK_LOWER
@@ -27,7 +27,7 @@
 #define PREDICTED_CHAINING
 #define JIT_CHAIN
 
-#define NUM_DEPENDENCIES 24 /* max number of dependencies from a LowOp */
+#define NCG_O1
 //compilaton flags used by NCG O1
 #define DUMP_EXCEPTION //to measure performance, required to have correct exception handling
 /*! multiple versions for hardcoded registers */
@@ -200,103 +200,14 @@ typedef enum JmpCall_type {
     JmpCall_call
 } JmpCall_type;
 
-////////////////////////////////////////////////////////////////
-/* data structure for native codes */
-/* Due to space considation, a lowered op (LowOp) has two operands (LowOpnd), depending on
-   the type of the operand, LowOpndReg or LowOpndImm or LowOpndMem will follow */
-/*! type of an operand can be immediate, register or memory */
-typedef enum LowOpndType {
-  LowOpndType_Imm = 0,
-  LowOpndType_Reg,
-  LowOpndType_Mem,
-  LowOpndType_Label,
-  LowOpndType_NCG,
-  LowOpndType_Chain
-} LowOpndType;
-typedef enum LowOpndDefUse {
-  LowOpndDefUse_Def = 0,
-  LowOpndDefUse_Use,
-  LowOpndDefUse_UseDef
-} LowOpndDefUse;
-
-/*!
-\brief base data structure for an operand */
-typedef struct LowOpnd {
-  LowOpndType type;
-  OpndSize size;
-  LowOpndDefUse defuse;
-} LowOpnd;
-/*!
-\brief data structure for a register operand */
-typedef struct LowOpndReg {
-  LowOpndRegType regType;
-  int logicalReg;
-  int physicalReg;
-} LowOpndReg;
-/*!
-\brief data structure for an immediate operand */
-typedef struct LowOpndImm {
-  union {
-    s4 value;
-    unsigned char bytes[4];
-  };
-} LowOpndImm;
-
-typedef struct LowOpndNCG {
-  union {
-    s4 value;
-    unsigned char bytes[4];
-  };
-} LowOpndNCG;
-
-#define LABEL_SIZE 256
-typedef struct LowOpndLabel {
-  char label[LABEL_SIZE];
-  bool isLocal;
-} LowOpndLabel;
-
-/* get ready for optimizations at LIR
-   add MemoryAccessType & virtualRegNum to memory operands */
-typedef enum MemoryAccessType {
-  MemoryAccess_GLUE,
-  MemoryAccess_VR,
-  MemoryAccess_SPILL,
-  MemoryAccess_Unknown
-} MemoryAccessType;
-typedef enum UseDefEntryType {
-  UseDefType_Ctrl = 0,
-  UseDefType_Float,
-  UseDefType_MemVR,
-  UseDefType_MemSpill,
-  UseDefType_MemUnknown,
-  UseDefType_Reg
-} UseDefEntryType;
-typedef struct UseDefProducerEntry {
-  UseDefEntryType type;
-  int index; //enum PhysicalReg for "Reg" type
-  int producerSlot;
-} UseDefProducerEntry;
-#define MAX_USE_PER_ENTRY 50 /* at most 10 uses for each entry */
-typedef struct UseDefUserEntry {
-  UseDefEntryType type;
-  int index;
-  int useSlots[MAX_USE_PER_ENTRY];
-  int num_uses_per_entry;
-} UseDefUserEntry;
-
-/*!
-\brief data structure for a memory operand */
-typedef struct LowOpndMem {
-  LowOpndImm m_disp;
-  LowOpndImm m_scale;
-  LowOpndReg m_index;
-  LowOpndReg m_base;
-  bool hasScale;
-  MemoryAccessType mType;
-  int index;
-} LowOpndMem;
-
-typedef enum AtomOpCode {
+//! \enum AtomOpCode
+//! \brief Pseudo-mnemonics for Atom
+//! \details Initially included to be in sync with ArmOpCode which specifies
+//! additional pseudo mnemonics for use during codegen, but it has
+//! diverted. Although there are references to this everywhere,
+//! very little of this is actually used for functionality.
+//! \todo Either refactor to match ArmOpCode or remove dependency on this.
+enum AtomOpCode {
     ATOM_PSEUDO_CHAINING_CELL_BACKWARD_BRANCH = -15,
     ATOM_NORMAL_ALU = -14,
     ATOM_PSEUDO_ENTRY_BLOCK = -13,
@@ -313,120 +224,338 @@ typedef enum AtomOpCode {
     ATOM_PSEUDO_EH_BLOCK_LABEL = -2,
     ATOM_PSEUDO_NORMAL_BLOCK_LABEL = -1,
     ATOM_NORMAL,
-} AtomOpCode;
-
-typedef enum DependencyType {
-  Dependency_RAW,
-  Dependency_WAW,
-  Dependency_WAR,
-  Dependency_FLAG
-} DependencyType;
-typedef struct DependencyStruct {
-  DependencyType dType;
-  int nodeId;
-  int latency;
-} DependencyStruct;
-
-typedef struct LowOpBlock {
-  LIR generic;
-  Mnemonic opCode;
-  AtomOpCode opCode2;
-} LowOpBlock;
+};
+
+//! \enum LowOpndType
+//! \brief Defines types of operands that a LowOp can have.
+//! \details The Imm, Mem, and Reg variants correspond literally to what
+//! the final encoded x86 instruction will have. The others are used for
+//! additional behavior needed before the x86 encoding.
+//! \see LowOp
+enum LowOpndType {
+    //! \brief Immediate
+    LowOpndType_Imm,
+    //! \brief Register
+    LowOpndType_Reg,
+    //! \brief Memory access
+    LowOpndType_Mem,
+    //! \brief Used for jumps to labels
+    LowOpndType_Label,
+    //! \brief Used for jumps to other blocks
+    LowOpndType_BlockId,
+    //! \brief Used for chaining
+    LowOpndType_Chain
+};
+
+//! \enum LowOpndDefUse
+//! \brief Defines type of usage that a LowOpnd can have.
+//! \see LowOpnd
+enum LowOpndDefUse {
+    //! \brief Definition
+    LowOpndDefUse_Def,
+    //! \brief Usage
+    LowOpndDefUse_Use,
+    //! \brief Usage and Definition
+    LowOpndDefUse_UseDef
+};
+
+//! \enum MemoryAccessType
+//! \brief Classifies type of memory access.
+enum MemoryAccessType {
+    //! \brief access Dalvik virtual register
+    MemoryAccess_VR,
+    //! \brief access spill region
+    MemoryAccess_SPILL,
+    //! \brief unclassified memory access
+    MemoryAccess_Unknown
+};
+
+//! \enum UseDefEntryType
+//! \brief Defines types of resources on which there can be a dependency.
+enum UseDefEntryType {
+    //! \brief Control flags
+    UseDefType_Ctrl,
+    //! \brief Floating-point stack
+    UseDefType_Float,
+    //! \brief Dalvik virtual register. Corresponds to MemoryAccess_VR
+    UseDefType_MemVR,
+    //! \brief Spill region. Corresponds to MemoryAccess_SPILL
+    UseDefType_MemSpill,
+    //! \brief Unclassified memory access. Corresponds to MemoryAccess_Unknown
+    UseDefType_MemUnknown,
+    //! \brief Register
+    UseDefType_Reg
+};
+
+//! \enum DependencyType
+//! \brief Defines types of dependencies on a resource.
+enum DependencyType {
+    //! \brief Read after Write
+    Dependency_RAW,
+    //! \brief Write after Write
+    Dependency_WAW,
+    //! \brief Write after Read
+    Dependency_WAR,
+};
+
+//! \brief Defines a relationship between a resource and its producer.
+struct UseDefProducerEntry {
+    //! \brief Resource type on which there is a dependency.
+    UseDefEntryType type;
+    //! \brief Logical or physical register this resource is
+    //! associated with.
+    //! \details When physical, this is of enum type PhysicalReg.
+    //! When logical, this is the index of the logical register.
+    //! When there is no register related dependency, this is
+    //! negative.
+    //! \todo Is this correct? What about VRs?
+    int regNum;
+    //! \brief Corresponds to LowOp::slotId to keep track of producer.
+    unsigned int producerSlot;
+};
+
+//! \brief Defines a relationship between a resource and its users.
+struct UseDefUserEntry {
+    //! \brief Resource type on which there is a dependency.
+    UseDefEntryType type;
+    //! \brief Logical or physical register this resource is
+    //! associated with.
+    //! \details When physical, this is of enum type PhysicalReg.
+    //! When logical, this is the index of the logical register.
+    //! When there is no register related dependency, this is
+    //! negative.
+    //! \todo Is this correct? What about VRs?
+    int regNum;
+    //! \brief A list of LowOp::slotId to keep track of all users
+    //! of this resource.
+    std::vector<unsigned int> useSlotsList;
+};
+
+//! \brief Holds information on the data dependencies
+struct DependencyInformation {
+    //! \brief Type of data hazard
+    DependencyType dataHazard;
+    //! \brief Holds the LowOp::slotId of the LIR that causes this
+    //! data dependence.
+    unsigned int lowopSlotId;
+    //! \brief Holds latency information for edges in the
+    //! dependency graph, not execute to execute latency for the
+    //! instructions.
+    int latency;
+};
+
+//! \brief Holds general information about an operand.
+struct LowOpnd {
+    //! \brief Classification of operand.
+    LowOpndType type;
+    //! \brief Size of operand.
+    OpndSize size;
+    //! \brief Usage, definition, or both of operand.
+    LowOpndDefUse defuse;
+};
+
+//! \brief Holds information about a register operand.
+struct LowOpndReg {
+    //! \brief Classification on type of register.
+    LowOpndRegType regType;
+    //! \brief Register number, either logical or physical.
+    int regNum;
+    //! \brief When false, register is logical.
+    bool isPhysical;
+};
+
+//! \brief Holds information about an immediate operand.
+struct LowOpndImm {
+    //! \brief Value of the immediate.
+    s4 value;
+};
 
-/*!
-\brief data structure for a lowered operation */
-typedef struct LowOp {
-  LIR generic;
-  Mnemonic opCode;
-  AtomOpCode opCode2;
-  LowOpnd opnd1;
-  LowOpnd opnd2;
-  int numOperands;
-} LowOp;
-
-typedef struct LowOpLabel {
-  LowOp lop;
-  LowOpndLabel labelOpnd;
-}LowOpLabel;
-
-typedef struct LowOpNCG {
-  LowOp lop;
-  LowOpndNCG ncgOpnd;
-}LowOpNCG;
-
-typedef struct LowOpBlockLabel {
-  LowOpBlock lop;
-  LowOpndImm immOpnd;
-} LowOpBlockLabel;
-
-typedef struct LowOpImm {
-  LowOp lop;
-  LowOpndImm immOpnd;
-} LowOpImm;
-
-typedef struct LowOpMem {
-  LowOp lop;
-  LowOpndMem memOpnd;
-} LowOpMem;
-
-typedef struct LowOpReg {
-  LowOp lop;
-  LowOpndReg regOpnd;
-} LowOpReg;
-
-typedef struct LowOpImmImm {
-  LowOp lop;
-  LowOpndImm immOpnd1;
-  LowOpndImm immOpnd2;
-} LowOpImmImm;
-
-typedef struct LowOpImmReg {
-  LowOp lop;
-  LowOpndImm immOpnd1;
-  LowOpndReg regOpnd2;
-} LowOpImmReg;
-
-typedef struct LowOpImmMem {
-  LowOp lop;
-  LowOpndImm immOpnd1;
-  LowOpndMem memOpnd2;
-} LowOpImmMem;
-
-typedef struct LowOpRegImm {
-  LowOp lop;
-  LowOpndReg regOpnd1;
-  LowOpndImm immOpnd2;
-} LowOpRegImm;
-
-typedef struct LowOpRegReg {
-  LowOp lop;
-  LowOpndReg regOpnd1;
-  LowOpndReg regOpnd2;
-} LowOpRegReg;
-
-typedef struct LowOpRegMem {
-  LowOp lop;
-  LowOpndReg regOpnd1;
-  LowOpndMem memOpnd2;
-} LowOpRegMem;
-
-typedef struct LowOpMemImm {
-  LowOp lop;
-  LowOpndMem memOpnd1;
-  LowOpndImm immOpnd2;
-} LowOpMemImm;
-
-typedef struct LowOpMemReg {
-  LowOp lop;
-  LowOpndMem memOpnd1;
-  LowOpndReg regOpnd2;
-} LowOpMemReg;
-
-typedef struct LowOpMemMem {
-  LowOp lop;
-  LowOpndMem memOpnd1;
-  LowOpndMem memOpnd2;
-} LowOpMemMem;
+//! \brief Holds information about an immediate operand where the immediate
+//! has not been generated yet.
+struct LowOpndBlock {
+    //! \brief Holds id of MIR level basic block.
+    s4 value;
+};
+
+//! \brief Defines maximum length of string holding label name.
+#define LABEL_SIZE 256
+
+//! \brief Holds information about an immediate operand where the immediate
+//! has not been generated yet from label.
+struct LowOpndLabel {
+    //! \brief Name of the label for which to generate immediate.
+    char label[LABEL_SIZE];
+    //! \brief This is true when label is short term distance from caller
+    //! and an 8-bit operand is sufficient.
+    bool isLocal;
+};
+
+//! \brief Holds information about a memory operand.
+struct LowOpndMem {
+    //! \brief Displacement
+    LowOpndImm m_disp;
+    //! \brief Scaling
+    LowOpndImm m_scale;
+    //! \brief Index Register
+    LowOpndReg m_index;
+    //! \brief Base Register
+    LowOpndReg m_base;
+    //! \brief If true, must use the scaling value.
+    bool hasScale;
+    //! \brief Defines type of memory access.
+    MemoryAccessType mType;
+    //! \brief
+    //! \todo What is this used for?
+    int index;
+};
+
+//! \brief Data structure for an x86 LIR.
+//! \todo Decouple fields used for scheduling from this struct.
+//! is a good idea if using it throughout the trace JIT and never
+//! actually passing it for scheduling.
+struct LowOp {
+    //! \brief Holds general LIR information (Google's implementation)
+    //! \warning Only offset information is used for x86 and the other
+    //! fields are not valid except in LowOpBlockLabel.
+    LIR generic;
+    //! \brief x86 mnemonic for instruction
+    Mnemonic opCode;
+    //! \brief x86 pseudo-mnemonic
+    AtomOpCode opCode2;
+    //! \brief Destination operand
+    //! \details This is not used when there are only 0 or 1 operands.
+    LowOpnd opndDest;
+    //! \brief Source operand
+    //! \details This is used when there is a single operand.
+    LowOpnd opndSrc;
+    //! \brief Holds number of operands for this LIR (0, 1, or 2)
+    unsigned short numOperands;
+    //! \brief Logical timestamp for ordering.
+    //! \details This value should uniquely identify an LIR and also
+    //! provide natural ordering depending on when it was requested.
+    //! This is used during scheduling to hold original order for the
+    //! native basic block.
+    unsigned int slotId;
+    //! \brief Logical time for when the LIR is ready.
+    //! \details This field is used only for scheduling.
+    int readyTime;
+    //! \brief Logical time for when the LIR is scheduled.
+    //! \details This field is used only for scheduling.
+    int scheduledTime;
+    //! \brief Execute to execute time for this instruction.
+    //! \details This field is used only for scheduling.
+    //! \see MachineModelEntry::executeToExecuteLatency
+    int instructionLatency;
+    //! \brief Issue port for this instruction.
+    //! \details This field is used only for scheduling.
+    //! \see MachineModelEntry::issuePortType
+    int portType;
+    //! \brief Holds information about LowOps on which current LowOp
+    //! depends on (predecessors).
+    //! \details For example, if a LowOp with slotId of 3 depends on
+    //! LowOp with slotId of 2 because of a RAW, then the LowOp with
+    //! slotId of 3 will have an entry in the predecessorDependencies
+    //! with a Dependency_RAW and slotId of 2. This field is used
+    //! only for scheduling.
+    std::vector<DependencyInformation> predecessorDependencies;
+    //! \brief Holds information about LowOps that depend on current
+    //! LowOp (successors).
+    //! \details For example, if a LowOp with slotId of 3 depends on
+    //! LowOp with slotId of 2 because of a RAW, then the LowOp with
+    //! slotId of 2 will have an entry in the successorDependencies
+    //! with a Dependency_RAW and slotId of 3. This field is used
+    //! only for scheduling.
+    std::vector<DependencyInformation> successorDependencies;
+    //! \brief Weight of longest path in dependency graph from
+    //! current instruction to end of the basic block.
+    //! \details This field is used only for scheduling.
+    int longestPath;
+};
+
+//! \brief Specialized LowOp with known label operand but
+//! whose offset immediate is not known yet.
+struct LowOpLabel : LowOp {
+    //! \brief Label operand whose immediate has not yet been
+    //! generated.
+    LowOpndLabel labelOpnd;
+};
+
+//! \brief Specialized LowOp for use with block operand whose id
+//! is known but the offset immediate has not been generated yet.
+struct LowOpBlock : LowOp {
+    //! \brief Non-generated immediate operand
+    LowOpndBlock blockIdOpnd;
+};
+
+//! \brief Specialized LowOp which is only used with
+//! pseudo-mnemonic.
+//! \see AtomOpCode
+struct LowOpBlockLabel {
+    //! \todo Does not use inheritance like the other LowOp
+    //! data structures because of a git merge issue. In future,
+    //! this can be safely updated.
+    LowOp lop;
+    //! \brief Holds offset information.
+    LowOpndImm immOpnd;
+};
+
+//! \brief Specialized LowOp with an immediate operand.
+struct LowOpImm : LowOp {
+    //! \brief Immediate
+    LowOpndImm immOpnd;
+};
+
+//! \brief Specialized LowOp with a memory operand.
+struct LowOpMem : LowOp {
+    //! \brief Memory Operand
+    LowOpndMem memOpnd;
+};
+
+//! \brief Specialized LowOp with register operand.
+struct LowOpReg : LowOp {
+    //! \brief Register
+    LowOpndReg regOpnd;
+};
+
+//! \brief Specialized LowOp for immediate to register.
+struct LowOpImmReg : LowOp {
+    //! \brief Immediate as source.
+    LowOpndImm immSrc;
+    //! \brief Register as destination.
+    LowOpndReg regDest;
+};
+
+//! \brief Specialized LowOp for register to register.
+struct LowOpRegReg : LowOp {
+    //! \brief Register as source.
+    LowOpndReg regSrc;
+    //! \brief Register as destination.
+    LowOpndReg regDest;
+};
+
+//! \brief Specialized LowOp for memory to register.
+struct LowOpMemReg : LowOp {
+    //! \brief Memory as source.
+    LowOpndMem memSrc;
+    //! \brief Register as destination.
+    LowOpndReg regDest;
+};
+
+//! \brief Specialized LowOp for immediate to memory.
+struct LowOpImmMem : LowOp {
+    //! \brief Immediate as source.
+    LowOpndImm immSrc;
+    //! \brief Memory as destination.
+    LowOpndMem memDest;
+};
+
+//! \brief Specialized LowOp for register to memory.
+struct LowOpRegMem : LowOp {
+    //! \brief Register as source.
+    LowOpndReg regSrc;
+    //! \brief Memory as destination.
+    LowOpndMem memDest;
+};
 
 /*!
 \brief data structure for labels used when lowering a method
@@ -553,8 +682,8 @@ extern LabelMap* VMAPIWorklist;
 extern int ncgClassNum;
 extern int ncgMethodNum;
 
-extern LowOp* lirTable[200]; //Number of LIRs for all bytecodes do not exceed 200
-extern int num_lirs_in_table;
+class Scheduler;
+extern Scheduler g_SchedulerInstance;
 
 bool existATryBlock(Method* method, int startPC, int endPC);
 // interface between register allocator & lowering
@@ -665,10 +794,10 @@ void move_ss_mem_to_reg(LowOp* op, int disp, int base_reg, bool isBasePhysical,
                         int reg, bool isPhysical);
 void move_ss_reg_to_mem(LowOp* op, int reg, bool isPhysical,
                          int disp, int base_reg, bool isBasePhysical);
-LowOpRegMem* move_ss_mem_to_reg_noalloc(int disp, int base_reg, bool isBasePhysical,
+LowOpMemReg* move_ss_mem_to_reg_noalloc(int disp, int base_reg, bool isBasePhysical,
                          MemoryAccessType mType, int mIndex,
                          int reg, bool isPhysical);
-LowOpMemReg* move_ss_reg_to_mem_noalloc(int reg, bool isPhysical,
+LowOpRegMem* move_ss_reg_to_mem_noalloc(int reg, bool isPhysical,
                          int disp, int base_reg, bool isBasePhysical,
                          MemoryAccessType mType, int mIndex);
 void move_sd_mem_to_reg(int disp, int base_reg, bool isBasePhysical,
@@ -680,6 +809,8 @@ void conditional_jump(ConditionCode cc, const char* target, bool isShortTerm);
 void unconditional_jump(const char* target, bool isShortTerm);
 void conditional_jump_int(ConditionCode cc, int target, OpndSize size);
 void unconditional_jump_int(int target, OpndSize size);
+void conditional_jump_block(ConditionCode cc, int targetBlockId);
+void unconditional_jump_block(int targetBlockId);
 void unconditional_jump_reg(int reg, bool isPhysical);
 void call(const char* target);
 void call_reg(int reg, bool isPhysical);
@@ -718,7 +849,7 @@ void push_reg_to_stack(OpndSize size, int reg, bool isPhysical);
 void move_reg_to_mem(OpndSize size,
                       int reg, bool isPhysical,
                       int disp, int base_reg, bool isBasePhysical);
-LowOpRegMem* move_mem_to_reg(OpndSize size,
+LowOpMemReg* move_mem_to_reg(OpndSize size,
                       int disp, int base_reg, bool isBasePhysical,
                       int reg, bool isPhysical);
 void movez_mem_to_reg(OpndSize size,
@@ -786,7 +917,7 @@ void move_reg_to_mem_noalloc(OpndSize size,
                       int reg, bool isPhysical,
                       int disp, int base_reg, bool isBasePhysical,
                       MemoryAccessType mType, int mIndex);
-LowOpRegMem* move_mem_to_reg_noalloc(OpndSize size,
+LowOpMemReg* move_mem_to_reg_noalloc(OpndSize size,
                       int disp, int base_reg, bool isBasePhysical,
                       MemoryAccessType mType, int mIndex,
                       int reg, bool isPhysical);
@@ -1141,19 +1272,16 @@ int op_invoke_super_quick_range();
 void set_reg_opnd(LowOpndReg* op_reg, int reg, bool isPhysical, LowOpndRegType type);
 void set_mem_opnd(LowOpndMem* mem, int disp, int base, bool isPhysical);
 void set_mem_opnd_scale(LowOpndMem* mem, int base, bool isPhysical, int disp, int index, bool indexPhysical, int scale);
-LowOpImm* dump_imm(Mnemonic m, OpndSize size,
-               int imm);
-LowOpNCG* dump_ncg(Mnemonic m, OpndSize size, int imm);
-LowOpImm* dump_imm_with_codeaddr(Mnemonic m, OpndSize size,
-               int imm, char* codePtr);
-LowOpImm* dump_special(AtomOpCode cc, int imm);
+LowOpImm* dump_imm(Mnemonic m, OpndSize size, int imm);
+void dump_imm_update(int imm, char* codePtr, bool updateSecondOperand);
+LowOpBlock* dump_blockid_imm(Mnemonic m, int targetBlockId);
 LowOpMem* dump_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
                int disp, int base_reg, bool isBasePhysical);
 LowOpReg* dump_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
                int reg, bool isPhysical, LowOpndRegType type);
 LowOpReg* dump_reg_noalloc(Mnemonic m, OpndSize size,
                int reg, bool isPhysical, LowOpndRegType type);
-LowOpMemImm* dump_imm_mem_noalloc(Mnemonic m, OpndSize size,
+LowOpImmMem* dump_imm_mem_noalloc(Mnemonic m, OpndSize size,
                            int imm,
                            int disp, int base_reg, bool isBasePhysical,
                            MemoryAccessType mType, int mIndex);
@@ -1163,39 +1291,39 @@ LowOpRegReg* dump_reg_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
 LowOpRegReg* dump_movez_reg_reg(Mnemonic m, OpndSize size,
                         int reg, bool isPhysical,
                         int reg2, bool isPhysical2);
-LowOpRegMem* dump_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
+LowOpMemReg* dump_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
                    int disp, int base_reg, bool isBasePhysical,
                    MemoryAccessType mType, int mIndex,
                    int reg, bool isPhysical, LowOpndRegType type);
-LowOpRegMem* dump_mem_reg_noalloc(Mnemonic m, OpndSize size,
+LowOpMemReg* dump_mem_reg_noalloc(Mnemonic m, OpndSize size,
                            int disp, int base_reg, bool isBasePhysical,
                            MemoryAccessType mType, int mIndex,
                            int reg, bool isPhysical, LowOpndRegType type);
-LowOpRegMem* dump_mem_scale_reg(Mnemonic m, OpndSize size,
+LowOpMemReg* dump_mem_scale_reg(Mnemonic m, OpndSize size,
                          int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
                          int reg, bool isPhysical, LowOpndRegType type);
-LowOpMemReg* dump_reg_mem_scale(Mnemonic m, OpndSize size,
+LowOpRegMem* dump_reg_mem_scale(Mnemonic m, OpndSize size,
                          int reg, bool isPhysical,
                          int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
                          LowOpndRegType type);
-LowOpMemReg* dump_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
+LowOpRegMem* dump_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
                    int reg, bool isPhysical,
                    int disp, int base_reg, bool isBasePhysical,
                    MemoryAccessType mType, int mIndex, LowOpndRegType type);
-LowOpMemReg* dump_reg_mem_noalloc(Mnemonic m, OpndSize size,
+LowOpRegMem* dump_reg_mem_noalloc(Mnemonic m, OpndSize size,
                            int reg, bool isPhysical,
                            int disp, int base_reg, bool isBasePhysical,
                            MemoryAccessType mType, int mIndex, LowOpndRegType type);
-LowOpRegImm* dump_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
+LowOpImmReg* dump_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
                    int imm, int reg, bool isPhysical, LowOpndRegType type, bool chaining);
-LowOpMemImm* dump_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
+LowOpImmMem* dump_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
                    int imm,
                    int disp, int base_reg, bool isBasePhysical,
                    MemoryAccessType mType, int mIndex, bool chaining);
-LowOpMemReg* dump_fp_mem(Mnemonic m, OpndSize size, int reg,
+LowOpRegMem* dump_fp_mem(Mnemonic m, OpndSize size, int reg,
                   int disp, int base_reg, bool isBasePhysical,
                   MemoryAccessType mType, int mIndex);
-LowOpRegMem* dump_mem_fp(Mnemonic m, OpndSize size,
+LowOpMemReg* dump_mem_fp(Mnemonic m, OpndSize size,
                   int disp, int base_reg, bool isBasePhysical,
                   MemoryAccessType mType, int mIndex,
                   int reg);
diff --git a/vm/compiler/codegen/x86/LowerHelper.cpp b/vm/compiler/codegen/x86/LowerHelper.cpp
index 1a70a91..7affeb8 100644
--- a/vm/compiler/codegen/x86/LowerHelper.cpp
+++ b/vm/compiler/codegen/x86/LowerHelper.cpp
@@ -46,12 +46,11 @@ When allocating a physical register for an operand, we can't spill the operands
 #include "NcgHelper.h"
 #include <math.h>
 #include "interp/InterpState.h"
+#include "Scheduler.h"
 
 extern "C" int64_t __divdi3(int64_t, int64_t);
 extern "C" int64_t __moddi3(int64_t, int64_t);
 bool isScratchPhysical;
-LowOp* lirTable[200];
-int num_lirs_in_table = 0;
 
 //4 tables are defined: GPR integer ALU ops, ALU ops in FPU, SSE 32-bit, SSE 64-bit
 //the index to the table is the opcode
@@ -103,291 +102,329 @@ const  Mnemonic map_of_64_opcode_2_mnemonic[] = {
     Mnemonic_Null
 };
 
-////////////////////////////////////////////////
-//!update fields of LowOpndReg
-
-//!
-void set_reg_opnd(LowOpndReg* op_reg, int reg, bool isPhysical, LowOpndRegType type) {
+//! \brief Simplifies update of LowOpndReg fields.
+void set_reg_opnd(LowOpndReg* op_reg, int reg, bool isPhysical,
+        LowOpndRegType type) {
     op_reg->regType = type;
-    if(isPhysical) {
-        op_reg->logicalReg = -1;
-        op_reg->physicalReg = reg;
-    }
-    else
-        op_reg->logicalReg = reg;
-    return;
+    op_reg->regNum = reg;
+    op_reg->isPhysical = isPhysical;
 }
-//!update fields of LowOpndMem
 
-//!
+//! \brief Simplifies update of LowOpndMem fields when only base and
+//! displacement is used.
 void set_mem_opnd(LowOpndMem* mem, int disp, int base, bool isPhysical) {
     mem->m_disp.value = disp;
     mem->hasScale = false;
     mem->m_base.regType = LowOpndRegType_gp;
-    if(isPhysical) {
-        mem->m_base.logicalReg = -1;
-        mem->m_base.physicalReg = base;
-    } else {
-        mem->m_base.logicalReg = base;
-    }
-    return;
+    mem->m_base.regNum = base;
+    mem->m_base.isPhysical = isPhysical;
 }
-//!update fields of LowOpndMem
 
-//!
-void set_mem_opnd_scale(LowOpndMem* mem, int base, bool isPhysical, int disp, int index, bool indexPhysical, int scale) {
+//! \brief Simplifies update of LowOpndMem fields when base, displacement, index,
+//! and scaling is used.
+void set_mem_opnd_scale(LowOpndMem* mem, int base, bool isPhysical, int disp,
+        int index, bool indexPhysical, int scale) {
     mem->hasScale = true;
     mem->m_base.regType = LowOpndRegType_gp;
-    if(isPhysical) {
-        mem->m_base.logicalReg = -1;
-        mem->m_base.physicalReg = base;
-    } else {
-        mem->m_base.logicalReg = base;
-    }
-    if(indexPhysical) {
-        mem->m_index.logicalReg = -1;
-        mem->m_index.physicalReg = index;
-    } else {
-        mem->m_index.logicalReg = index;
-    }
+    mem->m_base.regNum = base;
+    mem->m_base.isPhysical = isPhysical;
+    mem->m_index.regNum = index;
+    mem->m_index.isPhysical = indexPhysical;
     mem->m_disp.value = disp;
     mem->m_scale.value = scale;
-    return;
 }
-//!return either LowOpndRegType_xmm or LowOpndRegType_gp
 
-//!
+//! \brief Return either LowOpndRegType_xmm or LowOpndRegType_gp
+//! depending on operand size.
+//! \param size
 inline LowOpndRegType getTypeFromIntSize(OpndSize size) {
     return size == OpndSize_64 ? LowOpndRegType_xmm : LowOpndRegType_gp;
 }
 
-// copied from JIT compiler
-typedef struct AtomMemBlock {
-    size_t bytesAllocated;
-    struct AtomMemBlock *next;
-    char ptr[0];
-} AtomMemBlock;
-
-#define ATOMBLOCK_DEFAULT_SIZE 4096
-AtomMemBlock *atomMemHead = NULL;
-AtomMemBlock *currentAtomMem = NULL;
-void * atomNew(size_t size) {
-    lowOpTimeStamp++; //one LowOp constructed
-    if(atomMemHead == NULL) {
-        atomMemHead = (AtomMemBlock*)malloc(sizeof(AtomMemBlock) + ATOMBLOCK_DEFAULT_SIZE);
-        if(atomMemHead == NULL) {
-            ALOGE("Memory allocation failed");
-            return NULL;
-        }
-        currentAtomMem = atomMemHead;
-        currentAtomMem->bytesAllocated = 0;
-        currentAtomMem->next = NULL;
-    }
-    size = (size + 3) & ~3;
-    if (size > ATOMBLOCK_DEFAULT_SIZE) {
-        ALOGE("Requesting %d bytes which exceed the maximal size allowed", size);
-        return NULL;
-    }
-retry:
-    if (size + currentAtomMem->bytesAllocated <= ATOMBLOCK_DEFAULT_SIZE) {
-        void *ptr;
-        ptr = &currentAtomMem->ptr[currentAtomMem->bytesAllocated];
-        return ptr;
-    }
-    if (currentAtomMem->next) {
-        currentAtomMem = currentAtomMem->next;
-        goto retry;
-    }
-    /* Time to allocate a new arena */
-    AtomMemBlock *newAtomMem = (AtomMemBlock*)malloc(sizeof(AtomMemBlock) + ATOMBLOCK_DEFAULT_SIZE);
-    if(newAtomMem == NULL) {
-        ALOGE("Memory allocation failed");
+//! \brief Thin layer over encoder that makes scheduling decision and
+//! is used for dumping instruction whose immediate is a target label.
+//! \param m x86 mnemonic
+//! \param size operand size
+//! \param imm When scheduling is disabled, this is the actual immediate.
+//! When scheduling is enabled, this is 0 because immediate has not been
+//! generated yet.
+//! \param label name of label for which we need to generate immediate for
+//! using the label address.
+//! \param isLocal Used to hint the distance from this instruction to label.
+//! When this is true, it means that 8 bits should be enough.
+inline LowOpLabel* lower_label(Mnemonic m, OpndSize size, int imm,
+        const char* label, bool isLocal) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_imm(m, size, imm, stream);
         return NULL;
     }
-    newAtomMem->bytesAllocated = 0;
-    newAtomMem->next = NULL;
-    currentAtomMem->next = newAtomMem;
-    currentAtomMem = newAtomMem;
-    goto retry;
-    ALOGE("atomNew requesting %d bytes", size);
-    return NULL;
-}
-
-void freeAtomMem() {
-    //LOGI("free all atom memory");
-    AtomMemBlock * tmpMem = atomMemHead;
-    while(tmpMem != NULL) {
-        tmpMem->bytesAllocated = 0;
-        tmpMem = tmpMem->next;
-    }
-    currentAtomMem = atomMemHead;
-}
-
-LowOpImm* dump_special(AtomOpCode cc, int imm) {
-    LowOpImm* op = (LowOpImm*)atomNew(sizeof(LowOpImm));
-    op->lop.opCode = Mnemonic_NULL;
-    op->lop.opCode2 = cc;
-    op->lop.opnd1.type = LowOpndType_Imm;
-    op->lop.numOperands = 1;
-    op->immOpnd.value = imm;
-    //stream = encoder_imm(m, size, imm, stream);
+    LowOpLabel * op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpLabel>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Label;
+    op->numOperands = 1;
+    snprintf(op->labelOpnd.label, LABEL_SIZE, "%s", label);
+    op->labelOpnd.isLocal = isLocal;
+    g_SchedulerInstance.updateUseDefInformation_imm(op);
     return op;
 }
 
-LowOpLabel* lower_label(Mnemonic m, OpndSize size, int imm, const char* label, bool isLocal) {
-    stream = encoder_imm(m, size, imm, stream);
-    return NULL;
-}
-
-LowOpLabel* dump_label(Mnemonic m, OpndSize size, int imm,
-               const char* label, bool isLocal) {
+//! \brief Interface to encoder.
+LowOpLabel* dump_label(Mnemonic m, OpndSize size, int imm, const char* label,
+        bool isLocal) {
     return lower_label(m, size, imm, label, isLocal);
 }
 
-LowOpNCG* dump_ncg(Mnemonic m, OpndSize size, int imm) {
-    stream = encoder_imm(m, size, imm, stream);
-    return NULL;
+//! Used for dumping an instruction with a single immediate to the code stream
+//! but the immediate is not yet known because the target MIR block still needs
+//! code generated for it. This is only valid when scheduling is on.
+//! \pre Instruction scheduling must be enabled
+//! \param m x86 mnemonic
+//! \param targetBlockId id of the MIR block
+LowOpBlock* dump_blockid_imm(Mnemonic m, int targetBlockId) {
+    assert(gDvmJit.scheduling && "Scheduling must be turned on before "
+                "calling dump_blockid_imm");
+    LowOpBlock* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpBlock>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndSrc.type = LowOpndType_BlockId;
+    op->numOperands = 1;
+    op->blockIdOpnd.value = targetBlockId;
+    g_SchedulerInstance.updateUseDefInformation_imm(op);
+    return op;
 }
 
-//!update fields of LowOp and generate a x86 instruction with a single immediate operand
-
-//!
-LowOpImm* lower_imm(Mnemonic m, OpndSize size, int imm, bool updateTable) {
-    stream = encoder_imm(m, size, imm, stream);
-    return NULL;
+//! \brief Thin layer over encoder that makes scheduling decision and
+//! is used for dumping instruction with a known immediate.
+//! \param m x86 mnemonic
+//! \param size operand size
+//! \param imm immediate
+LowOpImm* lower_imm(Mnemonic m, OpndSize size, int imm) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_imm(m, size, imm, stream);
+        return NULL;
+    }
+    LowOpImm* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpImm>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Imm;
+    op->numOperands = 1;
+    op->immOpnd.value = imm;
+    g_SchedulerInstance.updateUseDefInformation_imm(op);
+    return op;
 }
 
+//! \brief Interface to encoder.
 LowOpImm* dump_imm(Mnemonic m, OpndSize size, int imm) {
-    return lower_imm(m, size, imm, true);
-}
-
-LowOpImm* dump_imm_with_codeaddr(Mnemonic m, OpndSize size,
-               int imm, char* codePtr) {
-    encoder_imm(m, size, imm, codePtr);
-    return NULL;
-}
-
-//!update fields of LowOp and generate a x86 instruction that takes a single memory operand
-
-//!With NCG O1, we call freeReg to free up physical registers, then call registerAlloc to allocate a physical register for memory base
-LowOpMem* lower_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
-               int disp, int base_reg) {
-    stream = encoder_mem(m, size, disp, base_reg, true, stream);
-    return NULL;
+    return lower_imm(m, size, imm);
+}
+
+//! \brief Used to update the immediate of an instruction already in the
+//! code stream.
+//! \warning This assumes that the instruction to update is already in the
+//! code stream. If it is not, the VM will abort.
+//! \param imm new immediate to use
+//! \param codePtr pointer to location in code stream where the instruction
+//! whose immediate needs updated
+//! \param updateSecondOperand This is true when second operand needs updated
+void dump_imm_update(int imm, char* codePtr, bool updateSecondOperand) {
+    // These encoder call do not need to go through scheduler since they need
+    // to be dumped at a specific location in code stream. However, there might
+    // be a request to update a location which hasn't had code lowered, but this
+    // shouldn't happen.
+    if(codePtr > stream) { //! \warning Assumes code stream is incremental
+        LOGE("ERROR: Trying to update the immediate of an instruction, "
+                "but instruction is not in code stream yet!");
+        dvmAbort();
+    }
+    if(updateSecondOperand)
+        encoder_update_imm_rm(imm, codePtr);
+    else // update first operand
+        encoder_update_imm(imm, codePtr);
+}
+
+//! \brief Thin layer over encoder that makes scheduling decision and
+//! is used for dumping instruction with a single memory operand.
+//! \param m x86 mnemonic
+//! \param m2 Atom pseudo-mnemonic
+//! \param size operand size
+//! \param disp displacement offset
+//! \param base_reg physical register (PhysicalReg type) or a logical register
+//! \param isBasePhysical notes if base_reg is a physical register. It must
+//! be true when scheduling is enabled or else VM will abort.
+LowOpMem* lower_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_mem(m, size, disp, base_reg, isBasePhysical, stream);
+        return NULL;
+    }
+    if (!isBasePhysical)
+        dvmAbort();
+    LowOpMem* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpMem>();
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Mem;
+    op->numOperands = 1;
+    op->memOpnd.mType = MemoryAccess_Unknown;
+    op->memOpnd.index = -1;
+    set_mem_opnd(&(op->memOpnd), disp, base_reg, isBasePhysical);
+    g_SchedulerInstance.updateUseDefInformation_mem(op);
+    return op;
 }
 
-LowOpMem* dump_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
-               int disp, int base_reg, bool isBasePhysical) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+//! \brief Interface to encoder which includes register allocation
+//! decision.
+//! \details With NCG O1, call freeReg to free up physical registers,
+//! then call registerAlloc to allocate a physical register for memory base
+LowOpMem* dump_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         freeReg(true);
         //type of the base is gpr
-        int regAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
-        return lower_mem(m, m2, size, disp, regAll);
+        int regAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+        return lower_mem(m, m2, size, disp, regAll, true /*isBasePhysical*/);
     } else {
-        stream = encoder_mem(m, size, disp, base_reg, isBasePhysical, stream);
-        return NULL;
+        return lower_mem(m, m2, size, disp, base_reg, isBasePhysical);
     }
 }
-//!update fields of LowOp and generate a x86 instruction that takes a single reg operand
 
-//!With NCG O1, wecall freeReg to free up physical registers, then call registerAlloc to allocate a physical register for the single operand
-LowOpReg* lower_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-               int reg, LowOpndRegType type) {
-    stream = encoder_reg(m, size, reg, true, type, stream);
-    return NULL;
+//!update fields of LowOp and generate a x86 instruction that takes a single reg operand
+LowOpReg* lower_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        LowOpndRegType type, bool isPhysical) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_reg(m, size, reg, isPhysical, type, stream);
+        return NULL;
+    }
+    if (!isPhysical)
+        dvmAbort();
+    LowOpReg* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpReg>();
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 1;
+    set_reg_opnd(&(op->regOpnd), reg, isPhysical, type);
+    g_SchedulerInstance.updateUseDefInformation_reg(op);
+    return op;
 }
 
-LowOpReg* dump_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-               int reg, bool isPhysical, LowOpndRegType type) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+//!With NCG O1, wecall freeReg to free up physical registers, then call registerAlloc to allocate a physical register for the single operand
+LowOpReg* dump_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        bool isPhysical, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         freeReg(true);
-        if(m == Mnemonic_MUL || m == Mnemonic_IDIV) {
-            //these two instructions use eax & edx implicitly
+        if (m == Mnemonic_MUL || m == Mnemonic_IMUL || m == Mnemonic_DIV
+                || m == Mnemonic_IDIV) {
+            //these four instructions use eax & edx implicitly
             touchEax();
             touchEdx();
         }
         int regAll = registerAlloc(type, reg, isPhysical, true);
-        return lower_reg(m, m2, size, regAll, type);
+        return lower_reg(m, m2, size, regAll, type, true /*isPhysical*/);
     } else {
-        stream = encoder_reg(m, size, reg, isPhysical, type, stream);
-        return NULL;
+        return lower_reg(m, m2, size, reg, type, isPhysical);
     }
 }
-LowOpReg* dump_reg_noalloc(Mnemonic m, OpndSize size,
-               int reg, bool isPhysical, LowOpndRegType type) {
-    return lower_reg(m, ATOM_NORMAL, size, reg, type);
+
+LowOpReg* dump_reg_noalloc(Mnemonic m, OpndSize size, int reg, bool isPhysical,
+        LowOpndRegType type) {
+    return lower_reg(m, ATOM_NORMAL, size, reg, type, true /*isPhysical*/);
 }
 
-LowOpRegReg* lower_reg_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-                 int reg, int reg2, LowOpndRegType type) {
-    if(m == Mnemonic_FUCOMP || m == Mnemonic_FUCOM) {
-        stream = encoder_compare_fp_stack(m == Mnemonic_FUCOMP,
-                                          reg-reg2, size==OpndSize_64, stream);
-    }
-    else {
-        stream = encoder_reg_reg(m, size, reg, true, reg2, true, type, stream);
+LowOpRegReg* lower_reg_to_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int regSrc,
+        bool isPhysical, int regDest, bool isPhysical2, LowOpndRegType type) {
+    if (!gDvmJit.scheduling) {
+        if (m == Mnemonic_FUCOMP || m == Mnemonic_FUCOM) {
+            stream = encoder_compare_fp_stack(m == Mnemonic_FUCOMP, regSrc - regDest,
+                    size == OpndSize_64, stream);
+        } else {
+            stream = encoder_reg_reg(m, size, regSrc, isPhysical, regDest,
+                    isPhysical2, type, stream);
+        }
+        return NULL;
     }
-    return NULL;
+    if (!isPhysical && !isPhysical2)
+        dvmAbort();
+    LowOpRegReg* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpRegReg>();
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Reg;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regDest), regDest, isPhysical2, type);
+    set_reg_opnd(&(op->regSrc), regSrc, isPhysical, type);
+    g_SchedulerInstance.updateUseDefInformation_reg_to_reg(op);
+    return op;
 }
 
 //!update fields of LowOp and generate a x86 instruction that takes two reg operands
 
-//Here, both registers are physical
-LowOpRegReg* dump_reg_reg_noalloc(Mnemonic m, OpndSize size,
-                           int reg, bool isPhysical,
-                           int reg2, bool isPhysical2, LowOpndRegType type) {
-    return lower_reg_reg(m, ATOM_NORMAL, size, reg, reg2, type);
+//!Here, both registers are physical
+LowOpRegReg* dump_reg_reg_noalloc(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
+    return lower_reg_to_reg(m, ATOM_NORMAL, size, reg, true /*isPhysical*/, reg2,
+            true /*isPhysical2*/, type);
 }
 
 inline bool isMnemonicMove(Mnemonic m) {
-    return (m == Mnemonic_MOV || m == Mnemonic_MOVQ ||
-            m == Mnemonic_MOVSS || m == Mnemonic_MOVSD);
+    return (m == Mnemonic_MOV || m == Mnemonic_MOVQ || m == Mnemonic_MOVSS
+            || m == Mnemonic_MOVSD);
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes two reg operands
 
 //!here dst reg is already allocated to a physical reg
 //! we should not spill the physical register for dst when allocating for src
-LowOpRegReg* dump_reg_reg_noalloc_dst(Mnemonic m, OpndSize size,
-                               int reg, bool isPhysical,
-                               int reg2, bool isPhysical2, LowOpndRegType type) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpRegReg* dump_reg_reg_noalloc_dst(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         int regAll = registerAlloc(type, reg, isPhysical, true);
         /* remove move from one register to the same register */
-        if(isMnemonicMove(m) && regAll == reg2) return NULL;
-        return lower_reg_reg(m, ATOM_NORMAL, size, regAll, reg2, type);
+        if (isMnemonicMove(m) && regAll == reg2)
+            return NULL;
+        return lower_reg_to_reg(m, ATOM_NORMAL, size, regAll, true /*isPhysical*/,
+                reg2, true /*isPhysical2*/, type);
     } else {
-        stream = encoder_reg_reg(m, size, reg, isPhysical, reg2, isPhysical2, type, stream);
-        return NULL;
+        return lower_reg_to_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg2,
+                isPhysical2, type);
     }
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes two reg operands
 
 //!here src reg is already allocated to a physical reg
 LowOpRegReg* dump_reg_reg_noalloc_src(Mnemonic m, AtomOpCode m2, OpndSize size,
-                               int reg, bool isPhysical,
-                               int reg2, bool isPhysical2, LowOpndRegType type) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+        int reg, bool isPhysical, int reg2, bool isPhysical2,
+        LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         int regAll2;
         if(isMnemonicMove(m) && checkTempReg2(reg2, type, isPhysical2, reg, -1)) { //dst reg is logical
             //only from get_virtual_reg_all
             regAll2 = registerAllocMove(reg2, type, isPhysical2, reg);
         } else {
             regAll2 = registerAlloc(type, reg2, isPhysical2, true);
-            return lower_reg_reg(m, m2, size, reg, regAll2, type);
+            return lower_reg_to_reg(m, m2, size, reg, true /*isPhysical*/, regAll2,
+                    true /*isPhysical2*/, type);
         }
     } else {
-        stream = encoder_reg_reg(m, size, reg, isPhysical, reg2, isPhysical2, type, stream);
-        return NULL;
+        return lower_reg_to_reg(m, m2, size, reg, isPhysical, reg2, isPhysical2,
+                type);
     }
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes two reg operands
 
 //!
-LowOpRegReg* dump_reg_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int reg, bool isPhysical,
-                   int reg2, bool isPhysical2, LowOpndRegType type) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpRegReg* dump_reg_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        bool isPhysical, int reg2, bool isPhysical2, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
         //reg is source if m is MOV
         freeReg(true);
@@ -396,103 +433,124 @@ LowOpRegReg* dump_reg_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
         LowOpRegReg* op = NULL;
 #ifdef MOVE_OPT2
         if(isMnemonicMove(m) &&
-           ((reg != PhysicalReg_EDI && reg != PhysicalReg_ESP && reg != PhysicalReg_EBP) || (!isPhysical)) &&
-           isPhysical2 == false) { //dst reg is logical
+                ((reg != PhysicalReg_EDI && reg != PhysicalReg_ESP && reg != PhysicalReg_EBP) || (!isPhysical)) &&
+                isPhysical2 == false) { //dst reg is logical
             //called from move_reg_to_reg
             regAll2 = registerAllocMove(reg2, type, isPhysical2, regAll);
         } else {
 #endif
-            donotSpillReg(regAll);
-            regAll2 = registerAlloc(type, reg2, isPhysical2, true);
-            op = lower_reg_reg(m, m2, size, regAll, regAll2, type);
+        donotSpillReg(regAll);
+        regAll2 = registerAlloc(type, reg2, isPhysical2, true);
+        op = lower_reg_to_reg(m, m2, size, regAll, true /*isPhysical*/, regAll2,
+                true /*isPhysical2*/, type);
 #ifdef MOVE_OPT2
-        }
+    }
 #endif
         endNativeCode();
         return op;
-    }
-    else {
-        stream = encoder_reg_reg(m, size, reg, isPhysical, reg2, isPhysical2, type, stream);
+    } else {
+        return lower_reg_to_reg(m, m2, size, reg, isPhysical, reg2, isPhysical2,
+                type);
     }
     return NULL;
 }
 
-LowOpRegMem* lower_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-                 int disp, int base_reg,
-                 MemoryAccessType mType, int mIndex,
-                 int reg, LowOpndRegType type, bool isMoves) {
-    if(m == Mnemonic_MOVSX) {
-        stream = encoder_moves_mem_to_reg(size, disp, base_reg, true,
-                                          reg, true, stream);
-    }
-    else if(m == Mnemonic_MOVZX) {
-        stream = encoder_movez_mem_to_reg(size, disp, base_reg, true,
-                                          reg, true, stream);
-    }
-    else {
-        stream = encoder_mem_reg(m, size, disp, base_reg, true,
-                                 reg, true, type, stream);
+LowOpMemReg* lower_mem_to_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
+        int reg, bool isPhysical, LowOpndRegType type) {
+    bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
+    OpndSize overridden_size = isMovzs ? OpndSize_32 : size;
+    LowOpndRegType overridden_type = isMovzs ? LowOpndRegType_gp : type;
+    if (!gDvmJit.scheduling) {
+        stream = encoder_mem_to_reg_2(m, size, disp, base_reg, isBasePhysical,
+                overridden_size, reg, isPhysical, overridden_type, stream);
+        return NULL;
     }
-    return NULL;
+    if (!isBasePhysical && !isPhysical)
+        dvmAbort();
+    LowOpMemReg* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpMemReg>();
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = overridden_size;
+    op->opndDest.type = LowOpndType_Reg;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Mem;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regDest), reg, isPhysical, overridden_type);
+    set_mem_opnd(&(op->memSrc), disp, base_reg, isBasePhysical);
+    op->memSrc.mType = mType;
+    op->memSrc.index = mIndex;
+    g_SchedulerInstance.updateUseDefInformation_mem_to_reg(op);
+    return op;
 }
 
 //!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
 
 //!Here, operands are already allocated to physical registers
-LowOpRegMem* dump_mem_reg_noalloc(Mnemonic m, OpndSize size,
-                           int disp, int base_reg, bool isBasePhysical,
-                           MemoryAccessType mType, int mIndex,
-                           int reg, bool isPhysical, LowOpndRegType type) {
-    return lower_mem_reg(m, ATOM_NORMAL, size, disp, base_reg, mType, mIndex, reg, type, false);
+LowOpMemReg* dump_mem_reg_noalloc(Mnemonic m, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
+        int reg, bool isPhysical, LowOpndRegType type) {
+    return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, base_reg,
+            true /*isBasePhysical*/, mType, mIndex, reg, true /*isPhysical*/,
+            type);
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
 
 //!Here, memory operand is already allocated to physical register
-LowOpRegMem* dump_mem_reg_noalloc_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
-                               int disp, int base_reg, bool isBasePhysical,
-                               MemoryAccessType mType, int mIndex,
-                               int reg, bool isPhysical, LowOpndRegType type) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpMemReg* dump_mem_reg_noalloc_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
+        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
+        int mIndex, int reg, bool isPhysical, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         int regAll = registerAlloc(type, reg, isPhysical, true);
-        return lower_mem_reg(m, m2, size, disp, base_reg, mType, mIndex, regAll, type, false);
+        return lower_mem_to_reg(m, m2, size, disp, base_reg,
+                true /*isBasePhysical*/, mType, mIndex, regAll,
+                true /*isPhysical*/, type);
     } else {
-        stream = encoder_mem_reg(m, size, disp, base_reg, isBasePhysical,
-                                 reg, isPhysical, type, stream);
+        return lower_mem_to_reg(m, m2, size, disp, base_reg, isBasePhysical, mType,
+                mIndex, reg, isPhysical, type);
     }
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
 
 //!
-LowOpRegMem* dump_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int disp, int base_reg, bool isBasePhysical,
-                   MemoryAccessType mType, int mIndex,
-                   int reg, bool isPhysical, LowOpndRegType type) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpMemReg* dump_mem_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex,
+        int reg, bool isPhysical, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
         freeReg(true);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
         //it is okay to use the same physical register
-        if(isMnemonicMove(m)) {
+        if (isMnemonicMove(m)) {
             freeReg(true);
         } else {
             donotSpillReg(baseAll);
         }
         int regAll = registerAlloc(type, reg, isPhysical, true);
         endNativeCode();
-        return lower_mem_reg(m, m2, size, disp, baseAll, mType, mIndex, regAll, type, false);
+        return lower_mem_to_reg(m, m2, size, disp, baseAll,
+                true /*isBasePhysical*/, mType, mIndex, regAll,
+                true /*isPhysical*/, type);
     } else {
-        stream = encoder_mem_reg(m, size, disp, base_reg, isBasePhysical,
-                                 reg, isPhysical, type, stream);
+        return lower_mem_to_reg(m, m2, size, disp, base_reg, isBasePhysical, mType,
+                mIndex, reg, isPhysical, type);
     }
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
 
 //!
-LowOpRegMem* dump_moves_mem_reg(Mnemonic m, OpndSize size,
+LowOpMemReg* dump_moves_mem_reg(Mnemonic m, OpndSize size,
                          int disp, int base_reg, bool isBasePhysical,
              int reg, bool isPhysical) {
+#if 0 /* Commented out because it is dead code. If re-enabling, this needs to be updated
+         to work with instruction scheduling and cannot call encoder directly. Please see
+         dump_movez_mem_reg for an example */
     if(gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
         freeReg(true);
@@ -505,25 +563,30 @@ LowOpRegMem* dump_moves_mem_reg(Mnemonic m, OpndSize size,
     } else {
         stream = encoder_moves_mem_to_reg(size, disp, base_reg, isBasePhysical, reg, isPhysical, stream);
     }
+#endif
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
 
 //!
-LowOpRegMem* dump_movez_mem_reg(Mnemonic m, OpndSize size,
-             int disp, int base_reg, bool isBasePhysical,
-             int reg, bool isPhysical) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpMemReg* dump_movez_mem_reg(Mnemonic m, OpndSize size, int disp,
+        int base_reg, bool isBasePhysical, int reg, bool isPhysical) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
         freeReg(true);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
         donotSpillReg(baseAll);
         int regAll = registerAlloc(LowOpndRegType_gp, reg, isPhysical, true);
         endNativeCode();
-        return lower_mem_reg(m, ATOM_NORMAL, size, disp, baseAll, MemoryAccess_Unknown, -1,
-            regAll, LowOpndRegType_gp, true/*moves*/);
+        return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, baseAll,
+                true /*isBasePhysical*/, MemoryAccess_Unknown, -1, regAll,
+                true /*isPhysical*/, LowOpndRegType_gp);
     } else {
-        stream = encoder_movez_mem_to_reg(size, disp, base_reg, isBasePhysical, reg, isPhysical, stream);
+        return lower_mem_to_reg(m, ATOM_NORMAL, size, disp, base_reg,
+                isBasePhysical, MemoryAccess_Unknown, -1, reg, isPhysical,
+                LowOpndRegType_gp);
     }
     return NULL;
 }
@@ -534,6 +597,9 @@ LowOpRegMem* dump_movez_mem_reg(Mnemonic m, OpndSize size,
 LowOpRegReg* dump_movez_reg_reg(Mnemonic m, OpndSize size,
              int reg, bool isPhysical,
              int reg2, bool isPhysical2) {
+#if 0 /* Commented out because it is dead code. If re-enabling, this needs to be updated
+         to work with instruction scheduling and cannot call encoder directly. Please see
+         dump_movez_mem_reg for an example */
     LowOpRegReg* op = (LowOpRegReg*)atomNew(sizeof(LowOpRegReg));
     op->lop.opCode = m;
     op->lop.opnd1.size = OpndSize_32;
@@ -557,229 +623,387 @@ LowOpRegReg* dump_movez_reg_reg(Mnemonic m, OpndSize size,
         stream = encoder_movez_reg_to_reg(size, reg, isPhysical, reg2,
                                         isPhysical2, LowOpndRegType_gp, stream);
     }
+#endif
     return NULL;
 }
 
 //!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
 
 //!
-LowOpRegMem* lower_mem_scale_reg(Mnemonic m, OpndSize size, int base_reg, int disp, int index_reg,
-                 int scale, int reg, LowOpndRegType type) {
+LowOpMemReg* lower_mem_scale_to_reg(Mnemonic m, OpndSize size, int base_reg,
+        bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical,
+        int scale, int reg, bool isPhysical, LowOpndRegType type) {
     bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
-    if(isMovzs)
-        stream = encoder_movzs_mem_disp_scale_reg(m, size, base_reg, true, disp, index_reg, true,
-                                                  scale, reg, true, type, stream);
-    else {
-        if(disp == 0)
-            stream = encoder_mem_scale_reg(m, size, base_reg, true, index_reg, true,
-                                           scale, reg, true, type, stream);
-        else
-            stream = encoder_mem_disp_scale_reg(m, size, base_reg, true, disp, index_reg, true,
-                                                scale, reg, true, type, stream);
+    OpndSize overridden_size = isMovzs ? OpndSize_32 : size;
+    LowOpndRegType overridden_type = isMovzs ? LowOpndRegType_gp : type;
+    if (!gDvmJit.scheduling) {
+        stream = encoder_mem_disp_scale_to_reg_2(m, size, base_reg, isBasePhysical,
+                disp, index_reg, isIndexPhysical, scale, overridden_size, reg,
+                isPhysical, overridden_type, stream);
+        return NULL;
     }
-    return NULL;
+    if (!isBasePhysical && !isIndexPhysical && !isPhysical)
+        dvmAbort();
+    LowOpMemReg* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpMemReg>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndDest.size = overridden_size;
+    op->opndDest.type = LowOpndType_Reg;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Mem;
+    op->numOperands = 2;
+    op->memSrc.mType = MemoryAccess_Unknown;
+    op->memSrc.index = -1;
+    set_reg_opnd(&(op->regDest), reg, isPhysical, overridden_type);
+    set_mem_opnd_scale(&(op->memSrc), base_reg, isBasePhysical, disp,
+            index_reg, isIndexPhysical, scale);
+    g_SchedulerInstance.updateUseDefInformation_mem_scale_to_reg(op);
+    return op;
 }
 
-LowOpRegMem* dump_mem_scale_reg(Mnemonic m, OpndSize size,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         int reg, bool isPhysical, LowOpndRegType type) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpMemReg* dump_mem_scale_reg(Mnemonic m, OpndSize size, int base_reg,
+        bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical,
+        int scale, int reg, bool isPhysical, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
         freeReg(true);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
         donotSpillReg(baseAll); //make sure index will not use the same physical reg
-        int indexAll = registerAlloc(LowOpndRegType_gp, index_reg, isIndexPhysical, true);
-        if(isMnemonicMove(m)) {
+        int indexAll = registerAlloc(LowOpndRegType_gp, index_reg,
+                isIndexPhysical, true);
+        if (isMnemonicMove(m)) {
             freeReg(true);
             doSpillReg(baseAll); //base can be used now
         } else {
             donotSpillReg(indexAll);
         }
         bool isMovzs = (m == Mnemonic_MOVZX || m == Mnemonic_MOVSX);
-        int regAll = registerAlloc(isMovzs ? LowOpndRegType_gp : type, reg, isPhysical, true);
+        int regAll = registerAlloc(isMovzs ? LowOpndRegType_gp : type, reg,
+                isPhysical, true);
         endNativeCode();
-        return lower_mem_scale_reg(m, size, baseAll, disp, indexAll, scale, regAll, type);
+        return lower_mem_scale_to_reg(m, size, baseAll, true /*isBasePhysical*/,
+                disp, indexAll, true /*isIndexPhysical*/, scale, regAll,
+                true /*isPhysical*/, type);
     } else {
-        stream = encoder_mem_scale_reg(m, size, base_reg, isBasePhysical, index_reg,
-                                       isIndexPhysical, scale, reg, isPhysical, type, stream);
+        return lower_mem_scale_to_reg(m, size, base_reg, isBasePhysical, disp,
+                index_reg, isIndexPhysical, scale, reg, isPhysical, type);
     }
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
 
 //!
-LowOpMemReg* lower_reg_mem_scale(Mnemonic m, OpndSize size, int reg,
-                 int base_reg, int disp, int index_reg, int scale, LowOpndRegType type) {
-    if(disp == 0)
-        stream = encoder_reg_mem_scale(m, size, reg, true, base_reg, true,
-                                       index_reg, true, scale, type, stream);
-    else
-        stream = encoder_reg_mem_disp_scale(m, size, reg, true, base_reg, true,
-                                            disp, index_reg, true, scale, type, stream);
-    return NULL;
+LowOpRegMem* lower_reg_to_mem_scale(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int base_reg, bool isBasePhysical, int disp,
+        int index_reg, bool isIndexPhysical, int scale, LowOpndRegType type) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_reg_mem_disp_scale(m, size, reg, isPhysical, base_reg,
+                isBasePhysical, disp, index_reg, isIndexPhysical, scale, type,
+                stream);
+        return NULL;
+    }
+    if (!isBasePhysical && !isIndexPhysical && !isPhysical)
+        dvmAbort();
+    LowOpRegMem* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpRegMem>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Mem;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    op->memDest.mType = MemoryAccess_Unknown;
+    op->memDest.index = -1;
+    set_reg_opnd(&(op->regSrc), reg, isPhysical, type);
+    set_mem_opnd_scale(&(op->memDest), base_reg, isBasePhysical, disp,
+            index_reg, isIndexPhysical, scale);
+    g_SchedulerInstance.updateUseDefInformation_reg_to_mem_scale(op);
+    return op;
 }
 
-LowOpMemReg* dump_reg_mem_scale(Mnemonic m, OpndSize size,
-                         int reg, bool isPhysical,
-                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         LowOpndRegType type) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpRegMem* dump_reg_mem_scale(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int base_reg, bool isBasePhysical, int disp,
+        int index_reg, bool isIndexPhysical, int scale, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
         freeReg(true);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
         donotSpillReg(baseAll);
-        int indexAll = registerAlloc(LowOpndRegType_gp, index_reg, isIndexPhysical, true);
+        int indexAll = registerAlloc(LowOpndRegType_gp, index_reg,
+                isIndexPhysical, true);
         donotSpillReg(indexAll);
         int regAll = registerAlloc(type, reg, isPhysical, true);
         endNativeCode();
-        return lower_reg_mem_scale(m, size, regAll, baseAll, disp, indexAll, scale, type);
+        return lower_reg_to_mem_scale(m, size, regAll, true /*isPhysical*/,
+                baseAll, true /*isBasePhysical*/, disp, indexAll,
+                true /*isIndexPhysical*/, scale, type);
     } else {
-        stream = encoder_reg_mem_scale(m, size, reg, isPhysical, base_reg, isBasePhysical,
-                                       index_reg, isIndexPhysical, scale, type, stream);
+        return lower_reg_to_mem_scale(m, size, reg, isPhysical, base_reg,
+                isBasePhysical, disp, index_reg, isIndexPhysical, scale, type);
     }
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
 
 //!Here operands are already allocated
-LowOpMemReg* lower_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
-                 int disp, int base_reg, MemoryAccessType mType, int mIndex,
-                 LowOpndRegType type) {
-    stream = encoder_reg_mem(m, size, reg, true, disp, base_reg, true, type, stream);
-    return NULL;
+LowOpRegMem* lower_reg_to_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        bool isPhysical, int disp, int base_reg, bool isBasePhysical,
+        MemoryAccessType mType, int mIndex, LowOpndRegType type) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_reg_mem(m, size, reg, isPhysical, disp, base_reg,
+                isBasePhysical, type, stream);
+        return NULL;
+    }
+    if (!isBasePhysical && !isPhysical)
+        dvmAbort();
+    LowOpRegMem* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpRegMem>();
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Mem;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regSrc), reg, isPhysical, type);
+    set_mem_opnd(&(op->memDest), disp, base_reg, isBasePhysical);
+    op->memDest.mType = mType;
+    op->memDest.index = mIndex;
+    g_SchedulerInstance.updateUseDefInformation_reg_to_mem(op);
+    return op;
 }
 
-LowOpMemReg* dump_reg_mem_noalloc(Mnemonic m, OpndSize size,
-                           int reg, bool isPhysical,
-                           int disp, int base_reg, bool isBasePhysical,
-                           MemoryAccessType mType, int mIndex, LowOpndRegType type) {
-    return lower_reg_mem(m, ATOM_NORMAL, size, reg, disp, base_reg, mType, mIndex, type);
+LowOpRegMem* dump_reg_mem_noalloc(Mnemonic m, OpndSize size, int reg,
+        bool isPhysical, int disp, int base_reg, bool isBasePhysical,
+        MemoryAccessType mType, int mIndex, LowOpndRegType type) {
+    return lower_reg_to_mem(m, ATOM_NORMAL, size, reg, true /*isPhysical*/, disp,
+            base_reg, true /*isBasePhysical*/, mType, mIndex, type);
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one reg operand and one mem operand
 
 //!
-LowOpMemReg* dump_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int reg, bool isPhysical,
-                   int disp, int base_reg, bool isBasePhysical,
-                   MemoryAccessType mType, int mIndex, LowOpndRegType type) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpRegMem* dump_reg_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int reg,
+        bool isPhysical, int disp, int base_reg, bool isBasePhysical,
+        MemoryAccessType mType, int mIndex, LowOpndRegType type) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         startNativeCode(-1, -1);
         freeReg(true);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
         donotSpillReg(baseAll);
         int regAll = registerAlloc(type, reg, isPhysical, true);
         endNativeCode();
-        return lower_reg_mem(m, m2, size, regAll, disp, baseAll, mType, mIndex, type);
+        return lower_reg_to_mem(m, m2, size, regAll, true /*isPhysical*/, disp,
+                baseAll, true /*isBasePhysical*/, mType, mIndex, type);
     } else {
-        stream = encoder_reg_mem(m, size, reg, isPhysical, disp, base_reg, isBasePhysical, type, stream);
+        return lower_reg_to_mem(m, m2, size, reg, isPhysical, disp, base_reg,
+                isBasePhysical, mType, mIndex, type);
     }
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one immediate and one reg operand
 
 //!The reg operand is allocated already
-LowOpRegImm* lower_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-                 int imm, int reg, LowOpndRegType type, bool chaining) {
-    stream = encoder_imm_reg(m, size, imm, reg, true, type, stream);
-    return NULL;
+LowOpImmReg* lower_imm_to_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
+        int reg, bool isPhysical, LowOpndRegType type, bool chaining) {
+    // size of opnd1 can be different from size of opnd2:
+    OpndSize overridden_size =
+            (m == Mnemonic_SAL || m == Mnemonic_SHR || m == Mnemonic_SHL
+                    || m == Mnemonic_SAR || m == Mnemonic_ROR) ?
+                    OpndSize_8 : size;
+    if (!gDvmJit.scheduling) {
+        // No need to pass overridden size to encoder because it has same logic
+        // for determining size of immediate
+        stream = encoder_imm_reg(m, size, imm, reg, isPhysical, type, stream);
+        return NULL;
+    }
+    if (!isPhysical)
+        dvmAbort();
+    LowOpImmReg* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpImmReg>();
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    op->opndSrc.size = overridden_size;
+    op->opndSrc.type = chaining ? LowOpndType_Chain : LowOpndType_Imm;
+    set_reg_opnd(&(op->regDest), reg, isPhysical, type);
+    op->immSrc.value = imm;
+    g_SchedulerInstance.updateUseDefInformation_imm_to_reg(op);
+    return op;
 }
 
-LowOpRegImm* dump_imm_reg_noalloc(Mnemonic m, OpndSize size,
-                           int imm, int reg, bool isPhysical, LowOpndRegType type) {
-    return lower_imm_reg(m, ATOM_NORMAL, size, imm, reg, type, false);
+LowOpImmReg* dump_imm_reg_noalloc(Mnemonic m, OpndSize size, int imm, int reg,
+        bool isPhysical, LowOpndRegType type) {
+    return lower_imm_to_reg(m, ATOM_NORMAL, size, imm, reg, true /*isPhysical*/,
+            type, false);
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one immediate and one reg operand
 
 //!
-LowOpRegImm* dump_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int imm, int reg, bool isPhysical, LowOpndRegType type, bool chaining) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpImmReg* dump_imm_reg(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
+        int reg, bool isPhysical, LowOpndRegType type, bool chaining) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         freeReg(true);
         int regAll = registerAlloc(type, reg, isPhysical, true);
-        return lower_imm_reg(m, m2, size, imm, regAll, type, chaining);
+        return lower_imm_to_reg(m, m2, size, imm, regAll, true /*isPhysical*/,
+                type, chaining);
     } else {
-        stream = encoder_imm_reg(m, size, imm, reg, isPhysical, type, stream);
+        return lower_imm_to_reg(m, m2, size, imm, reg, isPhysical, type, chaining);
     }
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one immediate and one mem operand
 
 //!The mem operand is already allocated
-LowOpMemImm* lower_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
-                 int disp, int base_reg, MemoryAccessType mType, int mIndex,
-                 bool chaining) {
-    stream = encoder_imm_mem(m, size, imm, disp, base_reg, true, stream);
-    return NULL;
+LowOpImmMem* lower_imm_to_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
+        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
+        int mIndex, bool chaining) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_imm_mem(m, size, imm, disp, base_reg, isBasePhysical,
+                stream);
+        return NULL;
+    }
+    if (!isBasePhysical)
+        dvmAbort();
+    LowOpImmMem* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpImmMem>();
+    op->opCode = m;
+    op->opCode2 = m2;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Mem;
+    op->opndSrc.size = size;
+    op->opndSrc.type = chaining ? LowOpndType_Chain : LowOpndType_Imm;
+    op->numOperands = 2;
+    set_mem_opnd(&(op->memDest), disp, base_reg, isBasePhysical);
+    op->immSrc.value = imm;
+    op->memDest.mType = mType;
+    op->memDest.index = mIndex;
+    g_SchedulerInstance.updateUseDefInformation_imm_to_mem(op);
+    return op;
 }
 
-LowOpMemImm* dump_imm_mem_noalloc(Mnemonic m, OpndSize size,
-                           int imm,
-                           int disp, int base_reg, bool isBasePhysical,
-                           MemoryAccessType mType, int mIndex) {
-    return lower_imm_mem(m, ATOM_NORMAL, size, imm, disp, base_reg, mType, mIndex, false);
+LowOpImmMem* dump_imm_mem_noalloc(Mnemonic m, OpndSize size, int imm, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex) {
+    return lower_imm_to_mem(m, ATOM_NORMAL, size, imm, disp, base_reg,
+            true /*isBasePhysical*/, mType, mIndex, false);
 }
+
 //!update fields of LowOp and generate a x86 instruction that takes one immediate and one mem operand
 
 //!
-LowOpMemImm* dump_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size,
-                   int imm,
-                   int disp, int base_reg, bool isBasePhysical,
-                   MemoryAccessType mType, int mIndex, bool chaining) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpImmMem* dump_imm_mem(Mnemonic m, AtomOpCode m2, OpndSize size, int imm,
+        int disp, int base_reg, bool isBasePhysical, MemoryAccessType mType,
+        int mIndex, bool chaining) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         /* do not free register if the base is %edi, %esp, or %ebp
-           make sure dump_imm_mem will only generate a single instruction */
-        if(!isBasePhysical || (base_reg != PhysicalReg_EDI &&
-                               base_reg != PhysicalReg_ESP &&
-                               base_reg != PhysicalReg_EBP)) {
+         make sure dump_imm_mem will only generate a single instruction */
+        if (!isBasePhysical
+                || (base_reg != PhysicalReg_EDI && base_reg != PhysicalReg_ESP
+                        && base_reg != PhysicalReg_EBP)) {
             freeReg(true);
         }
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
-        return lower_imm_mem(m, m2, size, imm, disp, baseAll, mType, mIndex, chaining);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+        return lower_imm_to_mem(m, m2, size, imm, disp, baseAll,
+                true /*isBasePhysical*/, mType, mIndex, chaining);
     } else {
-        stream = encoder_imm_mem(m, size, imm, disp, base_reg, isBasePhysical, stream);
+        return lower_imm_to_mem(m, m2, size, imm, disp, base_reg, isBasePhysical,
+                mType, mIndex, chaining);
     }
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that uses the FP stack and takes one mem operand
 
 //!
-LowOpMemReg* lower_fp_mem(Mnemonic m, OpndSize size, int reg,
-                  int disp, int base_reg, MemoryAccessType mType, int mIndex) {
-    stream = encoder_fp_mem(m, size, reg, disp, base_reg, true, stream);
-    return NULL;
+LowOpRegMem* lower_fp_to_mem(Mnemonic m, OpndSize size, int reg, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_fp_mem(m, size, reg, disp, base_reg, isBasePhysical,
+                stream);
+        return NULL;
+    }
+    if (!isBasePhysical)
+        dvmAbort();
+    LowOpRegMem* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpRegMem>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Mem;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Reg;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regSrc), PhysicalReg_ST0 + reg, true,
+            LowOpndRegType_fs);
+    set_mem_opnd(&(op->memDest), disp, base_reg, isBasePhysical);
+    op->memDest.mType = mType;
+    op->memDest.index = mIndex;
+    g_SchedulerInstance.updateUseDefInformation_fp_to_mem(op);
+    return op;
 }
 
-LowOpMemReg* dump_fp_mem(Mnemonic m, OpndSize size, int reg,
-                  int disp, int base_reg, bool isBasePhysical,
-                  MemoryAccessType mType, int mIndex) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpRegMem* dump_fp_mem(Mnemonic m, OpndSize size, int reg, int disp,
+        int base_reg, bool isBasePhysical, MemoryAccessType mType, int mIndex) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         freeReg(true);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
-        return lower_fp_mem(m, size, reg, disp, baseAll, mType, mIndex);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+        return lower_fp_to_mem(m, size, reg, disp, baseAll,
+                true /*isBasePhysical*/, mType, mIndex);
     } else {
-        stream = encoder_fp_mem(m, size, reg, disp, base_reg, isBasePhysical, stream);
+        return lower_fp_to_mem(m, size, reg, disp, base_reg, isBasePhysical, mType,
+                mIndex);
     }
     return NULL;
 }
+
 //!update fields of LowOp and generate a x86 instruction that uses the FP stack and takes one mem operand
 
 //!
-LowOpRegMem* lower_mem_fp(Mnemonic m, OpndSize size, int disp, int base_reg,
-                 MemoryAccessType mType, int mIndex, int reg) {
-    stream = encoder_mem_fp(m, size, disp, base_reg, true, reg, stream);
-    return NULL;
+LowOpMemReg* lower_mem_to_fp(Mnemonic m, OpndSize size, int disp, int base_reg,
+        bool isBasePhysical, MemoryAccessType mType, int mIndex, int reg) {
+    if (!gDvmJit.scheduling) {
+        stream = encoder_mem_fp(m, size, disp, base_reg, isBasePhysical, reg,
+                stream);
+        return NULL;
+    }
+    if (!isBasePhysical)
+        dvmAbort();
+    LowOpMemReg* op = g_SchedulerInstance.allocateNewEmptyLIR<LowOpMemReg>();
+    op->opCode = m;
+    op->opCode2 = ATOM_NORMAL;
+    op->opndDest.size = size;
+    op->opndDest.type = LowOpndType_Reg;
+    op->opndSrc.size = size;
+    op->opndSrc.type = LowOpndType_Mem;
+    op->numOperands = 2;
+    set_reg_opnd(&(op->regDest), PhysicalReg_ST0 + reg, true,
+            LowOpndRegType_fs);
+    set_mem_opnd(&(op->memSrc), disp, base_reg, isBasePhysical);
+    op->memSrc.mType = mType;
+    op->memSrc.index = mIndex;
+    g_SchedulerInstance.updateUseDefInformation_mem_to_fp(op);
+    return op;
 }
 
-LowOpRegMem* dump_mem_fp(Mnemonic m, OpndSize size,
-                  int disp, int base_reg, bool isBasePhysical,
-                  MemoryAccessType mType, int mIndex,
-                  int reg) {
-    if(gDvm.executionMode == kExecutionModeNcgO1) {
+LowOpMemReg* dump_mem_fp(Mnemonic m, OpndSize size, int disp, int base_reg,
+        bool isBasePhysical, MemoryAccessType mType, int mIndex, int reg) {
+    if (gDvm.executionMode == kExecutionModeNcgO1) {
         freeReg(true);
-        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical, true);
-        return lower_mem_fp(m, size, disp, baseAll, mType, mIndex, reg);
+        int baseAll = registerAlloc(LowOpndRegType_gp, base_reg, isBasePhysical,
+                true);
+        return lower_mem_to_fp(m, size, disp, baseAll, true /*isBasePhysical*/,
+                mType, mIndex, reg);
     } else {
-        stream = encoder_mem_fp(m, size, disp, base_reg, isBasePhysical, reg, stream);
+        return lower_mem_to_fp(m, size, disp, base_reg, isBasePhysical, mType,
+                mIndex, reg);
     }
     return NULL;
 }
@@ -1083,9 +1307,9 @@ void compare_imm_reg(OpndSize size, int imm,
         if(gDvm.executionMode == kExecutionModeNcgO1) {
             freeReg(true);
             int regAll = registerAlloc(type, reg, isPhysical, true);
-            lower_reg_reg(m, ATOM_NORMAL, size, regAll, regAll, type);
+            lower_reg_to_reg(m, ATOM_NORMAL, size, regAll, true /*isPhysical*/, regAll, true /*isPhysical2*/, type);
         } else {
-            stream = encoder_reg_reg(m, size, reg, isPhysical, reg, isPhysical, type, stream);
+            lower_reg_to_reg(m, ATOM_NORMAL, size, reg, isPhysical, reg, isPhysical, type);
         }
         return;
     }
@@ -1178,16 +1402,25 @@ void compare_sd_reg_with_reg(LowOp* op, int reg1, bool isPhysical1,
 //!
 void compare_fp_stack(bool pop, int reg, bool isDouble) { //compare ST(0) with ST(reg)
     Mnemonic m = pop ? Mnemonic_FUCOMP : Mnemonic_FUCOM;
-    lower_reg_reg(m, ATOM_NORMAL, isDouble ? OpndSize_64 : OpndSize_32,
-                  PhysicalReg_ST0+reg, PhysicalReg_ST0, LowOpndRegType_fs);
+    lower_reg_to_reg(m, ATOM_NORMAL, isDouble ? OpndSize_64 : OpndSize_32,
+                  PhysicalReg_ST0+reg, true /*isPhysical*/, PhysicalReg_ST0, true /*isPhysical2*/, LowOpndRegType_fs);
 }
+
 /*!
 \brief generate a single return instruction
 
 */
-LowOp* lower_return() {
-    stream = encoder_return(stream);
-    return NULL;
+inline LowOp* lower_return() {
+    if (gDvm.executionMode == kExecutionModeNcgO0 || !gDvmJit.scheduling) {
+        stream = encoder_return(stream);
+        return NULL;
+    }
+    LowOp * op = g_SchedulerInstance.allocateNewEmptyLIR<LowOp>();
+    op->numOperands = 0;
+    op->opCode = Mnemonic_RET;
+    op->opCode2 = ATOM_NORMAL;
+    g_SchedulerInstance.updateUseDefInformation(op);
+    return op;
 }
 
 void x86_return() {
@@ -1446,7 +1679,7 @@ void move_reg_to_mem_noalloc(OpndSize size,
 //!move from memory to reg
 
 //!
-LowOpRegMem* move_mem_to_reg(OpndSize size,
+LowOpMemReg* move_mem_to_reg(OpndSize size,
                       int disp, int base_reg, bool isBasePhysical,
                       int reg, bool isPhysical) {
     Mnemonic m = (size == OpndSize_64) ? Mnemonic_MOVQ : Mnemonic_MOV;
@@ -1455,7 +1688,7 @@ LowOpRegMem* move_mem_to_reg(OpndSize size,
 //!move from memory to reg
 
 //!Operands are already allocated
-LowOpRegMem* move_mem_to_reg_noalloc(OpndSize size,
+LowOpMemReg* move_mem_to_reg_noalloc(OpndSize size,
                   int disp, int base_reg, bool isBasePhysical,
                   MemoryAccessType mType, int mIndex,
                   int reg, bool isPhysical) {
@@ -1465,7 +1698,7 @@ LowOpRegMem* move_mem_to_reg_noalloc(OpndSize size,
 //!movss from memory to reg
 
 //!Operands are already allocated
-LowOpRegMem* move_ss_mem_to_reg_noalloc(int disp, int base_reg, bool isBasePhysical,
+LowOpMemReg* move_ss_mem_to_reg_noalloc(int disp, int base_reg, bool isBasePhysical,
                  MemoryAccessType mType, int mIndex,
                  int reg, bool isPhysical) {
     return dump_mem_reg_noalloc(Mnemonic_MOVSS, OpndSize_32, disp, base_reg, isBasePhysical, mType, mIndex, reg, isPhysical, LowOpndRegType_xmm);
@@ -1473,7 +1706,7 @@ LowOpRegMem* move_ss_mem_to_reg_noalloc(int disp, int base_reg, bool isBasePhysi
 //!movss from reg to memory
 
 //!Operands are already allocated
-LowOpMemReg* move_ss_reg_to_mem_noalloc(int reg, bool isPhysical,
+LowOpRegMem* move_ss_reg_to_mem_noalloc(int reg, bool isPhysical,
                  int disp, int base_reg, bool isBasePhysical,
                  MemoryAccessType mType, int mIndex) {
     return dump_reg_mem_noalloc(Mnemonic_MOVSS, OpndSize_32, reg, isPhysical, disp, base_reg, isBasePhysical, mType, mIndex, LowOpndRegType_xmm);
@@ -1484,8 +1717,7 @@ LowOpMemReg* move_ss_reg_to_mem_noalloc(int reg, bool isPhysical,
 void movez_mem_to_reg(OpndSize size,
                int disp, int base_reg, bool isBasePhysical,
                int reg, bool isPhysical) {
-    Mnemonic m = Mnemonic_MOVZX;
-    dump_movez_mem_reg(m, size, disp, base_reg, isBasePhysical, reg, isPhysical);
+    dump_movez_mem_reg(Mnemonic_MOVZX, size, disp, base_reg, isBasePhysical, reg, isPhysical);
 }
 
 //!movzx from one reg to another reg
@@ -1514,7 +1746,6 @@ void moves_mem_disp_scale_to_reg(OpndSize size,
                   disp, index_reg, isIndexPhysical, scale,
                   reg, isPhysical, LowOpndRegType_gp);
 }
-
 //!movsx from memory to reg
 
 //!
diff --git a/vm/compiler/codegen/x86/LowerInvoke.cpp b/vm/compiler/codegen/x86/LowerInvoke.cpp
index ae2bbca..80d8b0f 100644
--- a/vm/compiler/codegen/x86/LowerInvoke.cpp
+++ b/vm/compiler/codegen/x86/LowerInvoke.cpp
@@ -573,10 +573,11 @@ int common_invokeMethod_Jmp(ArgsDoneType form) {
     //    start of HotChainingCell for next bytecode: -4(%esp)
     //    start of InvokeSingletonChainingCell for callee: -8(%esp)
     load_effective_addr(-8, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
-    insertChainingWorklist(traceCurrentBB->fallThrough->id, stream);
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->fallThrough->id, stream);
     move_chain_to_mem(OpndSize_32, traceCurrentBB->fallThrough->id, 4, PhysicalReg_ESP, true);
     // for honeycomb: JNI call doesn't need a chaining cell, so the taken branch is null
-    if(traceCurrentBB->taken)
+    if(!gDvmJit.scheduling && traceCurrentBB->taken)
         insertChainingWorklist(traceCurrentBB->taken->id, stream);
     int takenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
     move_chain_to_mem(OpndSize_32, takenId, 0, PhysicalReg_ESP, true);
@@ -1378,7 +1379,8 @@ void predicted_chain_interface_O0(u2 tmp) {
     /* set up arguments for dvmJitToPatchPredictedChain */
     load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
     move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
-    insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
     move_chain_to_mem(OpndSize_32, traceCurrentBB->taken->id, 8, PhysicalReg_ESP, true); //predictedChainCell
     move_reg_to_mem(OpndSize_32, P_GPR_3, true, 12, PhysicalReg_ESP, true);
     scratchRegs[0] = PhysicalReg_EAX;
@@ -1446,7 +1448,8 @@ void predicted_chain_interface_O1(u2 tmp) {
     load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
     move_reg_to_mem(OpndSize_32, PhysicalReg_EAX, true, 0, PhysicalReg_ESP, true);
     move_reg_to_mem(OpndSize_32, PhysicalReg_SCRATCH_7, isScratchPhysical, 4, PhysicalReg_ESP, true);
-    insertChainingWorklist(traceCurrentBB->taken->id, stream);
+   if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
     move_chain_to_mem(OpndSize_32, traceCurrentBB->taken->id, 8, PhysicalReg_ESP, true); //predictedChainCell
     move_reg_to_mem(OpndSize_32, 40, false, 12, PhysicalReg_ESP, true);
     scratchRegs[0] = PhysicalReg_SCRATCH_8;
@@ -1480,7 +1483,8 @@ void predicted_chain_virtual_O0(u2 IMMC) {
     /* set up arguments for dvmJitToPatchPredictedChain */
     load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
     move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 0,  PhysicalReg_ESP, true);
-    insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
     move_chain_to_mem(OpndSize_32, traceCurrentBB->taken->id, 8, PhysicalReg_ESP, true); //predictedChainCell
     move_reg_to_mem(OpndSize_32, P_GPR_3, true, 12, PhysicalReg_ESP, true);
     scratchRegs[0] = PhysicalReg_EAX;
@@ -1522,7 +1526,7 @@ void predicted_chain_virtual_O1(u2 IMMC) {
     load_effective_addr(-16, PhysicalReg_ESP, true, PhysicalReg_ESP, true);
     move_reg_to_mem(OpndSize_32, PhysicalReg_ECX, true, 0, PhysicalReg_ESP, true);
     move_reg_to_mem(OpndSize_32, PhysicalReg_SCRATCH_7, isScratchPhysical, 4, PhysicalReg_ESP, true);
-    if(traceCurrentBB->taken)
+    if(!gDvmJit.scheduling && traceCurrentBB->taken)
         insertChainingWorklist(traceCurrentBB->taken->id, stream);
     int traceTakenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
     move_chain_to_mem(OpndSize_32, traceTakenId, 8, PhysicalReg_ESP, true); //predictedChainCell
@@ -1556,7 +1560,8 @@ void gen_predicted_chain_O0(bool isRange, u2 tmp, int IMMC, bool isInterface, in
     /* get predicted clazz
        get predicted method
     */
-    insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
     move_chain_to_reg(OpndSize_32, traceCurrentBB->taken->id, P_GPR_1, true); //predictedChainCell
     move_mem_to_reg(OpndSize_32, offChainingCell_clazz, P_GPR_1, true, P_SCRATCH_2, true);//predicted clazz
     move_mem_to_reg(OpndSize_32, offChainingCell_method, P_GPR_1, true, PhysicalReg_ECX, true);//predicted method
@@ -1601,7 +1606,8 @@ void gen_predicted_chain_O0(bool isRange, u2 tmp, int IMMC, bool isInterface, in
     move_imm_to_reg(OpndSize_32, 0xeeee, PhysicalReg_EBX, true);
     scratchRegs[0] = PhysicalReg_EAX;
     call_debug_dumpSwitch(); //%ebx, %eax, %edx
-    insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
     move_chain_to_reg(OpndSize_32, traceCurrentBB->taken->id, PhysicalReg_EBX, true);
     call_debug_dumpSwitch();
 #endif
@@ -1618,7 +1624,8 @@ void gen_predicted_chain_O0(bool isRange, u2 tmp, int IMMC, bool isInterface, in
     move_imm_to_reg(OpndSize_32, 0xdddd, PhysicalReg_EBX, true);
     scratchRegs[0] = PhysicalReg_EAX;
     call_debug_dumpSwitch(); //%ebx, %eax, %edx
-    insertChainingWorklist(traceCurrentBB->taken->id, stream);
+    if(!gDvmJit.scheduling)
+        insertChainingWorklist(traceCurrentBB->taken->id, stream);
     move_chain_to_reg(OpndSize_32, traceCurrentBB->taken->id, PhysicalReg_EBX, true);
     call_debug_dumpSwitch();
     move_reg_to_reg(OpndSize_32, PhysicalReg_ECX, true, PhysicalReg_EBX, true);
@@ -1643,7 +1650,7 @@ void gen_predicted_chain_O1(bool isRange, u2 tmp, int IMMC, bool isInterface, in
     /* get predicted clazz
        get predicted method
     */
-    if(traceCurrentBB->taken)
+    if(!gDvmJit.scheduling && traceCurrentBB->taken)
         insertChainingWorklist(traceCurrentBB->taken->id, stream);
     int traceTakenId = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
     move_chain_to_reg(OpndSize_32, traceTakenId, 41, false); //predictedChainCell
diff --git a/vm/compiler/codegen/x86/LowerJump.cpp b/vm/compiler/codegen/x86/LowerJump.cpp
index b181763..8a46b09 100644
--- a/vm/compiler/codegen/x86/LowerJump.cpp
+++ b/vm/compiler/codegen/x86/LowerJump.cpp
@@ -26,6 +26,7 @@
 #include "enc_wrapper.h"
 #include "interp/InterpDefs.h"
 #include "NcgHelper.h"
+#include "Scheduler.h"
 
 LabelMap* globalMap;
 LabelMap* globalShortMap;//make sure for each bytecode, there is no duplicated label
@@ -94,7 +95,7 @@ int updateJumpInst(char* jumpInst, OpndSize immSize, int relativeNCG) {
             dvmAbort();
         }
     }
-    encoder_update_imm(relativeNCG, jumpInst);
+    dump_imm_update(relativeNCG, jumpInst, false);
     return 0;
 }
 
@@ -123,6 +124,9 @@ int insertLabel(const char* label, bool checkDup) {
         return 0;
     }
 
+    if (gDvmJit.scheduling)
+        g_SchedulerInstance.signalEndOfNativeBasicBlock(); /* will update stream */
+
     item = (LabelMap*)malloc(sizeof(LabelMap));
     if(item == NULL) {
         ALOGE("Memory allocation failed");
@@ -396,7 +400,7 @@ int updateImmRMInst(char* moveInst, const char* label, int relativeNCG) {
 #ifdef DEBUG_NCG
     ALOGI("perform work ImmRM inst @ %p for label %s with %d", moveInst, label, relativeNCG);
 #endif
-    encoder_update_imm_rm(relativeNCG, moveInst);
+    dump_imm_update(relativeNCG, moveInst, true);
     return 0;
 }
 //! maximum instruction size for jump,jcc,call: 6 for jcc rel32
@@ -537,7 +541,8 @@ void conditional_jump(ConditionCode cc, const char* target, bool isShortTerm) {
     bool unknown;
     OpndSize size;
     int imm = 0;
-    imm = getRelativeOffset(target, isShortTerm, JmpCall_cond, &unknown, &size);
+    if(!gDvmJit.scheduling)
+        imm = getRelativeOffset(target, isShortTerm, JmpCall_cond, &unknown, &size);
     dump_label(m, size, imm, target, isShortTerm);
 }
 /*!
@@ -572,7 +577,8 @@ void unconditional_jump(const char* target, bool isShortTerm) {
         }
     }
     int imm = 0;
-    imm = getRelativeOffset(target, isShortTerm, JmpCall_uncond, &unknown, &size);
+    if(!gDvmJit.scheduling)
+        imm = getRelativeOffset(target, isShortTerm, JmpCall_uncond, &unknown, &size);
     dump_label(m, size, imm, target, isShortTerm);
     if(gDvm.executionMode == kExecutionModeNcgO1) {
         if(!strncmp(target, ".invokeArgsDone", 15)) {
@@ -586,7 +592,7 @@ void unconditional_jump(const char* target, bool isShortTerm) {
 */
 void conditional_jump_int(ConditionCode cc, int target, OpndSize size) {
     Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
-    dump_ncg(m, size, target);
+    dump_imm(m, size, target);
 }
 /*!
 \brief generate a single native instruction "jmp imm"
@@ -594,8 +600,28 @@ void conditional_jump_int(ConditionCode cc, int target, OpndSize size) {
 */
 void unconditional_jump_int(int target, OpndSize size) {
     Mnemonic m = Mnemonic_JMP;
-    dump_ncg(m, size, target);
+    dump_imm(m, size, target);
 }
+
+//! Used to generate a single native instruction for conditionally
+//! jumping to a block when the immediate is not yet known.
+//! This should only be used when instruction scheduling is enabled.
+//! \param cc type of conditional jump
+//! \param targetBlockId id of MIR basic block
+void conditional_jump_block(ConditionCode cc, int targetBlockId) {
+    Mnemonic m = (Mnemonic)(Mnemonic_Jcc + cc);
+    dump_blockid_imm(m, targetBlockId);
+}
+
+//! Used to generate a single native instruction for unconditionally
+//! jumping to a block when the immediate is not yet known.
+//! This should only be used when instruction scheduling is enabled.
+//! \param targetBlockId id of MIR basic block
+void unconditional_jump_block(int targetBlockId) {
+    Mnemonic m = Mnemonic_JMP;
+    dump_blockid_imm(m, targetBlockId);
+}
+
 /*!
 \brief generate a single native instruction "jmp reg"
 
@@ -617,7 +643,8 @@ void call(const char* target) {
     bool dummy;
     OpndSize size;
     int relOffset = 0;
-    relOffset = getRelativeOffset(target, false, JmpCall_call, &dummy, &size);
+    if(!gDvmJit.scheduling)
+        relOffset = getRelativeOffset(target, false, JmpCall_call, &dummy, &size);
     dump_label(m, size, relOffset, target, false);
     if(gDvm.executionMode == kExecutionModeNcgO1) {
         afterCall(target);
@@ -915,16 +942,68 @@ int common_backwardBranch() {
     unconditional_jump_reg(PhysicalReg_EAX, true);
     return 0;
 }
+#if !defined(WITH_JIT)
+/*!
+\brief common code to handle GOTO
+
+If it is a backward branch, call common_periodicChecks4 to handle GC request.
+Since this is the end of a basic block, constVREndOfBB and globalVREndOfBB are called right before the jump instruction.
+*/
+int common_goto(s4 tmp) { //tmp: relativePC
+    if(tmp < 0) {
+#ifdef ENABLE_TRACING
+#if !defined(TRACING_OPTION2)
+        insertMapWorklist(offsetPC + tmp, mapFromBCtoNCG[offsetPC+tmp], 1);
+#endif
+        //(target offsetPC * 2)
+        move_imm_to_reg(OpndSize_32, 2*(offsetPC+tmp), PhysicalReg_EDX, true);
+#endif
+        //call( ... ) will dump VRs to memory first
+        //potential garbage collection will work as designed
+        call_helper_API("common_periodicChecks4");
+    }
+    constVREndOfBB();
+    globalVREndOfBB(currentMethod);
+    bool unknown;
+    OpndSize size;
+    int relativeNCG = tmp;
+    if(!gDvmJit.scheduling)
+        relativeNCG = getRelativeNCG(tmp, JmpCall_uncond, &unknown, &size);
+    unconditional_jump_int(relativeNCG, size);
+    return 0;
+}
+//the common code to lower a if bytecode
+int common_if(s4 tmp, ConditionCode cc_next, ConditionCode cc_taken) {
+    if(tmp < 0) { //backward
+        conditional_jump(cc_next, ".if_next", true);
+        common_goto(tmp);
+        insertLabel(".if_next", true);
+    }
+    else {
+        //if(tmp < 0) LOGI("skip periodicCheck for if");
+        bool unknown;
+        OpndSize size;
+        int relativeNCG = tmp;
+        if(!gDvmJit.scheduling)
+            relativeNCG = getRelativeNCG(tmp, JmpCall_cond, &unknown, &size); //must be known
+        conditional_jump_int(cc_taken, relativeNCG, size); //CHECK
+    }
+    return 0;
+}
+#else
 //when this is called from JIT, there is no need to check GC
-int common_goto(s4 tmp) { //tmp: target basic block id
+int common_goto(s4 targetBlockId) {
     bool unknown;
     OpndSize size;
     constVREndOfBB();
     globalVREndOfBB(currentMethod);
 
-    int relativeNCG = tmp;
-    relativeNCG = getRelativeNCG(tmp, JmpCall_uncond, &unknown, &size);
-    unconditional_jump_int(relativeNCG, size);
+    if(gDvmJit.scheduling) {
+        unconditional_jump_block((int)targetBlockId);
+    } else {
+        int relativeNCG = getRelativeNCG(targetBlockId, JmpCall_uncond, &unknown, &size);
+        unconditional_jump_int(relativeNCG, size);
+    }
     return 1;
 }
 
@@ -936,45 +1015,76 @@ int common_if(s4 tmp, ConditionCode cc_next, ConditionCode cc) {
     if (traceMode == kJitLoop && !branchInLoop && hasVRStoreExitOfLoop()) {
         if (traceCurrentBB->taken && traceCurrentBB->taken->blockType == kChainingCellNormal) {
             conditional_jump(cc, ".vr_store_at_loop_exit", true);
-            relativeNCG = traceCurrentBB->fallThrough ? traceCurrentBB->fallThrough->id : 0;
-            if(!scheduling_is_on && traceCurrentBB->fallThrough)
-                relativeNCG = getRelativeNCG(traceCurrentBB->fallThrough->id, JmpCall_uncond, &unknown, &size);
-            unconditional_jump_int(relativeNCG, size);
+
+            if(gDvmJit.scheduling && traceCurrentBB->fallThrough) {
+                unconditional_jump_block(traceCurrentBB->fallThrough->id);
+            } else {
+                relativeNCG = traceCurrentBB->fallThrough ? traceCurrentBB->fallThrough->id : 0;
+                if(traceCurrentBB->fallThrough)
+                    relativeNCG = getRelativeNCG(traceCurrentBB->fallThrough->id, JmpCall_uncond, &unknown, &size);
+                unconditional_jump_int(relativeNCG, size);
+            }
+
             insertLabel(".vr_store_at_loop_exit", true);
             storeVRExitOfLoop();
-            relativeNCG = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
-            if(!scheduling_is_on && traceCurrentBB->taken)
-                relativeNCG = getRelativeNCG(traceCurrentBB->taken->id, JmpCall_uncond, &unknown, &size);
-            unconditional_jump_int(relativeNCG, size);
+
+            if(gDvmJit.scheduling && traceCurrentBB->taken) {
+                unconditional_jump_block(traceCurrentBB->taken->id);
+            } else {
+                relativeNCG = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
+                if(traceCurrentBB->taken)
+                    relativeNCG = getRelativeNCG(traceCurrentBB->taken->id, JmpCall_uncond, &unknown, &size);
+                unconditional_jump_int(relativeNCG, size);
+            }
         }
         else if (traceCurrentBB->fallThrough && traceCurrentBB->fallThrough->blockType == kChainingCellNormal) {
             conditional_jump(cc_next, ".vr_store_at_loop_exit", true);
-            relativeNCG = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
-            if(!scheduling_is_on && traceCurrentBB->taken)
-                relativeNCG = getRelativeNCG(traceCurrentBB->taken->id, JmpCall_uncond, &unknown, &size);
-            unconditional_jump_int(relativeNCG, size);
+
+            if(gDvmJit.scheduling && traceCurrentBB->taken) {
+                unconditional_jump_block(traceCurrentBB->taken->id);
+            } else {
+                relativeNCG = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
+                if(traceCurrentBB->taken)
+                    relativeNCG = getRelativeNCG(traceCurrentBB->taken->id, JmpCall_uncond, &unknown, &size);
+                unconditional_jump_int(relativeNCG, size);
+            }
+
             insertLabel(".vr_store_at_loop_exit", true);
             storeVRExitOfLoop();
-            relativeNCG = traceCurrentBB->fallThrough ? traceCurrentBB->fallThrough->id : 0;
-            if(!scheduling_is_on && traceCurrentBB->fallThrough)
-                relativeNCG = getRelativeNCG(traceCurrentBB->fallThrough->id, JmpCall_uncond, &unknown, &size);
-            unconditional_jump_int(relativeNCG, size);
+
+            if(gDvmJit.scheduling && traceCurrentBB->fallThrough) {
+                unconditional_jump_block(traceCurrentBB->fallThrough->id);
+            } else {
+                relativeNCG = traceCurrentBB->fallThrough ? traceCurrentBB->fallThrough->id : 0;
+                if(traceCurrentBB->fallThrough)
+                    relativeNCG = getRelativeNCG(traceCurrentBB->fallThrough->id, JmpCall_uncond, &unknown, &size);
+                unconditional_jump_int(relativeNCG, size);
+            }
         }
         else
            LOGE("ERROR in common_if\n");
     }
     else {
-        relativeNCG = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
-        if(!scheduling_is_on && traceCurrentBB->taken)
-            relativeNCG = getRelativeNCG(traceCurrentBB->taken->id, JmpCall_cond, &unknown, &size);
-        conditional_jump_int(cc, relativeNCG, size);
-        relativeNCG = traceCurrentBB->fallThrough ? traceCurrentBB->fallThrough->id : 0;
-        if(!scheduling_is_on && traceCurrentBB->fallThrough)
-            relativeNCG = getRelativeNCG(traceCurrentBB->fallThrough->id, JmpCall_uncond, &unknown, &size);
-        unconditional_jump_int(relativeNCG, size);
+        if(gDvmJit.scheduling) {
+            if(traceCurrentBB->taken)
+                conditional_jump_block(cc, traceCurrentBB->taken->id);
+            if(traceCurrentBB->fallThrough)
+                unconditional_jump_block(traceCurrentBB->fallThrough->id);
+        } else {
+            relativeNCG = traceCurrentBB->taken ? traceCurrentBB->taken->id : 0;
+            if(traceCurrentBB->taken)
+                relativeNCG = getRelativeNCG(traceCurrentBB->taken->id, JmpCall_cond, &unknown, &size);
+            conditional_jump_int(cc, relativeNCG, size);
+
+            relativeNCG = traceCurrentBB->fallThrough ? traceCurrentBB->fallThrough->id : 0;
+            if(traceCurrentBB->fallThrough)
+                relativeNCG = getRelativeNCG(traceCurrentBB->fallThrough->id, JmpCall_uncond, &unknown, &size);
+            unconditional_jump_int(relativeNCG, size);
+        }
     }
     return 2;
 }
+#endif
 
 /*!
 \brief helper function to handle null object error
diff --git a/vm/compiler/codegen/x86/Schedule.cpp b/vm/compiler/codegen/x86/Schedule.cpp
new file mode 100644
index 0000000..94d0b6b
--- /dev/null
+++ b/vm/compiler/codegen/x86/Schedule.cpp
@@ -0,0 +1,1748 @@
+/*
+ * Copyright (C) 2010-2011 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*! \file Schedule.cpp
+    \brief This file implements the Atom Instruction Scheduler.
+    \details Scheduling algorithm implemented is basic block scheduling.
+*/
+
+#include "Lower.h"
+#include "interp/InterpDefs.h"
+#include "Scheduler.h"
+
+//! \brief Used to replace all calls to printf to calls to LOGD.
+//! \details Needs to be commented out when running on host.
+#define printf LOGD
+
+//! \def DISABLE_ATOM_SCHEDULING_STATISTICS
+//! \brief Disables printing of scheduling statistics.
+//! \details Defining this macro disables printing of scheduling statistics pre
+//! and post scheduling. Undefine macro when statistics are needed.
+#define DISABLE_ATOM_SCHEDULING_STATISTICS
+
+//! \def DISABLE_DEBUG_ATOM_SCHEDULER
+//! \brief Disables debug printing for atom scheduler.
+//! \details Defining macro DISABLE_DEBUG_ATOM_SCHEDULER disables debug printing.
+//! Undefine macro when debugging scheduler implementation.
+#define DISABLE_DEBUG_ATOM_SCHEDULER
+
+//! \def ENABLE_BASIC_BLOCK_DUMP
+//! \brief Enables scheduler to wait until it has formed a basic block.
+//! \details Defining this macro allows scheduler to dump the x86 instructions as
+//! a basic block instead of doing it one-at-a-time as they come in.
+//! Undefined macro when trying to debug basic block detection.
+#define ENABLE_BASIC_BLOCK_DUMP
+
+//! \def DISABLE_MACHINE_MODEL_CHECK
+//! \brief Disables checks on the machine model.
+//! \details Defining this macro disables checks of the machine model.
+//! Undefining this leads to Dalvik aborting when invalid information
+//! is used from machine model. For example, Dalvik will abort when an
+//! instruction does not have latency information.
+#define DISABLE_MACHINE_MODEL_CHECK
+
+//! \def g_SchedulerInstance
+//! \brief Global Atom instruction scheduler instance
+Scheduler g_SchedulerInstance;
+
+//! \enum IssuePort
+//! \brief Defines possible combinations of port-binding information for use
+//! with information about each x86 mnemonic.
+enum IssuePort {
+    //! \brief invalid port, used for table only when some
+    //! operands are not supported for the mnemonic
+    INVALID_PORT = -1,
+    //! \brief the mnemonic can only be issued on port 0
+    PORT0 = 0,
+    //! \brief the mnemonic can only be issued on port 1
+    PORT1 = 1,
+    //! \brief the mnemonic can be issued on either port
+    EITHER_PORT,
+    //! \brief both ports are used for the mnemonic
+    BOTH_PORTS
+};
+
+//! \def MachineModelEntry
+//! \brief Information needed to define the machine model for each x86 mnemonic.
+struct MachineModelEntry {
+    //! \brief which port the instruction can execute on
+    IssuePort issuePortType;
+    //! \brief execute to execute time for one instruction
+    int executeToExecuteLatency;
+};
+
+//! \def INVP
+//! \brief This is an abbreviation of INVALID_PORT and is used for readability
+//! reasons.
+#define INVP INVALID_PORT
+
+//! \def INVN
+//! \brief This is an abbreviation of invalid node latency and is used for
+//! readability reasons.
+#define INVN -1
+
+//! \def REG_NOT_USED
+//! \brief This is an abbreviation for register not used and is used for
+//! readability reasons whenever a Scheduler method needs register type
+//! to update some data structure but a register number does not make
+//! sense in the context.
+#define REG_NOT_USED -1
+
+//! \brief This table lists the parameters for each Mnemonic in the Atom Machine Model.
+//! \details This table includes port and latency information for each mnemonic for each possible
+//! configuration of operands. 6 entries of MachineModelEntry are reserved for each Mnemonic:
+//! - If a Mnemonic has zero operand, only the first entry is valid
+//! - If a Mnemonic has a single operand, the first 3 entries are valid, for operand type
+//! imm, reg and mem respectively
+//! - If a Mnemonic has two operands, the last 5 entries are valid, for operand types
+//! imm_to_reg, imm_to_mem, reg_to_reg, mem_to_reg and reg_to_mem
+//!
+//! This table matches content from Intel 64 and IA-32 Architectures Optimization
+//! Reference Manual (April 2011), Section 13.4
+//! \todo Make sure that the invalid entries are never used in the algorithm
+MachineModelEntry atomMachineModel[Mnemonic_Count*6] = {
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NULL, Null
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //ADC
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //ADD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //ADDSD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //ADDSS
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //AND
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //BSF
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //BSR
+    {BOTH_PORTS,1},{BOTH_PORTS,1},{EITHER_PORT,2},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CALL
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMC
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CWD, CDQ
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_O
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_NO
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_B,NAE,C
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_NB,AE,NC
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_Z,E
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_NZ,NE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_BE,NA
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_NBE,A
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_S
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_NS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_P,PE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_NP,PO
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_L,NGE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_NL,GE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_LE,NG
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //CMOV_NLE,G
+
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //CMP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPXCHG
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPXCHG8B
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPSB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPSW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CMPSD
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTSD2SS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTSD2SI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTTSD2SI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTSS2SD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTSS2SI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTTSS2SI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTSI2SD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTSI2SS
+
+    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //COMISD
+    {INVP,INVN},{BOTH_PORTS,9},{INVP,INVN},{BOTH_PORTS,9},{BOTH_PORTS,10},{INVP,INVN}, //COMISS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //DEC
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,64},{BOTH_PORTS,64},{INVP,INVN}, //DIVSD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,34},{BOTH_PORTS,34},{INVP,INVN}, //DIVSS
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //ENTER
+    {INVP,INVN},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDCW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FADDP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDZ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FADD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSUBP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSUB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FISUB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FMUL
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FMULP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FDIVP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FDIV
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,1},{INVP,INVN},{INVP,INVN}, //FUCOM
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FUCOMI
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,1},{INVP,INVN},{INVP,INVN}, //FUCOMP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FUCOMIP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FUCOMPP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FRNDINT
+    {INVP,INVN},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FNSTCW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSTSW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FNSTSW
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,5},{INVP,INVN}, //FILD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{INVP,INVN}, //FLD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDLG2
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLDLN2
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FLD1
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FCLEX
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FCHS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FNCLEX
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,2}, //FIST
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,2}, //FISTP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FISTTP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPREM
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPREM1
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,2}, //FST fp_mem
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{BOTH_PORTS,2}, //FSTP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSQRT
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FABS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSIN
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FCOS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPTAN
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FYL2X
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FYL2XP1
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //F2XM1
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FPATAN
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FXCH
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //FSCALE
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //XCHG
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //DIV
+    {INVP,INVN},{BOTH_PORTS,57},{BOTH_PORTS,57},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //IDIV
+    {INVP,INVN},{BOTH_PORTS,6},{BOTH_PORTS,6},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MUL
+    {INVP,INVN},{PORT0,5},{PORT0,5},{PORT0,5},{PORT0,5},{INVP,INVN}, //IMUL
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //INC
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //INT3
+
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_O
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NO
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_B
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NB
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_Z
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NZ
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_BE
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NBE
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_S
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NS
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_P
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NP
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_L
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NL
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_LE
+    {PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //Jcc_NLE
+
+    {PORT1,1},{PORT1,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //JMP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,1},{INVP,INVN}, //LEA
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LEAVE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LOOP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LOOPE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LOOPNE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //LAHF
+
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //MOV
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVD
+    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{PORT0,1},{PORT0,1}, //MOVQ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS8
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS16
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS32
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVS64
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //MOVAPD
+    {INVP,INVN},{PORT0,1},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //MOVSD
+    {INVP,INVN},{PORT0,1},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{PORT0,1}, //MOVSS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{INVP,INVN}, //MOVSX
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,1},{INVP,INVN}, //MOVZX
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,5},{PORT0,5},{INVP,INVN}, //MULSD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT0,4},{PORT0,4},{INVP,INVN}, //MULSS
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,10},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NEG
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NOP
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,10},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //NOT
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //OR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //PREFETCH
+
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PADDQ
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PAND
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //POR
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PSUBQ
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PANDN
+    {INVP,INVN},{PORT0,1},{INVP,INVN},{BOTH_PORTS,2},{BOTH_PORTS,3},{INVP,INVN}, //PSLLQ
+    {INVP,INVN},{PORT0,1},{INVP,INVN},{BOTH_PORTS,2},{BOTH_PORTS,3},{INVP,INVN}, //PSRLQ
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //PXOR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //POP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //POPFD
+    {INVP,INVN},{BOTH_PORTS,1},{BOTH_PORTS,2},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //PUSH
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //PUSHFD
+    {BOTH_PORTS,1},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //RET
+
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_O
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NO
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_B
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_Z
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NZ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_BE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NBE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_S
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_P
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NP
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_L
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NL
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_LE
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SET_NLE
+
+    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{INVP,INVN},{INVP,INVN}, //SAL,SHL
+    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{INVP,INVN},{INVP,INVN}, //SAR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //ROR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //RCR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //ROL
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //RCL
+    {INVP,INVN},{PORT0,1},{PORT0,1},{PORT0,1},{INVP,INVN},{INVP,INVN}, //SHR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SHRD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SHLD
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //SBB
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //SUB
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //SUBSD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{PORT1,5},{BOTH_PORTS,5},{INVP,INVN}, //SUBSS
+
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{INVP,INVN},{INVP,INVN}, //TEST
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //UCOMISD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //UCOMISS
+    {INVP,INVN},{EITHER_PORT,1},{PORT0,1},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //XOR
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{EITHER_PORT,1},{PORT0,1},{INVP,INVN}, //XORPD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //XORPS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTDQ2PD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTTPD2DQ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTDQ2PS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CVTTPS2DQ
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //STD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //CLD
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //SCAS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //STOS
+    {INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN},{INVP,INVN}, //WAIT
+};
+
+//! \brief Get issue port for mnemonic with no operands
+inline IssuePort getAtomMnemonicPort(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6].issuePortType;
+}
+//! \brief Get issue port for mnemonic with one immediate operand
+inline IssuePort getAtomMnemonicPort_imm(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6].issuePortType;
+}
+//! \brief Get issue port for mnemonic with one register operand
+inline IssuePort getAtomMnemonicPort_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+1].issuePortType;
+}
+//! \brief Get issue port for mnemonic with one memory operand
+inline IssuePort getAtomMnemonicPort_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+2].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: immediate to register
+inline IssuePort getAtomMnemonicPort_imm_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+1].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: immediate to memory
+inline IssuePort getAtomMnemonicPort_imm_to_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+2].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: register to register
+inline IssuePort getAtomMnemonicPort_reg_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+3].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: memory to register
+inline IssuePort getAtomMnemonicPort_mem_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+4].issuePortType;
+}
+//! \brief Get issue port for mnemonic with two operands: register to memory
+inline IssuePort getAtomMnemonicPort_reg_to_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVALID_PORT;
+    return atomMachineModel[m*6+5].issuePortType;
+}
+
+//! \brief Get execute to execute latency for mnemonic with no operands
+inline int getAtomMnemonicLatency(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with one immediate operand
+inline int getAtomMnemonicLatency_imm(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with one register operand
+inline int getAtomMnemonicLatency_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+1].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with one memory operand
+inline int getAtomMnemonicLatency_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+2].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: immediate to register
+inline int getAtomMnemonicLatency_imm_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+1].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: immediate to memory
+inline int getAtomMnemonicLatency_imm_to_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+2].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: register to register
+inline int getAtomMnemonicLatency_reg_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+3].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: memory to register
+inline int getAtomMnemonicLatency_mem_to_reg(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+4].executeToExecuteLatency;
+}
+//! \brief Get execute to execute latency for mnemonic with two operands: register to memory
+inline int getAtomMnemonicLatency_reg_to_mem(Mnemonic m) {
+    if (m >= Mnemonic_Count) return INVN;
+    return atomMachineModel[m*6+5].executeToExecuteLatency;
+}
+
+//! \def EDGE_LATENCY
+//! \brief Defines weight of edges in the dependency graph.
+#define EDGE_LATENCY 0
+
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+//! \brief Transforms from LowOpndDefUse enum to string representation of the usedef
+//! \see LowOpndDefUse
+inline const char * getUseDefType(LowOpndDefUse defuse) {
+    switch (defuse) {
+    case LowOpndDefUse_Def:
+        return "Def";
+    case LowOpndDefUse_Use:
+        return "Use";
+    case LowOpndDefUse_UseDef:
+        return "UseDef";
+    }
+    return "-";
+}
+#endif
+
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+//! \brief Transforms from UseDefEntryType enum to a string representation.
+//! \see UseDefEntryType
+inline const char * getUseDefEntryType(UseDefEntryType type) {
+    switch (type) {
+    case UseDefType_Ctrl:
+        return "Ctrl";
+    case UseDefType_Float:
+        return "Float";
+    case UseDefType_MemVR:
+        return "MemVR";
+    case UseDefType_MemSpill:
+        return "MemSpill";
+    case UseDefType_MemUnknown:
+        return "MemUnknown";
+    case UseDefType_Reg:
+        return "Reg";
+    }
+    return "-";
+}
+#endif
+
+//! \brief Returns true if mnemonic is a variant of MOV.
+inline bool isMoveMnemonic(Mnemonic m) {
+    return m == Mnemonic_MOV || m == Mnemonic_MOVQ || m == Mnemonic_MOVSD
+            || m == Mnemonic_MOVSS || m == Mnemonic_MOVZX || m == Mnemonic_MOVSX;
+}
+
+//! \brief Returns true if mnemonic uses and then defines the FLAGS register
+inline bool usesAndDefinesFlags(Mnemonic m) {
+    return m == Mnemonic_ADC || m == Mnemonic_SBB;
+}
+
+//! \brief Detects whether the mnemonic is a native basic block delimiter.
+//! \details Unconditional jumps, conditional jumps, calls, and returns
+//! always end a native basic block.
+inline bool Scheduler::isBasicBlockDelimiter(Mnemonic m) {
+    return (m == Mnemonic_JMP || m == Mnemonic_CALL
+            || (m >= Mnemonic_Jcc && m < Mnemonic_JMP) || m == Mnemonic_RET);
+}
+
+//! \brief Given an access to a resource (Control, register, VR, unknown memory
+//! access), this updates dependency graph, usedef information, and control flags.
+//! \details Algorithm description for dependency update:
+//! - for Use or UseDef:
+//!   -# insert RAW dependency from producer for this op
+//! - for Def or UseDef:
+//!   -# insert WAR dependency from earlier user for this op
+//!   -# insert WAW dependency from earlier producer for this op
+//! - Internal data structure updates for record keeping:
+//!   -# for Def or UseDef: update producerEntries
+//!   -# for Def: clear corresponding use slots for entry in userEntries
+//!   -# for UseDef: clear corresponding use slots for entry in userEntries
+//!   -# for Use: update userEntries
+//!
+//! \param type resource that causes dependency
+//! \param regNum is a number corresponding to a physical register or a Dalvik
+//! virtual register. When physical, this is of enum type PhysicalReg.
+//! \param defuse definition, usage, or both
+//! \param op LIR for which to update dependencies
+void Scheduler::updateDependencyGraph(UseDefEntryType type, int regNum,
+        LowOpndDefUse defuse, LowOp* op) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+    const char * string_defuse = getUseDefType(defuse);
+    const char * string_type = getUseDefEntryType(type);
+    printf("updateDependencyGraph for <%s %d> at slot %d %s\n", string_type,
+            regNum, op->slotId, string_defuse);
+#endif
+
+    unsigned int k;
+    unsigned int index_for_user = userEntries.size();
+    unsigned int index_for_producer = producerEntries.size();
+
+    if (type != UseDefType_Ctrl) {
+        for (k = 0; k < producerEntries.size(); ++k) {
+            if (producerEntries[k].type == type
+                    && producerEntries[k].regNum == regNum) {
+                index_for_producer = k;
+                break;
+            }
+        }
+    }
+    for (k = 0; k < userEntries.size(); ++k) {
+        if (userEntries[k].type == type && userEntries[k].regNum == regNum) {
+            index_for_user = k;
+            break;
+        }
+    }
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+    printf("index_for_producer %d %d index_for_user %d %d\n",
+            index_for_producer, producerEntries.size(),
+            index_for_user, userEntries.size());
+#endif
+
+    if (defuse == LowOpndDefUse_Use) {
+        // insert RAW from producer
+        if (type != UseDefType_Ctrl
+                && index_for_producer != producerEntries.size()) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            printf("insert RAW from %d to %d due to <%s %d>\n",
+                    producerEntries[index_for_producer].producerSlot, op->slotId, string_type, regNum);
+#endif
+            DependencyInformation ds;
+            ds.dataHazard = Dependency_RAW;
+            ds.lowopSlotId = producerEntries[index_for_producer].producerSlot;
+            ds.latency = EDGE_LATENCY;
+            op->predecessorDependencies.push_back(ds);
+        }
+        if (type == UseDefType_Ctrl && ctrlEntries.size() > 0) {
+            // insert RAW from the last producer
+            int lastSlot = ctrlEntries.back();
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            printf("insert RAW from %d to %d due to Ctrl\n",
+                    lastSlot, op->slotId);
+#endif
+            DependencyInformation ds;
+            ds.dataHazard = Dependency_RAW;
+            ds.lowopSlotId = lastSlot;
+            ds.latency = EDGE_LATENCY;
+            op->predecessorDependencies.push_back(ds);
+            // insert WAW from earlier producers to the last producer
+            unsigned int k2;
+            LowOp* opLast = (queuedLIREntries[lastSlot]);
+            for (k2 = 0; k2 < ctrlEntries.size() - 1; k2++) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+                printf("insert WAW from %d to %d due to Ctrl\n", ctrlEntries[k2], lastSlot);
+#endif
+                DependencyInformation ds;
+                ds.dataHazard = Dependency_WAW;
+                ds.lowopSlotId = ctrlEntries[k2];
+                ds.latency = EDGE_LATENCY;
+                opLast->predecessorDependencies.push_back(ds);
+            }
+        }
+        // update userEntries
+        if (index_for_user == userEntries.size()) {
+            // insert an entry in userTable
+            UseDefUserEntry entry;
+            entry.type = type;
+            entry.regNum = regNum;
+            userEntries.push_back(entry);
+        } else if (type == UseDefType_Ctrl) {
+            userEntries[index_for_user].useSlotsList.clear();
+        }
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        printf("insert use for <%s %d> @ slot %d\n", string_type, regNum, op->slotId);
+#endif
+        userEntries[index_for_user].useSlotsList.push_back(op->slotId);
+        if (type == UseDefType_Ctrl)
+            ctrlEntries.clear();
+    } else if (defuse == LowOpndDefUse_Def) {
+        // insert WAR from earlier uses
+        // insert WAW from earlier producer
+        if (index_for_user != userEntries.size()) {
+            for (unsigned int k2 = 0; k2 < userEntries[index_for_user].useSlotsList.size();
+                    k2++) {
+                if (userEntries[index_for_user].useSlotsList[k2] == op->slotId)
+                    continue;
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+                printf("insert WAR from %d to %d due to <%s %d>\n",
+                        userEntries[index_for_user].useSlotsList[k2], op->slotId, string_type, regNum);
+#endif
+                DependencyInformation ds;
+                ds.dataHazard = Dependency_WAR;
+                ds.lowopSlotId = userEntries[index_for_user].useSlotsList[k2];
+                ds.latency = EDGE_LATENCY;
+                op->predecessorDependencies.push_back(ds);
+            }
+        }
+        if (type != UseDefType_Ctrl
+                && index_for_producer != producerEntries.size()) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            printf("insert WAW from %d to %d due to <%s %d>\n",
+                    producerEntries[index_for_producer].producerSlot, op->slotId, string_type, regNum);
+#endif
+            DependencyInformation ds;
+            ds.dataHazard = Dependency_WAW;
+            ds.lowopSlotId = producerEntries[index_for_producer].producerSlot;
+            ds.latency = EDGE_LATENCY;
+            op->predecessorDependencies.push_back(ds);
+        }
+        // update producerTable
+        // clear corresponding use slots for entry in userEntries
+        if (type != UseDefType_Ctrl
+                && index_for_producer == producerEntries.size()) {
+            // insert an entry in producerEntries
+            UseDefProducerEntry entry;
+            entry.type = type;
+            entry.regNum = regNum;
+            producerEntries.push_back(entry);
+        }
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        printf("insert def for <%s %d> @ slot %d\n", string_type, regNum, op->slotId);
+#endif
+        if (type != UseDefType_Ctrl)
+            producerEntries[index_for_producer].producerSlot = op->slotId;
+        else { // insert into ctrlEntries
+            ctrlEntries.push_back(op->slotId);
+        }
+        if (type != UseDefType_Ctrl && index_for_user != userEntries.size()) {
+            userEntries[index_for_user].useSlotsList.clear();
+        }
+    } else if (defuse == LowOpndDefUse_UseDef) {
+        // type can not be Ctrl
+        // insert RAW from producer
+        // insert WAR from earlier user
+        // insert WAW from earlier producer
+        if (index_for_producer != producerEntries.size()) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            printf("insert RAW from %d to %d due to <%s %d>\n",
+                    producerEntries[index_for_producer].producerSlot, op->slotId, string_type, regNum);
+#endif
+            DependencyInformation ds;
+            ds.dataHazard = Dependency_RAW;
+            ds.lowopSlotId = producerEntries[index_for_producer].producerSlot;
+            ds.latency = EDGE_LATENCY;
+            op->predecessorDependencies.push_back(ds);
+
+#if 0 // inserting WAW is possibly redundant so section is commented out
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            printf("insert WAW from %d to %d due to <%s %d>",
+                    producerEntries[index_for_producer].producerSlot, op->slotId, string_type, regNum);
+#endif
+            DependencyInformation ds_waw;
+            ds_waw.dataHazard = Dependency_WAW;
+            ds_waw.lowopSlotId =
+                    producerEntries[index_for_producer].producerSlot;
+            ds_waw.latency = EDGE_LATENCY;
+            op->predecessorDependencies.push_back(ds_waw);
+#endif
+
+
+        }
+        if (index_for_user != userEntries.size()) {
+            for (unsigned int k2 = 0; k2 < userEntries[index_for_user].useSlotsList.size();
+                    k2++) {
+                if (userEntries[index_for_user].useSlotsList[k2] == op->slotId)
+                    continue;
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+                printf("insert WAR from %d to %d due to <%s %d>\n",
+                        userEntries[index_for_user].useSlotsList[k2], op->slotId, string_type, regNum);
+#endif
+                DependencyInformation ds;
+                ds.dataHazard = Dependency_WAR;
+                ds.lowopSlotId = userEntries[index_for_user].useSlotsList[k2];
+                ds.latency = EDGE_LATENCY;
+                op->predecessorDependencies.push_back(ds);
+            }
+        }
+        // update producerEntries
+        // clear corresponding use slots for entry in userEntries
+        if (index_for_producer == producerEntries.size()) {
+            // insert an entry in producerEntries
+            UseDefProducerEntry entry;
+            entry.type = type;
+            entry.regNum = regNum;
+            producerEntries.push_back(entry);
+        }
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        printf("insert def for <%s %d> @ slot %d\n", string_type, regNum, op->slotId);
+#endif
+        producerEntries[index_for_producer].producerSlot = op->slotId;
+        if (index_for_user != userEntries.size()) {
+            userEntries[index_for_user].useSlotsList.clear();
+        }
+    }
+}
+
+//! \brief Given an access to a memory location this updates dependency graph,
+//! usedef information, and control flags.
+//! \details This method uses updateDependencyGraph internally to update
+//! dependency graph, but knows the type of memory resource that is being used.
+//! \param mOpnd reference to the structure for memory operand
+//! \param defuse definition, usage, or both
+//! \param op LIR for which to update dependencies
+void Scheduler::updateDependencyGraphForMem(LowOpndMem & mOpnd, LowOpndDefUse defuse,
+        LowOp* op) {
+    updateDependencyGraph(UseDefType_Reg, mOpnd.m_base.regNum, LowOpndDefUse_Use, op);
+    if (mOpnd.hasScale)
+        updateDependencyGraph(UseDefType_Reg, mOpnd.m_index.regNum, LowOpndDefUse_Use,
+                op);
+    MemoryAccessType mType = mOpnd.mType;
+    int index = mOpnd.index;
+
+    // be conservative, if one of the operands has size 64, assume it is size 64
+    bool is64 = false;
+    if (op->numOperands >= 1 && op->opndDest.size == OpndSize_64)
+        is64 = true;
+    if (op->numOperands >= 2 && op->opndSrc.size == OpndSize_64)
+        is64 = true;
+
+    if (mType == MemoryAccess_VR) {
+        updateDependencyGraph(UseDefType_MemVR, index, defuse, op);
+        if (is64)
+            updateDependencyGraph(UseDefType_MemVR, index + 1, defuse, op);
+    } else if (mType == MemoryAccess_SPILL) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        printf("UseDef MemSpill @%d slot %d\n", index, op->slotId);
+#endif
+        updateDependencyGraph(UseDefType_MemSpill, index, defuse, op);
+        if (is64) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            printf("UseDef MemSpill2 @%d slot %d\n", index+4, op->slotId);
+#endif
+            updateDependencyGraph(UseDefType_MemSpill, index + 4, defuse, op);
+        }
+    } else
+        updateDependencyGraph(UseDefType_MemUnknown, REG_NOT_USED, defuse, op);
+}
+
+//! \brief Updates dependency information for PUSH which uses then defines %esp
+//! and also updates native stack.
+void inline Scheduler::handlePushDependencyUpdate(LowOp* op) {
+    if (op->opCode != Mnemonic_PUSH)
+        return;
+    updateDependencyGraph(UseDefType_Reg, PhysicalReg_ESP, LowOpndDefUse_UseDef, op);
+    updateDependencyGraph(UseDefType_MemUnknown, REG_NOT_USED, LowOpndDefUse_Def, op);
+}
+
+//! \brief Updates dependency information for operations on floating point stack.
+void inline Scheduler::handleFloatDependencyUpdate(LowOp* op) {
+    updateDependencyGraph(UseDefType_Float, REG_NOT_USED, LowOpndDefUse_Def, op);
+}
+
+//! \brief Updates dependency information for LowOps with zero operands.
+//! \param op has mnemonic RET
+void Scheduler::updateUseDefInformation(LowOp * op) {
+    op->instructionLatency = getAtomMnemonicLatency(op->opCode);
+    op->portType = getAtomMnemonicPort(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with zero operands.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+    signalEndOfNativeBasicBlock(); // RET ends native basic block
+}
+
+//! \brief Updates dependency information for LowOps with a single immediate
+//! operand.
+//! \param op has mnemonic JMP, Jcc, or CALL
+void Scheduler::updateUseDefInformation_imm(LowOp * op) {
+    op->instructionLatency = getAtomMnemonicLatency_imm(op->opCode);
+    op->portType = getAtomMnemonicPort_imm(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with imm operand.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    if (op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_JMP)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+    else
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use, op);
+    if (isBasicBlockDelimiter(op->opCode))
+        signalEndOfNativeBasicBlock();
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with a single register operand.
+//! \param op has mnemonic JMP, CALL, alu_unary_reg, PUSH or
+//! alu_unary: neg, not, idiv, mul 32 bits operand
+void Scheduler::updateUseDefInformation_reg(LowOpReg * op) {
+    op->instructionLatency = getAtomMnemonicLatency_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_reg(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with reg operand.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    if (op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_JMP
+            || op->opCode == Mnemonic_PUSH)
+        op->opndSrc.defuse = LowOpndDefUse_Use;
+    if (op->opCode2 == ATOM_NORMAL_ALU)
+        op->opndSrc.defuse = LowOpndDefUse_UseDef;
+    // PUSH will not update control flag
+    if (op->opCode != Mnemonic_PUSH)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+    updateDependencyGraph(UseDefType_Reg, op->regOpnd.regNum, op->opndSrc.defuse, op);
+    Mnemonic m = op->opCode;
+    if (m == Mnemonic_MUL || m == Mnemonic_IMUL || m == Mnemonic_DIV
+            || m == Mnemonic_IDIV) {
+        updateDependencyGraph(UseDefType_Reg, PhysicalReg_EAX, LowOpndDefUse_UseDef, op);
+        updateDependencyGraph(UseDefType_Reg, PhysicalReg_EDX, LowOpndDefUse_UseDef, op);
+    }
+    handlePushDependencyUpdate(op);
+    if (isBasicBlockDelimiter(op->opCode))
+        signalEndOfNativeBasicBlock();
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for for LowOps with a single
+//! memory operand.
+//! \param op has mnemonic CALL, FLDCW, FNSTCW, alu_unary_mem, PUSH or
+//! alu_unary: neg, not, idiv, mul 32 bits operand
+void Scheduler::updateUseDefInformation_mem(LowOpMem * op) {
+    op->instructionLatency = getAtomMnemonicLatency_mem(op->opCode);
+    op->portType = getAtomMnemonicPort_mem(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with mem operand.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    // FLDCW: read memory location, affects FPU flags
+    if (op->opCode == Mnemonic_CALL || op->opCode == Mnemonic_FLDCW
+            || op->opCode == Mnemonic_PUSH)
+        op->opndSrc.defuse = LowOpndDefUse_Use;
+    if (op->opCode == Mnemonic_FNSTCW)
+        op->opndSrc.defuse = LowOpndDefUse_Def;
+    if (op->opCode2 == ATOM_NORMAL_ALU)
+        op->opndSrc.defuse = LowOpndDefUse_UseDef;
+    // PUSH will not update control flag
+    if (op->opCode != Mnemonic_PUSH)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+    updateDependencyGraphForMem(op->memOpnd, op->opndSrc.defuse, op);
+    handlePushDependencyUpdate(op);
+    if (op->opCode == Mnemonic_FLDCW || op->opCode == Mnemonic_FNSTCW)
+        handleFloatDependencyUpdate(op);
+    if (isBasicBlockDelimiter(op->opCode))
+        signalEndOfNativeBasicBlock();
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! immediate to register
+//! \param op has mnemonic CMP, TEST, MOVQ, MOV, MOVSS, MOVSD, alu_binary,
+//! COMISS, or COMISD
+void Scheduler::updateUseDefInformation_imm_to_reg(LowOpImmReg * op) {
+    bool isMove = isMoveMnemonic(op->opCode);
+    op->instructionLatency = getAtomMnemonicLatency_imm_to_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_imm_to_reg(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with imm to reg.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    if (usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                op);
+    if (!isMove)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+
+    if (isMove)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (op->opCode2 == ATOM_NORMAL_ALU)
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    else
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum, op->opndDest.defuse, op);
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! immediate and memory.
+//! \param op has mnemonic MOV, MOVQ, CMP, TEST, alu_binary
+void Scheduler::updateUseDefInformation_imm_to_mem(LowOpImmMem * op) {
+    bool isMove = isMoveMnemonic(op->opCode);
+    op->instructionLatency = getAtomMnemonicLatency_imm_to_mem(op->opCode);
+    op->portType = getAtomMnemonicPort_imm_to_mem(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with imm to mem.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    if (usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                op);
+    if (!isMove)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+
+    if (isMove)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (op->opCode2 == ATOM_NORMAL_ALU)
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    else
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! register to register.
+//! \param op has mnemonic TEST, CDQ, CMP, COMISS, COMISD,
+//! MOVQ, MOV, CMOVcc, MOVSS, MOVSD, FUCOM, FUCOMP, or
+//! alu_sd_binary, alu_ss_binary, alu_binary,
+void Scheduler::updateUseDefInformation_reg_to_reg(LowOpRegReg * op) {
+    Mnemonic m = op->opCode;
+    bool isMove = isMoveMnemonic(m);
+    op->instructionLatency = getAtomMnemonicLatency_reg_to_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_reg_to_reg(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with reg to reg.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    if ((m >= Mnemonic_CMOVcc && m < Mnemonic_CMP)
+            || usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                op);
+    else if (!isMove)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+
+    // CDQ: use & def eax, def edx
+    if (m == Mnemonic_CDQ)
+        op->opndSrc.defuse = LowOpndDefUse_UseDef; //use & def eax
+    else
+        op->opndSrc.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, op->regSrc.regNum, op->opndSrc.defuse, op);
+
+    if (isMove || (m >= Mnemonic_CMOVcc && m < Mnemonic_CMP)
+            || m == Mnemonic_CDQ)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (op->opCode2 == ATOM_NORMAL_ALU)
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    else
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum, op->opndDest.defuse, op);
+    if (m == Mnemonic_FUCOM || m == Mnemonic_FUCOMP)
+        handleFloatDependencyUpdate(op);
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! memory to register.
+//! \param op has mnemonic LEA, CMP, COMISS, COMISD, MOVQ, MOV, MOVSS,
+//! MOVSD, alu_binary, or alu_sd_binary
+void Scheduler::updateUseDefInformation_mem_to_reg(LowOpMemReg * op) {
+    Mnemonic m = op->opCode;
+    bool isMove = isMoveMnemonic(m);
+    op->instructionLatency = getAtomMnemonicLatency_mem_to_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_mem_to_reg(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with mem to reg.", op->opCode);
+        dvmAbort();
+    }
+#endif
+
+    if (usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                op);
+    if (!isMove && m != Mnemonic_LEA)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+
+    // Read from memory
+    // However, LEA does not load from memory, and instead it uses the register
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    if (m != Mnemonic_LEA)
+        updateDependencyGraphForMem(op->memSrc, op->opndSrc.defuse, op);
+    else {
+        updateDependencyGraph(UseDefType_Reg, op->memSrc.m_base.regNum,
+                op->opndSrc.defuse, op);
+    }
+    if (isMove || m == Mnemonic_LEA)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (op->opCode2 == ATOM_NORMAL_ALU)
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    else
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum, op->opndDest.defuse, op);
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! register to memory.
+//! \param op has mnemonic MOVQ, MOV, MOVSS, MOVSD, CMP, or alu_binary
+void Scheduler::updateUseDefInformation_reg_to_mem(LowOpRegMem * op) {
+    bool isMove = isMoveMnemonic(op->opCode);
+    op->instructionLatency = getAtomMnemonicLatency_reg_to_mem(op->opCode);
+    op->portType = getAtomMnemonicPort_reg_to_mem(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with reg to mem.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    if (usesAndDefinesFlags(op->opCode))
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Use,
+                op);
+    if (!isMove)
+        updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, op->regSrc.regNum, op->opndSrc.defuse, op);
+    if (isMove)
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    else if (op->opCode2 == ATOM_NORMAL_ALU)
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    else
+        op->opndDest.defuse = LowOpndDefUse_Use;
+    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! memory with scale to register.
+//! \param op has mnemonic LEA, MOVQ, MOV, MOVSX, or MOVZX
+void Scheduler::updateUseDefInformation_mem_scale_to_reg(LowOpMemReg * op) {
+    op->instructionLatency = getAtomMnemonicLatency_mem_to_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_mem_to_reg(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with mem scale to reg.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    Mnemonic m = op->opCode;
+    if (m != Mnemonic_LEA)
+        updateDependencyGraphForMem(op->memSrc, op->opndSrc.defuse, op);
+    else {
+        updateDependencyGraph(UseDefType_Reg, op->memSrc.m_base.regNum,
+                op->opndSrc.defuse, op);
+    }
+    op->opndDest.defuse = LowOpndDefUse_Def;
+    updateDependencyGraph(UseDefType_Reg, op->regDest.regNum, op->opndDest.defuse, op);
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! register to memory with scale.
+//! \param op has mnemonic MOVQ or MOV
+void Scheduler::updateUseDefInformation_reg_to_mem_scale(LowOpRegMem * op) {
+    op->instructionLatency = getAtomMnemonicLatency_reg_to_mem(op->opCode);
+    op->portType = getAtomMnemonicPort_reg_to_mem(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with reg to mem scale.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, op->regSrc.regNum, op->opndSrc.defuse, op);
+    op->opndDest.defuse = LowOpndDefUse_Def;
+    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! floating point stack to memory.
+//! \param op has mnemonic FSTP, FST, FISTP, or FIST (write to memory)
+void Scheduler::updateUseDefInformation_fp_to_mem(LowOpRegMem * op) {
+    op->instructionLatency = getAtomMnemonicLatency_reg_to_mem(op->opCode);
+    op->portType = getAtomMnemonicPort_reg_to_mem(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with fp to mem.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    updateDependencyGraph(UseDefType_Reg, PhysicalReg_ST0, op->opndSrc.defuse, op);
+    op->opndDest.defuse = LowOpndDefUse_Def;
+    updateDependencyGraphForMem(op->memDest, op->opndDest.defuse, op);
+    handleFloatDependencyUpdate(op);
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Updates dependency information for LowOps with two operands:
+//! memory to floating point stack.
+//! \param op has mnemonic FLD or FILD
+void Scheduler::updateUseDefInformation_mem_to_fp(LowOpMemReg * op) {
+    op->instructionLatency = getAtomMnemonicLatency_mem_to_reg(op->opCode);
+    op->portType = getAtomMnemonicPort_mem_to_reg(op->opCode);
+#ifndef DISABLE_MACHINE_MODEL_CHECK
+    if(op->instructionLatency == INVN || op->portType == INVP) {
+        LOGE("ERROR: Atom Scheduler bug! Invalid Atom machine model "
+                "entry for Mnemonic %d with mem to fp.", op->opCode);
+        dvmAbort();
+    }
+#endif
+    updateDependencyGraph(UseDefType_Ctrl, REG_NOT_USED, LowOpndDefUse_Def, op);
+    op->opndSrc.defuse = LowOpndDefUse_Use;
+    updateDependencyGraphForMem(op->memSrc, op->opndSrc.defuse, op);
+    if (op->opCode2 == ATOM_NORMAL_ALU)
+        op->opndDest.defuse = LowOpndDefUse_UseDef;
+    else
+        op->opndDest.defuse = LowOpndDefUse_Def;
+    updateDependencyGraph(UseDefType_Reg, PhysicalReg_ST0, op->opndDest.defuse, op);
+    handleFloatDependencyUpdate(op);
+#ifndef ENABLE_BASIC_BLOCK_DUMP
+    signalEndOfNativeBasicBlock();
+#endif
+}
+
+//! \brief Generates IA native code for given LowOp
+//! \details This method takes a LowOp and generates x86 instructions into the
+//! code stream by making calls to the encoder.
+//! \param op to be encoded and placed into code stream.
+void Scheduler::generateAssembly(LowOp * op) {
+    if (op->numOperands == 0) {
+        stream = encoder_return(stream);
+    } else if (op->numOperands == 1) {
+        if (op->opndSrc.type == LowOpndType_Label) {
+            bool unknown;
+            OpndSize size;
+            int imm;
+            if (op->opCode == Mnemonic_JMP)
+                imm = getRelativeOffset(((LowOpLabel*) op)->labelOpnd.label,
+                        ((LowOpLabel*) op)->labelOpnd.isLocal, JmpCall_uncond,
+                        &unknown, &size);
+            else if (op->opCode == Mnemonic_CALL)
+                imm = getRelativeOffset(((LowOpLabel*) op)->labelOpnd.label,
+                        ((LowOpLabel*) op)->labelOpnd.isLocal, JmpCall_call,
+                        &unknown, &size);
+            else
+                imm = getRelativeOffset(((LowOpLabel*) op)->labelOpnd.label,
+                        ((LowOpLabel*) op)->labelOpnd.isLocal, JmpCall_cond,
+                        &unknown, &size);
+            op->opndSrc.size = size;
+            stream = encoder_imm(op->opCode, op->opndSrc.size, imm, stream);
+        } else if (op->opndSrc.type == LowOpndType_BlockId) {
+            bool unknown;
+            OpndSize size;
+            int imm;
+            if (op->opCode == Mnemonic_JMP)
+                imm = getRelativeNCG(((LowOpBlock*) op)->blockIdOpnd.value,
+                        JmpCall_uncond, &unknown, &size);
+            else
+                imm = getRelativeNCG(((LowOpBlock*) op)->blockIdOpnd.value,
+                        JmpCall_cond, &unknown, &size);
+            op->opndSrc.size = size;
+            stream = encoder_imm(op->opCode, op->opndSrc.size, imm, stream);
+        } else if (op->opndSrc.type == LowOpndType_Imm) {
+            stream = encoder_imm(op->opCode, op->opndSrc.size,
+                    ((LowOpImm*) op)->immOpnd.value, stream);
+        } else if (op->opndSrc.type == LowOpndType_Reg) {
+            stream = encoder_reg(op->opCode, op->opndSrc.size,
+                    ((LowOpReg*) op)->regOpnd.regNum,
+                    ((LowOpReg*) op)->regOpnd.isPhysical,
+                    ((LowOpReg*) op)->regOpnd.regType, stream);
+        } else { // Corresponds to lower_mem
+            stream = encoder_mem(op->opCode, op->opndSrc.size,
+                    ((LowOpMem*) op)->memOpnd.m_disp.value,
+                    ((LowOpMem*) op)->memOpnd.m_base.regNum,
+                    ((LowOpMem*) op)->memOpnd.m_base.isPhysical, stream);
+        }
+    }
+    // Number of operands is 2
+    // Handles LowOps coming from  lower_imm_reg, lower_imm_mem,
+    // lower_reg_mem, lower_mem_reg, lower_mem_scale_reg,
+    // lower_reg_mem_scale, lower_reg_reg, lower_fp_mem, and lower_mem_fp
+    else if (op->opndDest.type == LowOpndType_Reg
+            && op->opndSrc.type == LowOpndType_Imm) {
+        stream = encoder_imm_reg(op->opCode, op->opndDest.size,
+                ((LowOpImmReg*) op)->immSrc.value,
+                ((LowOpImmReg*) op)->regDest.regNum,
+                ((LowOpImmReg*) op)->regDest.isPhysical,
+                ((LowOpImmReg*) op)->regDest.regType, stream);
+    } else if (op->opndDest.type == LowOpndType_Reg
+            && op->opndSrc.type == LowOpndType_Chain) {
+#if defined(WITH_JIT)
+        insertChainingWorklist(((LowOpImmReg*) op)->immSrc.value, stream);
+#endif
+        stream = encoder_imm_reg(op->opCode, op->opndDest.size,
+                ((LowOpImmReg*) op)->immSrc.value,
+                ((LowOpImmReg*) op)->regDest.regNum,
+                ((LowOpImmReg*) op)->regDest.isPhysical,
+                ((LowOpImmReg*) op)->regDest.regType, stream);
+    } else if (op->opndDest.type == LowOpndType_Mem
+            && op->opndSrc.type == LowOpndType_Imm) {
+        stream = encoder_imm_mem(op->opCode, op->opndDest.size,
+                ((LowOpImmMem*) op)->immSrc.value,
+                ((LowOpImmMem*) op)->memDest.m_disp.value,
+                ((LowOpImmMem*) op)->memDest.m_base.regNum,
+                ((LowOpImmMem*) op)->memDest.m_base.isPhysical, stream);
+    } else if (op->opndDest.type == LowOpndType_Mem
+            && op->opndSrc.type == LowOpndType_Chain) {
+#if defined(WITH_JIT)
+        insertChainingWorklist(((LowOpImmMem*) op)->immSrc.value, stream);
+#endif
+        stream = encoder_imm_mem(op->opCode, op->opndDest.size,
+                ((LowOpImmMem*) op)->immSrc.value,
+                ((LowOpImmMem*) op)->memDest.m_disp.value,
+                ((LowOpImmMem*) op)->memDest.m_base.regNum,
+                ((LowOpImmMem*) op)->memDest.m_base.isPhysical, stream);
+    } else if (op->opndDest.type == LowOpndType_Reg
+            && op->opndSrc.type == LowOpndType_Reg) {
+        if (op->opCode == Mnemonic_FUCOMP || op->opCode == Mnemonic_FUCOM) {
+            stream = encoder_compare_fp_stack(op->opCode == Mnemonic_FUCOMP,
+                    ((LowOpRegReg*) op)->regSrc.regNum
+                            - ((LowOpRegReg*) op)->regDest.regNum,
+                    op->opndDest.size == OpndSize_64, stream);
+        } else {
+            stream = encoder_reg_reg(op->opCode, op->opndDest.size,
+                    ((LowOpRegReg*) op)->regSrc.regNum,
+                    ((LowOpRegReg*) op)->regSrc.isPhysical,
+                    ((LowOpRegReg*) op)->regDest.regNum,
+                    ((LowOpRegReg*) op)->regDest.isPhysical,
+                    ((LowOpRegReg*) op)->regDest.regType, stream);
+        }
+    } else if (op->opndDest.type == LowOpndType_Reg
+            && op->opndSrc.type == LowOpndType_Mem) {
+        // Corresponds to lower_mem_reg, lower_mem_fp, or lower_mem_scale_reg
+        LowOpMemReg * regmem_op = (LowOpMemReg*) op;
+
+        if (regmem_op->regDest.regType == LowOpndRegType_fs)
+            stream = encoder_mem_fp(regmem_op->opCode, regmem_op->opndSrc.size,
+                    regmem_op->memSrc.m_disp.value,
+                    regmem_op->memSrc.m_base.regNum,
+                    regmem_op->memSrc.m_base.isPhysical,
+                    regmem_op->regDest.regNum - PhysicalReg_ST0, stream);
+        else if (regmem_op->memSrc.hasScale)
+            stream = encoder_mem_disp_scale_to_reg_2(regmem_op->opCode,
+                    regmem_op->opndSrc.size, regmem_op->memSrc.m_base.regNum,
+                    regmem_op->memSrc.m_base.isPhysical,
+                    regmem_op->memSrc.m_disp.value,
+                    regmem_op->memSrc.m_index.regNum,
+                    regmem_op->memSrc.m_index.isPhysical,
+                    regmem_op->memSrc.m_scale.value, regmem_op->opndDest.size,
+                    regmem_op->regDest.regNum, regmem_op->regDest.isPhysical,
+                    regmem_op->regDest.regType, stream);
+        else
+            stream = encoder_mem_to_reg_2(regmem_op->opCode,
+                    regmem_op->opndSrc.size, regmem_op->memSrc.m_disp.value,
+                    regmem_op->memSrc.m_base.regNum,
+                    regmem_op->memSrc.m_base.isPhysical,
+                    regmem_op->opndDest.size, regmem_op->regDest.regNum,
+                    regmem_op->regDest.isPhysical, regmem_op->regDest.regType,
+                    stream);
+    } else if (op->opndDest.type == LowOpndType_Mem
+            && op->opndSrc.type == LowOpndType_Reg) {
+        // Corresponds to lower_reg_mem, lower_fp_mem, or lower_reg_mem_scale
+        LowOpRegMem * memreg_op = (LowOpRegMem*) op;
+        if (memreg_op->regSrc.regType == LowOpndRegType_fs)
+            stream = encoder_fp_mem(memreg_op->opCode, memreg_op->opndDest.size,
+                    memreg_op->regSrc.regNum - PhysicalReg_ST0,
+                    memreg_op->memDest.m_disp.value,
+                    memreg_op->memDest.m_base.regNum,
+                    memreg_op->memDest.m_base.isPhysical, stream);
+        else if (memreg_op->memDest.hasScale)
+            stream = encoder_reg_mem_disp_scale(memreg_op->opCode,
+                    memreg_op->opndDest.size, memreg_op->regSrc.regNum,
+                    memreg_op->regSrc.isPhysical,
+                    memreg_op->memDest.m_base.regNum,
+                    memreg_op->memDest.m_base.isPhysical,
+                    memreg_op->memDest.m_disp.value,
+                    memreg_op->memDest.m_index.regNum,
+                    memreg_op->memDest.m_index.isPhysical,
+                    memreg_op->memDest.m_scale.value,
+                    memreg_op->regSrc.regType, stream);
+        else
+            stream = encoder_reg_mem(op->opCode, op->opndDest.size,
+                    memreg_op->regSrc.regNum, memreg_op->regSrc.isPhysical,
+                    memreg_op->memDest.m_disp.value,
+                    memreg_op->memDest.m_base.regNum,
+                    memreg_op->memDest.m_base.isPhysical,
+                    memreg_op->regSrc.regType, stream);
+    }
+}
+
+//! \brief Figures out which LowOps are ready after an instruction at chosenIdx
+//! is scheduled.
+//! \details It also updates the readyTime of every LowOp waiting to be scheduled.
+//! \param chosenIdx is the index of the chosen instruction for scheduling
+//! \param scheduledOps is an input list of scheduled LowOps
+//! \param pendingOps is an input list of pending LowOps
+//! \param readyOps is an output list of LowOps that are ready
+void Scheduler::updateReadyOps(int chosenIdx, bool* scheduledOps,
+        bool* pendingOps, bool* readyOps) {
+    // Go through each successor LIR that depends on selected LIR
+    for (unsigned int k = 0; k < queuedLIREntries[chosenIdx]->successorDependencies.size(); ++k) {
+        int dst = queuedLIREntries[chosenIdx]->successorDependencies[k].lowopSlotId;
+        bool isReady = true;
+        int readyTime = -1;
+        // If all predecessors are scheduled, insert into ready queue
+        for (unsigned int k2 = 0; k2 < queuedLIREntries[dst]->predecessorDependencies.size(); ++k2) {
+            int src = queuedLIREntries[dst]->predecessorDependencies[k2].lowopSlotId;
+            if (!scheduledOps[src]) {
+                isReady = false;
+                break;
+            }
+            int tmpTime = queuedLIREntries[src]->scheduledTime
+                    + queuedLIREntries[src]->instructionLatency;
+            if (readyTime < tmpTime)
+                // This is ready after ALL predecessors have finished executing
+                readyTime = tmpTime;
+        }
+        if (isReady) {
+            readyOps[dst] = true; // Update entry in ready queue
+            // All already pending instructions should commit ahead of this LowOp
+            for (unsigned int k2 = 0; k2 < queuedLIREntries.size(); ++k2) {
+                if (pendingOps[k2]) {
+                    int tmpTime = queuedLIREntries[k2]->scheduledTime
+                            + queuedLIREntries[k2]->instructionLatency
+                            - queuedLIREntries[dst]->instructionLatency; //TODO Is this correct?
+                    if (readyTime < tmpTime) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+                        printf("update readyTime from %d to %d of slot %d due to pending "
+                                "op at slot %d\n", readyTime, tmpTime, dst, k2);
+#endif
+                        readyTime = tmpTime;
+                    }
+                }
+            }
+            queuedLIREntries[dst]->readyTime = readyTime;
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            printf("update readyTime of slot %d: %d\n", dst, readyTime);
+#endif
+        }
+    }
+}
+
+//! \brief Defines the maximum latency in the dependency graph.
+//! \details This value should be larger than any possible valid dependency latency.
+#define MAXIMUM_LATENCY 10000
+
+//! \brief Finds longest path latency to end for every LowOp in the basic block
+//! \details This updates the longest path field of every LowOp in the current
+//! native basic block.
+//! \see queuedLIREntries
+//! \todo Find out what algorithm is used here.
+void Scheduler::findLongestPath() {
+#if 1 // Algorithm 1
+    std::vector<int> options1, options2;
+    for (unsigned int k = 0; k < queuedLIREntries.size() - 1; ++k)
+        options1.push_back(MAXIMUM_LATENCY);
+    //destination is the last LowOp
+    options1.push_back(-queuedLIREntries.back()->instructionLatency); //from dst to dst
+    options2.resize(options1.size());
+
+    bool inputIn1 = true;
+    for (unsigned int i = 0; i < queuedLIREntries.size() - 1; ++i) { //number of arcs in shortest path
+        for (unsigned int k = 0; k < queuedLIREntries.size(); ++k) { //for each v in V
+            int m = inputIn1 ? options1[k] : options2[k];
+            int m_prime = MAXIMUM_LATENCY;
+            for (unsigned int k2 = 0; k2 < queuedLIREntries[k]->successorDependencies.size();
+                    ++k2) { //for each edge from v
+                int dst = queuedLIREntries[k]->successorDependencies[k2].lowopSlotId;
+                int m_tmp = (inputIn1 ? options1[dst] : options2[dst])
+                        + (-queuedLIREntries[k]->instructionLatency)
+                        + queuedLIREntries[k]->successorDependencies[k2].latency;
+                m_prime = (m_tmp < m_prime) ? m_tmp : m_prime;
+            }
+            if (inputIn1)
+                options2[k] = (m_prime < m) ? m_prime : m;
+            else
+                options1[k] = (m_prime < m) ? m_prime : m;
+        }
+        inputIn1 = !inputIn1;
+    }
+    for (unsigned int k = 0; k < queuedLIREntries.size(); ++k) {
+        int m = inputIn1 ? options1[k] : options2[k];
+        if (m == MAXIMUM_LATENCY)
+            m = queuedLIREntries[k]->instructionLatency;
+        else
+            m = -m;
+        queuedLIREntries[k]->longestPath = m;
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        printf("longest path from %d to %d: %d\n", k, queuedLIREntries.size() - 1, m);
+#endif
+    }
+#else // Algorithm 2
+
+#endif
+}
+
+//! \brief Reorders basic block to minimize block latency and make use of both
+//! Atom issue ports.
+//! \details The result of scheduling is stored in scheduledLIREntries. Additionally,
+//! each LowOp individually stores its scheduled time (logical based on index ordering).
+//!
+//! Algorithm details:
+//! - select one LIR from readyQueue with 2 criteria:
+//!   -# smallest readyTime
+//!   -# on critical path
+//! - A pair of LowOps can be issued at the same time slot if they use different issue ports.
+//! - A LowOp can be issued if
+//!   -# all pending ops can commit ahead of this LowOp (restriction reflected in readyTime)
+//!   -# it is ready
+//! - At end, currentTime is advanced to readyTime of the selected LowOps
+//! - If any LIR has jmp, jcc, call, or ret mnemonic, it must be scheduled last
+//!
+//! \see scheduledLIREntries
+//! \post Scheduler::scheduledLIREntries is same size as Scheduler::queuedLIREntries
+//! \post If last LIR in Scheduler::queuedLIREntries is a jump, call, or return, it must
+//! also be the last LIR in Scheduler::scheduledLIREntries
+void Scheduler::schedule() {
+    // Initialize data structures for scheduling
+    bool* readyOps = (bool *) malloc(sizeof(bool) * (queuedLIREntries.size())); // ready for scheduling
+    bool* scheduledOps = (bool *) malloc(
+            sizeof(bool) * (queuedLIREntries.size())); // scheduled
+    bool* pendingOps = (bool *) malloc(
+            sizeof(bool) * (queuedLIREntries.size())); // scheduled but not yet finished executing
+    unsigned int* candidateArray = (unsigned int *) malloc(
+            sizeof(unsigned int) * queuedLIREntries.size());
+    unsigned int num_candidates = 0, numScheduled = 0, k;
+    int currentTime = 0;
+
+    // Predecessor dependencies have already been initialized in the dependency graph building.
+    // Now, initialize successor dependencies to complete dependency graph.
+    for (unsigned int k = 0; k < queuedLIREntries.size(); ++k) {
+        for (unsigned int k2 = 0; k2 < queuedLIREntries[k]->predecessorDependencies.size(); ++k2) {
+            int src = queuedLIREntries[k]->predecessorDependencies[k2].lowopSlotId;
+            DependencyInformation ds;
+            ds.lowopSlotId = k;
+            ds.latency = -queuedLIREntries[k]->predecessorDependencies[k2].latency; //negative weight
+            queuedLIREntries[src]->successorDependencies.push_back(ds);
+        }
+    }
+
+    // Find longest path from each LIR to end of basic block
+    findLongestPath();
+
+    // When a LowOp is ready, it means all its predecessors are scheduled
+    // and the readyTime of this LowOp has been set already.
+    for (k = 0; k < queuedLIREntries.size(); k++) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        printf("-- slot %d: latency %d port type %d\n", k, queuedLIREntries[k]->instructionLatency,
+                queuedLIREntries[k]->portType);
+#endif
+        scheduledOps[k] = false;
+        readyOps[k] = false;
+        pendingOps[k] = false;
+        if (queuedLIREntries[k]->predecessorDependencies.size() == 0) {
+            readyOps[k] = true;
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+            printf("slot %d is ready\n", k);
+#endif
+            queuedLIREntries[k]->readyTime = 0;
+        }
+    }
+
+    // Schedule each of LIRs in the basic block
+    while (numScheduled < queuedLIREntries.size()) {
+        int nextTime = currentTime;
+        unsigned int chosenIdx1 = queuedLIREntries.size(), chosenIdx2 =
+                queuedLIREntries.size(), idx;
+        num_candidates = 0;
+        // candidates: ops with readyTime <= currentTime
+        // if no ops are ready to be issued, ops with smallest readyTime
+        for (k = 0; k < queuedLIREntries.size(); k++) {
+            if (readyOps[k] && queuedLIREntries[k]->readyTime <= currentTime
+                    && (!isBasicBlockDelimiter(queuedLIREntries[k]->opCode)))
+                candidateArray[num_candidates++] = k;
+        }
+        if (num_candidates == 0) {
+            for (k = 0; k < queuedLIREntries.size(); k++) {
+                if (readyOps[k]
+                        && (!isBasicBlockDelimiter(queuedLIREntries[k]->opCode))) {
+                    if (chosenIdx1 == queuedLIREntries.size()
+                            || queuedLIREntries[k]->readyTime
+                                    < queuedLIREntries[chosenIdx1]->readyTime) {
+                        chosenIdx1 = k;
+                        nextTime = queuedLIREntries[k]->readyTime;
+                    }
+                }
+            }
+            for (k = 0; k < queuedLIREntries.size(); k++) {
+                if (readyOps[k] && (!isBasicBlockDelimiter(queuedLIREntries[k]->opCode))
+                        && queuedLIREntries[k]->readyTime <= nextTime) {
+                    candidateArray[num_candidates++] = k;
+                }
+            }
+        }
+        if (num_candidates == 0)
+            candidateArray[num_candidates++] = queuedLIREntries.size() - 1;
+        chosenIdx1 = queuedLIREntries.size();
+        for (idx = 0; idx < num_candidates; idx++) {
+            k = candidateArray[idx];
+            if (chosenIdx1 == queuedLIREntries.size()
+                    || queuedLIREntries[k]->longestPath
+                            > queuedLIREntries[chosenIdx1]->longestPath) {
+                chosenIdx1 = k;
+            }
+        }
+        // Pick 2 candidates if possible.
+        if (queuedLIREntries[chosenIdx1]->portType == BOTH_PORTS)
+            num_candidates = 0;
+        for (idx = 0; idx < num_candidates; idx++) {
+            k = candidateArray[idx];
+            if (k == chosenIdx1)
+                continue;
+            // Check whether candidateArray[k] can be issued at the same time.
+            // Namely, check port conflict
+            if (queuedLIREntries[k]->portType == BOTH_PORTS)
+                continue;
+            if (queuedLIREntries[chosenIdx1]->portType == EITHER_PORT
+                    || queuedLIREntries[k]->portType == EITHER_PORT
+                    || (queuedLIREntries[chosenIdx1]->portType == PORT0
+                            && queuedLIREntries[k]->portType == PORT1)
+                    || (queuedLIREntries[chosenIdx1]->portType == PORT1
+                            && queuedLIREntries[k]->portType == PORT0)) {
+                chosenIdx2 = k;
+                break;
+            }
+        }
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        printf("pick ready instructions at slots %d %d\n", chosenIdx1, chosenIdx2);
+#endif
+        currentTime = nextTime;
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        printf("advance time to %d\n", nextTime);
+#endif
+        for (k = 0; k < queuedLIREntries.size(); k++) {
+            if (scheduledOps[k]
+                    && currentTime
+                            >= queuedLIREntries[k]->scheduledTime
+                                    + queuedLIREntries[k]->instructionLatency)
+                // This instruction is retired and thus no longer pending
+                pendingOps[k] = false;
+        }
+        if (chosenIdx1 < queuedLIREntries.size()) {
+            scheduledLIREntries.push_back(queuedLIREntries[chosenIdx1]);
+            scheduledOps[chosenIdx1] = true;
+            pendingOps[chosenIdx1] = true;
+            readyOps[chosenIdx1] = false;
+            queuedLIREntries[chosenIdx1]->scheduledTime = currentTime;
+            numScheduled++;
+        }
+        if (chosenIdx2 < queuedLIREntries.size()) {
+            scheduledLIREntries.push_back(queuedLIREntries[chosenIdx2]);
+            scheduledOps[chosenIdx2] = true;
+            pendingOps[chosenIdx2] = true;
+            readyOps[chosenIdx2] = false;
+            queuedLIREntries[chosenIdx2]->scheduledTime = currentTime;
+            numScheduled++;
+        }
+
+        // When an instruction is scheduled, insert ready instructions to readyQueue.
+        // Check dependencies from chosen instructions to other instructions.
+        if (chosenIdx1 < queuedLIREntries.size())
+            updateReadyOps(chosenIdx1, scheduledOps, pendingOps, readyOps);
+        if (chosenIdx2 < queuedLIREntries.size())
+            updateReadyOps(chosenIdx2, scheduledOps, pendingOps, readyOps);
+        currentTime++;
+    }
+
+    // Free temporary data structures used during scheduling
+    free(readyOps);
+    free(scheduledOps);
+    free(pendingOps);
+    free(candidateArray);
+
+    // Make sure that original and scheduled basic blocks are same size
+    if (scheduledLIREntries.size() != queuedLIREntries.size()) {
+        LOGE("ERROR: Atom Scheduler bug! Original basic block is not same "
+                "size as the scheduled basic block");
+        dvmAbort();
+    }
+
+    // Make sure that basic block delimiter mnemonic is always last one in
+    // scheduled basic block
+    if (isBasicBlockDelimiter(queuedLIREntries.back()->opCode)
+            && !isBasicBlockDelimiter(scheduledLIREntries.back()->opCode)) {
+        LOGE("ERROR: Atom Scheduler bug! Sync point should be the last "
+        "scheduled instruction.");
+        dvmAbort();
+    }
+}
+
+//! \brief Called to signal the scheduler that the native basic block it has
+//! been building is finished.
+//! \details This method should be called from other modules to signal that the
+//! native basic block the Scheduler has been building is finished. This has
+//! side effects because it starts the scheduling process using already created
+//! dependency graphs and then updates the code stream with the scheduled
+//! instructions.
+//! \warning Jumps to immediate must signal end of native basic block for target.
+//! If the target has a label, then this is not a problem. But if jumping to an
+//! address without label, this method must be called before building dependency
+//! graph for target basic block.
+void Scheduler::signalEndOfNativeBasicBlock() {
+    if(queuedLIREntries.empty())
+            return; // No need to do any work
+
+    printStatistics(true /*prescheduling*/);
+    schedule();
+    printStatistics(false /*prescheduling*/);
+
+    unsigned int k;
+    for(k = 0; k < scheduledLIREntries.size(); ++k) {
+        generateAssembly(scheduledLIREntries[k]);
+        delete scheduledLIREntries[k]; // LowOp is no longer needed, can deallocate
+    }
+
+    // Clear all scheduler data structures:
+    queuedLIREntries.clear();
+    scheduledLIREntries.clear();
+    userEntries.clear();
+    producerEntries.clear();
+    ctrlEntries.clear();
+}
+
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+//! \brief Transforms LowOpndType enum to string
+//! \see LowOpndType
+inline const char * operandTypeToString(LowOpndType type) {
+    switch (type) {
+    case LowOpndType_Imm:
+        return "Imm";
+    case LowOpndType_Reg:
+        return "Reg";
+    case LowOpndType_Mem:
+        return "Mem";
+    case LowOpndType_Label:
+        return "Label";
+    case LowOpndType_BlockId:
+        return "BlockId";
+    case LowOpndType_Chain:
+        return "Chain";
+    }
+    return "-";
+}
+#endif
+
+//! \brief Returns a scaled distance between two basic blocks.
+//! \details Computes the Hamming distance between two basic blocks and then scales
+//! result by block size and turns it into percentage. block1 and block2 must
+//! have same size.
+//! \retval scaled Hamming distance
+inline double Scheduler::basicBlockEditDistance(const NativeBasicBlock & block1,
+        const NativeBasicBlock & block2) {
+#ifndef DISABLE_ATOM_SCHEDULING_STATISTICS
+    int distance = 0;
+    assert(block1.size() == block2.size());
+    for(unsigned int i = 0; i < block1.size(); ++i) {
+        if(block1[i] != block2[i]) {
+            distance += 1;
+        }
+    }
+    return (distance * 100.0) / block1.size();
+#else
+    return 0.0;
+#endif
+}
+
+//! \brief Prints Atom Instruction Scheduling statistics.
+//! Details prints block size and basic block difference.
+//! \todo Comparing basic block latencies pre and post scheduling is a useful
+//! statistic.
+//! \param prescheduling is used to indicate whether the statistics are requested
+//! before the scheduling
+void Scheduler::printStatistics(bool prescheduling) {
+#ifndef DISABLE_ATOM_SCHEDULING_STATISTICS
+    const char * message_tag =
+            prescheduling ?
+                    "Atom Sched Stats: Pre-schedule:" :
+                    "Atom Sched Stats: Post-schedule:";
+    NativeBasicBlock * lowOpList;
+    if (prescheduling)
+        lowOpList = &queuedLIREntries;
+    else
+        lowOpList = &scheduledLIREntries;
+
+    printf("%s The block size is %d\n", message_tag, lowOpList->size());
+    for (unsigned int i = 0; i < lowOpList->size(); ++i) {
+#ifndef DISABLE_DEBUG_ATOM_SCHEDULER
+        printf(
+                "   LIR with opcode %d and with %d operands which are %s and %s\n",
+                (*lowOpList)[i]->opCode, (*lowOpList)[i]->numOperands,
+                (*lowOpList)[i]->numOperands >= 1 ?
+                operandTypeToString((*lowOpList)[i]->opndDest.type) : "-",
+                (*lowOpList)[i]->numOperands >= 2 ?
+                operandTypeToString((*lowOpList)[i]->opndSrc.type) : "-");
+#endif
+    }
+    if (!prescheduling) {
+        printf("%s Difference in basic blocks after scheduling is %5.2f%%\n",
+                message_tag, basicBlockEditDistance(queuedLIREntries, scheduledLIREntries));
+    }
+#endif
+}
diff --git a/vm/compiler/codegen/x86/Scheduler.h b/vm/compiler/codegen/x86/Scheduler.h
new file mode 100644
index 0000000..9b6e8fb
--- /dev/null
+++ b/vm/compiler/codegen/x86/Scheduler.h
@@ -0,0 +1,120 @@
+/*
+ * Copyright (C) 2010-2011 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*! \file Scheduler.h
+    \brief This file implements the interface for Atom Scheduler
+*/
+
+#ifndef ATOM_SCHEDULER_H_
+#define ATOM_SCHEDULER_H_
+
+#include "Lower.h"
+
+//! \brief Interface for Atom Instruction Scheduler
+class Scheduler {
+private:
+    //! \brief Defines implementation of a native basic block for Atom LIRs.
+    typedef std::vector<LowOp*> NativeBasicBlock;
+
+    //! \brief Holds a list of all LIRs allocated via allocateNewEmptyLIR
+    //! which are not yet in code stream.
+    //! \details The field LowOp::slotId corresponds to index into this list
+    //! when the LIR is allocated by scheduler via allocateNewEmptyLIR.
+    //! \see allocateNewEmptyLIR
+    NativeBasicBlock queuedLIREntries;
+
+    //! \brief Holds a list of scheduled LIRs in their scheduled order.
+    //! \details It contains the same LIRs are queuedLIREntries, just
+    //! in a possibly different order.
+    //! \see queuedLIREntries
+    NativeBasicBlock scheduledLIREntries;
+
+    //! \brief Used to keep track of writes to a resource.
+    //! \details This is used only during dependency building but corresponding
+    //! LIRs are also updated to keep track of their own dependencies which is
+    //! used during scheduling.
+    //! \see LowOp::predecessorDependencies
+    //! \see UseDefUserEntry
+    std::vector<UseDefProducerEntry> producerEntries;
+
+    //! \brief Used to keep track of reads from a resource.
+    //! \details This is used only during dependency building but corresponding
+    //! LIRs are also updated to keep track of their own dependencies which is
+    //! used during scheduling.
+    //! \see LowOp::predecessorDependencies
+    //! \see UseDefUserEntry
+    std::vector<UseDefUserEntry> userEntries;
+
+    //! \brief Used to keep track of dependencies on control flags.
+    //! \details This is used only during dependency building but corresponding
+    //! LIRs are also updated to keep track of their own dependencies which is
+    //! used during scheduling. This list holds values from LowOp::slotId.
+    //! \see LowOp::predecessorDependencies
+    //! \see LowOp::slotId
+    std::vector<int> ctrlEntries;
+
+    // See documentation in Schedule.cpp or in doxygen output for undocumented prototypes.
+    void updateDependencyGraph(UseDefEntryType type, int regNum,
+            LowOpndDefUse defuse, LowOp* op);
+    void updateDependencyGraphForMem(LowOpndMem & mOpnd, LowOpndDefUse defuse,
+            LowOp* op);
+    void handlePushDependencyUpdate(LowOp* op);
+    void handleFloatDependencyUpdate(LowOp* op);
+    void updateReadyOps(int chosenIdx, bool* scheduledOps, bool* pendingOps,
+            bool* readyOps);
+    bool isBasicBlockDelimiter(Mnemonic m);
+    void generateAssembly(LowOp * op);
+    void findLongestPath();
+    void schedule();
+    double basicBlockEditDistance(const NativeBasicBlock & block1,
+            const NativeBasicBlock & block2);
+    void printStatistics(bool prescheduling);
+public:
+    //! \brief Called by users of scheduler to allocate an empty LIR (no mnemonic
+    //! or operands).
+    //! \details The caller of this method takes the LIR, updates the mnemonic
+    //! and operand information, and then calls one of the updateUseDefInformation
+    //! methods in the scheduler with this LIR as parameter. This method should not
+    //! be called when scheduling is not enabled because the LIR will never be freed.
+    //! Internally, the scheduler will add this LIR to the native basic block it
+    //! is building and also assign it an id.
+    //! \tparam is a LowOp or any of its specialized children.
+    //! \see LowOp
+    template<typename LowOpType> LowOpType * allocateNewEmptyLIR() {
+        LowOpType * op = new LowOpType;
+        op->slotId = queuedLIREntries.size();
+        queuedLIREntries.push_back(op);
+        return op;
+    }
+
+    // See documentation in Schedule.cpp or in doxygen output for undocumented prototypes.
+    void updateUseDefInformation(LowOp * op);
+    void updateUseDefInformation_imm(LowOp * op);
+    void updateUseDefInformation_reg(LowOpReg * op);
+    void updateUseDefInformation_mem(LowOpMem * op);
+    void updateUseDefInformation_imm_to_reg(LowOpImmReg * op);
+    void updateUseDefInformation_imm_to_mem(LowOpImmMem * op);
+    void updateUseDefInformation_reg_to_reg(LowOpRegReg * op);
+    void updateUseDefInformation_mem_to_reg(LowOpMemReg * op);
+    void updateUseDefInformation_reg_to_mem(LowOpRegMem * op);
+    void updateUseDefInformation_mem_scale_to_reg(LowOpMemReg * op);
+    void updateUseDefInformation_reg_to_mem_scale(LowOpRegMem * op);
+    void updateUseDefInformation_fp_to_mem(LowOpRegMem * op);
+    void updateUseDefInformation_mem_to_fp(LowOpMemReg * op);
+    void signalEndOfNativeBasicBlock();
+};
+
+#endif /* ATOM_SCHEDULER_H_ */
diff --git a/vm/compiler/codegen/x86/doxygen-config-x86-jit b/vm/compiler/codegen/x86/doxygen-config-x86-jit
new file mode 100644
index 0000000..912f649
--- /dev/null
+++ b/vm/compiler/codegen/x86/doxygen-config-x86-jit
@@ -0,0 +1,9 @@
+PROJECT_NAME = "X86 Atom JIT Compiler for Dalvik"
+INPUT = AnalysisO1.cpp GlueOpt.cpp LowerHelper.cpp  NcgAot.cpp AnalysisO1.h LowerAlu.cpp LowerInvoke.cpp NcgAot.h Schedule.cpp BytecodeVisitor.cpp LowerConst.cpp LowerJump.cpp NcgCodegenO1.cpp Scheduler.h CodegenInterface.cpp Lower.cpp LowerMove.cpp NcgHelper.cpp Translator.h DataFlow.cpp LowerGetPut.cpp LowerObject.cpp NcgHelper.h Lower.h LowerReturn.cpp NullCheckElim.cpp
+OUTPUT_DIRECTORY = x86-jit-docs
+GENERATE_HTML = YES
+HTML_OUTPUT = html/
+HTML_FILE_EXTENSION = .html
+EXTRACT_PRIVATE = YES
+EXTRACT_STATIC = YES
+EXTRACT_LOCAL_CLASSES = YES
diff --git a/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp b/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp
index 71e0e06..3c34b29 100644
--- a/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp
+++ b/vm/compiler/codegen/x86/libenc/enc_wrapper.cpp
@@ -239,12 +239,13 @@ extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_reg(Mnemonic m, OpndSize si
 #endif
     return stream;
 }
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_reg(Mnemonic m, OpndSize size,
-                   int disp, int base_reg, bool isBasePhysical,
+//! \brief Allows for different operand sizes
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_to_reg_2(Mnemonic m, OpndSize memOpndSize,
+                   int disp, int base_reg, bool isBasePhysical, OpndSize regOpndSize,
                    int reg, bool isPhysical, LowOpndRegType type, char * stream) {
     EncoderBase::Operands args;
-    add_r(args, reg, size);
-    add_m(args, base_reg, disp, size);
+    add_r(args, reg, regOpndSize);
+    add_m(args, base_reg, disp, memOpndSize);
     char* stream_start = stream;
     stream = (char *)EncoderBase::encode(stream, m, args);
 #ifdef PRINT_ENCODER_STREAM
@@ -253,6 +254,11 @@ extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_reg(Mnemonic m, OpndSize si
 #endif
     return stream;
 }
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_reg(Mnemonic m, OpndSize size,
+                   int disp, int base_reg, bool isBasePhysical,
+                   int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    return encoder_mem_to_reg_2(m, size, disp, base_reg, isBasePhysical, size, reg, isPhysical, type, stream);
+}
 extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_scale_reg(Mnemonic m, OpndSize size,
                          int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
                          int reg, bool isPhysical, LowOpndRegType type, char * stream) {
@@ -282,12 +288,13 @@ extern "C" ENCODER_DECLARE_EXPORT char * encoder_reg_mem_scale(Mnemonic m, OpndS
 #endif
     return stream;
 }
-extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_reg(Mnemonic m, OpndSize size,
+//! \brief Allows for different operand sizes
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_to_reg_2(Mnemonic m, OpndSize memOpndSize,
                          int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-                         int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+                         OpndSize regOpndSize, int reg, bool isPhysical, LowOpndRegType type, char * stream) {
     EncoderBase::Operands args;
-    add_r(args, reg, size);
-    add_m_disp_scale(args, base_reg, disp, index_reg, scale, size);
+    add_r(args, reg, regOpndSize);
+    add_m_disp_scale(args, base_reg, disp, index_reg, scale, memOpndSize);
     char* stream_start = stream;
     stream = (char *)EncoderBase::encode(stream, m, args);
 #ifdef PRINT_ENCODER_STREAM
@@ -296,6 +303,13 @@ extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_reg(Mnemonic m,
 #endif
     return stream;
 }
+extern "C" ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_reg(Mnemonic m, OpndSize size,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         int reg, bool isPhysical, LowOpndRegType type, char * stream) {
+    return encoder_mem_disp_scale_to_reg_2(m, size, base_reg, isBasePhysical,
+            disp, index_reg, isIndexPhysical, scale, size, reg, isPhysical,
+            type, stream);
+}
 extern "C" ENCODER_DECLARE_EXPORT char * encoder_movzs_mem_disp_scale_reg(Mnemonic m, OpndSize size,
                          int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
                          int reg, bool isPhysical, LowOpndRegType type, char * stream) {
@@ -310,7 +324,6 @@ extern "C" ENCODER_DECLARE_EXPORT char * encoder_movzs_mem_disp_scale_reg(Mnemon
 #endif
     return stream;
 }
-
 extern "C" ENCODER_DECLARE_EXPORT char* encoder_reg_mem_disp_scale(Mnemonic m, OpndSize size,
                          int reg, bool isPhysical,
                          int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
diff --git a/vm/compiler/codegen/x86/libenc/enc_wrapper.h b/vm/compiler/codegen/x86/libenc/enc_wrapper.h
index a2a223e..093fe69 100644
--- a/vm/compiler/codegen/x86/libenc/enc_wrapper.h
+++ b/vm/compiler/codegen/x86/libenc/enc_wrapper.h
@@ -187,6 +187,9 @@ ENCODER_DECLARE_EXPORT char* encoder_reg_reg(Mnemonic m, OpndSize size,
 ENCODER_DECLARE_EXPORT char* encoder_mem_reg(Mnemonic m, OpndSize size,
                    int disp, int base_reg, bool isBasePhysical,
                    int reg, bool isPhysical, LowOpndRegType type, char* stream);
+ENCODER_DECLARE_EXPORT char* encoder_mem_to_reg_2(Mnemonic m, OpndSize memOpndSize,
+                   int disp, int base_reg, bool isBasePhysical, OpndSize regOpndSize,
+                   int reg, bool isPhysical, LowOpndRegType type, char* stream);
 ENCODER_DECLARE_EXPORT char* encoder_mem_scale_reg(Mnemonic m, OpndSize size,
                          int base_reg, bool isBasePhysical, int index_reg, bool isIndexPhysical, int scale,
                          int reg, bool isPhysical, LowOpndRegType type, char* stream);
@@ -200,6 +203,9 @@ ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_reg(Mnemonic m, OpndSize si
 ENCODER_DECLARE_EXPORT char * encoder_movzs_mem_disp_scale_reg(Mnemonic m, OpndSize size,
                          int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
                          int reg, bool isPhysical, LowOpndRegType type, char * stream);
+ENCODER_DECLARE_EXPORT char * encoder_mem_disp_scale_to_reg_2(Mnemonic m, OpndSize memOpndSize,
+                         int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
+                         OpndSize regOpndSize, int reg, bool isPhysical, LowOpndRegType type, char * stream);
 ENCODER_DECLARE_EXPORT char* encoder_reg_mem_disp_scale(Mnemonic m, OpndSize size,
                          int reg, bool isPhysical,
                          int base_reg, bool isBasePhysical, int disp, int index_reg, bool isIndexPhysical, int scale,
-- 
1.7.4.1

